<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>清华叉院提出「GenH2R」框架，用百万场景打造基于视觉的通用人机交接策略 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="清华叉院提出「GenH2R」框架，用百万场景打造基于视觉的通用人机交接策略" />
<meta property="og:description" content="©作者 | 机器之心编辑部
来源 | 机器之心
来自清华大学交叉信息研究院的研究者提出了「GenH2R」框架，让机器人学习通用的基于视觉的人机交接策略（generalizable vision-based human-to-robot handover policies）。这种可泛化策略使得机器人能更可靠地从人们手中接住几何形状多样、运动轨迹复杂的物体，为人机交互提供了新的可能性。
‍‍‍
‍‍随着具身智能（Embodied AI）时代的来临，我们期待智能体能主动与环境进行交互。在这个过程中，让机器人融入人类生活环境、与人类进行交互（Human Robot Interaction）变得至关重要。我们需要思考如何理解人类的行为和意图，以最符合人类期望的方式满足其需求，将人类放在具身智能的中心（Human-Centered Embodied AI）。
其中一个关键的技能是可泛化的人机交接（Generalizable Human-to-Robot Handover），它使机器人能够更好地与人类合作，完成各种日常通用任务，如烹饪、居室整理和家具组装等。
大模型的火爆发展预示着海量高质量数据&#43;大规模学习是走向通用智能的一种可能方式，那么能否通过海量机器人数据与大规模策略模仿获取通用人机交接技能？然而，若考虑在现实世界中让机器人与人类进行大规模交互学习是危险且昂贵的，机器很有可能会伤害到人类：
‍
‍而在仿真环境中（Simulation）进行训练，用人物仿真和动态抓取运动规划来自动化提供海量多样的机器人学习数据，然后将其部署到真实机器人上（Sim-to-Real Transfer），是一种更可靠的基于学习的方法，可以大大拓展机器人与人协作交互的能力。
因此，「GenH2R」框架被提出，分别从仿真（Simulation），示例（Demonstration），模仿（Imitation）三个角度出发，让机器人第一次基于端到端的方式学习对任意抓取方式、任意交接轨迹、任意物体几何的通用交接：
1）在「GenH2R-Sim」环境中提供了百万级别的易于生成的各种复杂仿真交接场景；
2）引入一套自动化的基于视觉 - 动作协同的专家示例（Expert Demonstrations）生成流程；
3）使用基于 4D 信息和预测辅助（点云 &#43; 时间）的模仿学习（Imitation Learning）方法。
相比于 SOTA 方法（CVPR2023 Highlight），GenH2R 的方法在各种测试集上平均成功率提升 14%，时间上缩短 13%，并在真机实验中取得更加鲁棒的效果。
论文标题：
GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation
论文地址：
https://arxiv.org/abs/2401.00929
论文主页：
https://genh2r.github.io/
论文视频：
https://youtu.be/BbphK5QlS1Y
方法介绍
A. 仿真环境（GenH2R-Sim）
为了生成高质量、大规模的人手 - 物体数据集，GenH2R-Sim 环境从抓取姿势和运动轨迹两方面对场景建模。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/1481c2d2ea71aa9ffe7651aa47d27368/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-05T12:37:51+08:00" />
<meta property="article:modified_time" content="2024-01-05T12:37:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">清华叉院提出「GenH2R」框架，用百万场景打造基于视觉的通用人机交接策略</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p><img src="https://images2.imgbox.com/2e/ce/EsYc44i2_o.gif" alt="14b929b01a42600c458bcba35e3b7176.gif"></p> 
 <p style="text-align:right;"><strong>©作者 |</strong> 机器之心编辑部</p> 
 <p style="text-align:right;"><strong>来源 | </strong>机器之心</p> 
 <p style="text-align:justify;">来自清华大学交叉信息研究院的研究者提出了「GenH2R」框架，让机器人学习通用的基于视觉的人机交接策略（generalizable vision-based human-to-robot handover policies）。这种可泛化策略使得机器人能更可靠地从人们手中接住几何形状多样、运动轨迹复杂的物体，为人机交互提供了新的可能性。</p> 
 <p style="text-align:justify;">‍‍‍</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/21/e1/K3gjK0Ia_o.gif" alt="fb7ebfdfaede7d4013578025d2428a36.gif"></p> 
 <p style="text-align:justify;">‍‍随着具身智能（Embodied AI）时代的来临，我们期待智能体能主动与环境进行交互。在这个过程中，让机器人融入人类生活环境、与人类进行交互（Human Robot Interaction）变得至关重要。我们需要思考如何理解人类的行为和意图，以最符合人类期望的方式满足其需求，将人类放在具身智能的中心（Human-Centered Embodied AI）。</p> 
 <p style="text-align:justify;">其中一个关键的技能是可泛化的人机交接（Generalizable Human-to-Robot Handover），它使机器人能够更好地与人类合作，完成各种日常通用任务，如烹饪、居室整理和家具组装等。</p> 
 <p style="text-align:justify;">大模型的火爆发展预示着海量高质量数据+大规模学习是走向通用智能的一种可能方式，那么能否通过海量机器人数据与大规模策略模仿获取通用人机交接技能？然而，若考虑在现实世界中让机器人与人类进行大规模交互学习是危险且昂贵的，机器很有可能会伤害到人类：</p> 
 <p style="text-align:justify;"> ‍</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/2d/f7/QZ3jnrUK_o.gif" alt="1d1b0fd08b1383f459f0ace1265533ee.gif"></p> 
 <p style="text-align:justify;">‍而在仿真环境中（Simulation）进行训练，用人物仿真和动态抓取运动规划来自动化提供海量多样的机器人学习数据，然后将其部署到真实机器人上（Sim-to-Real Transfer），是一种更可靠的基于学习的方法，可以大大拓展机器人与人协作交互的能力。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/6c/2e/Uagc09Ut_o.png" alt="07813150718f688978d850c46ddbbd34.png"></p> 
 <p style="text-align:justify;">因此，「GenH2R」框架被提出，分别从仿真（Simulation），示例（Demonstration），模仿（Imitation）三个角度出发，<strong>让机器人第一次基于端到端的方式学习对任意抓取方式、任意交接轨迹、任意物体几何的通用交接</strong>：</p> 
 <p style="text-align:justify;">1）在「GenH2R-Sim」环境中提供了百万级别的易于生成的各种复杂仿真交接场景；</p> 
 <p style="text-align:justify;">2）引入一套自动化的基于视觉 - 动作协同的专家示例（Expert Demonstrations）生成流程；</p> 
 <p style="text-align:justify;">3）使用基于 4D 信息和预测辅助（点云 + 时间）的模仿学习（Imitation Learning）方法。</p> 
 <p style="text-align:justify;">相比于 SOTA 方法（CVPR2023 Highlight），GenH2R 的方法在各种测试集上平均成功率提升 14%，时间上缩短 13%，并在真机实验中取得更加鲁棒的效果。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/6d/a2/GnUAshIb_o.png" alt="e70397b9cac9b04e8084a69c0d757e5b.png"></p> 
 <p style="text-align:left;"><strong>论文标题：</strong></p> 
 <p style="text-align:left;">GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation</p> 
 <p style="text-align:left;"><strong>论文地址：</strong></p> 
 <p style="text-align:left;">https://arxiv.org/abs/2401.00929</p> 
 <p style="text-align:left;"><strong>论文主页：</strong></p> 
 <p style="text-align:left;">https://genh2r.github.io/</p> 
 <p style="text-align:left;"><strong>论文视频：</strong></p> 
 <p style="text-align:left;">https://youtu.be/BbphK5QlS1Y</p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/a5/6f/su9oZ7gs_o.png" alt="89592af9fa0d39aca187e078c52e49f4.png"></p> 
 <p style="text-align:left;"><strong>方法介绍</strong></p> 
 <p style="text-align:justify;"><strong>A. 仿真环境（GenH2R-Sim）</strong><strong></strong></p> 
 <p style="text-align:justify;">为了生成高质量、大规模的人手 - 物体数据集，GenH2R-Sim 环境从抓取姿势和运动轨迹两方面对场景建模。</p> 
 <p style="text-align:justify;">在抓取姿势方面，GenH2R-Sim 从 ShapeNet 中引入了丰富的 3D 物体模型，从中挑选出 3266 个适合交接的日常物体，使用灵巧抓取的生成方法（DexGraspNet），总共生成了 100 万个人手抓住物体的场景。在运动轨迹方面，GenH2R-Sim 使用若干控制点生成多段光滑的 Bézier 曲线，并引入人手和物体的旋转，模拟出手递物体的各种复杂运动轨迹。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/06/76/ZG5XqeNN_o.png" alt="c9c5308938d1e97b6c0e896190fa4d8a.png"></p> 
 <p style="text-align:justify;">GenH2R-Sim 的 100 万场景中，不仅在运动轨迹（1 千 vs 100 万）、物体数量（20 vs 3266）两方面远超之前最新工作，此外，还引入了接近真实情境的互动信息（如机械臂足够靠近物体时，人会配合停止运动，等待完成交接），而非简单的轨迹播放。尽管仿真生成的数据不能完全逼真，但实验结果表明，相比小规模的真实数据，大规模的仿真数据更有助于学习。</p> 
 <p style="text-align:left;"><strong>B. 大规模生成利于蒸馏的专家示例</strong></p> 
 <p style="text-align:justify;">基于大规模的人手和物体运动轨迹数据，GenH2R 自动化地生成了大量专家示例。GenH2R 寻求的 “专家” 是经过改进后的 Motion Planner（如 OMG Planner），这些方法是非学习、基于控制优化的，不依赖于视觉的点云，往往需要一些场景状态（比如物体的目标抓取位置）。</p> 
 <p style="text-align:justify;">为了确保后续的视觉策略网络能够蒸馏出有益于学习的信息，关键在于确保 “专家” 提供的示例具有视觉 - 动作相关性（Vision-action correlation）。规划时如果知道最后落点，那么机械臂可以忽略视觉而直接规划到最终位置 “守株待兔”，这样可能会导致机器人的相机无法看到物体，这种示例对于下游的视觉策略网络并没有任何帮助；而如果频繁地根据物体位置进行重新规划，可能会导致机械臂动作不连续，出现奇怪的形态，无法完成合理的抓取。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/09/70/G0c5S4uk_o.gif" alt="9b65ca52dca830b67ba12447e8b7c3a3.gif"></p> 
 <p style="text-align:justify;">为了生成出利于蒸馏（Distillation-friendly）的专家示例，GenH2R 引入了 Landmark Planning。人手的运动轨迹会按照轨迹光滑程度和距离被分成多段，以 Landmark 作为分割标记。在每一段中，人手轨迹是光滑的，专家方法会朝着 Landmark 点进行规划。这种方法可以同时保证视觉 - 动作相关性和动作连续性。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/36/26/11FqaCAQ_o.gif" alt="1ea54e35149c8fd939dbbbe6e3812499.gif"></p> 
 <p style="text-align:left;"><strong>C. 以预测为辅助的 4D 模仿学习网络</strong></p> 
 <p style="text-align:justify;">基于大规模专家示例，GenH2R 使用模仿学习的方法，构建 4D 策略网络，对观察到的时序点云信息进行几何和运动的分解。对于每一帧点云，通过迭代最近点算法（Iterative Closest Point）计算和上一帧点云之间的位姿变换，以估计出每个点的流（flow）信息，使得每一帧点云都具有运动特征。</p> 
 <p style="text-align:justify;">接着，使用 PointNet++ 对每一帧点云编码，最后不仅解码出最终需要的 6D egocentric 动作，还会额外输出一个物体未来位姿的预测，增强策略网络对未来手和物体运动的预测能力。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ea/f6/aKJUmD4Y_o.png" alt="9e6653590601d43425286bb645e48451.png"></p> 
 <p style="text-align:justify;">不同于更加复杂的 4D Backbone（例如 Transformer-based），这种网络架构的推理速度很快，更适用于交接物体这种需要低延时的人机交互场景，同时它也能有效地利用时序信息，做到了简单性和有效性的平衡。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e7/fd/1Aoli5DD_o.png" alt="460f0fc078cec853ce61feb90c25da42.png"></p> 
 <h3><strong>实验</strong></h3> 
 <p style="text-align:left;"><strong>A. 仿真环境实验</strong></p> 
 <p style="text-align:justify;">GenH2R 和 SOTA 方法进行了各种设定下的比较，相比于使用小规模真实数据训练的方法，在 GenH2R-Sim 中使用大规模仿真数据进行训练的方法，可以取得显著的优势（在各种测试集上成功率平均提升 14%，时间上缩短 13%）。</p> 
 <p style="text-align:justify;">在真实数据测试集 s0 中，GenH2R 的方法可以成功交接更复杂的物体，并且能够提前调整姿势，避免在夹爪靠近物体时再频繁进行姿势调整：</p> 
 <p style="text-align:center;">‍<img src="https://images2.imgbox.com/d4/25/Ov1qM7Id_o.gif" alt="b5f950d0e2279404b14c9f691757048d.gif">‍</p> 
 <p style="text-align:justify;">在仿真数据测试集 t0（GenH2R-sim 引入）中，GenH2R 的方法可以能够预测物体的未来姿势，以实现更加合理的接近轨迹：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b1/d2/ZfA4tCb0_o.gif" alt="80792db186fdeb807fc573aee53bbd00.gif"></p> 
 <p style="text-align:justify;">‍</p> 
 <p style="text-align:justify;">在真实数据测试集 t1（GenH2R-sim 从 HOI4D 引入，比之前工作的 s0 测试集增大约 7 倍）中，GenH2R 的方法可以泛化到没有见过的、具有不同几何形状的真实世界物体。</p> 
 <p style="text-align:left;"><strong>B. 真机实验</strong></p> 
 <p style="text-align:justify;">GenH2R 同时将学到的策略部署到现实世界中的机械臂上，完成 “sim-to-real“的跳跃。</p> 
 <p style="text-align:justify;">对于更复杂的运动轨迹（例如旋转），GenH2R 的策略展示出更强的适应能力；对于更复杂的几何，GenH2R 的方法更可以展现出更强的泛化性：</p> 
 <p style="text-align:center;">‍<img src="https://images2.imgbox.com/4a/92/ubfP1bsm_o.gif" alt="c794e81dae571ded91da41e374a03570.gif"></p> 
 <p style="text-align:justify;">GenH2R 完成了对于各种交接物体的真机测试以及用户调研，展示出很强的鲁棒性。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fe/c3/RG1izsCr_o.jpg" alt="07e8ff67447dc600375c7e9fdfa563d0.jpeg"></p> 
 <p style="text-align:justify;">了解更多实验、方法内容，请参考论文主页。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/8c/97/jyTthCCA_o.png" alt="fc3ee3837d68afdd957fdbf7241051d3.png"></p> 
 <h3><strong>团队介绍</strong></h3> 
 <p style="text-align:justify;">该论文来自清华大学 3DVICI Lab、上海人工智能实验室和上海期智研究院，论文的作者为清华大学学生汪子凡（共同一作）、陈峻宇（共同一作）、陈梓青和谢鹏威，指导老师是弋力和陈睿。</p> 
 <p style="text-align:justify;">清华大学三维视觉计算与机器智能实验室（简称 3DVICI Lab），是清华大学交叉信息研究院下的人工智能实验室，由弋力教授组建和指导。3DVICI Lab 瞄准人工智能最前沿的通用三维视觉与智能机器人交互问题，研究方向涵盖具身感知、交互规划与生成、人机协作等，与机器人、虚拟现实、自动驾驶等应用领域密切联系。团队研究目标是使智能体具备理解并与三维世界交互的能力，成果发表于各大顶级计算机会议、期刊上。</p> 
 <p><strong>更多阅读</strong></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><img src="https://images2.imgbox.com/a5/f5/JZlFEH89_o.png" alt="122920ef9efd682d88778359ddfdbab7.png"></a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><img src="https://images2.imgbox.com/34/68/BtwWbqPX_o.png" alt="11c4729b0ede30bd7992ed4abe0a194b.png"></a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><img src="https://images2.imgbox.com/3e/2e/iA2Xbe8Z_o.png" alt="350960cf1a3624dd9240ba55bb2ceb9d.png"></a></p> 
 <p><img src="https://images2.imgbox.com/f7/79/ueSoltaO_o.gif" alt="f8a0ebb3022efac642c273f729c9d286.gif"></p> 
 <p><strong>#投 稿 通 道#</strong></p> 
 <p><strong> 让你的文字被更多人看到 </strong></p> 
 <p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？<strong>答案就是：你不认识的人。</strong></p> 
 <p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 </p> 
 <p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<strong>最新论文解读</strong>，也可以是<strong>学术热点剖析</strong>、<strong>科研心得</strong>或<strong>竞赛经验讲解</strong>等。我们的目的只有一个，让知识真正流动起来。</p> 
 <p>📝 <strong>稿件基本要求：</strong></p> 
 <p>• 文章确系个人<strong>原创作品</strong>，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注 </p> 
 <p>• 稿件建议以 <strong>markdown</strong> 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p> 
 <p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<strong>业内具有竞争力稿酬</strong>，具体依据文章阅读量和文章质量阶梯制结算</p> 
 <p>📬 <strong>投稿通道：</strong></p> 
 <p>• 投稿邮箱：hr@paperweekly.site </p> 
 <p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p> 
 <p>• 您也可以直接添加小编微信（<strong>pwbot02</strong>）快速投稿，备注：姓名-投稿</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f2/0a/HMMsVk3i_o.png" alt="ffcd8a25e35f5fdc62d7ab7fca59ee24.png"></p> 
 <p style="text-align:center;"><strong>△长按添加PaperWeekly小编</strong></p> 
 <p style="text-align:center;">🔍</p> 
 <p style="text-align:center;">现在，在<strong>「知乎」</strong>也能找到我们了</p> 
 <p style="text-align:center;">进入知乎首页搜索<strong>「PaperWeekly」</strong></p> 
 <p style="text-align:center;">点击<strong>「关注」</strong>订阅我们的专栏吧</p> 
 <p>·</p> 
 <p>·</p> 
 <p style="text-align:right;"><img src="https://images2.imgbox.com/d3/61/YkicdDW2_o.jpg" alt="5c7ed7b17c87048fcbc253607af045a0.jpeg"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/913fa035a6ea5a04f982c5091de9eb0f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Golang语言基础—函数调用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4f7ce1b73e3dc74c569afd506e71c9d6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">博士申请 | 香港科技大学（广州）孙莹老师招收人工智能方向全奖博士生/RA</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>