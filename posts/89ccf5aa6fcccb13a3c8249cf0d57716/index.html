<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>NIPS 2015 Deep Learning Symposium - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="NIPS 2015 Deep Learning Symposium" />
<meta property="og:description" content="今天小S 决定咬着牙写写 NIPS 2015 的论文总结。NIPS 文章理论较多，耗时耗力，如果有总结不周的地方，大家多多包涵，多多指正。因为 NIPS 文章也很多，我将会按照官方的 topic 分类。今天先分享 Deep Learning Symposium 中一些 references。依然很多，今天是第一部分，周五争取分享剩下的。
今天包括的：
《Character-aware Neural Language Models》. Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush.
《Character-level Convolutional Networks for Text Classification》. Xiang Zhang, Junbo Zhao, Yann LeCun.
《A Neural Algorithm Of Artistic Style》. Leon A. Gatys, Alexander S. Ecker, Matthias Bethge.
《Listen, attend and spell》. William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals.
《Skip-thought vectors》." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/89ccf5aa6fcccb13a3c8249cf0d57716/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2015-12-15T14:38:00+08:00" />
<meta property="article:modified_time" content="2015-12-15T14:38:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NIPS 2015 Deep Learning Symposium</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body" class="blogpost-body"> 
 <p class="p2">今天小S 决定咬着牙写写 NIPS 2015 的论文总结。NIPS 文章理论较多，耗时耗力，如果有总结不周的地方，大家多多包涵，多多指正。因为 NIPS 文章也很多，我将会按照官方的 topic 分类。今天先分享 Deep Learning Symposium 中一些 references。依然很多，今天是第一部分，周五争取分享剩下的。</p> 
 <p class="p2"><img src="https://images2.imgbox.com/20/b2/MwfkVUAL_o.jpg" alt=""></p> 
 <p class="p2"><strong>今天包括的：</strong></p> 
 <p class="p2"><strong>《Character-aware Neural Language Models》. </strong>Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush.</p> 
 <p class="p2"><strong>《Character-level Convolutional Networks for Text Classification》</strong>. Xiang Zhang, Junbo Zhao, Yann LeCun.</p> 
 <p class="p2"><strong>《A Neural Algorithm Of Artistic Style》</strong>. Leon A. Gatys, Alexander S. Ecker, Matthias Bethge.</p> 
 <p class="p2">《<strong>Listen, attend and spell</strong>》. William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals.</p> 
 <p class="p2"><strong>《Skip-thought vectors》</strong>. Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, et al.</p> 
 <p class="p2"><strong>《Ask me anything: Dynamic memory networks for natural language processing》</strong>. Ankit Kumar, Ozan Irsoy, Peter Ondruska, et al.</p> 
 <p class="p2"><strong>《Teaching machines to read and comprehend》</strong>. Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, et al.</p> 
 <p class="p2"><strong>《Towards AI-complete question answering: A set of prerequisite toy tasks》</strong>. JasonWeston, Antoine Bordes, Sumit Chopra, et al.</p> 
 <p class="p1">《<strong>Visualizing and understanding recurrent networks</strong>》. Andrej Karpathy, Justin Johnson, Li Fei-Fei.</p> 
 <p class="p2">《<strong>End-to-end memory networks</strong>》. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus.</p> 
 <p class="p2"><strong>《Grid long-short term memory》</strong>. Nal Kalchbrenner, Ivo Danihelka, Alex Graves.</p> 
 <p> </p> 
 <p> </p> 
 <p><img src="https://images2.imgbox.com/c3/4b/5cQOjwFH_o.jpg" alt=""></p> 
 <p><strong>Character-aware Neural Language Models</strong></p> 
 <p> </p> 
 <p>这篇文章之前挂在 arXiv 上，现在已经被 AAAI 2016 正式接收。<strong>推荐系数5星。是一篇 NLP 和 DL 结合的非常好的论文，而且信息量非常大</strong>。他们的 model 由两个部分组成，character-level 的输入，输入给 CNN，CNN 的输出，输入给 RNNLM，但最终预测仍然是 word-level。</p> 
 <blockquote> 
  <p><em>In this work, we propose a language model that leverages subword information through a character-level convolutional neural network (CNN), whose output is used as an input to a recurrent neural network language model (RNNLM).</em></p> 
 </blockquote> 
 <p><img src="https://images2.imgbox.com/b4/4e/MNURFT7w_o.jpg" alt=""></p> 
 <p>只用了一次 convolutional + pooling (max-over-time)，并且作者表示用多次 conv+pooling 的组合（stacked，就像 CNN for Sentence Modeling 那里一样，并没有提升效果）。<em>Whereas a conventional NLM takes word embeddings as inputs, our model instead takes the output from a single-layer character-level CNN with max-over-time pooling. </em>除了在 stacked 与否的问题上，和 Sentence Modeling 的 CNN 不同的第二个地方在于，这里不再是 wide convolutional，而是 narrow convolutional。</p> 
 <p>特殊的地方在于 pooling 的 output 不是直接输入给 LSTM，而是先经过一个基于 <strong>Highway Network （HW-Net）</strong>改造的 module。而且在实验中，作者验证了，如果没有这个 module，效果会差。这个 HW-Net Module 只对 character-level 的模型有效，对于 word-level input，则无提升。HW-Net 相当于另一个 nonlinear 隐层，作用类似于别的模型中的 MLP（multilayer perceptron），增强 feature 之间的 interaction 的。在这篇论文中的这个模型里，因为 conv+pooling 只是单层，没有那么多 interaction 被 model 进去，所以考虑了这层 interaction。<strong>但是作者声称尝试了 stacked CNN，没有提升，所以是否可以推论 highway networks 的 interaction 和 stacked CNN 是不同的？</strong></p> 
 <blockquote> 
  <ul class="list-paddingleft-2"><li> <p><em>Similar to the adaptive memory cells in LSTM networks, HW-Net allows for training of deep networks by adaptively carrying some dimensions of the input directly to the output.</em></p> </li><li> <p><em>Applying HW-Net to the CharCNN has the following interpretation: since each output  is essentially detecting a character n-gram (where n equals the width of the filter), HW-Net allows some character n-grams to be combined to build new features (dimensions where transform ≈ 1), while allowing other character n-grams to remain ‘as-is’ (dimensions where carry ≈ 1).</em></p> </li></ul> 
 </blockquote> 
 <p>最后作者在实验结论部分表示这个 highway 对于 character-aware compositonal 的 model 非常重要，对于 word-level 不重要。加不加这个东西，可以从学出来的 word representation 周围都是啥词看出效果（见 Table 5）. 这个结果还是不要太明显……震惊。<img src="https://images2.imgbox.com/6c/6d/oJCGkIOu_o.jpg" alt=""></p> 
 <blockquote> 
  <ul class="list-paddingleft-2"><li> <p><em>Before the highway layers the representations seem to solely rely on surface forms—for example the nearest neighbors of you are your, young, four, youth, which are close to you in terms of edit distance. The highway layers however, seem to enable encoding of semantic features that are not discernable from orthography alone. After highway layers the nearest neighbor of you is we, which is orthographically distinct from you. Another example is while and though— these words are far apart edit distance-wise yet the composition model is able to place them near each other.</em></p> </li></ul> 
 </blockquote> 
 <p>HW Net 对于 word-level 没用的原因是：<em>dimensions of word embeddings do not (a priori) encode features that benefit from nonlinear, hierarchical composition availed by highway layers</em>. 最后这篇的 Related Work 也很值得看。</p> 
 <p> </p> 
 <p> </p> 
 <p><img src="https://images2.imgbox.com/8f/03/SVXpOfvK_o.jpg" alt=""></p> 
 <p><strong>Character-level Convolutional Networks for Text Classification</strong></p> 
 <p> </p> 
 <p>这篇论文前身是《Text Understanding from Scratch》，当时在微博上一时因为实验效果太过拔群，引起轰动和过分关注。最后被发现是实验数据测试集训练集有严重重叠，暂时撤稿。重新修改后被 NIPS’15 接收。这篇论文的风格给人感觉非常不 NLP，从术语到模型思路，到写作风格，都感觉是纯做 Deep Learning 的人，来做了 NLP 的任务而已。</p> 
 <p><img src="https://images2.imgbox.com/90/b4/Rt6VoIyE_o.jpg" alt=""><br><br>文章中的模型是完全基于 ConvNet 改造。按照作者的原话是，<em>This article is the first to apply ConvNets only on characters。</em>是很规矩的, temporal convolutional，temporal max-pooling (max-over-time)，然后有 dropout 在最后三个全连接层。值得注意的是他们用到的 nonlinear funtion 是 rectifier/thresholding：h(x) = max{0,x}，使得很像 ReLUs。</p> 
 <p>细节上，这个模型十分“简单”，并没有做变长的 convolutional 处理。也就是说，一旦一个输入的 text chunk，超过了他们预定的一个 length，那么后面的 character 就都不要了。输入的时候就是把一个定长的 character embedding <strong>sequence</strong> input 进去。另外的细节是，尽管不是用 RNN 这样的 recurrent 模型 encoding decoding，但他们依然用 reverse 的 input，<em>The character quantization order is backward so that the latest reading on characters is always placed near the begin of the output, making it easy for fully connected layers to associate weights with the latest reading</em>。</p> 
 <p>在实验设置上考虑了大小写区分开，然后发现很多时候区分反而不如不区分。<em>We report experiments on this choice and observed that it usually (but not always) gives worse results when such distinction is made. One possible explanation might be that semantics do not change with different letter cases, therefore there is a benefit of regularization. </em>他们使用了 Data Augmentation，用同义词去制造更多的“正例”。这部分细节在 Section 2.4。</p> 
 <p> </p> 
 <p> </p> 
 <p><img src="https://images2.imgbox.com/e5/2f/TFb0bA7g_o.jpg" alt=""></p> 
 <p><strong>A Neural Algorithm Of Artistic Style</strong></p> 
 <p> </p> 
 <p class="p1">这篇文章应该是这次<span class="s1"> Symposium 中最著名的了。看看下面这些作品：</span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/af/84/m0OnTO6H_o.jpg" alt=""></p> 
 <p class="p1">这个工作被叫做<span class="s1"> neural art，就是用<span class="s1"> Deep Neural Networks 的方法，将一些作品，进行特定风格化（<span class="s1">photorealistic rendering）。比如可以将梵高在《<span class="s1">Starry Night》中的那种，高对比和清晰笔触的风格，渲染在各种风景摄影作品上。</span></span></span></span></p> 
 <p class="p2"><span class="s2">这个工作主要基于 CNN，其核心是对一张图片中的 content 和 style 分别建模 representation，而<strong><span class="s2">核心的核心就是 style 的建模</span></strong><span class="s2">。由于 CNN 的 hierarchy，content 建模中，越是 high-level 的 content representation 可能越 general，越难 reconstruct；但另一方面，style representation，则是在 high-level 的地方更不容易被“image content”的局部信息所“迷惑”。<br></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/91/4a/pNwcDFxP_o.jpg" alt=""></p> 
 <p class="p3"><span class="s2">因为 content <span class="s2">和 style <span class="s2">是很难完全独立开的，在网络设计上，作者也考虑了这点。所以 style representation <span class="s2">并不是基于某一层的 content representation<span class="s2">，而是在 CNN <span class="s2">中的每一层<span class="s2">都有一个 style representation<span class="s2">。style representation <span class="s2">的建模就是利用“<span class="s2">不变性”——<span class="s2">这背后的假设其实是，不管你在画什么东西，画哪个局部，你的 style <span class="s2">应该保证了一种局部性；不变的 feature<span class="s2">，才是 style<span class="s2">。所以它采用的是对比每一个层 filter <span class="s2">之间的 correlation<span class="s2">，进行 style representation<span class="s2">。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2">这个工作已经有相当多开源代码，在线应用，也被集成在各种<span class="s1"> NN 框架中。大家有兴趣可以找来玩玩。<br></span></p> 
 <p> </p> 
 <p> </p> 
 <p><img src="https://images2.imgbox.com/94/93/DsNgV3K7_o.jpg" alt=""></p> 
 <p><strong>Listen, attend and spell</strong></p> 
 <p> </p> 
 <p class="p1"><span class="s1">这篇文章其实挺简单的。核心思想就是用一个 listener-speller encoder-decoder <span class="s1">的结果做 speech recognition<span class="s1">（speeach utterances -&gt; characters)<span class="s1">。listener<span class="s1">（encoder<span class="s1">）部分用的是 pyramidal RNN<span class="s1">，这个比较特别，作者 argue <span class="s1">说 pyramidal RNN <span class="s1">对于这个任务提速显著。</span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/16/60/abYBJlhD_o.jpg" alt=""><br>pyramidal RNN <span class="s1">的部分，实质是一个 hierarchical <span class="s1">的 Bi-LSTM <span class="s1">（pBLSTM<span class="s1">）。而这个架构，就像 CNN <span class="s1">一样，high-level<span class="s1">（在“<span class="s1">金字塔”<span class="s1">顶端）的 features <span class="s1">会比较少，比较“<span class="s1">浓缩”<span class="s1">，用这样少一些的 features <span class="s1">传给 decoders<span class="s1">，会减少 decoder <span class="s1">解析的耗时，提高解析的能力，并且整体上提高 encoder-decoder <span class="s1">的 inference <span class="s1">速度。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2">另一方面，speller <span class="s1">端用的 attention-mechansim<span class="s1">，好处很显然，防止 overfitting<span class="s1">。<br></span></span></span></p> 
 <blockquote> 
  <p class="p1">Without the attention mechanism, the model overfits the training data significantly, in spite of our large training set of three million utterances - it memorizes the training transcripts without paying attention to the acoustics. Without the pyramid structure in the encoder side, our model converges too slowly - even after a month of training, the error rates were significantly higher than the errors we report here.</p> 
 </blockquote> 
 <p> </p> 
 <p> </p> 
 <p><img src="https://images2.imgbox.com/98/ae/DTam0Z1b_o.jpg" alt=""></p> 
 <p><strong>Skip-Thought Vectors</strong></p> 
 <p> </p> 
 <p class="p1">已经是相当有名的工作。模型上，具体使用是 RNN-RNN 的 encoder-decoder 模型；<strong>其中两个 RNN 都用了 GRU 去“模拟” LSTM 的更优表现</strong>。在 encoder 阶段，只是一个 RNN；在 decoder 阶段，则是两个（分别对应前一个句子和后一个句子——也就是说不能预测多个前面的句子和后面的句子）。<img src="https://images2.imgbox.com/9c/53/gdhCguC1_o.jpg" alt=""></p> 
 <p>这样的模型可以保留一个 encoding for each sentence，这个 encoding 会很有用，就被称为 skip-thoughts vector，用来作为特征提取器，进行后续 task。注意是 Figure 1 中所谓的 unattached arrows，对应在 decoder 阶段，是有一个 words conditioned on previous word + previous hidden state 的 probability 束缚。同时，因为 decoder 也是 RNN，所以可用于 generation（在论文结尾处也给出了一些例子）。</p> 
 <p>本文的另一个贡献是 <strong>vocabulary mapping</strong>。因为 RNN 的复杂性，但作者又不希望不能同时 learn word embedding，所以只好取舍一下——我们 learn 一部分 word embedding（words in training vocabulary）；对于没出现的部分，我们做一个 mapping from word embedding pre-trained from word2vec。这里的思想就是 Mikolov'13 年那篇 word similarity for MT 的，用一个没有正则的 L2 学好 mapping。</p> 
 <p>在实验中，他们用此方法将 20K 的 vocabulary 扩充到了 930K。</p> 
 <blockquote> 
  <p><em>In our experiments we consider 8 tasks: semantic-relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks. In these experiments, we extract skip-thought vectors and train linear models to evaluate the representations directly, without any additional fine-tuning. As it turns out, skip-thoughts yield generic representations that perform robustly across all tasks considered.</em></p> 
 </blockquote> 
 <p>首先是他们有三种 feature vectors，uni-skip/bi-skip/combine-skip。分别对应 encoder 是 unbidirectional，bidirectional，和 combine 的。分别都是 4800 dimensions。对于不同的 task，可能用不同的 feature indicator，比如把两个 skip-thoughts-vectors u 和 v，点乘或者相减，作为两个 feature，再用 linear classifier(logistic)。</p> 
 <p> </p> 
 <p> </p> 
 <p><img src="https://images2.imgbox.com/e6/06/oGRd0MAi_o.jpg" alt=""></p> 
 <p><strong>Ask me anything: Dynamic memory networks for natural language processing</strong></p> 
 <p> </p> 
 <p class="p1"><span class="s1">这篇文章也是相当早就放在 arXiv <span class="s1">上了，ACL 2015 <span class="s1">的论文中就已经有人引用。文章来自 Richard Socher <span class="s1">的 MetaMind <span class="s1">团队。主要就是利用一个 dynamic memory network<span class="s1">（DMN<span class="s1">）框架去进行 QA<span class="s1">（甚至是 Understanding Natural Language<span class="s1">）。</span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/9a/41/ufo2uapp_o.jpg" alt=""><br><span class="s1">这个框架是由几个模块组成，可以进行 end-to-end 的 training。其中核心的 module 就是Episodic Memory module<span class="s1">，可以进行 iterative 的 semantic + reasoning processing。DMN 先从 input 接受 raw input（question），然后生成 question representation，送给 semantic memory module，semantic module 再将 question representation + explicit knowledge basis（只是设想） 一起传给核心的 Episodic Memory module。这个 Episodic Memory module 会首先 retrieve question 中涉及到的 facts 和 concepts，再逐步推理得到一个 answer representation。由于可能有多个涉及到的 facts 和 questions，所以这里还用到了 attention mechanism。最后，Answer Module 就可以用接收到的 answer representation 去 generate 一个真正的 answer。</span></span></p> 
 <p><img src="https://images2.imgbox.com/52/91/JSWUQUOJ_o.jpg" alt=""></p> 
 <p> </p> 
 <p> </p> 
 <p><img src="https://images2.imgbox.com/b1/c0/0lPxgaas_o.jpg" alt=""><strong>Teaching machines to read and comprehend</strong></p> 
 <p> </p> 
 <p>这篇论文有两个主要贡献，一个在于 <strong>attention-based model</strong> 的运用和改进，一个在于构造了一个 supervised document-query based 的数据集，虽然说是供 machine comprehend 使用，其实依然没有超过 QA 范畴，就是基于一篇 document，一个 query（document-query pair）回答一个 entity form 的 answer。数据集在此不表。来看他们 attention-based 的相关 model。</p> 
 <p><img src="https://images2.imgbox.com/c5/cb/1PXwoDPN_o.jpg" alt=""><br>论文一共提出<strong>三个新 model</strong>，其中只有后两个（图中 (a)(b)）是 attention-based 的。input 都是一个 document query pair。作者尝试了两种机制，一种是 document 按一小段句子（以标点分割）输入，一小段句子+一个query，这样算一次输入；另一种是一篇 document 全部输入完毕再输入 query，这种方法被认为失去了 query 的 mention 作用。</p> 
 <p>两个 attention-based model，(a) Attentive Reader 和 (b) Impatient Reader。(a) 非常好理解，看 (a) 左边就是标准的 attention mechanism 的结构啊，对比一下：</p> 
 <p><img src="https://images2.imgbox.com/d8/22/kjVcHGBd_o.jpg" alt=""></p> 
 <p>再看 (b) Impatient Reader，这个 model 很有趣，尤其和我下面想说的非 NLP 那篇有点像。我按我的理解来解读，这个 model 强调<strong>“reread”</strong>，就是说，对于每一个 query，有许多个 token，按照 query token 一个个输入，每一个 query token（不再是每一个 query），就都读一遍 document，然后下一个 token，再来一遍——reread。</p> 
 <p>我会把这种 reread 机制，理解为一种“逐渐”获取（理解）文章的过程，就像我们读一篇艰深的文章，读一遍不成，读两遍，读三遍。这个机制的 motivation 很好，但是如果只用来预测一个 token（the answer），我会认为起不到 motivation 的作用。个人理解。</p> 
 <p> </p> 
 <p> </p> 
 <p><img src="https://images2.imgbox.com/d2/c5/wYUvQjhr_o.jpg" alt=""><strong>Towards AI-complete question answering: A set of prerequisite toy tasks</strong></p> 
 <p class="p1"> </p> 
 <p class="p1">和《<span class="s1">Ask Me Anything》那篇一样，也是很早就放在<span class="s1"> arXiv 上的工作。这个数据集不仅被《<span class="s1">Ask Me Anything》引用，也被很多<span class="s1"> ACL 2015 和后续的<span class="s1"> QA 工作引用。文章主要就是介绍他们的<span class="s1"> AI-related <span class="Apple-tab-span"> QA 数据集，因为是<span class="s1"> Facebook 团队制作且有<span class="s1"> 20类问题，所以这个数据集后来被缩写为<span class="s1"> FB20。</span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p1"><span class="s2">按照上次介绍的 ICLR2016 <span class="s2">中 Jianfeng Gao <span class="s2">团队的工作，在这 20<span class="s2">类问题中，最难做的是 position reasoning <span class="s2">和 path finding <span class="s2">两类任务。</span></span></span></span></span></span></p> 
 <blockquote> 
  <p class="p3">We achieve near-perfect accuracy on all categories, including positional reasoning and pathfinding that have proved difficult for all previous approaches due to the special two-dimensional relationships identified from this study.</p> 
 </blockquote> 
 <p class="p2"> </p> 
 <p class="p2">如果想了解在这个数据集上的一些工作，可以看下面几篇论文：</p> 
 <p class="p3">1.《Learning Answer-Entailing Structures for Machine Comprehension》Mrinmaya Sachan, Kumar Dubey, Eric Xing, Matthew Richardson. ACL 2015. CMU <span class="s2">出品，Eric Xing <span class="s2">老师的组。本文不是 NN<span class="s2">，数学上还算简单。个人觉得有两个亮点，一个就是假设了一个中间的 hypothesis<span class="s2">，一个是在数学的地方结合了 multi-task<span class="s2">，并使用了 feature map <span class="s2">的 technique <span class="s2">把 multi-task <span class="s2">给“<span class="s2">退化”<span class="s2">成了原始问题。他们先用 Question <span class="s2">和 Answer<span class="s2">，学出一个 hypothesis<span class="s2">，这个 hypothesis <span class="s2">就是一种 latent variable<span class="s2">，也可以认为是一种 embedding <span class="s2">后的 fact<span class="s2">。如果我们认为 question + answer <span class="s2">共同描述了一个 fact/truth/event <span class="s2">的话。基于这个 hypothesis<span class="s2">，再去 match <span class="s2">原始 paragraph/text <span class="s2">里的 relevant words<span class="s2">。具体可以看看 Figure 1.<span class="s2">我觉得这个蛮有趣的。因为让我想起编码解码。Question + Answer <span class="s2">的组合就是一种 <span class="s2">对于这篇 doc <span class="s2">的一种表达；而这篇 doc <span class="s2">本身是另一种表达。这两种表达就是两种 representation <span class="s2">的结果，那么中间真实的事情是什么？所谓的完整的 information <span class="s2">是什么？他这样直接结合的 hypothesis <span class="s2">肯定也是 reduce <span class="s2">了信息的。实际我觉得现在 Machine Translation/Conversation <span class="s2">那边也在做类似的事情。我们不要直接一对一，要有中间一个看不见的“hypothesis”<span class="s2">。第二个 multi-task<span class="s2">，他们用了FB20<span class="s2">这20<span class="s2">类，把任务细分，细分成 20<span class="s2">个 subtask<span class="s2">。这样就变成了 multi-task <span class="s2">的问题。然后使用了 feature map<span class="s2">（Evgeniou 2004<span class="s2">）的技术，把 multi-task <span class="s2">又给转化成了原始问题。我觉得还蛮有趣的。当然 multi-task <span class="s2">已经有非常多的解决办法了，这个只是一种适用于他的模型的有效简单的办法。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p3"><span class="s2">2.《Machine Comprehension with Discourse Relations》. Karthik Narasimhan and Regina Barzilay. ACL 2015. MIT CSAIL <span class="s2">出品。开源。是一篇很 neat <span class="s2">的论文，而且不是 NN<span class="s2">。这篇文章的卖点是：discourse information + less human annotation<span class="s2">所以他们的 model<span class="s2">，可以使用 discourse relation<span class="s2">（relations between sentences, learned, not annotated) <span class="s2">去增强 machine comprehension <span class="s2">的 performance<span class="s2">。具体的，他们先使用 parsing <span class="s2">等方法，去选出和 question <span class="s2">最 relevant <span class="s2">的一个句子（Model 1<span class="s2">）或者多个句子（Model 2 <span class="s2">和 Model 3<span class="s2">），并在这个过程中建立 relation<span class="s2">，最后预测。思想都是 discriminative model <span class="s2">的最简单的思想，找 hidden variable<span class="s2">，概率连乘。如果对本文有兴趣，推荐看 Section 3.1<span class="s2">，讨论了一下他们认为这个 task <span class="s2">上可能相关的四【类】feature<span class="s2">。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p3"><span class="s2">3.《Reasoning in Vector Space: An Exploratory Study of Question Answering》. In submission to ICLR 2016. <span class="s2">文章来自 Microsoft Jianfeng Gao, Xiaodong He <span class="s2">团队。是一份比较详细的针对 Facebook 20 tasks<span class="s2">（FB20<span class="s2">）的分析和工作。所谓分析是指，过去针对 FB20 <span class="s2">的 Reasoning Work <span class="s2">基本都是 end-to-end <span class="s2">的，所以对于 error case <span class="s2">的分析不够明确，不知道到底是作为 basis <span class="s2">的 semantics <span class="s2">就没建模好；还是 reasoning <span class="s2">的过程出了问题。为了进一步提高在这个 tasks <span class="s2">上的 performances<span class="s2">，作者就将 end-to-end <span class="s2">拆分开来，利用 tensor product representation<span class="s2">（TPR<span class="s2">）的方法，融合一些 common-sense inference<span class="s2">（比如东和西是 opposite <span class="s2">的两个方向），将 FB20 <span class="s2">的正确率提高到了几乎完美的程度。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p3"> </p> 
 <p class="p3"> </p> 
 <p class="p3"><img src="https://images2.imgbox.com/c8/2d/ZasIXvoD_o.jpg" alt=""><strong>Visualizing and understanding recurrent networks</strong></p> 
 <p class="p2"> </p> 
 <p class="p2">作者是写出《The Unreasonable Effectiveness Of RNN》 博文的 Stanford 学生，Andrej Karpathy。同时 Andrej Karpathy 也是 Fei-Fei Li 教授的高徒。</p> 
 <p class="p2"><img src="https://images2.imgbox.com/a8/85/Ci6yN516_o.jpg" alt=""></p> 
 <p class="p2">这篇工作从几个月前递交到 arXiv，前几日又更新了一版，投到了 ICLR 2016，内容上是博文的扩展。主要是通过 controlled experiment 的实验方式，结合可视化的方法，去“量化”展示 char-LSTM 到底为什么 powerful，是否真的如 often cited/claimed 的那样，可以 model long term dependency。这篇工作最后的结论也和之前 Yoav Goldberg 澄清 char-LSTM 令人惊奇之处的文章一致——指出 char-LSTM 厉害之处，<strong>不在于它能 generate 出看起来还不错的 char sequence，而是在于其对于 bracket, quote 等显著 long distance information 的 retrieval 能力。</strong></p> 
 <p class="p2"><img src="https://images2.imgbox.com/c6/a5/BD5ZfCmy_o.jpg" alt=""></p> 
 <p class="p2">它们通过 visulization cell 的激活，gate activation statistics, error type/case analysis 的方式，展现了许多 LSTM 确实是“对应”和“负责”某些 character position 的，同时 LSTM 确实比 n-gram character language model 大幅降低了 bracket, quote 等 long distance information 的 error case。</p> 
 <p class="p2"> </p> 
 <p class="p2"> </p> 
 <p class="p2"><img src="https://images2.imgbox.com/39/9e/99XebAD2_o.jpg" alt=""></p> 
 <p class="p2"><strong>End-to-end memory networks</strong></p> 
 <p class="p2"> </p> 
 <p class="p1"><span class="s1">这篇文章，及 Neural Turing Machine<span class="s1">，其实是很多类似思想的前身工作，下次会把相关一起对比一下。这类工作的 motivation <span class="s1">是，如何把 large body <span class="s1">的 external memory <span class="s1">用在 Neural Networks <span class="s1">里。</span></span></span></span></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/31/9d/w3iOgC4t_o.jpg" alt=""><br><span class="s1">在这篇工作中，他们就是尝试性地探究了几种方式。首先，是 single-layer or multi-layer<span class="s1">，其次是 feature <span class="s1">空间如何转换。如果将这样的 end-to-end memory networks <span class="s1">的输出拆分成两种，就可以和 typical RNN <span class="s1">的工作映射起来。将 output <span class="s1">分为 internal output <span class="s1">和 external output<span class="s1">，那么分别就可以对应到 RNN <span class="s1">中的 memory <span class="s1">和 predicted label<span class="s1">。</span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p1"> </p> 
 <p class="p1"><span class="s1"> </span></p> 
 <p class="p1"><img src="https://images2.imgbox.com/ca/9b/69mfRQ9Z_o.jpg" alt=""><strong>Grid Long-Short Term Memory</strong></p> 
 <p> </p> 
 <p class="p1"><span class="s1">总的来说，这篇的贡献应该是给出了一个更 flexible <span class="s1">还 computation capability <span class="s1">更高的框架。 <span class="s1">要理解这个论文，可能首先要理解三个概念：<strong>grid/block, stacked, depth。</strong><span class="s1">（1<span class="s1">）Grid/Block <span class="s1">是把一个 LSTM <span class="s1">机制改造后的一个 component<span class="s1">，这个 component <span class="s1">可以是 multi-dimensional <span class="s1">的，决定了几个方向进行 propagate<span class="s1">。每一个 dimension <span class="s1">都有 memory <span class="s1">和 hidden cell<span class="s1">。1-dimensional <span class="s1">的Grid LSTM <span class="s1">就很像上面所说的 Highway Networks<span class="s1">。（2<span class="s1">）Stacked <span class="s1">和 LSTM stacked <span class="s1">一样，是指把 output <span class="s1">和 input <span class="s1">连在一起。但是 stacked <span class="s1">并不会改变 Grid LSTM <span class="s1">的 dimension<span class="s1">。stacked 2D Grid LSTM <span class="s1">依然是 2D <span class="s1">的，而不是 3D <span class="s1">的。从 visualize <span class="s1">来看，无非就是把一个个方块/<span class="s1">方形，平铺在空间里（每个 dimension <span class="s1">都要延展）。（3<span class="s1">）Depth <span class="s1">则是会增加 dimension<span class="s1">。在一个 block<span class="s1">内部，变 deep<span class="s1">，就是增加 layers<span class="s1">。一个 block <span class="s1">由几个 layer <span class="s1">组成，就是几层 deep <span class="s1">的 Grid LSTM<span class="s1">。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><span class="s1">只是 1D/2D <span class="s1">的时候，Grid LSTM <span class="s1">看不出特别大的优点。但是当变成 multidimensional <span class="s1">的时候，就会比传统的 multidimensional LSTM <span class="s1">更好的解决 gradient vanishing <span class="s1">的问题。原因是，传统multidimensional LSTM <span class="s1">在计算每层的 memory cell <span class="s1">的时候，是把每个 dimensional <span class="s1">的 gate <span class="s1">信息集合起来的。显然这样有问题。Grid LSTM <span class="s1">就不是这样。它是每个 dimensional <span class="s1">分开计算memory cell<span class="s1">。对于每一个 grid<span class="s1">，有 N <span class="s1">个 incoming memory cells <span class="s1">和 hidden cells<span class="s1">，同时还有 N <span class="s1">个outgoing memory cells <span class="s1">和 hidden cells<span class="s1">。N <span class="s1">是 dimension <span class="s1">的个数。而 Grid LSTM share <span class="s1">的其实大的隐层 H<span class="s1">。这样既保证了 interaction <span class="s1">又保证了 information flow<span class="s1">。<br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/09/50/fQVvqeUR_o.jpg" alt=""></p> 
 <p class="p1"><span class="s1">这篇论文后面还有挺有趣的应用，把 MT <span class="s1">的任务转换成一个 3D Grid LSTM <span class="s1">的问题，其中两个dimensions <span class="s1">分别是 bi-LSTM <span class="s1">正向逆向读写，第三个 dimension <span class="s1">是 depth<span class="s1">。效果不俗。</span></span></span></span></span></span></span></p> 
 <p class="p2">可能这篇论文的这个框架的提出，在于让<span class="s2"> LSTM 的变种稍微有迹可循了一点，到底有多大<span class="s2">performance 的提高，我还是比较怀疑的。</span></span></p> 
 <p class="p1"><span class="s1">今天继续把 NIPS 2015 Deep Learning Symposium 的论文笔记写完，这次的论文个人感觉有很多有联系，让我隐隐感到 Bayes 圣战已经再度打响……比如，early stopping, batch normalization 这些都可以在 Bayesian Neural Networks 的框架下有较好的解释了。是否可以预期，NN 这种 black-box 已经越来越透明了？</span></p> 
 <p class="p1"><span class="s1"> </span></p> 
 <p class="p1"><strong>今天会包括的论文有：</strong></p> 
 <p class="p1"><span class="s1">《<strong>Spatial Transformer Networks</strong><span class="s1">》. Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu.</span></span></p> 
 <p class="p1"><span class="s1">《<strong>Semi-Supervised Learning with Ladder Networks</strong><span class="s1">》. Antti Rasmus, Harri Valpola, Mikko Honkala, et al.</span></span></p> 
 <p class="p1"><span class="s1">《<strong>Neural Turing Machines</strong><span class="s1">》. Alex Graves, Greg Wayne, Ivo Danihelka.</span></span></p> 
 <p class="p1"><span class="s1">《<strong>Deep Generative Image Models Using A Laplacian Pyramid Of Adversarial Networks</strong><span class="s1">》. Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus.</span></span></p> 
 <p class="p1"><span class="s1">《<strong>Natural neural networks</strong><span class="s1">》. Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray Kavukcuoglu.</span></span></p> 
 <p class="p1"><span class="s1">《<strong>Early stopping is nonparametric variational inference</strong><span class="s1">》. Dougal Maclaurin, David Duvenaud, Ryan P. Adams.</span></span></p> 
 <p class="p1"><span class="s1">《<strong>Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</strong><span class="s1">》. Yarin Gal, Zoubin Ghahramani.</span></span></p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p2"><span class="s2"><img src="https://images2.imgbox.com/d1/f4/zArDxv1r_o.jpg" alt=""> Spatial Transformer Networks<img src="https://images2.imgbox.com/c7/0d/QpqKiguJ_o.jpg" alt=""></span></p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p1"><span class="s1">来自 Google DeepMind <span class="s1">的工作。主要是说，尽管 CNN <span class="s1">一直号称可以做 spatial invariant feature extraction<span class="s1">，但是这种 invariant <span class="s1">是很有局限性的。因为 CNN <span class="s1">的 max-pooling <span class="s1">首先只是在一个非常小的、rigid <span class="s1">的范围内（2×2 pixels<span class="s1">）进行，其次即使是 stacked <span class="s1">以后，也需要非常 deep <span class="s1">才可以得到大一点范围的 invariant feature<span class="s1">，三者来说，相比 attention <span class="s1">那种只能抽取 relevant <span class="s1">的 feature<span class="s1">，我们需要的是更广范围的、更 canonical <span class="s1">的 features<span class="s1">。为此它们提出了一种新的完全 self-contained transformation module<span class="s1">，可以加入在网络中的任何地方，灵活高效地提取 invariant image features.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/a3/ad/pzX0Sr67_o.jpg" alt=""><br><span class="s1">具体上，这个 module 就叫做 <strong>Spatial Transformers</strong>，由三个部分组成：<span class="s1"> Localization Network, Grid generator 和 Sampler。Localization Network 非常灵活，可以认为是一个非常 general 的进一步生成 feature map 和 map 对应的 parameter 的网络。因此，它不局限于用某一种特定的 network，但是它要求在 network 最后有一层 regression<span class="s1">，因为需要将 feature map 的 parameter 输出到下一个部分：Grid generator。Grid generator 可以说是 Spatial Transformers 的核心<span class="s1">，它主要就是生成一种“蒙版”，用于“抠图”（Photoshop 附体……）。Grid generator 定义了 Transformer function，这个 function 的决定了能不能提取好 invariant features。如果是 regular grid，就好像一张四四方方没有倾斜的蒙版，是 affined grid，就可以把蒙版“扭曲”变换，从而提取出和这个蒙版“变换”一致的特征。在这个工作中，只需要六个参数就可以把 cropping, translation, rotation, scale and skew 这几种 transformation 都涵盖进去，还是很强大的；而最后的 Sampler 就很好理解了，就是用于把“图”抠出来。</span></span></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/54/52/bpjDsGUw_o.jpg" alt=""><br><span class="s1">这个工作有非常多的优点<span class="s1">：（1）它是 self-contained module，可以加在网络中的任何地方，加任何数量，不需要改变原网络；（2）它是 differentiable 的，所以可以直接进行各种 end-to-end 的训练；（3）它这个 differentiable simple and fast，所以不会使得原有网络变慢；（4）相比于 pooling 和 attention 机制，它抽取出的 invariant features 更 general。</span></span></p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p2"><span class="s2"> <img src="https://images2.imgbox.com/de/2a/vyMxjOIP_o.jpg" alt=""> Semi-Supervised Learning with Ladder Networks<img src="https://images2.imgbox.com/98/26/xAcYTtA5_o.jpg" alt=""></span></p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p1"><span class="s1"><img src="https://images2.imgbox.com/e8/b2/MgFvSQJS_o.jpg" alt=""><br>这篇论文并没有特别多的创新点，主要是将 Ladder Networks <span class="s1">从纯 unsupervised fashion <span class="s1">改成了 semi-supervised fashion<span class="s1">。<strong>Ladder Networks</strong> <span class="s1">其实就是把 stacked autoencoder <span class="s1">中 layer <span class="s1">和 decoded reconstruction <span class="s1">之间加上了 skip-connection<span class="s1">，所以就像在 encoder <span class="s1">和 decoder <span class="s1">之间有了 ladder<span class="s1">，因此命名。那么这篇论文的改进就是在 Ladder Networks <span class="s1">上，encoder <span class="s1">部分的每一层 layer <span class="s1">都加入了 Gaussian noise<span class="s1">，并保持 decoder <span class="s1">部分是 noise-free <span class="s1">的。加了 noise <span class="s1">的部分用于 unsupervised autoencoder loss<span class="s1">，noise-free <span class="s1">的就是用来提供 supervised loss<span class="s1">。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p1"><span class="s1">但是这篇论文的实验结果实在是太 outstanding<span class="s1">。在 MNIST <span class="s1">数据集上，达到了 1.13% 的超低错误率<span class="s1">。这也可以一定程度上证明 semi-supervised <span class="s1">的 improvements<span class="s1">。不过，这种 semi-supervised <span class="s1">暂时来看还没被运用得很好，因为这篇工作中，在 validation set <span class="s1">上用的依然是全部的 10K label<span class="s1">，而不是小范围的 label<span class="s1">。这点上，个人认为是有点 cheating <span class="s1">的。</span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p2"><span class="s2"> <img src="https://images2.imgbox.com/67/7b/qlWAyDDa_o.png" alt=""> Neural Turing Machines<img src="https://images2.imgbox.com/8e/b4/bLmbIEfA_o.png" alt=""></span></p> 
 <p class="p1"><strong>Neural Turing Machines（NTM）</strong><span class="s1"> 这个工作应该是整个 DL Symposium 中最出名的了。跟这篇工作相关的工作有个五六篇（比如同是这个 Symposium 中的另外两篇《Large-scale simple question answering with memory networks》和《End-to-end memory networks》<span class="s1">），下次有机会专门写一下。这次只讲这篇原始的 NTM。NTM 的 motivation 我个人理解，主要有两点<span class="s1">：（1）neural networks 虽然可以提供很好的 hidden units 计算，去 model internal memory，但是我们在真实生活中有时候更需要 external memory 的辅助和交互（这是两件事，比如和 NTM 很像的 memory networks 其实就只有辅助，没有交互，而 NTM 是有交互的）；（2）RNN 作为一种出色的 neural networks，其实是图灵完备的（已被证明）。既然如此，是否可以去把它设计成图灵机？出于这两个目的，就有了 NTM 这个工作。</span></span></span></p> 
 <p class="p1"> </p> 
 <p class="p2"><img src="https://images2.imgbox.com/7c/58/UvW2Q4FI_o.jpg" alt=""></p> 
 <p class="p3"><span class="s2">一个 NTM 包括 Controller，Read+Write Heads 和 External Memory<span class="s2">；Controller <span class="s2">就是 NN<span class="s2">。换言之，NTM <span class="s2">比一般的 NN <span class="s2">多在了读写头和外部存储交互（memory networks <span class="s2">就没有读写头）。个人理解，如果把 NTM 中的 Controller 比作计算机的 CPU，那么其中的 memory 就是计算机的 RAM，而 hidden states 就是 CPU 中的 registers。NTM <span class="s2">中的 Read+Write Heads <span class="s2">非常重要，首先它们可以实现 content-based/location-based <span class="s2">的相关操作，也因此就可以模拟 Focus/Attention <span class="s2">的效果——<span class="s2">于是就可以用 content addressing <span class="s2">实现查找 similar data<span class="s2">（content-based<span class="s2">）。Content addressing <span class="s2">之后，interpolation<span class="s2">，提供的是 gate <span class="s2">机制；convolutional shift <span class="s2">提供的是 location-based addressing<span class="s2">。有了上面这些模块，NTM <span class="s2">就可以模拟图灵机，实现一些算法。不仅如此，NTM <span class="s2">是 end-to-end differentiable <span class="s2">的。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/44/67/v3VU2zaY_o.jpg" alt=""><br><span class="s2">从 NTM 的两个 motivation 出发，就可以看出NTM 的两个 goal<span class="s2">：（1）NTM 是为了增强 RNN 的学习能力，那么它也应该像 RNN 一样能 solve problems；（2）NTM 是模拟图灵机，是否有可能学习出内部算法？基于这两个 goal，这篇工作中设计了很多种 tasks，比如 copy，比如 priority sort，同时横向对比了三种架构，NTM with LSTM, NTM with feedforward, standard LSTM。</span></span></p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p2"><span class="s2"> <img src="https://images2.imgbox.com/36/b2/PC1eiLlZ_o.jpg" alt=""> Deep Generative Image Models Using A Laplacian Pyramid Of Adversarial Networks<img src="https://images2.imgbox.com/d4/12/rs4PKeKC_o.jpg" alt=""></span></p> 
 <p class="p1">这个工作虽然知名度不那么大，但是也已经被广泛引用和改进。同样是来自<span class="s1"> NYU 和<span class="s1"> Facebook AI team 的合作（这次<span class="s1"> DL Symposium 中入选的很多篇都出自他们）。工作的思想上很像之前推荐过很多次的<span class="s1"> Google DeepMind 的DRAW（可以回复代码【GH022】查看），就是说，我们在<span class="s1"> generate 图片时，不要强迫<span class="s1"> model 一步到位，而是让它一步步来。<br></span></span></span></span></span></span></p> 
 <p class="p2"><span class="s2">这篇工作中的 model 叫做<strong> Laplacian Generative Adversarial Networks（LAPGAN）</strong><span class="s2">，由 conditional GAN 和 Laplacian pyramid 结构组成。前者，conditional GAN<span class="s2"> 是 GAN 的一种改造，而 GAN 是由一个用于生成 sample 的 generative model（G） 和 一个用于比较 G 生成的 sample 和真实 training data 的 discriminative model（D）构成的框架。那么 conditional GAN 就是在此基础上，再增加上 additional information，比如 sample class/label。后者，Laplacian pyramid<span class="s2"> 则是一种层次化的图像特征表达，主要体现的图像不同 scale 之间的差异。具体公式可以见 Equation (3)-(4)。那么这篇工作就是将这两点结合起来，使得 GAN 也变成一种层次化的 framework，变成了 multi-scale 的。<br></span></span></span></span></p> 
 <p class="p2"><img src="https://images2.imgbox.com/02/37/d1EC9AZk_o.jpg" alt=""><br><span class="s2">个人理解，这样的 LAPGAN 有两个好处<span class="s2">：（1）是 unsupervised，这是 GAN 的优势。可以直接从 finest/highest-scale/level 的图像，一直利用 Adversial Network 逐步进行 training；（2）就像 DRAW 一样，LAPGAN 的核心思想就是把 generation 的过程给“分解”了，变成了一种逐步的“refinement”，所以降低了网络每次需要记忆的内容量，也同时就提高了网络的 capacity 和 scalability。反过来，这样的网络也有一个劣势<span class="s2">，就是它抛弃了图像的 global feature 和 representation，缺少了对于一个 image 的 probability，所以也就在进行 evaluate 时，需要用一些特殊的技巧（比如这篇文章中采用的 Gaussian Parzen window）。</span></span></span></p> 
 <p class="p1"> </p> 
 <blockquote> 
  <p class="p1"><em>Breaking the generation into successive refinements is the key idea in this work. Note that we give up any “global” notion of fidelity; we never make any attempt to train a network to discriminate </em><em>between the output of a cascade and a real image and instead focus on making each step plausible. Furthermore, the independent training of each pyramid level has the advantage that it is far more difficult for the model to memorize training examples – a hazard when high capacity deep networks </em><em>are used.</em></p> 
 </blockquote> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p2"><span class="s2"> <img src="https://images2.imgbox.com/da/21/EBVtchgm_o.jpg" alt=""> Natural Neural Networks<img src="https://images2.imgbox.com/78/1f/XPoRpbUh_o.jpg" alt=""></span></p> 
 <p class="p1"><span class="s1">这篇论文的 motivation <span class="s1">也很 fundamental<span class="s1">，是说 SGD <span class="s1">这样基于 point gradient <span class="s1">的优化方法，在日趋复杂的 NN <span class="s1">架构上越来越无力。另一方面，distribution gradient <span class="s1">的方法则还有很多值得探索的空间。毕竟 distribution <span class="s1">在优化的过程中，是一直可被捕捉的（见今天的另一篇论文《Early stopping is nonparametric variational inference<span class="s1">》）。Distribution gradient <span class="s1">的求解就需要 KL divergence measurement <span class="s1">和 Fisher matrix<span class="s1">。然而，Fisher matrix <span class="s1">的求解计算量非常大（matrix size <span class="s1">大，且包括逆运算等等），使得过去想用 Fisher matrix <span class="s1">的工作都不太 scalable<span class="s1">。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><span class="s1">基于这个 distribution gradient <span class="s1">（也许）可以帮助提高 convergence <span class="s1">效率的想法，这篇工作开始探究 Fisher matrix <span class="s1">的性质。最终通过假设和实验，设计出了一种基于特定 Fisher matrix <span class="s1">的 NN<span class="s1">（给 Fisher matrix <span class="s1">加了一些限定条件，并忽略了一些 interaction<span class="s1">）。在这种 NN <span class="s1">下，它们的优化算法与更有名的 Mirror Descent <span class="s1">很像。<br></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><span class="s1">个人认为这篇工作很直观的贡献是，过去的一些 NN tricks，比如 batch normalization （before non-linearity），zero-mean activations 等等，在这个框架下，都可以有一些理论上的解释。也算是 theoretical Deep Learning<span class="s1">的一种进展吧。<br></span></span></p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p2"><span class="s2"> <img src="https://images2.imgbox.com/a9/b6/xOg6MbrZ_o.jpg" alt=""> Early stopping is nonparametric variational inference<img src="https://images2.imgbox.com/fd/c9/jzYt7qbZ_o.jpg" alt=""></span></p> 
 <p class="p1"><strong>这篇文章很推荐</strong><span class="s1">，是一篇优化相关的工作。出发点是，我们除了去优化 training loss<span class="s1">，我们也可以优化 marginal likelihood<span class="s1">。这样有很多优势，首先，我们就不需要哪些基于 validation set <span class="s1">的 trick <span class="s1">了（比如 early stopping<span class="s1">），我们可以直接用 marginal likelihood estimator <span class="s1">去 evaluate performance<span class="s1">。</span></span></span></span></span></span></span></span></p> 
 <p class="p2"><span class="s1">那么如何实现这件事呢，这篇工作给优化过程找了一些 Bayesian <span class="s1">的解释：优化过程中，每一步都会“<span class="s1">生成”<span class="s1">一个 distribution<span class="s1">。这样，整个优化过程中，就会产生一个 distribution sequence<span class="s1">。这个 sequence <span class="s1">从 Bayesian <span class="s1">的角度，可以看成是被某个 true posterior distribution <span class="s1">不断 sample <span class="s1">出来的，sample <span class="s1">的样本数 N<span class="s1">，也就是优化的迭代次数，就可以被看成是 variational parameter<span class="s1">。有了这样一个解释，作者进一步就把 early stopping <span class="s1">这个 trick <span class="s1">解释成了对 varitional lower bound <span class="s1">的优化；ensembling random initializations <span class="s1">就可以看成是 ensembling various independent variational samples.<br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p2"><span class="s1">上面所说的，就是这篇论文的第一个贡献<span class="s1">（也是论文的标题）。除此以外，本文利用这样的解释，进一步去构造了 marginal likelihood estimator，并用这个 estimator 去做了 training stop 选择，model selection capacity 选择和 model hypermeter 选择。<br></span></span></p> 
 <p class="p2"> </p> 
 <p class="p1"><span class="s1">之所以推荐这篇文章，并不是说它给出的这种优化方法就比以前 SGD <span class="s1">等等优化 training loss <span class="s1">的好；而是基于两个原因<span class="s1">：（1<span class="s1">）首先，它里面提到了非常多对于优化的思考。比如 training loss <span class="s1">和 marginal likelihood <span class="s1">两个“<span class="s1">指标”<span class="s1">，到底应该更“<span class="s1">相信”<span class="s1">哪个？varational lower bound <span class="s1">这个东西越高，是否真的代表 model <span class="s1">的 accuracy <span class="s1">越准？它和 validation error/test error <span class="s1">指标相反的时候该怎么理解？这些是很有趣的。（2<span class="s1">）对于优化过程中 distribution sequence <span class="s1">的解释我个人觉得很有用，现在 variational sequence learning <span class="s1">的工作也越来越多，但是被优化方法局限。这个工作也是一个启发。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p1"> </p> 
 <p class="p2"><span class="s2"> <img src="https://images2.imgbox.com/8c/fc/aMspo7yc_o.png" alt=""> Dropout as a Bayesian approximation: Representing model uncertainty in deep learning<img src="https://images2.imgbox.com/b5/3e/7toPbi58_o.png" alt=""></span></p> 
 <p class="p1"><span class="s1">这篇论文<strong>从 Bayesian 角度，解释了 why dropout works</strong><span class="s1">。虽然在2013<span class="s1">年，也有人试图解释过 dropout<span class="s1">，但当时是从 sparse regularization <span class="s1">的角度解释的，有一定局限性。这篇工作更 general<span class="s1">，更 provoking<span class="s1">。</span></span></span></span></span></span></span></p> 
 <p class="p2"><span class="s1">首先作者论证了 dropout 在理论上，是等价于一种 Gaussian Process 的 Bayesian approximation 的。这个证明过程很简单，大家可以去看一下。个人感觉，这个解释其实和dropout as noise regularization 很相似，毕竟 approximation 也在引入 noise<span class="s1">。只不过它们这种解释更数学化。 <br></span></span></p> 
 <p class="p2"><span class="s1">随后，有了这样一种解释，就可以从使用了 dropout 的 NN 中，得到 model uncertainty。这个 uncertainty 其实才是作者的 motivation<span class="s1">（当然也是 Bayesian 学派的 motivation）。比如现在的 NN，有一个 softmax layer 去 output 出一个 prediction，比如就是预测一个 label 吧，这个 output 只是对这个 label 的 propability，但是并不包含它对于自己这个 prediction 的 certainty 程度。设想一种情况，我们一直用 dog 的图像去 train 一个网络，最后让这个网络预测的全是 cat 的图片；最后很可能它预测的 probability 有些比较高，但其实 uncertainty 应该更高。以前的框架下，都无法很好地涵盖这种 uncertainty as output。现在，有了 dropout as approximation 的解释，就可以通过 moment-matching 的技术，从 NN 中得到这种 uncertainty 了。</span></span></p> 
 <p class="p1"><span class="s1">得到这种 uncertainty <span class="s1">后，可以把它用于 regression, classification <span class="s1">甚至是 reinforcement learning <span class="s1">的各种任务上。从实验结果来看，增加了 uncertainty <span class="s1">之后，各种 task <span class="s1">都有提升。</span></span></span></span></span></span></p> 
 <p class="p2">另外可以想到，这样的<span class="s2"> Bayesian 解释，有助于提高<span class="s2"> model interpretation 的能力，也算是一个非常大的<span class="s2"> motivation。最后，如果觉得论文读起来比较枯燥，可以去作者主页找他相关的<span class="s2"> slides，看起来非常生动。主页上还有他自己写的一篇<span class="s2"> blog，详细地展示了他的<span class="s2"> motivation。这次 DL Symposium 中的另外两篇，《Stochastic backpropagation and approximate inference in deep generative models》和《Scalable Bayesian optimization using deep neural networks》也和这个工作非常相似，就不单独介绍了。</span></span></span></span></span></span></p> 
 <p class="p1"> </p> 
 <p class="p2"> </p> 
 <p class="p1"> </p> 
 <p>至此，NIPS 2015 Deep Learning Symposium 就总结到这里。下次暂时准备总结 RAM workshop 和 main conference 中的一些 ML-related oral paper。大家有什么推荐的文章可以在本文下方【写评论】给我。</p> 
 <p> </p> 
 <p>相关网站：http://mp.weixin.qq.com/s?__biz=MzAwMjM3MTc5OA==&amp;mid=401673360&amp;idx=1&amp;sn=a39b7d1aa90422bab9d46bb523065967&amp;scene=20#rd</p> 
</div> 
<p>转载于:https://www.cnblogs.com/Yiutto/articles/5048260.html</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cfc7eecbb6bc9a185d990f623ffe240b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">人工神经网络简介</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/eefde60b3929cbc102520f1488cc4663/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">最近邻搜索之乘积量化（Product Quantizer）理解（一）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>