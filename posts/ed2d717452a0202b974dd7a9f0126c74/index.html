<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hive 实现脱敏以及ETL 过程（开启kerberos） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Hive 实现脱敏以及ETL 过程（开启kerberos）" />
<meta property="og:description" content="一. 业务场景及实现原理： 源数据存储在Hbase中，需要将源数据中敏感部分（如身份证、电话号码等）进行脱敏再供用户使用。只需要脱敏少量数据供页面呈现便可（这个呈现主要是给用户看样例数据，便于用户确定是否需要订阅资料）。为了防止请求被非法模仿，因而编写了一个访问Ip 鉴权类，也就是设置了访问ip白名单，只有在白名单上的ip才可以访问接口。具体实现见如下链接：
http://blog.csdn.net/u013850277/article/details/77900765 注：运用该方法脱敏后的数据将存在Hive中而不是Hbase中。 实现原理：
通过 自定义Hive 函数，然后运用自定义函数结合Hive sql 对Hive 数据进行脱敏并入到脱敏库（这里的脱敏库与贴源库保存在同一个集群上）
本人为了方便部署与对外提供接口采用的是Spring boot 方式来部署
二. 执行过程简述 创建Hive 自定义函数
原先脱敏备份的表若存在则将其删除掉
如果存在该脱敏表则重命名备份
执行ETL及脱敏过程
创建表及脱敏成功则将备份表删除掉
测试：一百万条数据完成上述步骤用时1分钟之内；集群机器节点数量为3，内存分别为16G
三. 功能部署成功后的访问接口 请求URL： http://localhost:8003/hive_data_sensitive?tableName=sensitive.t_person_sen&amp;sourceDataTable=admin.t_person1&amp;param=name_0,address_2,sex_1,birthday_3,idcards_4 注：如果源数据表有字段不想出现在脱敏后的表中，则可以不拼接到请求参数中。
请求方式： ​ GET
请求参数：
| 参数名 | 必选 | 说明 |
| --------------- | :–: | :--------------------------------------: |
| tableName | 是 | 脱敏后的表名，eg:sensitive.t_person_sen 其中 sensitive脱敏库，t_person_sen为脱敏后的表名 |
| sourceDataTable | 是 | 源数据，eg: admin.t_person1,admin为脱敏源库，t_person1为源数据 |
| param | 是 | 脱敏参数，字段及其脱敏规则，多个用英文符逗号分割，eg：name_0,address_2,sex_1,birthday_3,idcards_4 | 脱敏规则说明" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/ed2d717452a0202b974dd7a9f0126c74/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-08-18T00:01:12+08:00" />
<meta property="article:modified_time" content="2017-08-18T00:01:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hive 实现脱敏以及ETL 过程（开启kerberos）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="__1"></a>一. 业务场景及实现原理：</h3> 
<ol><li>源数据存储在Hbase中，需要将源数据中敏感部分（如身份证、电话号码等）进行脱敏再供用户使用。</li><li>只需要脱敏少量数据供页面呈现便可（这个呈现主要是给用户看样例数据，便于用户确定是否需要订阅资料）。</li><li>为了防止请求被非法模仿，因而编写了一个访问Ip 鉴权类，也就是设置了访问ip白名单，只有在白名单上的ip才可以访问接口。具体实现见如下链接：<br> <a href="http://blog.csdn.net/u013850277/article/details/77900765">http://blog.csdn.net/u013850277/article/details/77900765</a></li></ol> 
<ul><li>注：运用该方法脱敏后的数据将存在Hive中而不是Hbase中。</li></ul> 
<p><strong>实现原理：</strong></p> 
<ul><li> <p>通过 自定义Hive 函数，然后运用自定义函数结合Hive sql 对Hive 数据进行脱敏并入到脱敏库（这里的脱敏库与贴源库保存在同一个集群上）</p> </li><li> <p>本人为了方便部署与对外提供接口采用的是Spring boot 方式来部署</p> </li></ul> 
<h3><a id="__13"></a>二. 执行过程简述</h3> 
<ol><li> <p>创建Hive 自定义函数</p> </li><li> <p>原先脱敏备份的表若存在则将其删除掉</p> </li><li> <p>如果存在该脱敏表则重命名备份</p> </li><li> <p>执行ETL及脱敏过程</p> </li><li> <p>创建表及脱敏成功则将备份表删除掉</p> </li></ol> 
<p>测试：一百万条数据完成上述步骤用时1分钟之内；集群机器节点数量为3，内存分别为16G</p> 
<h3><a id="__27"></a>三. 功能部署成功后的访问接口</h3> 
<ul><li>请求URL：</li></ul> 
<pre><code>http://localhost:8003/hive_data_sensitive?tableName=sensitive.t_person_sen&amp;sourceDataTable=admin.t_person1&amp;param=name_0,address_2,sex_1,birthday_3,idcards_4
</code></pre> 
<p>注：如果源数据表有字段不想出现在脱敏后的表中，则可以不拼接到请求参数中。</p> 
<ul><li>请求方式：</li></ul> 
<p>​ GET</p> 
<ul><li>请求参数：<br> | 参数名 | 必选 | 说明 |<br> | --------------- | :–: | :--------------------------------------: |<br> | tableName | 是 | 脱敏后的表名，eg:sensitive.t_person_sen 其中 sensitive脱敏库，t_person_sen为脱敏后的表名 |<br> | sourceDataTable | 是 | 源数据，eg: admin.t_person1,admin为脱敏源库，t_person1为源数据 |<br> | param | 是 | 脱敏参数，字段及其脱敏规则，多个用英文符逗号分割，eg：name_0,address_2,sex_1,birthday_3,idcards_4 |</li></ul> 
<p>脱敏规则说明</p> 
<table><thead><tr><th>参数</th><th align="center">规则</th><th align="center">说明</th></tr></thead><tbody><tr><td>0</td><td align="center">不做处理</td><td align="center">原样输出</td></tr><tr><td>1</td><td align="center">替换算法</td><td align="center">默认将后4位替换成*号，如果少于4位大于1位将只保留前一位后几位用星号代替，对性别进行特殊处理，男替换成"M",女替换成"F"，电话号码将中间五位替换成星号，eg: 134*****2152。</td></tr><tr><td>2</td><td align="center">截取</td><td align="center">默认将后4位载掉，少于4位大于1位则只保留一位，eg:上海普陀区外海110号,脱敏后是上海普陀区外海</td></tr><tr><td>3</td><td align="center">加密</td><td align="center">目前只简单实现MD5加密，eg:1983/7/15 脱敏后：70d7d532bd97bb0e5c7edbf128257f07</td></tr><tr><td>4</td><td align="center">身份证脱敏</td><td align="center">将后6位通过短网址替换，短网址替换后同样保证其唯一性，eg: 530502197207131215 脱敏后：5305021972079356h7</td></tr></tbody></table> 
<ul><li> <p>返回参数</p> 
  <table><thead><tr><th>参数名</th><th align="center">说明</th></tr></thead><tbody><tr><td>布尔类型</td><td align="center">true : 成功; false：失败</td></tr></tbody></table></li><li> <p>示例</p> <p>请求：</p> </li></ul> 
<pre><code>http://localhost:8003/hive_data_sensitive?tableName=sensitive.t_person_sen&amp;sourceDataTable=admin.t_person1&amp;param=name_0,address_2,sex_1,birthday_3,idcards_4
</code></pre> 
<p>返回：true</p> 
<ul><li> <p>源表数据如下所示：<br> <img src="https://images2.imgbox.com/ec/fc/e0hUXwxz_o.png" alt="这里写图片描述"></p> </li><li> <p>脱敏后结果如下所示：<br> <img src="https://images2.imgbox.com/0b/76/ahcjJMPZ_o.png" alt="这里写图片描述"></p> </li><li> <p>示例2：源表birthday字段不进行ETL到脱敏表<br> 请求：</p> </li></ul> 
<pre><code>http://localhost:8003/hive_data_sensitive?tableName=sensitive.t_person_sen&amp;sourceDataTable=admin.t_person1&amp;param=name_0,address_2,sex_1,idcards_4
</code></pre> 
<p>返回：true</p> 
<p>脱敏后结果如下所示：<br> <img src="https://images2.imgbox.com/bb/b4/pmrLXfmk_o.png" alt="这里写图片描述"></p> 
<h3><a id="__89"></a>四. 功能实现</h3> 
<p>1、 Main (Spring boot 启动类)</p> 
<pre><code>package com.bms;

import org.jboss.logging.Logger;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer;
import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

import com.bms.service.HiveDataEtl;

/**
 * @author YeChunBo
 * @time 2017年8月2日
 *
 *       类说明 ：项目启动类，启动该项目只需启动该类便可
 */

@SpringBootApplication
@RestController
public class Main implements EmbeddedServletContainerCustomizer{

	private static Logger log = Logger.getLogger(HiveDataEtl.class);
	@RequestMapping("/")
	public String getHello() {
        log.info("Hello Spring Boot .....启动项目成功");
		return "Hello Spring Boot .....启动项目成功";
	}

	// 修改访问的默认端口
	public void customize(ConfigurableEmbeddedServletContainer configurableEmbeddedServletContainer) {
		configurableEmbeddedServletContainer.setPort(Integer.parseInt("8003"));
	}

	public static void main(String[] args) {
		SpringApplication.run(Main.class, args);
	}
}

</code></pre> 
<p>2、HiveDataEtlController(Spring boot 对应的controller类)</p> 
<pre><code>package com.bms.controller;

import java.util.ArrayList;

import org.jboss.logging.Logger;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.ResponseBody;
import org.springframework.web.bind.annotation.RestController;

import com.bms.service.HiveDataEtl;

/**
 * @author YeChunBo
 * @time 2017年8月15日
 *
 *       类说明 Hive 数据库脱敏接口控制类
 */

@SpringBootApplication
@RestController
public class HiveDataEtlController {

	private static Logger log = Logger.getLogger(HiveDataEtlController.class);
	HiveDataEtl hiveDataEtl = new HiveDataEtl();

	/**
	 * 请求样例：http://localhost:8003/hive_data_sensitive?tableName=sensitive.t_person_sen&amp;sourceDataTable=admin.t_person1&amp;param=name_0,address_2,sex_1,birthday_3,idcards_4
	 * @param tableName
	 * @param sourceDataTable
	 * @param param : 字段及其要进行的脱敏规则：eg:name_1,address_2,birthday,idcards_4,其中0:不脱敏；1：替换算法，2：截取；3：加密；4：身份证脱敏
	 * @return
	 */
	@RequestMapping("/hive_data_sensitive")
	@ResponseBody
	Boolean hiveDataSensitive(@RequestParam(value = "tableName", required = true) String tableName,
			@RequestParam(value = "sourceDataTable", required = true) String sourceDataTable,
			@RequestParam(value = "param", required = true) String param) { // 带上脱敏规则的字段字符串，eg:param=name_1,address_2,birthday,idcards_4,其中1：替换算法，2：截取；3：加密；4：身份证脱敏

		 log.info("HiveDataSensitive req tableName=" + tableName + ",sourceDataTable=" + sourceDataTable + ",param=" + param);
          
		return HiveDataEtl.hiveDataSen2OtherTable(tableName, sourceDataTable, param);
	}
	
	/**
	 * 查看脱敏后数据的样例数据,带上库的表名，eg:dbName.tableName,http://localhost:8003/query_table?tableName=sensitive.t_person_sen
	 * @param tableName
	 * @return
	 */
	@RequestMapping("/query_table")
	@ResponseBody
	ArrayList&lt;String&gt; queryTable(@RequestParam(value = "tableName", required = true) String tableName) { 
		log.info("Query table req tableName=" + tableName);
		return HiveDataEtl.queryTable(tableName);
	}

}

</code></pre> 
<p>3、Hive 实现脱敏过程的业务类</p> 
<pre><code>package com.bms.service;

import org.apache.commons.lang3.StringUtils;

/**
* @author YeChunBo
* @time 2017年8月11日 
*
* 类说明 
*/

import org.apache.hadoop.security.UserGroupInformation;
import org.jboss.logging.Logger;

import java.io.IOException;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.ResultSetMetaData;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.ArrayList;

public class HiveDataEtl {
	/**
	 * 用于连接Hive所需的一些参数设置 driverName:用于连接hive的JDBC驱动名 When connecting to
	 * HiveServer2 with Kerberos authentication, the URL format is:
	 * jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;db&gt;;principal= &lt;Server_Principal_of_HiveServer2&gt;
	 */
	private static String driverName = "org.apache.hive.jdbc.HiveDriver";
	// 注意：这里的principal是固定不变的，其指的hive服务所对应的principal
	private static String url = "jdbc:hive2://bigdata40:10000/admin;principal=hive/bigdata40@KK.COM";
	private static Logger log = Logger.getLogger(HiveDataEtl.class);

	// 其中 0:不处理，1：替换算法，2：截取；3：加密；4：身份证脱敏
	public static final String NO_SEN = "0";
	public static final String REPLACE_SEN = "1";
	public static final String CUT_SEN = "2";
	public static final String MD5_SEN = "3";
	public static final String IDCARDS_SEN = "4";

	public static Connection get_conn() throws SQLException, ClassNotFoundException {
		org.apache.hadoop.conf.Configuration conf = new org.apache.hadoop.conf.Configuration();
		conf.set("hadoop.security.authentication", "Kerberos");
		if (System.getProperty("os.name").toLowerCase().startsWith("win")) {
			System.setProperty("java.security.krb5.conf", "C:/Windows/krbconf/krb5.ini");
		}
		try {
			UserGroupInformation.setConfiguration(conf);
			UserGroupInformation.loginUserFromKeytab("hive/bigdata39@KK.COM", "./conf/hive.service.keytab"); //
		} catch (IOException e1) {
			e1.printStackTrace();
		}
		Class.forName(driverName);
		Connection conn = DriverManager.getConnection(url);
		return conn;
	}

	/**
	 * Hive数据脱敏到另一张表
	 * 
	 * @param statement
	 * @param tableName
	 * @param sourceDataTable
	 * @return
	 */
	public static boolean hiveDataSen2OtherTable(String tableName, String sourceDataTable, String param) {

		String sqlAdd = "ADD jar /usr/bigdata/2.5.3.0-37/hive/auxlib/HiveEtl-0.0.1-SNAPSHOT.jar"; // Server_Principal_of_HiveServer2 对应的位置
		String cutSql = "CREATE TEMPORARY FUNCTION cut_sensitive as 'com.bms.udf.CutOfSensitive'";
		String md5Sql = "CREATE TEMPORARY FUNCTION md5_sensitive as 'com.bms.udf.EncAlgorithmSensitive'";
		String replaceSql = "CREATE TEMPORARY FUNCTION replace_sensitive as 'com.bms.udf.ReplaceOfSensitive'";
		String idCardsSql = "CREATE TEMPORARY FUNCTION idcards_sensitive as 'com.bms.udf.IDCardsOfSensitive'";
		
		String dropBakTableSql = "DROP TABLE  IF EXISTS " + tableName + "_bak "; // 先删除原先的备份数据
		String tableBakSql = "ALTER TABLE " + tableName + " RENAME TO " + tableName + "_bak";// 将要修改的表进行备份
		
		String paramSql = tranReqparamToSql(param); // 将请求参数转换成sql语句
		String createTableSql = "CREATE TABLE " + tableName
				+ " AS select " + paramSql + " from " + sourceDataTable;  // 进行ETL 过程的sql 语句
		
		Connection conn = null;
		Statement statement = null;
		try {
			conn = get_conn();
			statement = conn.createStatement();

		    // 将jar 包加入到HIVE类路径下  
			statement.execute(sqlAdd);     
			log.info(sqlAdd + " is successed....");
			
			// 将自定义的函数类加入到classpath下
			statement.execute(cutSql);
			log.info(cutSql + " is successed....");

			statement.execute(md5Sql);
			log.info(md5Sql + " is successed....");

			statement.execute(replaceSql);
			log.info(replaceSql + " is successed....");

			statement.execute(idCardsSql);
			log.info(idCardsSql + " is successed....");

			statement.execute(dropBakTableSql);  // 原先脱敏备份的表若存在则将其删除掉
			log.info(dropBakTableSql + " is successed....");
		
			if (isExistTable(statement, tableName.substring(tableName.indexOf(".") + 1 ), tableName.substring(0, tableName.indexOf(".")))) {
				statement.execute(tableBakSql);// 如果存在该脱敏表则重命名备份
				log.info(tableBakSql + " is successed....");
			}

			statement.execute(createTableSql); // 执行ETL及脱敏过程
			log.info(createTableSql + " is successed....");
			
			statement.execute(dropBakTableSql); // 创建表及脱敏成功则将备份表删除掉
			log.info(dropBakTableSql + " is successed....");
			return true;
		} catch (Exception e) {

			log.error("hiveDataSen2OtherTable is failed , the errorMessage is " + e.getMessage());
			e.printStackTrace();
			return false;
		}  finally {
			if(statement != null) {
				try {
					statement.close();
				} catch (SQLException e) {
					log.error("close statement is failed , the errorMessage is " + e.getMessage());
					e.printStackTrace();
				}
			}
			if (conn != null) {
				try {
					conn.close();
				} catch (SQLException e1) {
					log.error("close conn is failed , the errorMessage is " + e1.getMessage());
					e1.printStackTrace();
				}
			}
		}
	}

	/**
	 * 将请求参数（字段及其要进行的脱敏规则）
	 * @param param 字段与脱敏参数：eg:name_0,address_2,sex=1,birthday=3,idcards_4
	 * @return 	转换成sql, eg:name,cut_sensitive(address) as address,replace_sensitive(sex) as sex,md5_sensitive(birthday) as birthday,idcards_sensitive(idcards) as idcards
	 */
	public static String tranReqparamToSql(String param) {

		StringBuffer paramSql = new StringBuffer();
		String col = null;
		String sensiType = null;
		if (StringUtils.isNotBlank(param)) {
			String[] colArr = param.split(",");
			for (int i = 0; i &lt; colArr.length; i++) {
				col = colArr[i].substring(0, colArr[i].indexOf("_"));
				sensiType = colArr[i].substring(colArr[i].indexOf("_") + 1);
				if (NO_SEN.equals(sensiType))
					paramSql.append(col).append(",");
				if (REPLACE_SEN.equals(sensiType))
					paramSql.append(" replace_sensitive" + "(" + col + ") ").append(" AS " + col).append(",");
				if (CUT_SEN.equals(sensiType))
					paramSql.append(" cut_sensitive" + "(" + col + ") ").append(" AS " + col).append(",");
				if (MD5_SEN.equals(sensiType))
					paramSql.append(" md5_sensitive" + "(" + col + ") ").append(" AS " + col).append(",");
				if (IDCARDS_SEN.equals(sensiType))
					paramSql.append(" idcards_sensitive" + "(" + col + ") ").append(" AS " + col).append(",");
			}
			return paramSql.toString().substring(0, paramSql.toString().lastIndexOf(","));
		} else
			return param;
	}
	
	public static ArrayList&lt;String&gt; queryTable(String tableName) {
		String querySql = "Select * from " + tableName + " limit 20 ";
		log.info("Query sql is " + querySql);
		Connection conn;
		ArrayList&lt;String&gt; arrayList = new ArrayList&lt;String&gt;();

		try {
			conn = get_conn();
			
			Statement statement = conn.createStatement();
			ResultSet rs = statement.executeQuery(querySql);

			ResultSetMetaData m = rs.getMetaData();

			int columns = m.getColumnCount();
			// 显示列,表格的表头
			for (int i = 1; i &lt;= columns; i++) {
				arrayList.add(m.getColumnName(i));
			}

			System.out.println();
			// 显示表格内容
			while (rs.next()) {
				for (int i = 1; i &lt;= columns; i++) {
					arrayList.add(rs.getString(i));
				}
			}

		} catch (Exception e) {
			e.printStackTrace();
		}

		return arrayList;
	}
	
	/**
	 * 判断表是否存在对应的库中
	 * @param statement
	 * @param tableName
	 * @param dbName
	 * @return
	 */
	public static boolean isExistTable(Statement statement, String tableName, String dbName) {

		ResultSet rs;
		try {
			statement.execute(" use " + dbName);
			rs = statement.executeQuery(" show tables ");
			while(rs.next()) {
				if (tableName.equalsIgnoreCase(rs.getString(1))) // 不区分大小写，因为Hive查出来的表名就没有区分大小写
					return true;
			}
		} catch (SQLException e) {
			e.printStackTrace();
		}
		return false;
	}
	

	public static void main(String[] args) throws Exception {
//		Connection conn = get_conn();
//		Statement statement = conn.createStatement();
//		System.out.println(isExistTable(statement, "t_person1_sen", "sensitive"));
		// hiveDataSen2OtherTable("sensitive.t_person1_sen","t_person1", "");
		// tranReqparamToSql("name_0,address_2,sex_1,birthday_3,idcards_4");
		/*
		 * // 创建的表名 String tableName = "sensitive.t_person_100_sen"; String
		 * sourceDataTable = "t_person_100"; createTable( tableName,
		 * sourceDataTable); // 将t_person_100脱敏后存入default库的t_person_100_sen表
		 * System.out.println("...........End........");
		 */
	}

}

</code></pre> 
<p>4、Hive UDF 定义类（身份证处理算法）</p> 
<pre><code>package com.bms.udf;

import java.util.ArrayList;
import java.util.List;
import java.util.Random;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;

import com.bms.utils.TokenUtils;

/**
 * @author YeChunBo
 * @time 2017年8月11日
 * 将身份证后6位使用短网址的方式进行脱敏生成
 * 类说明 :脱敏替换的UDF
 */

public class IDCardsOfSensitive extends UDF {

	public String evaluate(String str) {
		return idcarts(str);
	}

	/** 
     * 身份证后6位用短网址的token代替,保证了它的唯一性
     *  
     * @param idcards
     * @return 
     */  
    public static String idcarts(String idcards) {  
    	if (StringUtils.isBlank(idcards)) {  
    		return idcards;  
    	}  
    	// 用正则判断传进来的是否为身份号再处理
		boolean isIdcardsFlag = idcards.matches(
				"(^[1-9]\\d{5}(18|19|([23]\\d))\\d{2}((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\\d{3}[0-9Xx]$)|(^[1-9]\\d{5}\\d{2}((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\\d{2}[0-9Xx]$)");
    	if(isIdcardsFlag) {
          // return idcards.replace(idcards.substring(idcards.length() - 6), genCodes(6,1).get(0));
    	    return idcards.replace(idcards.substring(idcards.length() - 6), TokenUtils.getShortUrlToken(idcards.substring(idcards.length() - 6)));
    	}
    	else
    		return idcards;
    }

	/**
	 * 随机生成6位字符串
	 * 
	 * @param length
	 * @param num
	 *            生成字符串的组数
	 * @return
	 */
	public static List&lt;String&gt; genCodes(int length, long num) {

		List&lt;String&gt; results = new ArrayList&lt;String&gt;();

		for (int j = 0; j &lt; num; j++) {
			String val = "";

			Random random = new Random();
			for (int i = 0; i &lt; length; i++) {
				String charOrNum = random.nextInt(2) % 2 == 0 ? "char" : "num"; // 输出字母还是数字

				if ("char".equalsIgnoreCase(charOrNum)) // 字符串
				{
					int choice = random.nextInt(2) % 2 == 0 ? 65 : 97; // 取得大写字母还是小写字母
					val += (char) (choice + random.nextInt(26));
				} else if ("num".equalsIgnoreCase(charOrNum)) // 数字
				{
					val += String.valueOf(random.nextInt(10));
				}
			}
			val = val.toLowerCase();
			if (results.contains(val)) {
				continue;
			} else {
				results.add(val);
			}
		}
		return results;
	}

	 public static void main(String[] args) {
	    System.out.println(idcarts("130503670401001"));//15位：13为河北，05为邢台，03为桥西区，出生日期为1967年4月1日，顺序号为001
	    System.out.println(idcarts("340100196011010134"));//18位
	 }
}

</code></pre> 
<p>5、Hive UDF 简单替换算法</p> 
<pre><code>package com.bms.udf;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;

/**
 * @author YeChunBo
 * @time 2017年8月11日
 *
 *       类说明 :替换的 hive 自定义函数
 */

public class ReplaceOfSensitive extends UDF {

	public String evaluate(String str) {
		return replace(str, 4);
	}

	/**
	 * 替换算法 如果字段值为性别，男替换成M,女替换成F,电话号码将中间5位替换成*， 地址将后四位替换成*
	 * &lt;例子：北京市海淀区****&gt;
	 * 
	 * @param address
	 * @param sensitiveSize
	 *            敏感信息长度
	 * @return
	 */
	public static String replace(String inputStr, int sensitiveSize) {
		int length = StringUtils.length(inputStr);
		if (StringUtils.isBlank(inputStr))
			return inputStr;
		else if(inputStr.matches("^((13[0-9])|(14[5|7])|(15([0-3]|[5-9]))|(18[0,5-9]))\\d{8}$")) // 手机号特殊处理，eg:处理后138******1234
			return StringUtils.left(inputStr, 3).concat(StringUtils.removeStart(StringUtils.leftPad(StringUtils.right(inputStr, 4), StringUtils.length(inputStr), "*"), "***"));
		else if ("男".equals(inputStr))
			return "M";
		else if ("女".equals(inputStr))
			return "F";
		else if (StringUtils.isBlank(inputStr) || length == 1)  // 如果为空或只有一位则不做任何处理
			return inputStr;
		else if (length == 2)
			return StringUtils.rightPad(StringUtils.left(inputStr, length - 1), length, "*");
		else if (length == 3)
			return StringUtils.rightPad(StringUtils.left(inputStr, length - 2), length, "*");
		else if (length == 4)
			return StringUtils.rightPad(StringUtils.left(inputStr, length - 3), length, "*");
		else {
			return StringUtils.rightPad(StringUtils.left(inputStr, length - sensitiveSize), length, "*");
		}
	}

	public static void main(String[] args) {
		System.out.println(replace("男", 5));
		System.out.println(replace("女", 5));
		System.out.println(replace("叶问", 5));
		System.out.println(replace("迪力热巴", 5));
		System.out.println(replace("上海普陀区外海110号", 4));
		System.out.println(replace("13428282152", 4));
	}
}
// M
// F
// 叶*
// 迪***
//上海普陀区外海****
// 134****2152
</code></pre> 
<p>5、 短网址实现算法</p> 
<pre><code>// 字符表
	private static String[] chars = new String[] { "a", "b", "c", "d", "e",
			"f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r",
			"s", "t", "u", "v", "w", "x", "y", "z", "0", "1", "2", "3", "4",
			"5", "6", "7", "8", "9", "A", "B", "C", "D", "E", "F", "G", "H",
			"I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U",
			"V", "W", "X", "Y", "Z" };
/**
	
	 * 生成短网址的token
	 * @param data 数据
	 * @return 6位长度的token字符串
	 */
	public static String getShortUrlToken(String data) {
		if (data == null)
			return null;
		
		// 将数据进行md5加密生成32位签名串
		String md5Hex = new String(DigestUtils.md5Hex(data));
		
		// 将32位签名串，分为4段，每段8位，进行求和
		int hexLen = md5Hex.length();
		int subHexLen = hexLen / 8;
		long subHexSum = 0;
		for (int i = 0; i &lt; subHexLen; i++) {
			int j = i + 1;
			String subHex = md5Hex.substring(i * 8, j * 8);
			subHexSum += Long.valueOf(subHex, 16);
		}
		
		// 生成token
		long idx = Long.valueOf("3FFFFFFF", 16) &amp; subHexSum;
		StringBuffer token = new StringBuffer();
		for (int k = 0; k &lt; 6; k++) {
			int index = (int) (Long.valueOf("0000003D", 16) &amp; idx);
			token.append(chars[index]);
			idx = idx &gt;&gt; 5;
		}
		
		return token.toString();
	}
</code></pre> 
<p>6、pom.xml</p> 
<pre><code>&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
	&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
	&lt;groupId&gt;HiveEtl&lt;/groupId&gt;
	&lt;artifactId&gt;HiveEtl&lt;/artifactId&gt;
	&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;

	&lt;parent&gt;
		&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
		&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
		&lt;version&gt;1.5.4.RELEASE&lt;/version&gt;
	&lt;/parent&gt;

	&lt;dependencies&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
			&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;

			&lt;exclusions&gt;
				&lt;exclusion&gt;
					&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
					&lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;
				&lt;/exclusion&gt;
			&lt;/exclusions&gt;

		&lt;/dependency&gt;

		&lt;!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-log4j --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
			&lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt;
			&lt;version&gt;1.3.0.RELEASE&lt;/version&gt;
		&lt;/dependency&gt;

		&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
			&lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
			&lt;version&gt;1.2.1&lt;/version&gt;

			&lt;exclusions&gt;
				&lt;exclusion&gt;
					&lt;groupId&gt;org.eclipse.jetty.aggregate&lt;/groupId&gt;
					&lt;artifactId&gt;jetty-all&lt;/artifactId&gt;
				&lt;/exclusion&gt;
				&lt;exclusion&gt;
					&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
					&lt;artifactId&gt;hive-shims&lt;/artifactId&gt;
				&lt;/exclusion&gt;
			&lt;/exclusions&gt;
		&lt;/dependency&gt;
		&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
			&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
			&lt;version&gt;2.7.1&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
			&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;
			&lt;version&gt;1.2.1&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-metastore --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
			&lt;artifactId&gt;hive-metastore&lt;/artifactId&gt;
			&lt;version&gt;1.2.1&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-common --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
			&lt;artifactId&gt;hive-common&lt;/artifactId&gt;
			&lt;version&gt;1.2.1&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-service --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
			&lt;artifactId&gt;hive-service&lt;/artifactId&gt;
			&lt;version&gt;1.2.1&lt;/version&gt;

			&lt;exclusions&gt;
				&lt;exclusion&gt;
					&lt;groupId&gt;org.eclipse.jetty.aggregate&lt;/groupId&gt;
					&lt;artifactId&gt;jetty-all&lt;/artifactId&gt;
				&lt;/exclusion&gt;
				&lt;exclusion&gt;
					&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
					&lt;artifactId&gt;hive-shims&lt;/artifactId&gt;
				&lt;/exclusion&gt;
			&lt;/exclusions&gt;

		&lt;/dependency&gt;

		&lt;dependency&gt;
			&lt;groupId&gt;jdk.tools&lt;/groupId&gt;
			&lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;
			&lt;version&gt;1.8&lt;/version&gt;
			&lt;scope&gt;system&lt;/scope&gt;
			&lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt;
		&lt;/dependency&gt;

	&lt;/dependencies&gt;
	
	&lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;fork&gt;true&lt;/fork&gt;
                    &lt;mainClass&gt;${start-class}&lt;/mainClass&gt;
                &lt;/configuration&gt;
                  &lt;executions&gt;
                    &lt;execution&gt;
                      &lt;goals&gt;
                        &lt;goal&gt;repackage&lt;/goal&gt;
                      &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
              &lt;version&gt;2.2-beta-5&lt;/version&gt;
              &lt;configuration&gt;
                &lt;archive&gt;
                  &lt;manifest&gt;
                    &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                    &lt;mainClass&gt;com.bms.Main&lt;/mainClass&gt;
                  &lt;/manifest&gt;
                &lt;/archive&gt;
                &lt;descriptorRefs&gt;
                  &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                &lt;/descriptorRefs&gt;
              &lt;/configuration&gt;
              &lt;executions&gt;
                &lt;execution&gt;
                  &lt;id&gt;assemble-all&lt;/id&gt;
                  &lt;phase&gt;package&lt;/phase&gt;
                  &lt;goals&gt;
                    &lt;goal&gt;single&lt;/goal&gt;
                  &lt;/goals&gt;
                &lt;/execution&gt;
              &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre> 
<h3><a id="_linux__810"></a>五. linux 部署过程</h3> 
<p><strong>1. 部署Hive UDF 环境</strong><br> 用普通的maven 打包成 jar包（大小只有几十K），上传的Hive服务器上(目录是：/usr/bigdata/2.5.3.0-37/hive/auxlib), 这个 jar 的主要作用是 创建 Hive自定义函数用的。之所以打普通包，是因为普通包Hive加载起来快。</p> 
<ul><li> <p>注：/usr/bigdata/2.5.3.0-37/hive/auxlib 这个目录一般是不存在的，需要新建，主要作用是因为将 Hive包放到这个目录，hive会自动加载。</p> </li><li> <p>特别提醒：这个jar 包是要放到Hive的安装目录下。例如，公司测试环境Hive安装在bigdata40，则将其上传到bigdata40。</p> </li><li> <p>打成普通包时需要先将pom.xml文件中加载启动类的打包方式注释掉，也就下面这段代码</p> </li></ul> 
<pre><code>&lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;fork&gt;true&lt;/fork&gt;
                    &lt;mainClass&gt;${start-class}&lt;/mainClass&gt;
                &lt;/configuration&gt;
                  &lt;executions&gt;
                    &lt;execution&gt;
                      &lt;goals&gt;
                        &lt;goal&gt;repackage&lt;/goal&gt;
                      &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
              &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
              &lt;version&gt;2.2-beta-5&lt;/version&gt;
              &lt;configuration&gt;
                &lt;archive&gt;
                  &lt;manifest&gt;
                    &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                    &lt;mainClass&gt;com.bms.Main&lt;/mainClass&gt;
                  &lt;/manifest&gt;
                &lt;/archive&gt;
                &lt;descriptorRefs&gt;
                  &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                &lt;/descriptorRefs&gt;
              &lt;/configuration&gt;
              &lt;executions&gt;
                &lt;execution&gt;
                  &lt;id&gt;assemble-all&lt;/id&gt;
                  &lt;phase&gt;package&lt;/phase&gt;
                  &lt;goals&gt;
                    &lt;goal&gt;single&lt;/goal&gt;
                  &lt;/goals&gt;
                &lt;/execution&gt;
              &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
</code></pre> 
<p><strong>2. 部署HiveETL 项目</strong></p> 
<p>运用Maven 打包（注意将上一步注释掉的pom.xml文件加上，因为这个要生成的是运行Jar包，需要指定项目的运行Main方法）</p> 
<p>将打包后的jar包上传到部署服务器，编写启停脚本</p> 
<p>检查安装的服务器，是否有/etc/krb5.conf 文件存在，没有则到安装kerberos的服务复制一份</p> 
<p>在创建部署目录下创建 conf 目录并上传对应的keytab上去（注意：这个用户所对应的权限是必须拥有创建 Hive UDF权限的）</p> 
<p>启动脚本，查看日志输出</p> 
<p>其中脚本如下</p> 
<ul><li><a href="http://start.sh" rel="nofollow">start.sh</a></li></ul> 
<pre><code>
#!/bin/bash
PROJECTNAME=HiveEtl
pid=`ps -ef |grep $PROJECTNAME |grep -v "grep" |awk '{print $2}'`   
if [ $pid ]; then  
    echo "$PROJECTNAME  is  running  and pid=$pid"  
else  
   echo "Start success to start $PROJECTNAME ...."
   nohup java -jar HiveEtl-0.0.1-SNAPSHOT.jar  &gt;&gt; catalina.out  2&gt;&amp;1 &amp;
fi 
</code></pre> 
<ul><li><a href="http://stop.sh" rel="nofollow">stop.sh</a></li></ul> 
<pre><code>
#!/bin/bash

PROJECTNAME=HiveEtl
pid=`ps -ef |grep $PROJECTNAME |grep -v "grep" |awk '{print $2}' `   
if [ $pid ]; then  
    echo "$PROJECTNAME is  running  and pid=$pid"  
    kill -9 $pid  
    if [[ $? -eq 0 ]];then   
       echo "success to stop $PROJECTNAME "   
    else   
       echo "fail to stop $PROJECTNAME "  
     fi  
fi  
</code></pre> 
<h3><a id="__912"></a>五. 遇到的问题</h3> 
<ul><li>使用spring boot整合Hive，在启动Spring boot项目时，报出异常：<br> java.lang.NoSuchMethodError: org.eclipse.jetty.servlet.ServletMapping.setDefault(Z)V</li></ul> 
<pre><code>经过排查，是maven的包冲突引起的，具体做法，排除：jetty-all、hive-shims依赖包。对应的pom配置如下：
    &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
            &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
            &lt;version&gt;1.2.1&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.eclipse.jetty.aggregate&lt;/groupId&gt;
                    &lt;artifactId&gt;jetty-all&lt;/artifactId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
                    &lt;artifactId&gt;hive-shims&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a5913a51a2540b088d03b38ad74713b5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">c# Win Form程序调用Web API</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c427adc5474655984e40a986852ca7c9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">html中设置table的各个td的宽度</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>