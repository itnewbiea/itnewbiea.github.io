<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>23 DMA与零拷贝技术 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="23 DMA与零拷贝技术" />
<meta property="og:description" content="磁盘可以说是计算机系统中最慢的硬件之一，读写速度相差内存10倍以上，所以针对优化磁盘的技术非常的多，比如：零拷贝，直接I/O,异步I/O等等。
这里我们就以文件传输为切入点，分析I/O工作方式，以及如何优化文件传输的性能。
为什么要有DMA技术 DMA技术，全称为Direct Memory Access（又称直接内存访问技术）。
没有DMA技术前的I/O过程 可以看到整个数据传输的过程：
首先在用户进程进行read()系统调用的时候，操作系统会由用户态切换到内核态，然后由CPU向磁盘发起IO请求。磁盘在接收到IO请求以后，进行数据准备工作，将数据放在磁盘控制器缓冲区里。在数据准备工作完成以后，磁盘向CPU发出IO中断信号。CPU收到中断信号后，会先将磁盘缓冲区中的文件copy到PageCache中，再将数据从PageCache中copy到用户缓冲区中。在这期间CPU是无法执行其他任务的。copy完成之后read()调用返回，操作系统刚从内核态切换回用户态。 故可以得知：在数据传输的过程，需要CPU亲自的去拷贝数据，并且在这期间CPU无法去做其他事情。简单的搬运几个字符没有问题，但是当处理大量数据的时候，如果每次都让CPU来搬运，显然忙不过来。
有DMA技术之后的I/O过程 当有了DMA控制器以后，在进程向CPU发出read()调用以后，CPU向DMA控制器发起IO请求，然后DMA控制器再向磁盘发出IO请求。
当磁盘接收到IO请求以后，会进行数据准备工作，将数据放到磁盘数据缓冲区当中。
当磁盘的数据准备工作完成以后，不再向CPU发出中断信号，而是通知DMA控制器。
DMA控制器在接收到通知以后，将数据从磁盘控制器缓冲区中copy到内核缓冲区中。在这期间并不占用CPU，CPU可以处理其他的事情，执行其他的任务。
DMA控制器处理完之后向CPU发出中断信号，由CPU将数据从内核缓冲区copy至用户缓冲区中。copy完成之后read()调用返回，操作系统从内核态切换回用户态。
注意：起初的DMA控制器只在主板上，但是现在IO设备越来越多，数据传输的需求也不尽相同，所以现在每个IO设备中都有DMA控制器。
在进行read调用的时候，操作系统由用户态转为内核态，需要进行DMA拷贝和CPU拷贝各一次。（DMA拷贝是指由DMA控制器将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，CPU拷贝是指将数据从内核缓冲区中copy到用户缓冲区中），在read调用结束以后，操作系统再从内核态切换回用户态。
在进行write调用的时候，操作系统由用户态转为内核态，需要进行CPU拷贝和DMA拷贝各一次。（CPU拷贝是指将用户缓冲区的数据拷贝到socket缓冲区中。而DMA拷贝是指将socket缓冲区中的数据拷贝到网卡中。）
在文件传输场景下，我们无需在用户空间对数据进行再加工。因此用户缓冲区的存在是没有必要的。
如何实现零拷贝 实现零拷贝的技术主要有两种：mmap&#43;write，sendfile
下面就谈一谈，他们是如何减少上下文切换和数据拷贝的次数的。
mmap&#43;write 利用mmap替换read系统调用函数。
mmap系统调用函数会直接将内核缓冲区的数据映射到用户空间，这样操作系统内核与用户空间之间就不需要任何的拷贝操作。
具体流程如下：
应用进程调用了mmap系统调用函数之后，DMA会把磁盘缓冲区的数据拷贝到内核缓冲区中。接着，应用进程和操作系统内核共享这个缓冲区。
应用进程再调用write函数，操作系统直接将内核缓冲区的数据拷贝到socket缓冲区。再由DMA控制器copy到网卡。
这还不是最理想的零拷贝因为还是需要两次系统调用，四次上下文切换，3次拷贝。
sendfile 在linux内核版本2.1中，提供了一个新的系统调用函数sendfile。这个系统调用函数可以代替read和write两个系统调用函数。
这样系统调用的次数就变成了一次，上下文切换的次数减小到了两次，数据拷贝次数为3次。
这还不是真正的零拷贝技术：
如果网卡支持SG—DMA，就可以将数据拷贝次数减小为两次。
这就是所谓的零拷贝技术，我们没有在内存层面区拷贝数据，也就是说全过程都没有使用CPU搬运数据，所有数据都是通过DMA进行传输的。
整个过程只需要两次上下文的切换和两次数据拷贝。
kafka、rocketMQ、Nginx都使用到了零拷贝技术。
PageCache 零拷贝的内核缓冲区正是使用了PageCache技术。在零拷贝技术中，用PageCache来缓存最近被访问过的数据，当空间不足时淘汰最久未使用的缓存。
并且给予局部性原理，进行预读，减少IO次数。
但是在传输大文件的时候，性能损失非常大。因为在传输大文件的时候如果采用PageCache，PageCache容量有限，则PageCache由于长时间被大文件占据，其他热点的小文件可能就无法充分使用到。这样磁盘的读写性能就会降低。并且PageCache中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次，造成资源的浪费。
因此PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。
大文件传输采用什么实现 在最初的方案中：
调用read方法的时候，操作系统会由用户态切换为内核态。然后内核向磁盘发起IO请求，磁盘收到IO请求以后，会将数据放在磁盘控制器缓冲区中。磁盘完成该操作以后，向内核发起中断信号，然后内核将磁盘控制器缓冲区中的信号拷贝到PageCache中。然后内核再将PageCache中的数据拷贝到用户缓冲区中。完成以后，read调用返回。操作系统由内核态窃魂会用户态。 在read方法返回前，进程一直处于阻塞的状态。
针对阻塞的问题，可以采用异步IO来解决。
异步IO的流程图如下所示：
核心思想在于：在发起异步IO读以后，不等待数据就位就可以立即返回，然后去处理其他任务。直到收到内核返回的读取成功通知，才去处理数据。这样就解决了高并发场景下零拷贝技术大文件读取的阻塞问题。
因此，对于传输大文件，应该使用异步IO＋直接IO的方法。而对于小文件，则应该使用零拷贝。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/13b91ab761c4f1c0182b51d89c770ad5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-15T22:27:55+08:00" />
<meta property="article:modified_time" content="2023-04-15T22:27:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">23 DMA与零拷贝技术</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>磁盘可以说是计算机系统中最慢的硬件之一，读写速度相差内存10倍以上，所以针对优化磁盘的技术非常的多，比如：零拷贝，直接I/O,异步I/O等等。</p> 
<p>这里我们就以文件传输为切入点，分析I/O工作方式，以及如何优化文件传输的性能。</p> 
<h2><a id="DMA_4"></a>为什么要有DMA技术</h2> 
<p>DMA技术，全称为Direct Memory Access（又称直接内存访问技术）。</p> 
<h3><a id="DMAIO_8"></a>没有DMA技术前的I/O过程</h3> 
<p><img src="https://images2.imgbox.com/06/e9/BrdVDFNa_o.png" alt="image.png"></p> 
<p>可以看到整个数据传输的过程：</p> 
<ol><li>首先在用户进程进行read()系统调用的时候，操作系统会由用户态切换到内核态，然后由CPU向磁盘发起IO请求。</li><li>磁盘在接收到IO请求以后，进行数据准备工作，将数据放在磁盘控制器缓冲区里。</li><li>在数据准备工作完成以后，磁盘向CPU发出IO中断信号。</li><li>CPU收到中断信号后，会先将磁盘缓冲区中的文件copy到PageCache中，再将数据从PageCache中copy到用户缓冲区中。在这期间CPU是无法执行其他任务的。copy完成之后read()调用返回，操作系统刚从内核态切换回用户态。</li></ol> 
<p>故可以得知：在数据传输的过程，需要CPU亲自的去拷贝数据，并且在这期间CPU无法去做其他事情。简单的搬运几个字符没有问题，但是当处理大量数据的时候，如果每次都让CPU来搬运，显然忙不过来。</p> 
<h3><a id="DMAIO_21"></a>有DMA技术之后的I/O过程</h3> 
<p><img src="https://images2.imgbox.com/2a/19/7uaM3nuC_o.png" alt="image.png"></p> 
<ol><li> <p>当有了DMA控制器以后，在进程向CPU发出read()调用以后，CPU向DMA控制器发起IO请求，然后DMA控制器再向磁盘发出IO请求。</p> </li><li> <p>当磁盘接收到IO请求以后，会进行数据准备工作，将数据放到磁盘数据缓冲区当中。</p> </li><li> <p>当磁盘的数据准备工作完成以后，不再向CPU发出中断信号，而是通知DMA控制器。</p> </li><li> <p>DMA控制器在接收到通知以后，将数据从磁盘控制器缓冲区中copy到内核缓冲区中。在这期间并不占用CPU，CPU可以处理其他的事情，执行其他的任务。</p> </li><li> <p>DMA控制器处理完之后向CPU发出中断信号，由CPU将数据从内核缓冲区copy至用户缓冲区中。copy完成之后read()调用返回，操作系统从内核态切换回用户态。</p> <p>注意：起初的DMA控制器只在主板上，但是现在IO设备越来越多，数据传输的需求也不尽相同，所以现在每个IO设备中都有DMA控制器。</p> </li></ol> 
<p><img src="https://images2.imgbox.com/90/05/52YCXIGh_o.png" alt="image.png"></p> 
<p>在进行read调用的时候，操作系统由用户态转为内核态，需要进行DMA拷贝和CPU拷贝各一次。（DMA拷贝是指由DMA控制器将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，CPU拷贝是指将数据从内核缓冲区中copy到用户缓冲区中），在read调用结束以后，操作系统再从内核态切换回用户态。</p> 
<p>在进行write调用的时候，操作系统由用户态转为内核态，需要进行CPU拷贝和DMA拷贝各一次。（CPU拷贝是指将用户缓冲区的数据拷贝到socket缓冲区中。而DMA拷贝是指将socket缓冲区中的数据拷贝到网卡中。）</p> 
<p>在文件传输场景下，我们无需在用户空间对数据进行再加工。因此用户缓冲区的存在是没有必要的。</p> 
<h2><a id="_42"></a>如何实现零拷贝</h2> 
<p>实现零拷贝的技术主要有两种：mmap+write，sendfile</p> 
<p>下面就谈一谈，他们是如何减少上下文切换和数据拷贝的次数的。</p> 
<h3><a id="mmapwrite_48"></a>mmap+write</h3> 
<p>利用mmap替换read系统调用函数。</p> 
<p>mmap系统调用函数会直接将内核缓冲区的数据映射到用户空间，这样操作系统内核与用户空间之间就不需要任何的拷贝操作。</p> 
<p><img src="https://images2.imgbox.com/b2/13/gsEQ68za_o.png" alt="image.png"></p> 
<p>具体流程如下：<br> 应用进程调用了mmap系统调用函数之后，DMA会把磁盘缓冲区的数据拷贝到内核缓冲区中。接着，应用进程和操作系统内核共享这个缓冲区。<br> 应用进程再调用write函数，操作系统直接将内核缓冲区的数据拷贝到socket缓冲区。再由DMA控制器copy到网卡。</p> 
<p>这还不是最理想的零拷贝因为还是需要两次系统调用，四次上下文切换，3次拷贝。</p> 
<h3><a id="sendfile_63"></a>sendfile</h3> 
<p>在linux内核版本2.1中，提供了一个新的系统调用函数sendfile。这个系统调用函数可以代替read和write两个系统调用函数。</p> 
<p><img src="https://images2.imgbox.com/95/c5/P9aB8CQP_o.png" alt="image.png"></p> 
<p>这样系统调用的次数就变成了一次，上下文切换的次数减小到了两次，数据拷贝次数为3次。</p> 
<p>这还不是真正的零拷贝技术：<br> 如果网卡支持SG—DMA，就可以将数据拷贝次数减小为两次。</p> 
<p><img src="https://images2.imgbox.com/06/42/hwE9ii8t_o.png" alt="image.png"></p> 
<p>这就是所谓的零拷贝技术，我们没有在内存层面区拷贝数据，也就是说全过程都没有使用CPU搬运数据，所有数据都是通过DMA进行传输的。</p> 
<p>整个过程只需要两次上下文的切换和两次数据拷贝。</p> 
<p>kafka、rocketMQ、Nginx都使用到了零拷贝技术。</p> 
<h3><a id="PageCache_82"></a>PageCache</h3> 
<p>零拷贝的内核缓冲区正是使用了PageCache技术。在零拷贝技术中，用PageCache来缓存最近被访问过的数据，当空间不足时淘汰最久未使用的缓存。</p> 
<p>并且给予局部性原理，进行预读，减少IO次数。</p> 
<p>但是在传输大文件的时候，性能损失非常大。因为在传输大文件的时候如果采用PageCache，PageCache容量有限，则PageCache由于长时间被大文件占据，其他<strong>热点</strong>的<strong>小文件</strong>可能就无法充分使用到。这样磁盘的读写性能就会降低。并且PageCache中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次，造成资源的浪费。</p> 
<p>因此PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。</p> 
<h3><a id="_92"></a>大文件传输采用什么实现</h3> 
<p>在最初的方案中：</p> 
<ol><li>调用read方法的时候，操作系统会由用户态切换为内核态。</li><li>然后内核向磁盘发起IO请求，磁盘收到IO请求以后，会将数据放在磁盘控制器缓冲区中。</li><li>磁盘完成该操作以后，向内核发起中断信号，然后内核将磁盘控制器缓冲区中的信号拷贝到PageCache中。</li><li>然后内核再将PageCache中的数据拷贝到用户缓冲区中。</li><li>完成以后，read调用返回。操作系统由内核态窃魂会用户态。</li></ol> 
<p><img src="https://images2.imgbox.com/2b/6d/Lrlqel78_o.png" alt="image.png"></p> 
<p>在read方法返回前，进程一直处于阻塞的状态。</p> 
<p>针对阻塞的问题，可以采用异步IO来解决。</p> 
<p>异步IO的流程图如下所示：</p> 
<p><img src="https://images2.imgbox.com/e6/17/pluR3djM_o.png" alt="image.png"></p> 
<p>核心思想在于：在发起异步IO读以后，不等待数据就位就可以立即返回，然后去处理其他任务。直到收到内核返回的读取成功通知，才去处理数据。这样就解决了高并发场景下零拷贝技术大文件读取的阻塞问题。</p> 
<p>因此，对于传输大文件，应该使用异步IO＋直接IO的方法。而对于小文件，则应该使用零拷贝。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/913bef7914cec8076f0115845f48715e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">22 操作系统之内存管理</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/93919c1f421cc5090d4ce33946edefae/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">HBase架构篇 - Hadoop家族的天之骄子HBase</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>