<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习】Diffusion模型 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【深度学习】Diffusion模型" />
<meta property="og:description" content="Diffusion基础理论入门 方向：ddpm ==》 ddim 论文
公式推导：科学空间-苏剑林
Diffusion Models 视频：唐宇迪——Diffusion
1 t时刻，给图片1增加高斯分布噪声为图片2
2 使用噪声预测模型，输入为t时刻，输出为噪声分布，标签为步骤1中产生的高斯分布噪声
3 将时刻t和图片2作为预测模型(UNet)输入，t-1时刻的图片作为预测模型输出
4 推理时，使用随机(h,w,3)的高斯分布和时刻t(=2000)作为输入，输入为t-1时刻的图片；重复上述步骤2000次，即t-0时刻为生成图片
Latent Diffusion ==》Stable diffusion Latent Diffusion 论文：https://arxiv.org/abs/2112.10752
Stable diffusion由Stability AI 推出，基于Latent Diffusion制作，无论文。
Code: https://github.com/CompVis/latent-diffusion
原理 PPT1_Math_Reading_Group_Stable_Diffusion.pdf
视频讲解（建议看下面的视频讲解，文章中的图片来源于【2】Lightning AI）：
【1】试图做一个正常讲解Latent / Stable Diffusion的成年人
【2】Youtube_Lightning AI——Stable Diffusion Explained 总体训练流程
1 使用VAE模型的Encoder部分对图片进行编码，即生成特征图片1（其符合正太分布？）
VAE模型：下图源于VAE(Variational Autoencoder)简单推导及理解
2 在 t 时刻对特征图片1添加噪声
3 使用CLIP模型对条件信息进行处理，特征图片中信息作为q 文本或图片信息(condition)作为k和v
4 将特征图片2与条件信息再进行cross attention处理输入去噪UNet网络，重复本次操作 t-1次
5 将处理后的图片输入VAE模型的Decoder部分，生成原始图片
推理阶段
1 使用CLIP编码条件信息，并生成tN 高斯分布图片A
2 将未加条件信息和加了条件信息的图片输入 UNet网络，输出两张噪声编码图片B C" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/ffedfbad46abdf112e2dd4f4453e68b9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-23T19:02:07+08:00" />
<meta property="article:modified_time" content="2023-03-23T19:02:07+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习】Diffusion模型</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Diffusion_0"></a>Diffusion基础理论入门</h2> 
<p>方向：ddpm ==》 ddim 论文</p> 
<p>公式推导：<a href="https://kexue.fm/tag/DDPM/" rel="nofollow">科学空间-苏剑林</a></p> 
<h3><a id="Diffusion_Models_8"></a>Diffusion Models</h3> 
<p><strong>视频</strong>：<a href="https://www.bilibili.com/video/BV1ne411u7J6/?p=2&amp;spm_id_from=333.880.my_history.page.click&amp;vd_source=88716fc82b0d84ac76ab6dbc7bb756d5" rel="nofollow">唐宇迪——Diffusion</a><br> 1 t时刻，给图片1增加高斯分布噪声为图片2</p> 
<p>2 使用噪声预测模型，输入为t时刻，输出为噪声分布，标签为步骤1中产生的高斯分布噪声</p> 
<p>3 将时刻t和图片2作为预测模型(UNet)输入，t-1时刻的图片作为预测模型输出</p> 
<p>4 推理时，使用随机(h,w,3)的高斯分布和时刻t(=2000)作为输入，输入为t-1时刻的图片；重复上述步骤2000次，即t-0时刻为生成图片</p> 
<h3><a id="Latent_Diffusion_Stable_diffusion_20"></a>Latent Diffusion ==》Stable diffusion</h3> 
<p>Latent Diffusion 论文：https://arxiv.org/abs/2112.10752</p> 
<p>Stable diffusion由<a href="https://stability.ai/" rel="nofollow">Stability AI</a> 推出，基于Latent Diffusion制作，无论文。</p> 
<p>Code: https://github.com/CompVis/latent-diffusion</p> 
<h4><a id="_28"></a>原理</h4> 
<p><a href="diffusion.assets/1_Math_Reading_Group_Stable_Diffusion.pdf" rel="nofollow">PPT1_Math_Reading_Group_Stable_Diffusion.pdf</a></p> 
<p>视频讲解（<font color="red"><strong>建议看下面的视频讲解，文章中的图片来源于【2】Lightning AI</strong></font>）：</p> 
<p><a href="https://www.bilibili.com/video/BV1tY4y1Z7eR" rel="nofollow">【1】试图做一个正常讲解Latent / Stable Diffusion的成年人</a></p> 
<p><a href="https://www.youtube.com/watch?v=AQrMWH8aC0Q" rel="nofollow">【2】Youtube_Lightning AI——Stable Diffusion Explained </a></p> 
<p><strong>总体训练流程</strong></p> 
<p>1 使用<a href="https://arxiv.org/abs/1312.6114" rel="nofollow">VAE</a>模型的Encoder部分对图片进行编码，即生成特征图片1（其符合正太分布？）</p> 
<p>VAE模型：下图源于<a href="https://blog.csdn.net/cjh_jinduoxia/article/details/84995156?spm=1001.2014.3001.5501;size_1">VAE(Variational Autoencoder)简单推导及理解</a><br> <img src="https://images2.imgbox.com/92/47/oRPFhqck_o.png" alt="在这里插入图片描述"></p> 
<p>2 在 t 时刻对特征图片1添加噪声</p> 
<p>3 使用CLIP模型对条件信息进行处理，特征图片中信息作为q 文本或图片信息(condition)作为k和v</p> 
<p>4 将特征图片2与条件信息再进行cross attention处理输入去噪UNet网络，重复本次操作 t-1次</p> 
<p>5 将处理后的图片输入VAE模型的Decoder部分，生成原始图片</p> 
<p><img src="https://images2.imgbox.com/4a/e5/ItW0xlRl_o.png" alt="在这里插入图片描述"></p> 
<p><strong>推理阶段</strong></p> 
<p>1 使用CLIP编码条件信息，并生成t<sub>N</sub> 高斯分布图片A</p> 
<p>2 将未加条件信息和加了条件信息的图片输入 UNet网络，输出两张噪声编码图片B C</p> 
<p>3 将图片B和C进行信息整合，更新Attention UNet的输入，重复上述2， 3步骤直到 i-1=0</p> 
<p><img src="https://images2.imgbox.com/04/31/9P6rizd1_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="httpsimgblogcsdnimgcnc6a5213fe4674386800b66191b74dec6png_67"></a><strong>整体架构</strong><img src="https://images2.imgbox.com/36/04/bMapshkP_o.png" alt="在这里插入图片描述"></h4> 
<h4><a id="_69"></a><strong>详细</strong></h4> 
<h5><a id="UNet_70"></a>UNet网络</h5> 
<p>1 使用ResNet和Spatial Transformer替换卷积和Relu激活作为layer结构<br> <img src="https://images2.imgbox.com/0e/bb/0uVLB24f_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="ResNet_74"></a>ResNet模块细节</h5> 
<p>加噪过程？<br> <img src="https://images2.imgbox.com/3a/e8/py7NrdsF_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="Cross_Attention_81"></a><strong>Cross Attention机制</strong></h5> 
<p><strong>QKV来源于不同的特征，Latent Diffusion中Q来源于图片，KV来源于conditions</strong></p> 
<p><img src="https://images2.imgbox.com/b9/8f/j9I4CVwx_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="_87"></a>各模块应用细节</h5> 
<p><img src="https://images2.imgbox.com/9a/91/y1Qb1eHI_o.png" alt="在这里插入图片描述"></p> 
<ul><li><strong>Number of Resnet+SpatiaTransformers per blockdepends on application.</strong></li><li><strong>Number of downsamplingand upsampling blocksdepends on application.</strong></li><li><strong>Timestep embedding enters all Resnets.</strong></li><li><strong>Context embedding enters all Spatial Transformers.</strong></li></ul> 
<p><strong>训练</strong><br> <img src="https://images2.imgbox.com/7f/d5/Rq9MR3I4_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="VAE_98"></a>VAE</h5> 
<p><a href="https://zhuanlan.zhihu.com/p/364178598" rel="nofollow">VAE – Reparameterization Trick</a></p> 
<p><img src="https://images2.imgbox.com/70/1d/RApgiWhs_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/c2/99/QdEe83ep_o.png" alt="在这里插入图片描述"></p> 
<p><strong>self-attention</strong></p> 
<p><img src="https://images2.imgbox.com/0c/48/5y5psPLe_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ENDPGoh0-1678769503337)(diffusion.assets/image-20230308123930650.png)]"></p> 
<p><strong>训练VAE</strong><br> <img src="https://images2.imgbox.com/39/cf/lYIRyRYl_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-s5e2SF2f-1678769503337)(diffusion.assets/image-20230308124113687.png)]"></p> 
<p>数据集：<a href="https://laion.ai/blog/laion-5b/" rel="nofollow">LAION-5B</a></p> 
<h4><a id="_119"></a>其他</h4> 
<p><a href="https://zhuanlan.zhihu.com/p/526438544" rel="nofollow">DALL·E 2 解读</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/493489688" rel="nofollow">神器CLIP</a></p> 
<p><a href="https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F" rel="nofollow">midjourney</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a0f374e96dbe562ff7267b5f0f125238/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">微信小程序抓包-夜神模拟器结合BurpSuite抓包(可用于现在最新版本微信)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1f96ced99e308249195c8bfd030a2f58/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C语言--编写程序，从键盘输入月份号，输出该月的英文名。（用指针知识）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>