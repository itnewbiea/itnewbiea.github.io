<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>视频编码 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="视频编码" />
<meta property="og:description" content="视频编码 图像基础知识 1、像素
像素点的英文叫Pixel，缩写为PX，它是图像显示的基本单位，我们通常说一幅图片的大小，如1920X1080，就是图片长度为1920个像素点，宽度为1080个像素点，长宽乘积是2073600，也就是说，这个图片是200w像素的。1920X1080也被称为这幅图片的分辨率
2、PPI
PPI是“Pixels Per Inch”，每英寸像素数，也就是显示器屏幕上每英寸面积。PPI的值越高，图像就越清晰细腻；相反，PPI的值越低，屏幕看起来就有很强的颗粒感。
3、图像中的颜色表示
在计算机里，R、G、B也被称为“基色分量”。它们的取值，分别从0-255，一工256个等级。所以任何颜色都可以用R、G、B三个值的组合表示。
通过这种方式，一共能表达多少种颜色呢?256×256×256=16,777,216种，因此也简称为1600万色。RGB三色，每色有8bit，这种方式表达出来的颜色，也被称为24位色(占用24bit)。这个颜色范围已经超过了人眼可见的全部色彩，所以又叫真彩色。再高的话，对于我们人眼来说，已经没有意义了，完全识别不出来。
视频编码基础知识 视频和图像的关系
大量的图片连续起来就是视频。衡量视频的指标参数—帧率。
在视频中，一个帧就是指一幅静止的画面。帧率，就是指视频每秒钟包括的画面数量，帧率越高，视频就越逼真、越流畅。
对视频编码的必要性
一个视频若未经编码，它的体积是非常庞大的。
以一个分辨率1920×1280，帧率30的视频为例：
共：1920×1280=2,073,600(Pixels 像素)，每个像素点是24bit;
也就是：每幅图片2073600×24=49766400 bit，8 bit(位)=1 byte(字节);
所以：49766400bit=6220800byte≈6.22MB。
这是一幅1920×1280图片的原始大小，再乘以帧率30。
也就是说：每秒视频的大小是186.6MB，每分钟大约是11GB，一部90分钟的电影，约是1000GB。
什么是视频编码
编码：就是按照指定的方法，将信息从一种形式转换成另外一种形式。
视频编码：就是将一种视频分时转换成另一种视频格式。
编码的终极目的，就是为了压缩。各种编码格式都是为了让视频变得提价更小，有利于存储和传输。
视频录制播放的整个过程如下：
摄像机（图像传感器）—&gt;预处理（A/D转换）—&gt;预处理（YUV转换等）—&gt;压缩编码—&gt;数据封装及传输—&gt;解码—&gt;图像格式变换—&gt;播放
首先时视频采集，通常我们会使用摄像机、摄像头进行视频采集。
采集了视频数据之后，就要进行模数转换，将模拟信号变成数字信号。其实现在很多摄像机直接输出数字信号。信号输出之后，还要进行预处理，将RGB信号变成YUV信号。
那么什么时YUV信号呢？
简单来说，YUV就是另外一种颜色数字化表示方式。视频通信系统之所以要采用YUV，而不是RGB，主要时因为RGB信号不利于压缩。在YUV这种方式里面，加入了亮度这一概念。在最近十年中，视频工程师发现，眼睛对于亮和暗的分辨要比对颜色的分辨更精细一些，也就是说，人眼对色度的敏感程度要低于对亮度的敏感程度。
所以，工程师认为，在我们的视频存储中，没有必要存储全部颜色信号。我们可以把更多带宽留给黑—白信号(被称作“亮度”)，将稍少的带宽留给彩色信号(被称作“色度”)。于是，就有了YUV。
YUV里面的“Y”，就是亮度(Luma)，“U”和“V”则是色度(Chroma)。
YUV主流的采样方式有三种：
1）YUV4:4:4;
2)YUV4:2:2;
3)YUV4:2:0。
通常来说，YUV4:2:0的采样方式，能获得1/2的压缩率。
视频编码的实现原理 1、视频编码技术的基本原理
所谓编码算法，就是寻找规律，构建模型。谁能找到更精准的规律，建立更高效的模型，谁就是厉害的算法。
通常来说，视频里面的冗余信息包括：
种类内容压缩方法空间冗余像素间的相关性变换编码，预测编码时间冗余时间方向上的相关性帧间预测，运动补偿图像构造冗余图像本身的构造轮廓编码，区域分割知识冗余收发两端对人物的共有认识基于知识的编码视觉冗余人的视觉特性非线性量化，位分配其他不确定性因素 视频编码技术优先消除的目标，就是空间冗余和时间冗余。
2、视频编码技术的实现方法
视频是由不同的帧画面连续播放形成的。
这些帧，主要分为三类：
1）I帧；
2）B帧；
3）P帧。
I帧：是自带全部信息的独立帧，是最完整的画面(占用的空间最大)，无需参考其它图像便可独立进行解码。视频序列中的第一个帧，始终都是I帧。
P帧：“帧间预测编码帧”，需要参考前面的I帧和/或P帧的不同部分，才能进行编码。P帧对前面的P和I参考帧有依赖性。但是，P帧压缩率比较高，占用的空间较小。
B帧：“双向预测编码帧”，以前帧后帧作为参考帧。不仅参考前面，还参考后面的帧，所以，它的压缩率最高，可以达到200:1。不过，因为依赖后面的帧，所以不适合实时传输(例如视频会议)。
通过对帧的分类处理，可以大幅压缩视频的大小。
我们举个例子来看一下。
这有两个帧：
好像是一样的？
不对，我做个GIF动图就能看出来，是不一样的
人在动，背景时没有在动的
第一帧是I帧，第二帧是P帧。两个帧之间的差值，就是如下：
也就是说，图中的部分像素，进行了移动。移动轨迹如下：
这个就是运动估计和补偿
当然了，如果总是按照像素来算，数据量会比较大，所以，一般都是把图像切割为不同的“块(Block)”或“宏块(MacroBlock)”，对它们进行计算。一个宏块一般为16像素×16像素。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/2c56e5a6413b80e60cb37b75a42944a3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-27T14:39:43+08:00" />
<meta property="article:modified_time" content="2021-04-27T14:39:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">视频编码</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_2"></a>视频编码</h2> 
<h3><a id="_5"></a>图像基础知识</h3> 
<p><strong>1、像素</strong></p> 
<p>像素点的英文叫Pixel，缩写为PX，它是图像显示的基本单位，我们通常说一幅图片的大小，如1920X1080，就是图片长度为1920个像素点，宽度为1080个像素点，长宽乘积是2073600，也就是说，这个图片是200w像素的。1920X1080也被称为这幅图片的分辨率</p> 
<p><strong>2、PPI</strong></p> 
<p>PPI是“Pixels Per Inch”，每英寸像素数，也就是显示器屏幕上每英寸面积。PPI的值越高，图像就越清晰细腻；相反，PPI的值越低，屏幕看起来就有很强的颗粒感。</p> 
<p><img src="https://images2.imgbox.com/d0/c7/hnwGYIld_o.png" alt="在这里插入图片描述"></p> 
<p><strong>3、图像中的颜色表示</strong></p> 
<p>在计算机里，R、G、B也被称为“基色分量”。它们的取值，分别从0-255，一工256个等级。所以任何颜色都可以用R、G、B三个值的组合表示。</p> 
<p><img src="https://images2.imgbox.com/6c/a7/qFZfSeAF_o.png" alt="在这里插入图片描述"></p> 
<p>通过这种方式，一共能表达多少种颜色呢?256×256×256=16,777,216种，因此也简称为1600万色。RGB三色，每色有8bit，这种方式表达出来的颜色，也被称为24位色(占用24bit)。这个颜色范围已经超过了人眼可见的全部色彩，所以又叫真彩色。再高的话，对于我们人眼来说，已经没有意义了，完全识别不出来。</p> 
<h3><a id="_24"></a>视频编码基础知识</h3> 
<p><strong>视频和图像的关系</strong></p> 
<p>大量的图片连续起来就是视频。衡量视频的指标参数—帧率。</p> 
<p>在视频中，一个帧就是指一幅静止的画面。帧率，就是指视频每秒钟包括的画面数量，帧率越高，视频就越逼真、越流畅。</p> 
<p><img src="https://images2.imgbox.com/39/9b/AcUvHdcI_o.png" alt="在这里插入图片描述"></p> 
<p><strong>对视频编码的必要性</strong></p> 
<p>一个视频若未经编码，它的体积是非常庞大的。</p> 
<p>以一个分辨率1920×1280，帧率30的视频为例：</p> 
<p>共：1920×1280=2,073,600(Pixels 像素)，每个像素点是24bit;</p> 
<p>也就是：每幅图片2073600×24=49766400 bit，8 bit(位)=1 byte(字节);</p> 
<p>所以：49766400bit=6220800byte≈6.22MB。</p> 
<p>这是一幅1920×1280图片的原始大小，再乘以帧率30。</p> 
<p>也就是说：每秒视频的大小是186.6MB，每分钟大约是11GB，一部90分钟的电影，约是1000GB。</p> 
<p><strong>什么是视频编码</strong></p> 
<p>编码：就是按照指定的方法，将信息从一种形式转换成另外一种形式。</p> 
<p>视频编码：就是将一种视频分时转换成另一种视频格式。</p> 
<p>编码的终极目的，就是为了压缩。各种编码格式都是为了让视频变得提价更小，有利于存储和传输。</p> 
<p>视频录制播放的整个过程如下：</p> 
<p>摄像机（图像传感器）—&gt;预处理（A/D转换）—&gt;预处理（YUV转换等）—&gt;压缩编码—&gt;数据封装及传输—&gt;解码—&gt;图像格式变换—&gt;播放</p> 
<hr> 
<p>首先时视频采集，通常我们会使用摄像机、摄像头进行视频采集。</p> 
<p>采集了视频数据之后，就要进行模数转换，将模拟信号变成数字信号。其实现在很多摄像机直接输出数字信号。信号输出之后，还要进行预处理，将RGB信号变成YUV信号。</p> 
<p>那么什么时YUV信号呢？</p> 
<p>简单来说，YUV就是另外一种颜色数字化表示方式。视频通信系统之所以要采用YUV，而不是RGB，主要时因为RGB信号不利于压缩。在YUV这种方式里面，加入了亮度这一概念。在最近十年中，视频工程师发现，<strong>眼睛对于亮和暗的分辨要比对颜色的分辨更精细一些</strong>，也就是说，人眼对色度的敏感程度要低于对亮度的敏感程度。</p> 
<p>所以，工程师认为，在我们的视频存储中，没有必要存储全部颜色信号。我们可以把更多带宽留给黑—白信号(被称作“亮度”)，将稍少的带宽留给彩色信号(被称作“色度”)。于是，就有了YUV。</p> 
<p>YUV里面的“Y”，就是亮度(Luma)，“U”和“V”则是色度(Chroma)。</p> 
<p>YUV主流的采样方式有三种：</p> 
<p>1）YUV4:4:4;</p> 
<p>2)YUV4:2:2;</p> 
<p>3)YUV4:2:0。</p> 
<p>通常来说，<strong>YUV4:2:0的采样方式，能获得1/2的压缩率。</strong></p> 
<h3><a id="_85"></a>视频编码的实现原理</h3> 
<p><strong>1、视频编码技术的基本原理</strong></p> 
<p>所谓编码算法，就是寻找规律，构建模型。谁能找到更精准的规律，建立更高效的模型，谁就是厉害的算法。</p> 
<p>通常来说，视频里面的冗余信息包括：</p> 
<table><thead><tr><th>种类</th><th>内容</th><th>压缩方法</th></tr></thead><tbody><tr><td>空间冗余</td><td>像素间的相关性</td><td>变换编码，预测编码</td></tr><tr><td>时间冗余</td><td>时间方向上的相关性</td><td>帧间预测，运动补偿</td></tr><tr><td>图像构造冗余</td><td>图像本身的构造</td><td>轮廓编码，区域分割</td></tr><tr><td>知识冗余</td><td>收发两端对人物的共有认识</td><td>基于知识的编码</td></tr><tr><td>视觉冗余</td><td>人的视觉特性</td><td>非线性量化，位分配</td></tr><tr><td>其他</td><td>不确定性因素</td><td></td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/6f/15/vvjpszT5_o.png" alt="在这里插入图片描述"></p> 
<p><strong>视频编码技术优先消除的目标，就是空间冗余和时间冗余。</strong></p> 
<p><strong>2、视频编码技术的实现方法</strong></p> 
<p>视频是由不同的帧画面连续播放形成的。</p> 
<p>这些帧，主要分为三类：</p> 
<p>1）I帧；</p> 
<p>2）B帧；</p> 
<p>3）P帧。</p> 
<p>I帧：是自带全部信息的独立帧，是最完整的画面(占用的空间最大)，无需参考其它图像便可独立进行解码。视频序列中的第一个帧，始终都是I帧。</p> 
<p>P帧：“帧间预测编码帧”，需要参考前面的I帧和/或P帧的不同部分，才能进行编码。P帧对前面的P和I参考帧有依赖性。但是，P帧压缩率比较高，占用的空间较小。</p> 
<p><img src="https://images2.imgbox.com/37/d3/cFINsK16_o.png" alt=""></p> 
<p>B帧：“双向预测编码帧”，以前帧后帧作为参考帧。不仅参考前面，还参考后面的帧，所以，它的压缩率最高，可以达到200:1。不过，因为依赖后面的帧，所以不适合实时传输(例如视频会议)。</p> 
<p><img src="https://images2.imgbox.com/5e/7b/WEdt68tr_o.png" alt="在这里插入图片描述"><br> 通过对帧的分类处理，可以大幅压缩视频的大小。</p> 
<p><img src="https://images2.imgbox.com/30/b2/nsSDHH7Z_o.png" alt="在这里插入图片描述"><br> 我们举个例子来看一下。</p> 
<p>这有两个帧：</p> 
<p><img src="https://images2.imgbox.com/96/42/8mWyx3WV_o.png" alt="在这里插入图片描述"><br> 好像是一样的？</p> 
<p>不对，我做个GIF动图就能看出来，是不一样的</p> 
<p>人在动，背景时没有在动的</p> 
<p>第一帧是I帧，第二帧是P帧。两个帧之间的差值，就是如下：<br> <img src="https://images2.imgbox.com/58/72/E1lblWsY_o.png" alt="在这里插入图片描述"><br> 也就是说，图中的部分像素，进行了移动。移动轨迹如下：</p> 
<p><img src="https://images2.imgbox.com/65/4b/5BK8C3YQ_o.png" alt="在这里插入图片描述"><br> 这个就是运动估计和补偿</p> 
<p><img src="https://images2.imgbox.com/55/a0/10m0MXho_o.png" alt="在这里插入图片描述"><br> 当然了，如果总是按照像素来算，数据量会比较大，所以，一般都是把图像切割为不同的“块(Block)”或“宏块(MacroBlock)”，对它们进行计算。一个宏块一般为16像素×16像素。</p> 
<p><img src="https://images2.imgbox.com/53/fb/F796PGPm_o.png" alt="在这里插入图片描述"></p> 
<p>对I帧的处理，是采用帧内编码方式，只利用本帧图像内的空间相关性。对P帧的处理，采用帧间编码(前向运动估计)，同时利用空间和时间上的相关性。简单来说，采用运动补偿(motion compensation)算法来去掉冗余信息。</p> 
<p><img src="https://images2.imgbox.com/3d/7d/FOlnpFlI_o.png" alt="在这里插入图片描述"><br> 需要特别注意，I帧(帧内编码)，虽然只有空间相关性，但整个编码过程也不简单。</p> 
<p><img src="https://images2.imgbox.com/ca/c7/b3nwy9MV_o.png" alt="在这里插入图片描述"><br> 如上图所示，每个帧内鞭名马，还要经过DCT（离散余弦变换）、量化、编码等多个过程。</p> 
<p>那么，视频经过编码之后，如何来衡量和评价编解码的效果呢？</p> 
<p>一般来说，分为客观评价和主观评价。客观评价，就是拿数字来说话。例如计算“信噪比/峰值信噪比”。</p> 
<p><img src="https://images2.imgbox.com/95/3f/JlmcP234_o.png" alt="在这里插入图片描述"><br> 除了客观评价，就是主观评价了。主观评价，就是用人的主观感知直接测量，额，说人话就是——“好不好看我说了算”。</p> 
<h3><a id="_166"></a>视频编码的国际标准</h3> 
<p><strong>1、视频编码格式的便准化</strong></p> 
<p>ITU（国际电信联盟）</p> 
<p>ITU是联合国下属的一个专门机构，其总部在瑞士的日内瓦。</p> 
<p>ITU下属有三个部门：</p> 
<p>1)分别是ITU-R(前身是国际无线电咨询委员会CCIR);</p> 
<p>2)ITU-T(前身是国际电报电话咨询委员会CCITT);</p> 
<p>3)ITU-D。<br> <img src="https://images2.imgbox.com/57/20/tDVGNd2C_o.png" alt="在这里插入图片描述"><br> ISO/IEC</p> 
<p>ISO大家都知道，就是推出ISO9001质量认证的那个“国际标准化组织”。IEC，是“国际电工委员会”。1988年，ISO和IEC联合成立了一个专家组，负责开发电视图像数据和声音数据的编码、解码和它们的同步等标准。这个专家组，就是大名鼎鼎的MPEG，Moving Picture Expert Group(动态图像专家组)。</p> 
<p>三十多年以来，世界上主流的视频编码标准，基本上都是它们提出来的：</p> 
<p>1)ITU提出了H.261、H.262、H.263、H.263+、H.263++，这些统称为H.26X系列，主要应用于实时视频通信领域，如会议电视、可视电话等;</p> 
<p>2)ISO/IEC提出了MPEG1、MPEG2、MPEG4、MPEG7、MPEG21，统称为MPEG系列。</p> 
<p>ITU和ISO/IEC一开始是各自捣鼓，后来，两边成立了一个联合小组，名叫JVT(Joint Video Team，视频联合工作组)。</p> 
<p>JVT致力于新一代视频编码标准的制定，后来推出了包括H.264在内的一系列标准。</p> 
<p><img src="https://images2.imgbox.com/82/e2/q1E9wg5F_o.png" alt=""><br> 如下是编码标准的发展关系：<br> <img src="https://images2.imgbox.com/96/15/OX81czbF_o.png" alt="在这里插入图片描述"><br> 上图里面的HEVC，也就是现在风头正盛的H.265，作为一种新编码标准，相比H.264有极大的性能提升，目前已经成为最新视频编码系统的标配。</p> 
<p><img src="https://images2.imgbox.com/1b/f7/46BCgO3D_o.png" alt="在这里插入图片描述"><br> <strong>2、视频数据的封装</strong></p> 
<p>对于任何一部视频来说，只有图像，没有声音，肯定是不行的。所以，视频编码后，加上音频编码，要一起进行封装。</p> 
<p>封装：就是封装格式，简单来说，就是将已经编码压缩好的视频轨和音频轨按照一定的格式放到一个文件中。再通俗点，视频轨相当于饭，而音频轨相当于菜，封装格式就是一个饭盒，用来盛放饭菜的容器。</p> 
<p>目前主要的视频容器有如下：MPG、VOB、MP4、3GP、ASF、RMVB、WMV、MOV、Divx、MKV、FLV、TS/PS等。</p> 
<p>封装之后的视频，就可以传输了，你也可以通过视频播放器进行解码观看。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/32ef01fb6ebbe5981e8611139bb6f8ae/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">树模型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c5b23c4f001833758939c1f42d9ec986/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python函数与方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>