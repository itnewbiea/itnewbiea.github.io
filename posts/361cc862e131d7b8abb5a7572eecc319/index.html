<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python3实战（1）：网络小说爬取工具 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python3实战（1）：网络小说爬取工具" />
<meta property="og:description" content="一、前言 本文是http://blog.csdn.net/c406495762/article/details/71158264的学习笔记 作者:Jack-Cui 博主链接:http://blog.csdn.net/c406495762 运行平台： OSX Python版本： Python3.x IDE： pycharm Beautiful Soup 简单来说，BeautifulSoup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下：
BeautifulSoup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。BeautifulSoup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。BeautifulSoup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。 lxml
lxml是个非常有用的python库，它可以灵活高效地解析xml，与BeautifulSoup、requests结合，是编写爬虫的标准姿势。
二、实战步骤 爬虫在github的地址
1、环境安装
这里我们需要用到python的pip或者pip3来安装相关库和依赖，做法是运行cmd，转到你本地python安装目录下，有个script文件夹，到该文件夹的路径下运行安装命令就行了
安装BeautifulSoup：
pip3 install beautifulsoup4 安装lxml：
pip3 install lxml 2、PyCharm配置
如果使用开发工具的话，如果没有添加beautifulsoup4和lxml库的话，也会报错
打开File/Settings/Project:xxx/Project Interpreter，首先，在Project Interpreter选择本地python安装目录下的python.exe，
然后，在下面的列表中添加beautifulsoup4和lxml库
3、预备知识
想了解爬虫知识，首先得了解BeautifulSoup相关知识
详细内容可参照官方文档：http://beautifulsoup.readthedocs.io/zh_CN/latest/
但是我觉得参照的教程的博主写的更为通俗易懂：http://blog.csdn.net/c406495762/article/details/71158264
4、小说内容的爬取
这边同样的，以《笔趣看》小说网站（URL：http://www.biqukan.com/）为例
（1）单章小说内容的爬取
打开《一念永恒》小说的第一章（URL：http://www.biqukan.com/1_1094/5403177.html），进行审查元素分析
由审查结果可知，文章的内容存放在id为content，class为showtxt的div标签中：
局部放大：
因此我们，可以使用如下方法将本章小说内容爬取下来：
# -*- coding:UTF-8 -*- from urllib import request from bs4 import BeautifulSoup if __name__ == &#34;__main__&#34;: download_url = &#39;http://www.biqukan.com/1_1094/5403177.html&#39; head = {} head[&#39;User-Agent&#39;] = &#39;Mozilla/5." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/361cc862e131d7b8abb5a7572eecc319/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-03-14T03:36:16+08:00" />
<meta property="article:modified_time" content="2018-03-14T03:36:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python3实战（1）：网络小说爬取工具</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="article fmt article__content"> 
 <h4>一、前言</h4> 
 <pre><code>本文是http://blog.csdn.net/c406495762/article/details/71158264的学习笔记 
作者:Jack-Cui 
博主链接:http://blog.csdn.net/c406495762

运行平台： OSX 
Python版本： Python3.x 
IDE： pycharm 
</code></pre> 
 <blockquote> 
  <p><strong>Beautiful Soup</strong> </p> 
  <p>简单来说，BeautifulSoup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下：</p> 
  <ul><li>BeautifulSoup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。</li><li>BeautifulSoup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。</li><li>BeautifulSoup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。</li></ul> 
  <p><strong>lxml</strong></p> 
  <p>lxml是个非常有用的python库，它可以灵活高效地解析xml，与BeautifulSoup、requests结合，是编写爬虫的标准姿势。</p> 
 </blockquote> 
 <h4>二、实战步骤</h4> 
 <p>爬虫在<a href="https://github.com/MealiaLin/myPython/tree/master/novelDownload">github的地址</a></p> 
 <p><strong>1、环境安装</strong></p> 
 <p>这里我们需要用到python的pip或者pip3来安装相关库和依赖，做法是运行cmd，转到你本地python安装目录下，有个script文件夹，到该文件夹的路径下运行安装命令就行了</p> 
 <p>安装BeautifulSoup：</p> 
 <pre><code>pip3 install beautifulsoup4</code></pre> 
 <p>安装lxml：</p> 
 <pre><code>pip3 install lxml</code></pre> 
 <p><strong>2、PyCharm配置</strong></p> 
 <p>如果使用开发工具的话，如果没有添加beautifulsoup4和lxml库的话，也会报错</p> 
 <p>打开File/Settings/Project:xxx/Project Interpreter，首先，在Project Interpreter选择本地python安装目录下的python.exe，<br>然后，在下面的列表中添加beautifulsoup4和lxml库</p> 
 <p><span class="img-wrap"><img src="https://images2.imgbox.com/48/f3/xQ4YABjv_o.png" alt="图片描述" title="图片描述"></span></p> 
 <p><strong>3、预备知识</strong></p> 
 <p>想了解爬虫知识，首先得了解<strong>BeautifulSoup</strong>相关知识</p> 
 <p>详细内容可参照官方文档：<a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" rel="nofollow">http://beautifulsoup.readthedocs.io/zh_CN/latest/</a></p> 
 <p>但是我觉得参照的教程的博主写的更为通俗易懂：<a href="http://blog.csdn.net/c406495762/article/details/71158264">http://blog.csdn.net/c406495762/article/details/71158264</a></p> 
 <p><strong>4、小说内容的爬取</strong></p> 
 <p>这边同样的，以《笔趣看》小说网站（URL：<a href="http://www.biqukan.com/" rel="nofollow">http://www.biqukan.com/</a>）为例</p> 
 <p><strong>（1）单章小说内容的爬取</strong></p> 
 <p>打开《一念永恒》小说的第一章（URL：<a href="http://www.biqukan.com/1_1094/5403177.html" rel="nofollow">http://www.biqukan.com/1_1094/5403177.html</a>），进行审查元素分析</p> 
 <p>由审查结果可知，文章的内容存放在id为content，class为showtxt的div标签中：</p> 
 <p><span class="img-wrap"><img src="https://images2.imgbox.com/2b/61/Of09FrVT_o.png" alt="图片描述" title="图片描述"></span></p> 
 <p>局部放大：</p> 
 <p><span class="img-wrap"><img src="https://images2.imgbox.com/8e/43/KpbwwABT_o.png" alt="图片描述" title="图片描述"></span></p> 
 <p>因此我们，可以使用如下方法将本章小说内容爬取下来：</p> 
 <pre><code class="python"># -*- coding:UTF-8 -*-
from urllib import request
from bs4 import BeautifulSoup

if __name__ == "__main__":
    download_url = 'http://www.biqukan.com/1_1094/5403177.html'
    head = {}
    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'
    download_req = request.Request(url = download_url, headers = head)
    download_response = request.urlopen(download_req)
    download_html = download_response.read().decode('gbk','ignore')
    soup_texts = BeautifulSoup(download_html, 'lxml')
    texts = soup_texts.find_all(id = 'content', class_ = 'showtxt')
    soup_text = BeautifulSoup(str(texts), 'lxml')
    #将\xa0无法解码的字符删除
    print(soup_text.div.text.replace('\xa0',''))</code></pre> 
 <p>运行结果：</p> 
 <p><span class="img-wrap"><img src="https://images2.imgbox.com/f9/d6/jKduorA0_o.png" alt="图片描述" title="图片描述"></span></p> 
 <p>可以看到，我们已经顺利爬取第一章内容，但是我们不能满足于此，我们想要的是正本小说都爬取下来</p> 
 <p>接下来就是如何爬取所有章的内容，爬取之前需要知道每个章节的地址。因此，我们需要审查《一念永恒》小说目录页的内容。</p> 
 <p><strong>（2）各章小说链接的爬取</strong></p> 
 <p>URL：<a href="http://www.biqukan.com/1_1094/" rel="nofollow">http://www.biqukan.com/1_1094/</a></p> 
 <p>由审查结果可知，小说每章的链接放在了class为listmain的div标签中。链接具体位置放在html-&gt;body-&gt;div-&gt;dd-&gt;dl-&gt;a的href属性中，例如下图的第759章的href属性为/1_1094/14235101.html，<br> 那么该章节的地址为：<a href="http://www.biqukan.com/1_1094/14235101.html" rel="nofollow">http://www.biqukan.com/1_1094/14235101.html</a></p> 
 <p>因此，我们可以使用如下方法获取正文所有章节的地址：</p> 
 <pre><code class="python"># -*- coding:UTF-8 -*-
from urllib import request
from bs4 import BeautifulSoup

if __name__ == "__main__":
    target_url = 'http://www.biqukan.com/1_1094/'
    head = {}
    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'
    target_req = request.Request(url = target_url, headers = head)
    target_response = request.urlopen(target_req)
    target_html = target_response.read().decode('gbk','ignore')
    #创建BeautifulSoup对象
    listmain_soup = BeautifulSoup(target_html,'lxml')
    #搜索文档树,找出div标签中class为listmain的所有子标签
    chapters = listmain_soup.find_all('div',class_ = 'listmain')
    #使用查询结果再创建一个BeautifulSoup对象,对其继续进行解析
    download_soup = BeautifulSoup(str(chapters), 'lxml')
    #开始记录内容标志位,只要正文卷下面的链接,最新章节列表链接剔除
    begin_flag = False
    #遍历dl标签下所有子节点
    for child in download_soup.dl.children:
        #滤除回车
        if child != '\n':
            #找到《一念永恒》正文卷,使能标志位
            if child.string == u"《一念永恒》正文卷":
                begin_flag = True
            #爬取链接
            if begin_flag == True and child.a != None:
                download_url = "http://www.biqukan.com" + child.a.get('href')
                download_name = child.string
                print(download_name + " : " + download_url)</code></pre> 
 <p>运行结果：</p> 
 <p><span class="img-wrap"><img src="https://images2.imgbox.com/af/b0/K8Cjq3qz_o.png" alt="图片描述" title="图片描述"></span></p> 
 <p><strong>（3）爬取所有章节的内容，并保存在文件中</strong></p> 
 <p>综合上面两点，我们要做的就是根据爬取下来所有章节的链接地址，再遍历爬取所有章节的内容，并保存在本地文件中</p> 
 <pre><code class="python"># -*- coding:UTF-8 -*-
from urllib import request
from bs4 import BeautifulSoup
import re
import sys

if __name__ == "__main__":
    #创建txt文件
    file = open('一念永恒.txt', 'w', encoding='utf-8')
    #一念永恒小说目录地址
    target_url = 'http://www.biqukan.com/1_1094/'
    #User-Agent
    head = {}
    head['User-Agent'] = 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19'
    target_req = request.Request(url = target_url, headers = head)
    target_response = request.urlopen(target_req)
    target_html = target_response.read().decode('gbk','ignore')
    #创建BeautifulSoup对象
    listmain_soup = BeautifulSoup(target_html,'lxml')
    #搜索文档树,找出div标签中class为listmain的所有子标签
    chapters = listmain_soup.find_all('div',class_ = 'listmain')
    #使用查询结果再创建一个BeautifulSoup对象,对其继续进行解析
    download_soup = BeautifulSoup(str(chapters), 'lxml')
    #计算章节个数
    numbers = (len(download_soup.dl.contents) - 1) / 2 - 8
    index = 1
    #开始记录内容标志位,只要正文卷下面的链接,最新章节列表链接剔除
    begin_flag = False
    #遍历dl标签下所有子节点
    for child in download_soup.dl.children:
        #滤除回车
        if child != '\n':
            #找到《一念永恒》正文卷,使能标志位
            if child.string == u"《一念永恒》正文卷":
                begin_flag = True
            #爬取链接并下载链接内容
            if begin_flag == True and child.a != None:
                download_url = "http://www.biqukan.com" + child.a.get('href')
                download_req = request.Request(url = download_url, headers = head)
                download_response = request.urlopen(download_req)
                download_html = download_response.read().decode('gbk','ignore')
                download_name = child.string
                soup_texts = BeautifulSoup(download_html, 'lxml')
                texts = soup_texts.find_all(id = 'content', class_ = 'showtxt')
                soup_text = BeautifulSoup(str(texts), 'lxml')
                write_flag = True
                file.write(download_name + '\n\n')
                #将爬取内容写入文件
                for each in soup_text.div.text.replace('\xa0',''):
                    if each == 'h':
                        write_flag = False
                    if write_flag == True and each != ' ':
                        file.write(each)
                    if write_flag == True and each == '\r':
                        file.write('\n')
                file.write('\n\n')
                #打印爬取进度
                sys.stdout.write("已下载:%.3f%%" % float(index/numbers) + '\r')
                sys.stdout.flush()
                index += 1
    file.close()</code></pre> 
 <p>代码略显粗糙，运行效率不高，还有很多可以改进的地方</p> 
 <p><a href="https://mealialin.github.io/2018/03/python3-novel-download/" rel="nofollow">本文原文地址</a></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5dff27877827ed948b1d8aee20800160/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">c&#43;&#43;Double类型不能直接比较大小</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/73e6dcdb5f75235361423d439705cc40/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">手把手教你脑电波采集及信号处理分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>