<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于 SmartX 分布式存储的 RDMA 与 TCP/IP 技术与性能对比 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于 SmartX 分布式存储的 RDMA 与 TCP/IP 技术与性能对比" />
<meta property="og:description" content="作者：深耕行业的 SmartX 金融团队
背景 上一篇 “分布式块存储 ZBS 的自主研发之旅｜架构篇” 文章中，我们简单介绍了 SmartX 分布式块存储 ZBS 的架构原理。接下来，我们将为读者深入解析 ZBS 存储中最为重要的技术之一“RDMA”。
目前 ZBS 在两个层面会使用到 RDMA 技术，分别是存储接入网络和存储内部数据同步网络。为了使读者更加容易理解，以及更有针对性地做存储性能对比，特通过两篇独立的文章分别进行介绍。本期，我们将聚焦 RDMA 远程内存直接访问技术，并结合 ZBS 内部存储数据同步进行详细的展开（ZBS 支持 RDMA 能力，也是在存储内部数据同步中最先实现）。
ZBS 存储内部数据同步 分布式存储系统与集中存储最重要的区别之一就是架构实现。分布式架构要保证数据的存储一致性和可靠性，就必须依赖网络进行数据同步。这里举一个例子，一个由 3 节点（A/B/C）组成的 ZBS 存储集群，数据采用两副本保护（数据存储两份，放置在不同的物理节点），假设数据分别存放在节点 A 和节点 B 上，当数据发生修改，ZBS 分布式存储必须完成对节点 A 和 B 的数据修改再返回确认。在这个过程中，A 和 B 节点同步数据修改，所使用的网络，即是存储网络。
通过例子，相信读者已经理解，数据同步效率对于分布式存储的性能表现有着非常大的影响，是分布式存储性能优化的重要方向之一，也是本篇文章重点讨论的内容。
图 1：分布式存储数据同步网络
在目前常规的工作负载需求下，ZBS 存储网络通常使用 10GbE 以太网交换机和服务器网卡配置，采用标准 TCP/IP 作为网络传输协议。但对于高带宽和低延时的业务工作负载，这样的配置明显会成为内部存储数据同步的性能瓶颈。同时，为了发挥新型的高速存储介质（例如 NVMe 磁盘）更强劲的 I/O 性能，采用 RDMA 技术并结合 25GbE 或更高的网络规格，是满足业务端更高的存储性能诉求的更优选择。
TCP/IP 通过软件定义实现的分布式存储，基于通用标准的硬件构建，是其有别于传统存储的重要的特点之一。多年以来，ZBS 使用 TCP/IP 网络协议栈作为存储内部通信方式，优势是具备与现有以太网最大的兼容性，同时满足绝大多数客户的业务工作负载需求。但 TCP/IP 网络通信逐渐不能适应更高性能计算的业务诉求，其主要限制有以下两点：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/21b0e54e1173384a19f51dce7521ffd4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-08T19:36:54+08:00" />
<meta property="article:modified_time" content="2023-02-08T19:36:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于 SmartX 分布式存储的 RDMA 与 TCP/IP 技术与性能对比</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="kdocs-document"> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">作者：深耕行业的 SmartX 金融团队</span></p> 
 <p style="text-align:null;"></p> 
 <h3 style="text-align:left;">背景</h3> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">上一篇 “</span><a class="kdocs-link" style="color:#0A6CFF;" href="https://www.smartx.com/blog/2022/08/zbs-architecture/" rel="nofollow noopener noreferrer" target="_blank"><span class="kdocs-fontSize" style="font-size:13pt;">分布式块存储 ZBS 的自主研发之旅｜架构篇</span></a><span class="kdocs-fontSize" style="font-size:13pt;">” 文章中，我们简单介绍了 SmartX 分布式块存储 ZBS 的架构原理。接下来，我们将为读者深入解析 ZBS 存储中最为重要的技术之一“RDMA”。</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">目前 ZBS 在两个层面会使用到 RDMA 技术，分别是存储接入网络和存储内部数据同步网络。为了使读者更加容易理解，以及更有针对性地做存储性能对比，特通过两篇独立的文章分别进行介绍。<span class="kdocs-bold" style="font-weight:bold;">本期，我们将聚焦 RDMA 远程内存直接访问技术，并结合 ZBS 内部存储数据同步进行详细的展开</span>（ZBS 支持 RDMA 能力，也是在存储内部数据同步中最先实现）。</span></p> 
 <p style="text-align:null;"></p> 
 <h3 style="text-align:left;">ZBS 存储内部数据同步</h3> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">分布式存储系统与集中存储最重要的区别之一就是架构实现。分布式架构要保证数据的存储一致性和可靠性，就必须依赖网络进行数据同步。这里举一个例子，一个由 3 节点（A/B/C）组成的 ZBS 存储集群，数据采用两副本保护（数据存储两份，放置在不同的物理节点），假设数据分别存放在节点 A 和节点 B 上，当数据发生修改，ZBS 分布式存储必须完成对节点 A 和 B 的数据修改再返回确认。在这个过程中，A 和 B 节点同步数据修改，所使用的网络，即是存储网络。</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">通过例子，相信读者已经理解，<span class="kdocs-bold" style="font-weight:bold;">数据同步效率对于分布式存储的性能表现有着非常大的影响，是分布式存储性能优化的重要方向之一，</span>也是本篇文章重点讨论的内容。</span></p> 
 <div class="kdocs-line-container" style="display:flex;justify-content:null;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:830px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:32.04819%;height:0;"> 
    <img src="https://images2.imgbox.com/7c/f4/kNKXs7eK_o.png" style="margin-left:;display:block;width:830px;margin-top:-32.04819%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="text-align:center;"><span class="kdocs-fontSize" style="font-size:13pt;">图 1：分布式存储数据同步网络</span></p> 
 <p style="text-align:null;"></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">在目前常规的工作负载需求下，ZBS 存储网络通常使用 10GbE 以太网交换机和服务器网卡配置，采用标准 TCP/IP 作为网络传输协议。但对于高带宽和低延时的业务工作负载，这样的配置明显会成为内部存储数据同步的性能瓶颈。同时，为了发挥新型的高速存储介质（例如 NVMe 磁盘）更强劲的 I/O 性能，采用 RDMA 技术并结合 25GbE 或更高的网络规格，是满足业务端更高的存储性能诉求的更优选择。</span></p> 
 <p style="text-align:null;"></p> 
 <h3 style="text-align:left;">TCP/IP</h3> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">通过软件定义实现的分布式存储，基于通用标准的硬件构建，是其有别于传统存储的重要的特点之一。多年以来，ZBS 使用 TCP/IP 网络协议栈作为存储内部通信方式，优势是具备与现有以太网最大的兼容性，同时满足绝大多数客户的业务工作负载需求。但 TCP/IP 网络通信逐渐不能适应更高性能计算的业务诉求，其主要限制有以下两点：</span></p> 
 <p style="text-align:null;"></p> 
 <ul><li style="margin-left:1.4em;list-style-type:disc;text-indent:0;"><p><span class="kdocs-bold" style="font-weight:bold;">TCP/IP 协议栈处理带来的时延</span></p></li></ul> 
 <p style="">TCP 协议栈在接收/发送数据报文时，系统内核需要做多次上下文切换，这个动作无疑将增加传输时延。另外在数据传输过程中，还需要多次数据复制和依赖 CPU 进行协议封装处理，这就导致仅仅是协议栈处理就带来数十微秒的时延。</p> 
 <ul><li style="margin-left:1.4em;list-style-type:disc;text-indent:0;"><p><span class="kdocs-bold" style="font-weight:bold;">TCP 协议栈处理导致服务器更高的 CPU 消耗 </span></p></li></ul> 
 <p style="">除了时延问题，TCP/IP 网络需要主机 CPU 多次参与到协议栈的内存复制。分布式存储网络规模越大，网络带宽要求越高，CPU 收发数据时的处理负担也就越大，导致 CPU 资源的更高消耗（对于超融合架构，是非常不友好的）。</p> 
 <p style=""></p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:660px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:87.87879%;height:0;"> 
    <img src="https://images2.imgbox.com/df/24/fyQnyX2x_o.png" style="margin-left:;display:block;width:660px;margin-top:-87.87879%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="text-align:center;"><span class="kdocs-fontSize" style="font-size:13pt;">图 2：TCP/IP Socket 通信</span></p> 
 <p style="text-align:null;"></p> 
 <h3 style="text-align:left;">RDMA</h3> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">RDMA 是 Remote Direct Memory Access 的缩写。其中 DMA 是指设备直接读写内存技术（无需经过 CPU）。</span></p> 
 <div class="kdocs-line-container" style="display:flex;justify-content:null;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:1769px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:33.91747%;height:0;"> 
    <img src="https://images2.imgbox.com/02/7a/6SpRUiAg_o.png" style="margin-left:;display:block;width:1769px;margin-top:-33.91747%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="text-align:center;"><span class="kdocs-fontSize" style="font-size:13pt;">图 3：DMA</span></p> 
 <p style="text-align:null;"></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">RDMA 技术的出现，为降低 TCP/IP 网络传输时延和 CPU 资源消耗，提供了一种全新且高效的解决思路。通过直接内存访问技术，数据从一个系统快速移动到远程系统的内存中，无需经过内核网络协议栈，不需要经过中央处理器耗时的处理，最终达到高带宽、低时延和低 CPU 资源占用的效果。</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">目前实现 RDMA 的方案有如下 3 种：</span></p> 
 <div class="kdocs-line-container" style="display:flex;justify-content:null;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:964px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:62.24066%;height:0;"> 
    <img src="https://images2.imgbox.com/8a/f3/Z3WZy0rb_o.png" style="margin-left:;display:block;width:964px;margin-top:-62.24066%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="text-align:center;"><span class="kdocs-fontSize" style="font-size:13pt;">图 4：RDMA 实现方案 （图片来源：SNIA）</span></p> 
 <p style="text-align:null;"></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">InfiniBand（IB）是一种提供了 RDMA 功能的全栈架构，包含了编程接口、二到四层协议、网卡接口和交换机等一整套 RDMA 解决方案。InfiniBand 的编程接口也是 RDMA 编程接口的事实标准，RoCE 和 iWARP 都使用 InfiniBand 的接口进行编程。</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">RoCE（RDMA over Converged Ethernet）和 iWARP（常被解释为 Internet Wide Area RDMA Protocol，这并不准确，RDMA Consortium 专门做出解释 iWARP 并不是缩写），两个技术都是将 InfiniBand 的编程接口封装在以太网进行传输的方案实现。RoCE 分为两个版本，RoCEv1 包含了网络层和传输层的协议，所以不支持路由（更像是过渡协议，应用并不多）；RoCEv2 基于 UDP/IP 协议，具有可路由能力。iWARP 是构建于 TCP 协议之上的。</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">跟 RoCE 协议继承自 Infiniband 不同，iWARP 本身不是直接从 Infiniband 发展而来的。Infiniband 和 RoCE 协议都是基于“Infiniband Architecture Specification”，也就是常说的“IB 规范”。而 iWARP 是自成一派，遵循着一套 IETF 设计的协议标准。虽然遵循着不同的标准，但是 iWARP 的设计思想受到了很多 Infiniband 的影响，并且目前使用同一套编程接口（Verbs*）。这三种协议在概念层面并没有差异。</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;"><span class="kdocs-italic" style="font-style:italic;">* Verb 是 RDMA 语境中对网络适配器功能的一个抽象，每个 Verb 是一个函数，代表了一个 RDMA 的功能，实现发送或接收数据、创建或删除 RDMA 对象等动作。</span></span></p> 
 <p style="text-align:null;"></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">RDMA 需要设备厂商（网卡和交换机）的生态支持，主流网络厂家的协议支持能力如下：</span></p> 
 <div class="kdocs-line-container" style="display:flex;justify-content:null;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:1101px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:31.88011%;height:0;"> 
    <img src="https://images2.imgbox.com/86/55/RX7g0WAF_o.png" style="margin-left:;display:block;width:1101px;margin-top:-31.88011%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="text-align:null;"></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">Infiniband 从协议到软硬件封闭，其性能虽然最优，但成本也最高，因为需要更换全套设备，包括网卡、光缆和交换机等。这对于通用标准化的分布式存储场景并不友好，在 ZBS 选择时首先被排除掉。</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">对于 RoCE 和 iWARP 选择上，虽然 RoCE 在数据重传和拥塞控制上受到 UDP 协议自身的限制，需要无损网络的环境支持，但在综合生态、协议效率和复杂度等多方面因素评估下，SmartX 更加看好 RoCE 未来的发展，在极致的性能诉求下，RoCE 也会比 iWARP 具有更强的潜力。<span class="kdocs-bold" style="font-weight:bold;">当前 ZBS 存储内部数据同步网络采用的是 RoCEv2 的 RDMA 技术路线。</span></span></p> 
 <div class="kdocs-line-container" style="display:flex;justify-content:null;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:1654px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:35.066505%;height:0;"> 
    <img src="https://images2.imgbox.com/d3/95/qm89uJkZ_o.png" style="margin-left:;display:block;width:1654px;margin-top:-35.066505%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="text-align:center;"><span class="kdocs-fontSize" style="font-size:13pt;">图 5：ZBS RDMA RoCEv2</span></p> 
 <p style="text-align:null;"></p> 
 <h3 style="text-align:left;">性能验证数据</h3> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">为了使测试数据有更直观的对比性（RDMA vs TCP/IP），将控制测试环境严格一致性，包括硬件配置、系统版本以及相关软件版本，唯一变量仅为开启/关闭存储内部数据同步 RDMA 能力，基于此，测试集群在两种状态下的性能表现。</span></p> 
 <p style="text-align:null;"></p> 
 <h4 style="text-align:left;"><span class="kdocs-bold" style="font-weight:bold;">环境信息</span></h4> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">存储集群，由 3 节点组成，安装 SMTX OS 5.0，分层存储结构，所有存储节点的硬件配置相同，节点环境信息如下：</span></p> 
 <div class="kdocs-line-container" style="display:flex;justify-content:null;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:1106px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:45.479202%;height:0;"> 
    <img src="https://images2.imgbox.com/eb/57/msOUFSVr_o.png" style="margin-left:;display:block;width:1106px;margin-top:-45.479202%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;"> </span></p> 
 <h4 style="text-align:left;"><span class="kdocs-bold" style="font-weight:bold;">性能数据</span></h4> 
 <p style="text-align:null;"></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;"><span class="kdocs-bold" style="font-weight:bold;">在相同的测试环境和测试方法下，分别使用 RDMA 和 TCP/IP 协议进行性能验证。</span>为了更好地观测读写 I/O 跨节点的性能表现（ZBS 分布式存储默认具有数据本地化特点，对读 I/O 模型有明显优化作用），本次测试基于 Data Channel 平面（ZBS 内部的 RPC 通道，用于节点间收发数请求）。本测试仅用于评估网络性能差异，I/O 读写操作并不落盘。</span></p> 
 <p style="text-align:null;"></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;"><span class="kdocs-bold" style="font-weight:bold;">性能对比数据</span></span></p> 
 <div class="kdocs-line-container" style="display:flex;justify-content:null;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:1091px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:45.27956%;height:0;"> 
    <img src="https://images2.imgbox.com/c7/00/HgnPCDnm_o.png" style="margin-left:;display:block;width:1091px;margin-top:-45.27956%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <h4 style="text-align:left;"><span class="kdocs-bold" style="font-weight:bold;">测试结论</span></h4> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">通过以上基准测试数据，可以看出，<span class="kdocs-bold" style="font-weight:bold;">相同软硬件环境以及测试方法下，使用 RDMA 作为存储内部数据同步协议，可以取得更优的 I/O 性能输出。</span>其表现为更高的 4K 随机 IOPS 和更低的延时，以及在 256K 顺序读写场景，充分释放网络带宽（25GbE）条件，提供更高的数据吞吐表现。</span></p> 
 <p style="text-align:null;"></p> 
 <h3 style="text-align:left;">总结</h3> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">通过本篇文章的理论介绍和客观的性能测试数据，希望读者能够对于 RDMA 协议有了更加全面的了解。RDMA 对于数据跨网络通信性能的优化，已经应用于很多企业场景中，分布式存储作为其中一个重要场景，借助 RDMA 实现了存储内部数据同步效率的提升，进而为更高工作负载需求的业务应用提供了更好的存储性能表现。</span></p> 
 <p style="text-align:null;"></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">参考文章：</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">1. RDMA over Converged Ethernet. Wikipedia.</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">https://en.wikipedia.org/wiki/RDMA_over_Converged_Ethernet</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">2. How Ethernet RDMA Protocols iWARP and RoCE Support NVMe over Fabrics.</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">https://www.snia.org/sites/default/files/ESF/How_Ethernet_RDMA_Protocols_Support_NVMe_over_Fabrics_Final.pdf</span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;"> </span></p> 
 <p style="text-align:null;"><span class="kdocs-fontSize" style="font-size:13pt;">点击了解 </span><a class="kdocs-link" style="color:#0A6CFF;" href="https://www.smartx.com/resource/doc/distributed-storage/?utm_source=onwed&amp;utm_medium=article&amp;utm_campaign=content&amp;utm_id=zbs-rdma" rel="nofollow noopener noreferrer" target="_blank"><span class="kdocs-fontSize" style="font-size:13pt;">SMTX ZBS 更多产品特性与技术实现亮点</span></a><span class="kdocs-fontSize" style="font-size:13pt;">。</span></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e2fc3f7282844c608b8976fd6fc76154/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java：记录一下第一次面试经历（新希望六和）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5d173bf7237d56a085e48d688d7ed5a9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">遗传算法(Genetic Algorithm)之deap学习笔记(一): 基础概念</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>