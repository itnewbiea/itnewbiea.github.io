<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【神经网络】用python从底层实现一个卷积神经网络 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【神经网络】用python从底层实现一个卷积神经网络" />
<meta property="og:description" content="1. 背景介绍：卷积神经网络(CNN) 卷积神经网络（ConvolutionalNeural Network,CNN）是人工神经网络的一种。当前已经成为图像和语音识别领域有十分广泛的应用，特别是在识别位移、缩放及其他形式扭曲不变性的二维图形方面有十分优异的表现，已经成为一个十分重要的研究方向。
关于CNN的详细解释可以看这篇论文1。
接下来我将实现一个卷积神经网络，用来识别CIFAR-10数据集中的图片，我会给出每一层的前向和后向推导，以及python底层实现，如有什么错误，欢迎指出。
2. 矩阵知识 矩阵部分知识可以参考一文搞定BP神经网络——从原理到应用（原理篇）
3.网络框架 这个是我们所要搭建的网络，这里讲卷积，池化，激活层，flatten层，全连接层，softmax层都看做独立的层来分析。 因为神经网络的批处理和权重共享特点，这里我设置训练网络是输入每个batch=10的幅图片。这10张图片正向传播时是数据互不干扰的，反向传播时数据共同加和作用残差传递。所以我们可以以单幅图像为参考理解整个网络，在反向传播涉及批处理时我会提出。 如图所示，我只列出了一幅图像的RGB3个通道作为输入，图像像素大小是32*32，所以输入层是[10 3 32 32]的python-list格式。
4.从数据处理开始讲起 如果你翻阅我的博客，你会发现，我是从7月19日写的“在Windows10下安装anaconda”，之后陆陆续续更新了十几篇文章，而这十几篇文章源于一个目的：从0编写一个底层的CNN网络并可视化每层的map特征。为什么要做这个事，一方面是项目要求，另一方面是我从去年开始入门深度学习，陆陆续续看了几十篇深度学习方面的文献，跑了十几个文献开源代码，当我开始有了新的思路想要仿照着别人的代码实现自己的深度学习网络时，却怎么也不得法门。所以我需要从原理层面和代码层面从头推导一边。 我的底层CNN代码时基于python3&#43;和numpy的，所以你可以在Ubuntu【推荐】或者Windows系统上安装anaconda，然后创建一个python3&#43;的环境。 数据是CIRAF-10，数据的读取请看【cifar-10】：数据读取代码中出现ImportError: cannot import name ‘imread’当然，为什么会写一个报错博客，是因为CIFAR-10数据python提取代码需要安装依赖。 下面是数据去均值，请看我之前博客：【数据预处理】：图像去均值：image mean 和 pixel mean 5.正向传播 正向传播有卷积，池化，激活，全连接等，这里我们按照在网络框架的出场顺序依次介绍，注意这里我把每个网络层的正向传播与反向传播写到一篇博客里了，所有，在看正向传播的时候可以将反向传播部分跳过： 正向传播第一站：【Python实现卷积神经网络】：卷积层的正向传播与反向传播&#43;python实现代码 正向传播第二站：【Python实现卷积神经网络】：池化层的正向传播与反向传播&#43;python实现代码 正向传播第三站：【Python实现卷积神经网络】：激活层RELU的正向与反向传播&#43;python实现代码 正向传播第四站：【Python实现卷积神经网络】：全连接层的正向传播与反向传播&#43;python实现代码 正向传播第五站：【Python实现卷积神经网络】：神经网络的Loss函数：Softmax&#43;Cross Entropy前向传播原理&#43;python实现代码
6.反向传播 反向传播是从softmax开始的，所以，我们先要了解softmax的传播原理，然后从softmax开始反向传播。由于反向传播是倒着传播的，所以我们按照网络框架的反向顺序开始讲解： 反向传播第一站是：【交叉熵】：神经网络的Loss函数编写：Softmax&#43;Cross Entropy 反向传播第二站是：【反向传播】：从softmax层开始反向传播 反向传播第三站是：【反向传播】：全连接层的正向传播与反向传播 反向传播第四站是：【反向传播】：激活层RELU的正向与反向传播 反向传播第五站是：【反向传播】：池化层的正向传播与反向传播 反向传播第六站是：【反向传播】：卷积层的正向传播与反向传播
下边儿是我在组织本博客时的想法，不构成主要内容。
我的博客特点：其它人讲每层的原理的时候只是单幅图像来介绍公式原理，但是应用到代码时是batch处理的，所以前向传播，反向传播，矩阵的加减，求导等和单幅图像不一样。我讲每层原理时，先从简单的一幅图像开始，然后公式均扩展到batch=10的数组上边处理。这样，你再写代码时就会理解它的代码为什么这样写了。
每层主要讲几部分 1.该层作用 2.数学原理及正反向公式 3.批处理公式 4.在公式落实到代码时的步骤 5.在代码中如何将层写成函数
训练网络代码 在我的github中，分步代码讲解在前向传播反向传播讲解的时候附上，总的代码在我的github上边儿
待更新： 1.增加loss下降可视化：https://blog.csdn.net/sinat_28113749/article/details/79079832 2.
会随着我的想法的更新和新的网络的出现而增加。。。
测试网络代码 。。。
。。。统一的残差更新公式【这里我们先分层计算残差公式，然后最后总结推导出 这个统一的残差公式】
【未完待续】
参考：23 https://www.cnblogs.com/charlotte77/p/7783261.html https://www.cnblogs.com/pinard/p/6494810.html
http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf ↩https://blog." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/28ae67dec4f1a85fab579b7722d7153c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-07-30T19:32:01+08:00" />
<meta property="article:modified_time" content="2018-07-30T19:32:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【神经网络】用python从底层实现一个卷积神经网络</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2 id="1-背景介绍卷积神经网络cnn">1. 背景介绍：卷积神经网络(CNN)</h2> 
<p>　　卷积神经网络（ConvolutionalNeural Network,CNN）是人工神经网络的一种。当前已经成为图像和语音识别领域有十分广泛的应用，特别是在识别位移、缩放及其他形式扭曲不变性的二维图形方面有十分优异的表现，已经成为一个十分重要的研究方向。</p> 
<p>　　关于CNN的详细解释可以看这篇论文<a href="#fn:1" rel="nofollow" id="fnref:1" title="See footnote" class="footnote">1</a>。</p> 
<p>　　接下来我将实现一个卷积神经网络，用来识别CIFAR-10数据集中的图片，我会给出每一层的前向和后向推导，以及python底层实现，如有什么错误，欢迎指出。</p> 
<h2 id="2-矩阵知识">2. 矩阵知识</h2> 
<p>　　矩阵部分知识可以参考<a href="https://blog.csdn.net/u014303046/article/details/78200010">一文搞定BP神经网络——从原理到应用（原理篇）</a></p> 
<h2 id="3网络框架">3.网络框架</h2> 
<p><img src="https://images2.imgbox.com/af/44/50AFrSbe_o.png" alt="这里写图片描述" title=""> <br> 　　这个是我们所要搭建的网络，这里讲卷积，池化，激活层，flatten层，全连接层，softmax层都看做独立的层来分析。 <br> 　　 <br> 　　因为神经网络的批处理和权重共享特点，这里我设置训练网络是输入每个batch=10的幅图片。<strong>这10张图片正向传播时是数据互不干扰的，反向传播时数据共同加和作用残差传递。</strong>所以我们可以以单幅图像为参考理解整个网络，在反向传播涉及批处理时我会提出。 <br> 　　 <br> 　　如图所示，我只列出了一幅图像的RGB3个通道作为输入，图像像素大小是32*32，所以输入层是[10 3 32 32]的python-list格式。</p> 
<h2 id="4从数据处理开始讲起">4.从数据处理开始讲起</h2> 
<p>　　如果你翻阅我的博客，你会发现，我是从7月19日写的<a href="https://blog.csdn.net/weixin_37251044/article/details/81118851">“在Windows10下安装anaconda”</a>，之后陆陆续续更新了十几篇文章，而这十几篇文章源于一个目的：<strong>从0编写一个底层的CNN网络并可视化每层的map特征</strong>。为什么要做这个事，一方面是项目要求，另一方面是我从去年开始入门深度学习，陆陆续续看了几十篇深度学习方面的文献，跑了十几个文献开源代码，当我开始有了新的思路想要仿照着别人的代码实现自己的深度学习网络时，却怎么也不得法门。所以我需要从原理层面和代码层面从头推导一边。 <br> 　　 <br> 　　我的底层CNN代码时基于python3+和numpy的，所以你可以在Ubuntu【推荐】或者Windows系统上安装anaconda，然后创建一个python3+的环境。 <br> 　　 <br> 　　数据是CIRAF-10，数据的读取请看<a href="https://blog.csdn.net/weixin_37251044/article/details/81131701">【cifar-10】：数据读取代码中出现ImportError: cannot import name ‘imread’</a>当然，为什么会写一个报错博客，是因为CIFAR-10数据python提取代码需要安装依赖。 <br> 　　 <br> 　　下面是数据去均值，请看我之前博客：<a href="https://blog.csdn.net/weixin_37251044/article/details/81157344">【数据预处理】：图像去均值：image mean 和 pixel mean</a> <br> 　　</p> 
<h2 id="5正向传播">5.正向传播</h2> 
<p>　　 <br> 　　正向传播有卷积，池化，激活，全连接等，这里我们按照在网络框架的出场顺序依次介绍，注意这里我把每个网络层的正向传播与反向传播写到一篇博客里了，所有，在看正向传播的时候可以将反向传播部分跳过： <br> 　　 <br> 　　正向传播第一站：<a href="https://blog.csdn.net/weixin_37251044/article/details/81349287">【Python实现卷积神经网络】：卷积层的正向传播与反向传播+python实现代码</a> <br> 　　正向传播第二站：<a href="https://blog.csdn.net/weixin_37251044/article/details/81328494">【Python实现卷积神经网络】：池化层的正向传播与反向传播+python实现代码</a> <br> 　　正向传播第三站：<a href="https://blog.csdn.net/weixin_37251044/article/details/81321515">【Python实现卷积神经网络】：激活层RELU的正向与反向传播+python实现代码</a> <br> 　　正向传播第四站：<a href="https://blog.csdn.net/weixin_37251044/article/details/81274479">【Python实现卷积神经网络】：全连接层的正向传播与反向传播+python实现代码</a> <br> 　　正向传播第五站：<a href="https://blog.csdn.net/weixin_37251044/article/details/81180449">【Python实现卷积神经网络】：神经网络的Loss函数：Softmax+Cross Entropy前向传播原理+python实现代码</a></p> 
<h2 id="6反向传播">6.反向传播</h2> 
<p>反向传播是从softmax开始的，所以，我们先要了解softmax的传播原理，然后从softmax开始反向传播。由于反向传播是倒着传播的，所以我们按照网络框架的反向顺序开始讲解： <br> 　　反向传播第一站是：<a href="https://blog.csdn.net/weixin_37251044/article/details/81180449">【交叉熵】：神经网络的Loss函数编写：Softmax+Cross Entropy</a> <br> 　　反向传播第二站是：<a href="https://blog.csdn.net/weixin_37251044/article/details/81206236">【反向传播】：从softmax层开始反向传播</a> <br> 　　反向传播第三站是：<a href="https://blog.csdn.net/weixin_37251044/article/details/81274479">【反向传播】：全连接层的正向传播与反向传播</a> <br> 　　反向传播第四站是：<a href="https://blog.csdn.net/weixin_37251044/article/details/81321515">【反向传播】：激活层RELU的正向与反向传播</a> <br> 　　反向传播第五站是：<a href="https://blog.csdn.net/weixin_37251044/article/details/81328494">【反向传播】：池化层的正向传播与反向传播</a> <br> 　　反向传播第六站是：<a href="https://blog.csdn.net/weixin_37251044/article/details/81349287">【反向传播】：卷积层的正向传播与反向传播</a></p> 
<hr> 
<p>下边儿是我在组织本博客时的想法，不构成主要内容。</p> 
<p><strong>我的博客特点：</strong>其它人讲每层的原理的时候只是单幅图像来介绍公式原理，但是应用到代码时是batch处理的，所以前向传播，反向传播，矩阵的加减，求导等和单幅图像不一样。我讲每层原理时，先从简单的一幅图像开始，然后公式均扩展到batch=10的数组上边处理。这样，你再写代码时就会理解它的代码为什么这样写了。</p> 
<h2 id="每层主要讲几部分">每层主要讲几部分</h2> 
<p>1.该层作用 <br> 2.数学原理及正反向公式 <br> 3.批处理公式 <br> 4.在公式落实到代码时的步骤 <br> 5.在代码中如何将层写成函数</p> 
<h2 id="训练网络代码">训练网络代码</h2> 
<p>在我的github中，分步代码讲解在前向传播反向传播讲解的时候附上，总的代码在我的github上边儿</p> 
<p>待更新： <br> 1.增加loss下降可视化：<a href="https://blog.csdn.net/sinat_28113749/article/details/79079832">https://blog.csdn.net/sinat_28113749/article/details/79079832</a> <br> 2.</p> 
<p>会随着我的想法的更新和新的网络的出现而增加。。。</p> 
<h2 id="测试网络代码">测试网络代码</h2> 
<p>。。。</p> 
<p>。。。统一的残差更新公式【这里我们先分层计算残差公式，然后最后总结推导出 这个统一的残差公式】</p> 
<p>【未完待续】</p> 
<p>参考：<a href="#fn:2" rel="nofollow" id="fnref:2" title="See footnote" class="footnote">2</a><a href="#fn:3" rel="nofollow" id="fnref:3" title="See footnote" class="footnote">3</a> <br> <a href="https://www.cnblogs.com/charlotte77/p/7783261.html" rel="nofollow">https://www.cnblogs.com/charlotte77/p/7783261.html</a> <br> <a href="https://www.cnblogs.com/pinard/p/6494810.html" rel="nofollow">https://www.cnblogs.com/pinard/p/6494810.html</a></p> 
<div class="footnotes"> 
 <hr> 
 <ol><li id="fn:1"><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" rel="nofollow">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a> <a href="#fnref:1" rel="nofollow" title="Return to article" class="reversefootnote">↩</a></li><li id="fn:2"><a href="https://blog.csdn.net/l691899397/article/details/52233454">https://blog.csdn.net/l691899397/article/details/52233454</a> <a href="#fnref:2" rel="nofollow" title="Return to article" class="reversefootnote">↩</a></li><li id="fn:3"><a href="https://blog.csdn.net/u010866505/article/details/77857394">https://blog.csdn.net/u010866505/article/details/77857394</a> <a href="#fnref:3" rel="nofollow" title="Return to article" class="reversefootnote">↩</a></li></ol> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c754846e259553aadaeedd7c5a995f77/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">一种快速的自适应二值化算法-wallner</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ba4581dd20061e94eb221dc5de41d099/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">TP5运行workerman的操作</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>