<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Policy gradient(策略梯度详解) - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Policy gradient(策略梯度详解)" />
<meta property="og:description" content="文章目录 策略梯度基本知识什么是策略梯度？强化学习案例 策略梯度公式详解如何使你的损失函数更好增加一个基准为每一个action分配不同的权重 策略梯度基本知识 什么是策略梯度？ 直接根据状态输出动作或者动作的概率。那么怎么输出呢，最简单的就是使用神经网络啦！我们使用神经网络输入当前的状态，网络就可以输出我们在这个状态下采取每个动作的概率，那么网络应该如何训练来实现最终的收敛呢？我们之前在训练神经网络时，使用最多的方法就是反向传播算法，我们需要一个误差函数，通过梯度下降来使我们的损失最小。但对于强化学习来说，我们不知道动作的正确与否，只能通过奖励值来判断这个动作的相对好坏。基于上面的想法，我们有个非常简单的想法：如果一个动作得到的reward多，那么我们就使其出现的概率增加，如果一个动作得到的reward少，我们就使其出现的概率减小。
强化学习案例 在强化学习中，环境与rewardnfunction你是不能控制的，玩video game时，环境就是你的游戏机，然后reword function就是得分规则，你所能改的只有actor。下围棋也类似。
神经网络的输入是机器观察到的场景转化的向量或者矩阵，输出是每一个行为的概率。像这种彩图我们一般用CNN，然后取代了最早的查表方式，以前的actor是table，然后遇到某张图片就去table里找对应的行为，用在下棋里还可能穷举，如果在自动驾驶领域，这图片是无法用表存完的。可能你之前没有给神经网络看过某张图，但是它还是能得出比较靠谱的结果，所以他具有generalization的特性。
机器先观察画面，然后做出了一个action，向右移动，这个action的奖励是0，然后机器又观察画面，做出了开火的action，然后观察画面，发现有外星人被击落，然后获得reward。
从游戏开始到游戏结束被称为一个episode，那么机器就是要找到每一个episode中，谁的reward总和最大，然后总和最大的reward的episode所包含的各个action是比较好的！
策略梯度公式详解 那么这个Actor的损失函数该怎么定义呢？给定一个actor，记为Π，然后下表θ代表该神经网络的参数，然后input的s就是机器所看到的场景，然后让机器实际去玩一下这个游戏，然后我们要求总的Total reward最大，我们就要将所有的r加起来。但由于即使是使用相同的actor，每一次的总的奖励也可能不同，于是我们就求总的奖励的期望即可。
┏是场景，行动，奖励所组成的向量，如下图所示，比如说玩游戏，一个┏就代表了机器看到了第一个画面，做了某个行为，然后得到什么奖励，然后看到第二个画面，做了某个行为，然后得到什么奖励，以此类推，循环往复，直到游戏结束。然后每一个┏都有可能被经历。当你选择了某一个actor,也就是选择了某一个神经网络，那么会使某一些┏容易出现，某一些不容易出现。那么Rθ的期望就等于每一次游戏过程┏的奖励R与该过程┏出现的几率的乘积之和。那么穷举所有的┏显然不可能，那么我们就玩N场游戏，相当于N个训练数据。
下面的这图看起来不就是一个巨大的network吗？然后环境和reward是无法改变的，就相当于下棋的时候机器无法控制对手的操作，机器也无法改变奖励制度，唯一能改变的就是自己的参数，去适应环境，来使奖励最大化。
下面我们来看看P(┏|θ)打开是什么，画黄线的部分与你的actor无关，取决于外部环境，也就是游戏，然后红线部分与你的actor有关。
求最大你会想到梯度上升的求法，这里的梯度上升是策略梯度的一部分。
那么对Rθ期望求微分具体应该怎么做呢？如下图，R(┏)肯定不可微，但是没关系，它的表达式里没有θ，所以我只需对P(┏|θ)求导数，然后我们要对其做一下变换，巧用log！！然后画红框的两部分之前推导过可以化为1/N，然后整个表达式就可以化为一个近似的表达式。使用Πθ这个神经网络去玩N次游戏，可以得到N个┏
利用上述打开的结论，然后我们利用对数的性质化简，如下图所示：然后我们对θ求导！忽视与θ无关的项。然后得出来一个等式。
然后我们将Rθ得期望求导之后得算式写出来，如下图：其中，log里面的那个p所代表的意思就是当前第n回玩游戏时，t时刻机器所看到的画面的情况下做出a行为的概率，R(┏n)是指第n回完游戏时的总的奖励，然后利用上述得到的结果进行梯度上升。注意：如果我将R(┏n)换成rtn，也就是将第n回玩游戏的总的奖励换成第n回玩游戏时t时刻的奖励，那么会发生什么后果？？如果在刚刚那个射击游戏里，只有开火能得到奖励，那么就会导致机器只会开火。
如果在某一次玩游戏时，机器在看到某个场景时，采取了一个行动，然后总的奖励是正的，那么机器就会自己去增加看到这个场景下做出该行动的概率。
下面我们来看看更新model的过程，先是给了一个actor，然后给actor一个┏，然后算出奖励总和，然后用梯度上升，更新θ，然后再将另一个┏传给actor，循环往复执行。
如何使你的损失函数更好 增加一个基准 那么这里有一个问题，我们看下图，ideal case的第一张图，a和c的会使总的奖励变多，那么机器会倾向于执行a和c的操作，所以a和c的执行几率就变大了，相对的b的几率就减少了。然后我们再看看sampling那一行，b和c可能使我的总的奖励一直是正的，那么机器根本就不知道a的情况，万一a的操作更好呢？？机器只会去学更positive的，b和c的几率也会越来越大，a只会越来越小。这时，我们需要引入一个baseline，如下图的b，我们将总的奖励减去一个b值，也就是某一步的奖励一定要达到某一个标准我才能说它好，否则就是不好。通常我们可以将这个b设为与R(┏)的期望接近的值。
为每一个action分配不同的权重 比如下面是一个简单的游戏，就三步，第一组(s,a)得到的奖励是&#43;5，第二组是0，第三组是-2，然后最后的奖励是&#43;3,如果我们用之前的那个损失函数，那么就默认了每一个(s,a)的组合的权重都是&#43;3,这显然是不靠谱的，虽然总的奖励是正的，但是明显里面有些组合不靠谱，我们就可以给那些不靠谱的组合负的权重。于是我们将R(┏n)换成下图的样子。这样就可以对每一个组合的权重加以区分。
同时，我们再增加一个衰减因子γ，意味着随着时间推移，组合越来越多，那么前面的组合对很后面的组合的影响就越来越小。然后我们将红框框住的那部分重新命名一个函数，叫Advantage function" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/bdf79fd12a205a7cdf1a11214595eb83/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-11T15:07:47+08:00" />
<meta property="article:modified_time" content="2020-10-11T15:07:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Policy gradient(策略梯度详解)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#_1" rel="nofollow">策略梯度基本知识</a></li><li><ul><li><a href="#_2" rel="nofollow">什么是策略梯度？</a></li><li><a href="#_7" rel="nofollow">强化学习案例</a></li></ul> 
   </li><li><a href="#_18" rel="nofollow">策略梯度公式详解</a></li><li><a href="#_48" rel="nofollow">如何使你的损失函数更好</a></li><li><ul><li><a href="#_49" rel="nofollow">增加一个基准</a></li><li><a href="#action_52" rel="nofollow">为每一个action分配不同的权重</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_1"></a>策略梯度基本知识</h3> 
<h4><a id="_2"></a>什么是策略梯度？</h4> 
<p><em>直接根据状态输出动作或者动作的概率。那么怎么输出呢，最简单的就是使用神经网络啦！我们使用神经网络输入当前的状态，网络就可以输出我们在这个状态下采取每个动作的概率，那么网络应该如何训练来实现最终的收敛呢？我们之前在训练神经网络时，使用最多的方法就是反向传播算法，我们需要一个误差函数，通过梯度下降来使我们的损失最小。但对于强化学习来说，我们不知道动作的正确与否，只能通过奖励值来判断这个动作的相对好坏。基于上面的想法，我们有个非常简单的想法：如果一个动作得到的reward多，那么我们就使其出现的概率增加，如果一个动作得到的reward少，我们就使其出现的概率减小。</em></p> 
<h4><a id="_7"></a>强化学习案例</h4> 
<p>在强化学习中，环境与rewardnfunction你是不能控制的，玩video game时，环境就是你的游戏机，然后reword function就是得分规则，你所能改的只有actor。下围棋也类似。<br> <img src="https://images2.imgbox.com/b6/9d/DrB7PknH_o.png" alt="在这里插入图片描述"><br> 神经网络的输入是机器观察到的场景转化的向量或者矩阵，输出是每一个行为的概率。像这种彩图我们一般用CNN，然后取代了最早的查表方式，以前的actor是table，然后遇到某张图片就去table里找对应的行为，用在下棋里还可能穷举，如果在自动驾驶领域，这图片是无法用表存完的。可能你之前没有给神经网络看过某张图，但是它还是能得出比较靠谱的结果，所以他具有generalization的特性。<br> <img src="https://images2.imgbox.com/4c/47/2HCc0wpB_o.png" alt="在这里插入图片描述"><br> 机器先观察画面，然后做出了一个action，向右移动，这个action的奖励是0，然后机器又观察画面，做出了开火的action，然后观察画面，发现有外星人被击落，然后获得reward。<br> <img src="https://images2.imgbox.com/52/2e/GWBRMruG_o.png" alt="在这里插入图片描述"><br> 从游戏开始到游戏结束被称为一个episode，那么机器就是要找到每一个episode中，谁的reward总和最大，然后总和最大的reward的episode所包含的各个action是比较好的！</p> 
<p><img src="https://images2.imgbox.com/54/60/C8fDIc3P_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_18"></a>策略梯度公式详解</h3> 
<p>那么这个Actor的损失函数该怎么定义呢？给定一个actor，记为Π，然后下表θ代表该神经网络的参数，然后input的s就是机器所看到的场景，然后让机器实际去玩一下这个游戏，然后我们要求总的Total reward最大，我们就要将所有的r加起来。但由于即使是使用相同的actor，每一次的总的奖励也可能不同，于是我们就求总的奖励的期望即可。<br> <img src="https://images2.imgbox.com/f5/71/o3uWxowt_o.png" alt="在这里插入图片描述"><br> ┏是场景，行动，奖励所组成的向量，如下图所示，比如说玩游戏，一个┏就代表了机器看到了第一个画面，做了某个行为，然后得到什么奖励，然后看到第二个画面，做了某个行为，然后得到什么奖励，以此类推，循环往复，直到游戏结束。然后每一个┏都有可能被经历。当你选择了某一个actor,也就是选择了某一个神经网络，那么会使某一些┏容易出现，某一些不容易出现。那么Rθ的期望就等于每一次游戏过程┏的奖励R与该过程┏出现的几率的乘积之和。那么穷举所有的┏显然不可能，那么我们就玩N场游戏，相当于N个训练数据。<br> <img src="https://images2.imgbox.com/11/96/0DTndBdc_o.png" alt="在这里插入图片描述"></p> 
<p>下面的这图看起来不就是一个巨大的network吗？然后环境和reward是无法改变的，就相当于下棋的时候机器无法控制对手的操作，机器也无法改变奖励制度，唯一能改变的就是自己的参数，去适应环境，来使奖励最大化。<br> <img src="https://images2.imgbox.com/67/cf/FXL0g7Uf_o.png" alt="在这里插入图片描述"><br> 下面我们来看看P(┏|θ)打开是什么，画黄线的部分与你的actor无关，取决于外部环境，也就是游戏，然后红线部分与你的actor有关。<br> <img src="https://images2.imgbox.com/80/ca/jeCdVJt6_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/0a/1c/eaP1ZjZt_o.png" alt="在这里插入图片描述"></p> 
<p><strong>求最大你会想到梯度上升的求法，这里的梯度上升是策略梯度的一部分。</strong><br> <img src="https://images2.imgbox.com/e4/d8/7Es7wS5h_o.png" alt="在这里插入图片描述"></p> 
<p>那么对Rθ期望求微分具体应该怎么做呢？如下图，R(┏)肯定不可微，但是没关系，它的表达式里没有θ，所以我只需对P(┏|θ)求导数，然后我们要对其做一下变换，巧用log！！然后画红框的两部分之前推导过可以化为1/N，然后整个表达式就可以化为一个近似的表达式。使用Πθ这个神经网络去玩N次游戏，可以得到N个┏<br> <img src="https://images2.imgbox.com/c9/7a/cfOUO83x_o.png" alt="在这里插入图片描述"><br> 利用上述打开的结论，然后我们利用对数的性质化简，如下图所示：然后我们对θ求导！忽视与θ无关的项。然后得出来一个等式。<br> <img src="https://images2.imgbox.com/e4/ec/pdwG3vC3_o.png" alt="在这里插入图片描述"><br> 然后我们将Rθ得期望求导之后得算式写出来，如下图：其中，log里面的那个p所代表的意思就是当前第n回玩游戏时，t时刻机器所看到的画面的情况下做出a行为的概率，R(┏n)是指第n回完游戏时的总的奖励，然后利用上述得到的结果进行梯度上升。<code>注意：如果我将R(┏n)换成rtn，也就是将第n回玩游戏的总的奖励换成第n回玩游戏时t时刻的奖励，那么会发生什么后果？？如果在刚刚那个射击游戏里，只有开火能得到奖励，那么就会导致机器只会开火。</code><br> 如果在某一次玩游戏时，机器在看到某个场景时，采取了一个行动，然后总的奖励是正的，那么机器就会自己去增加看到这个场景下做出该行动的概率。<br> <img src="https://images2.imgbox.com/01/df/YNNZxcg7_o.png" alt="在这里插入图片描述"></p> 
<p>下面我们来看看更新model的过程，先是给了一个actor，然后给actor一个┏，然后算出奖励总和，然后用梯度上升，更新θ，然后再将另一个┏传给actor，循环往复执行。<br> <img src="https://images2.imgbox.com/b7/b1/qEd10FAU_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_48"></a>如何使你的损失函数更好</h3> 
<h4><a id="_49"></a>增加一个基准</h4> 
<p>那么这里有一个问题，我们看下图，ideal case的第一张图，a和c的会使总的奖励变多，那么机器会倾向于执行a和c的操作，所以a和c的执行几率就变大了，相对的b的几率就减少了。然后我们再看看sampling那一行，b和c可能使我的总的奖励一直是正的，那么机器根本就不知道a的情况，万一a的操作更好呢？？机器只会去学更positive的，b和c的几率也会越来越大，a只会越来越小。这时，我们需要引入一个baseline，如下图的b，我们将总的奖励减去一个b值，也就是某一步的奖励一定要达到某一个标准我才能说它好，否则就是不好。通常我们可以将这个b设为与R(┏)的期望接近的值。<br> <img src="https://images2.imgbox.com/37/c8/Ki1KMIyb_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="action_52"></a>为每一个action分配不同的权重</h4> 
<p>比如下面是一个简单的游戏，就三步，第一组(s,a)得到的奖励是+5，第二组是0，第三组是-2，然后最后的奖励是+3,如果我们用之前的那个损失函数，那么就默认了每一个(s,a)的组合的权重都是+3,这显然是不靠谱的，虽然总的奖励是正的，但是明显里面有些组合不靠谱，我们就可以给那些不靠谱的组合负的权重。于是我们将R(┏n)换成下图的样子。这样就可以对每一个组合的权重加以区分。<br> <img src="https://images2.imgbox.com/b7/59/8AV3Bq0d_o.png" alt="在这里插入图片描述"><br> 同时，我们再增加一个衰减因子γ，意味着随着时间推移，组合越来越多，那么前面的组合对很后面的组合的影响就越来越小。然后我们将红框框住的那部分重新命名一个函数，叫Advantage function<br> <img src="https://images2.imgbox.com/c9/2b/6GcXPOZP_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8ab5e363f84f3584ee8790ffcf7a4673/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ADC、DMA、EXTI、定时器、NVIC等片上外设篇</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c91272b3a5b291a5823c9a680bed2639/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">小论文正文内容双栏，将脚注只分布在左侧栏设置方法：</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>