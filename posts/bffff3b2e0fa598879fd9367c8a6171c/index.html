<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习入门（二十九）卷积神经网络——VGG - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习入门（二十九）卷积神经网络——VGG" />
<meta property="og:description" content="深度学习入门（二十九）卷积神经网络——VGG 前言卷积神经网络——VGG课件VGGVGG块VGG架构进度总结 教材1 VGG块2 VGG网络3 训练模型4 小结参考文献 前言 核心内容来自博客链接1博客连接2希望大家多多支持作者
本文记录用，防止遗忘
卷积神经网络——VGG 课件 使用块的网络VGG
VGG AlexNet比LeNet更深更大来得到更好的精度能不能更深和更大?
选项更多的全连接层(太贵)更多的卷积层将卷积层组合成块
VGG块 深VS宽？
5×5卷积3×3卷积深但窄效果更好 VGG块 3×3卷积（填充1）（n层m通道）2×2最大池化层（步幅2）
VGG架构 多个VGG块后接全连接层 不同次数的重复块得到不同的架构VGG-16，VGG-19 进度 LeNet (1995)
2卷积＋池化层 2全连接层 AlexNet
更大更深 ReLu, Dropout,数据增强 VGG
更大更深的AlexNet(重复的VGG块)
GluonCV Model Zoo https://cv.gluon.ai/model_zoo/classification.html
总结 1、VGG使用可重复使用的卷积块来构建深度卷积神经网络
2、不同的卷积块个数和超参数可以得到不同复杂度的变种
教材 虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 在下面的几节中，我们将介绍一些常用于设计深层神经网络的启发式概念。
与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。
使用块的想法首先出现在牛津大学的视觉几何组（visualgeometry group）的VGG网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。
1 VGG块 经典卷积神经网络的基本组成部分是下面的这个序列：
1、带填充以保持分辨率的卷积层；
2、非线性激活函数，如ReLU；
3、汇聚层，如最大汇聚层。
对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。
而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的VGG论文中，作者使用了带有卷积核、填充为1（保持高度和宽度）的卷积层，和带有汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层。在下面的代码中，我们定义了一个名为vgg_block的函数来实现一个VGG块。
该函数有三个参数，分别对应于卷积层的数量num_convs、输入通道的数量in_channels 和输出通道的数量out_channels.
import torch from torch import nn from d2l import torch as d2l def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/bffff3b2e0fa598879fd9367c8a6171c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-09T08:50:59+08:00" />
<meta property="article:modified_time" content="2022-11-09T08:50:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习入门（二十九）卷积神经网络——VGG</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>深度学习入门（二十九）卷积神经网络——VGG</h4> 
 <ul><li><a href="#_1" rel="nofollow">前言</a></li><li><a href="#VGG_4" rel="nofollow">卷积神经网络——VGG</a></li><li><ul><li><a href="#_5" rel="nofollow">课件</a></li><li><ul><li><a href="#VGG_8" rel="nofollow">VGG</a></li><li><a href="#VGG_19" rel="nofollow">VGG块</a></li><li><a href="#VGG_29" rel="nofollow">VGG架构</a></li><li><a href="#_35" rel="nofollow">进度</a></li><li><a href="#_55" rel="nofollow">总结</a></li></ul> 
   </li><li><a href="#_59" rel="nofollow">教材</a></li><li><ul><li><a href="#1_VGG_65" rel="nofollow">1 VGG块</a></li><li><a href="#2_VGG_93" rel="nofollow">2 VGG网络</a></li><li><a href="#3__151" rel="nofollow">3 训练模型</a></li><li><a href="#4__173" rel="nofollow">4 小结</a></li><li><a href="#_177" rel="nofollow">参考文献</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>前言</h2> 
<p>核心内容来自<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" rel="nofollow">博客链接1</a><a href="https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#id7" rel="nofollow">博客连接2</a>希望大家多多支持作者<br> 本文记录用，防止遗忘</p> 
<h2><a id="VGG_4"></a>卷积神经网络——VGG</h2> 
<h3><a id="_5"></a>课件</h3> 
<p>使用块的网络VGG<br> <img src="https://images2.imgbox.com/38/df/yOsrFn5T_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="VGG_8"></a>VGG</h4> 
<ul><li>AlexNet比LeNet更深更大来得到更好的精度</li><li><p>能不能更深和更大?</p> 
</li><li>选项</li><li>更多的全连接层(太贵)</li><li>更多的卷积层</li><li>将卷积层组合成块</li><li><p><img src="https://images2.imgbox.com/27/8f/1WlG0qjK_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="VGG_19"></a>VGG块</h4> 
<p>深VS宽？</p> 
</li><li>5×5卷积</li><li>3×3卷积</li><li>深但窄效果更好</li><li> VGG块 
</li><li>3×3卷积（填充1）（n层m通道）</li><li>2×2最大池化层（步幅2）</li><li><p><img src="https://images2.imgbox.com/d6/75/R2X7Au4V_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="VGG_29"></a>VGG架构</h4> 
</li><li>多个VGG块后接全连接层 </li><li>不同次数的重复块得到不同的架构VGG-16，VGG-19 </li><li><p><img src="https://images2.imgbox.com/3a/88/uQLj3M1t_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_35"></a>进度</h4> 
<p>LeNet (1995)</p> 
</li><li>2卷积＋池化层 </li><li>2全连接层 </li><li><p>AlexNet</p> 
</li><li>更大更深 </li><li>ReLu, Dropout,数据增强 </li><li><p>VGG</p> 
</li><li>更大更深的AlexNet(重复的VGG块)</li><li><br> 
<br> GluonCV Model Zoo 
<p><a href="https://cv.gluon.ai/model_zoo/classification.html" rel="nofollow">https://cv.gluon.ai/model_zoo/classification.html</a><br> <img src="https://images2.imgbox.com/3d/08/jUznJ810_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_55"></a>总结</h4> 
<p>1、VGG使用可重复使用的卷积块来构建深度卷积神经网络<br> 2、不同的卷积块个数和超参数可以得到不同复杂度的变种</p> 
<h3><a id="_59"></a>教材</h3> 
<p>虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 在下面的几节中，我们将介绍一些常用于设计深层神经网络的启发式概念。</p> 
<p>与芯片设计中工程师从放置晶体管到逻辑元件再到逻辑块的过程类似，神经网络架构的设计也逐渐变得更加抽象。研究人员开始从单个神经元的角度思考问题，发展到整个层，现在又转向块，重复层的模式。</p> 
<p>使用块的想法首先出现在牛津大学的视觉几何组（visualgeometry group）的VGG网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。</p> 
<h4><a id="1_VGG_65"></a>1 VGG块</h4> 
<p>经典卷积神经网络的基本组成部分是下面的这个序列：<br> 1、带填充以保持分辨率的卷积层；<br> 2、非线性激活函数，如ReLU；<br> 3、汇聚层，如最大汇聚层。</p> 
<blockquote> 
 <p>对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p> 
</blockquote> 
<p>而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。在最初的VGG论文中，作者使用了带有卷积核、填充为1（保持高度和宽度）的卷积层，和带有汇聚窗口、步幅为2（每个块后的分辨率减半）的最大汇聚层。在下面的代码中，我们定义了一个名为<code>vgg_block</code>的函数来实现一个VGG块。</p> 
<p>该函数有三个参数，分别对应于卷积层的数量<code>num_convs</code>、输入通道的数量<code>in_channels</code> 和输出通道的数量<code>out_channels</code>.</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">def</span> <span class="token function">vgg_block</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span>
                                kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        in_channels <span class="token operator">=</span> out_channels
    layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="2_VGG_93"></a>2 VGG网络</h4> 
<p>与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。如图所示。<br> <img src="https://images2.imgbox.com/88/70/A9pnqbky_o.png" alt="在这里插入图片描述"><br> VGG神经网络连接中的几个VGG块（在vgg_block函数中定义）。其中有超参数变量<code>conv_arch</code>。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则与AlexNet中的相同。</p> 
<p>原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</p> 
<pre><code class="prism language-python">conv_arch <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>下面的代码实现了VGG-11。可以通过在<code>conv_arch</code>上执行for循环来简单实现。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">vgg</span><span class="token punctuation">(</span>conv_arch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    conv_blks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    in_channels <span class="token operator">=</span> <span class="token number">1</span>
    <span class="token comment"># 卷积层部分</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span> <span class="token keyword">in</span> conv_arch<span class="token punctuation">:</span>
        conv_blks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>vgg_block<span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        in_channels <span class="token operator">=</span> out_channels

    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        <span class="token operator">*</span>conv_blks<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token comment"># 全连接层部分</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>out_channels <span class="token operator">*</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

net <span class="token operator">=</span> vgg<span class="token punctuation">(</span>conv_arch<span class="token punctuation">)</span>
</code></pre> 
<p>接下来，我们将构建一个高度和宽度为224的单通道数据样本，以观察每个层输出的形状。</p> 
<pre><code class="prism language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> blk <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>blk<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape:\t'</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Sequential output shape<span class="token punctuation">:</span>     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Sequential output shape<span class="token punctuation">:</span>     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Sequential output shape<span class="token punctuation">:</span>     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Sequential output shape<span class="token punctuation">:</span>     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Sequential output shape<span class="token punctuation">:</span>     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Flatten output shape<span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">25088</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Linear output shape<span class="token punctuation">:</span>         torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
ReLU output shape<span class="token punctuation">:</span>   torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Dropout output shape<span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Linear output shape<span class="token punctuation">:</span>         torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
ReLU output shape<span class="token punctuation">:</span>   torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Dropout output shape<span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Linear output shape<span class="token punctuation">:</span>         torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>正如你所看到的，我们在每个块的高度和宽度减半，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。<br> 与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。</p> 
<h4><a id="3__151"></a>3 训练模型</h4> 
<p>由于VGG-11比AlexNet计算量更大，因此我们构建了一个通道数较少的网络，足够用于训练Fashion-MNIST数据集。</p> 
<pre><code class="prism language-python">ratio <span class="token operator">=</span> <span class="token number">4</span>
small_conv_arch <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>pair<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> pair<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> ratio<span class="token punctuation">)</span> <span class="token keyword">for</span> pair <span class="token keyword">in</span> conv_arch<span class="token punctuation">]</span>
net <span class="token operator">=</span> vgg<span class="token punctuation">(</span>small_conv_arch<span class="token punctuation">)</span>
</code></pre> 
<p>除了使用略高的学习率外，模型训练过程与AlexNet类似。</p> 
<pre><code class="prism language-python">lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">128</span>
train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> resize<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">)</span>
d2l<span class="token punctuation">.</span>train_ch6<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">loss <span class="token number">0.177</span><span class="token punctuation">,</span> train acc <span class="token number">0.934</span><span class="token punctuation">,</span> test acc <span class="token number">0.911</span>
<span class="token number">2562.3</span> examples<span class="token operator">/</span>sec on cuda<span class="token punctuation">:</span><span class="token number">0</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/c2/fc/EV558VxA_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="4__173"></a>4 小结</h4> 
<p>1、VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。<br> 2、块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。<br> 3、在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即）比较浅层且宽的卷积更有效。</p> 
<h4><a id="_177"></a>参考文献</h4> 
<p>[1] Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</p></li></ul>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1f4601b4a75bded9e3b70f36635a694a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SQL Server 2019数据库还原数据报错</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3dc9298c9222abe10d4f59789adb0d44/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">IDEA最牛逼的五款插件，没有之一，堪称神器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>