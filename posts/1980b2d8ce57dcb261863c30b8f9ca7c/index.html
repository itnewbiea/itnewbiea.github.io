<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【标准化方法】(2) Layer Normalization 原理解析、代码复现，附Pytorch代码 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【标准化方法】(2) Layer Normalization 原理解析、代码复现，附Pytorch代码" />
<meta property="og:description" content="大家好，今天和各位分享一下深度学习中常见的标准化方法，在 Transformer 模型中常用的 Layer Normalization，从数学公式的角度复现一下代码。
看本节前建议各位先看一下 Batch Normalization：https://blog.csdn.net/dgvv4/article/details/130567501
Layer Normalization 的论文地址如下：https://arxiv.org/pdf/1607.06450.pdf
1. 原理介绍 深层网络训练时，网络层数的增加会增加模型计算负担，同时也会导致模型变得难以训练。随着网络层数的增加，数据的分布方式也会随着层与层之间的变化而变化，这种现象被称为内部协变量偏移（Internal Convariate Shift, ICS）。这要求模型训练时必须使用较小的学习率，且需要慎重地选择权重初值。ICS 导致训练速度减慢，同时也导致使用饱和的非线性激活函数（如sigmoid，正负两边都会饱和梯度为 0）时出现梯度消失问题。
为解决内部协方差变化（ICS），思路是固定每一层输出的均值和方差，即层归一化算法（Layer Normalization，LN），层归一化算法用每个样本的均值和方差对输入进行归一化。LN 是在单个样本上操作，可以应用于小批次和 RNN。LN 和 BN 有相同的形式，只是不同的归一化方式。
层归一化与批归一化算法的区别只在于统计值的获取方式上，下式是层归一化算法中均值和方差的计算方式，H 表示层的隐藏单元数， 代表第 层中的第 个神经元。
层归一化算法通过计算在一个训练样本上某一层所有的神经元的均值和方差来对输入进行归一化，像批归一化算法那样，同样也给每个神经元加入了增益 和偏置 来实现线性变换，这在归一化后激活函数前使用。
层归一化 LN 和批归一化 BN 不同的是，层归一化在训练和测试时执行同样的计算，由于 LN 与批次大小没有关系，LN 能够在递归神经网络的每个时间步上分别计算归一化操作所需要的均值和方差的值。实验结果表明，层归一化技术相对批归一化技术训练时间更短。
层归一化算法比较适合应用于全连接网络和递归神经网络，有学者尝试在卷积神经网络上采用层归一化算法，但是发现层归一化算法的效果没有批归一化算法好。这是因为对于全连接层，隐藏层中的全部单元对最终预测和重新定位做出相似的贡献，将所有输入缩放到一个图层效果很好。但是，类似贡献的假设在卷积神经网络不再适用，大量的隐藏单位的感受野位于图像边界附近很少被打开，因此来自同一层内其他隐藏单元的统计数据有很大不同。有学者认为认为需要进一步的研究使卷积网络中的层归一化工作取得好的效果。
总体来说，LN 较 BN 简单，它也是通过减少 ICS 来加速神经网络的训练。LN 在训练和测试时没有区别，只需要对当前隐藏层计算均值和方差而不需要保存每层的移动平均和方差用于测试且不受批次大小的限制，可以通过在线学习的方式一条一条的输入训练数据。
优点：批量较小时，效果好；适用于自然语言处理任务。
缺点：批量较大时，效果不如BN。
2. 代码展示 构造一个输入 shape=[B,C,H*W] 的张量，对每个样本在 [C, H*W] 这两个维度上做 LN
import torch from torch import nn class LN(nn.Module): # 初始化 def __init__(self, normalized_shape, # 在哪个维度上做LN eps:float = 1e-5, # 防止分母为0 elementwise_affine:bool = True): # 是否使用可学习的缩放因子和偏移因子 super(LN, self)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/1980b2d8ce57dcb261863c30b8f9ca7c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-09T10:31:52+08:00" />
<meta property="article:modified_time" content="2023-05-09T10:31:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【标准化方法】(2) Layer Normalization 原理解析、代码复现，附Pytorch代码</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#0d0016;">大家好，今天和各位分享一下深度学习中常见的标准化方法，在 Transformer 模型中常用的 Layer Normalization，从数学公式的角度复现一下代码。</span></p> 
<p><span style="color:#0d0016;">看本节前建议各位先看一下<strong> Batch Normalization</strong>：<a class="link-info" href="https://blog.csdn.net/dgvv4/article/details/130567501" title="https://blog.csdn.net/dgvv4/article/details/130567501">https://blog.csdn.net/dgvv4/article/details/130567501</a></span></p> 
<p><span style="color:#0d0016;">Layer Normalization 的论文地址如下：<a href="https://arxiv.org/pdf/1607.06450.pdf" rel="nofollow" title="https://arxiv.org/pdf/1607.06450.pdf">https://arxiv.org/pdf/1607.06450.pdf</a></span></p> 
<hr> 
<h2><span style="color:#ff9900;"><strong>1. 原理介绍</strong></span></h2> 
<p><span style="color:#0d0016;">深层网络训练时，网络层数的增加会增加模型计算负担，同时也会导致模型变得难以训练。<strong>随着网络层数的增加，数据的分布方式也会随着层与层之间的变化而变化</strong>，这种现象被称为内部协变量偏移（Internal Convariate Shift, ICS）。这要求</span><span style="color:#1c7331;"><strong>模型训练时必须使用较小的学习率，且需要慎重地选择权重初值</strong></span><span style="color:#0d0016;">。</span><span style="color:#1a439c;"><strong>ICS 导致训练速度减慢，同时也导致使用饱和的非线性激活函数（如sigmoid，正负两边都会饱和梯度为 0）时出现梯度消失问题。</strong></span></p> 
<p><span style="color:#0d0016;">为解决</span><span style="color:#b95514;"><strong>内部协方差变化（ICS）</strong></span><span style="color:#0d0016;">，思路是固定每一层输出的均值和方差，即层归一化算法（Layer Normalization，LN），</span><span style="color:#be191c;"><strong>层归一化算法用每个样本的均值和方差对输入进行归一化</strong></span><span style="color:#0d0016;">。<strong>LN 是在单个样本上操作</strong>，可以应用于小批次和 RNN。LN 和 BN 有相同的形式，只是不同的归一化方式。</span></p> 
<p class="img-center"><img alt="" height="211" src="https://images2.imgbox.com/5b/fc/czSlXj8Z_o.png" width="637"></p> 
<p><span style="color:#0d0016;">层归一化与批归一化算法的区别只在于统计值的获取方式上，下式是</span><span style="color:#be191c;"><strong>层归一化算法中均值和方差</strong></span><span style="color:#0d0016;">的计算方式，H 表示层的隐藏单元数，<img alt="a_i^l" class="mathcode" src="https://images2.imgbox.com/b0/d4/vW7bVUo0_o.png"> 代表第 <img alt="l" class="mathcode" src="https://images2.imgbox.com/d9/6a/RlYgEn0a_o.png"> 层中的第 <img alt="i" class="mathcode" src="https://images2.imgbox.com/a5/a0/RSPLeXYk_o.png"> 个神经元。</span></p> 
<p style="text-align:center;"><img alt="u^l=\frac{1}{H}\sum\limits_{i=1}^H a_i^l" class="mathcode" src="https://images2.imgbox.com/df/b7/BKpz8ssJ_o.png"></p> 
<p style="text-align:center;"><img alt="\sigma^l=\sqrt{\frac{1}{H}\sum\limits_{i=1}^H\left(a_i^l-u^l\right)^2}" class="mathcode" src="https://images2.imgbox.com/98/73/aOHdWSts_o.png"></p> 
<p><span style="color:#1a439c;"><strong>层归一化算法通过计算在一个训练样本上某一层所有的神经元的均值和方差来对输入进行归一化</strong></span><span style="color:#0d0016;">，像批归一化算法那样，同样也给</span><span style="color:#be191c;"><strong>每个神经元加入了增益 <img alt="\gamma" class="mathcode" src="https://images2.imgbox.com/de/24/9sWQCw5T_o.png"> 和偏置 <img alt="\beta" class="mathcode" src="https://images2.imgbox.com/4a/b3/Jan5PHtf_o.png"> 来实现线性变换</strong></span><span style="color:#0d0016;">，这在归一化后激活函数前使用。</span></p> 
<p><span style="color:#1c7892;"><strong>层归一化 LN 和批归一化 BN 不同的是</strong></span><span style="color:#0d0016;">，层归一化在训练和测试时执行同样的计算，由于</span><span style="color:#1c7331;"><strong> LN 与批次大小没有关系</strong></span><span style="color:#0d0016;">，LN 能够在递归神经网络的每个时间步上分别计算归一化操作所需要的均值和方差的值。实验结果表明，</span><span style="color:#1a439c;"><strong>层归一化技术相对批归一化技术训练时间更短</strong></span><span style="color:#0d0016;">。</span></p> 
<p class="img-center"><img alt="" height="362" src="https://images2.imgbox.com/27/07/LFHzx2Ln_o.png" width="444"></p> 
<p><span style="color:#1c7331;"><strong>层归一化算法比较适合应用于全连接网络和递归神经网络</strong></span><span style="color:#0d0016;">，有学者尝试在卷积神经网络上采用层归一化算法，但是发现层归一化算法的效果没有批归一化算法好。这是因为<strong>对于</strong></span><span style="color:#b95514;"><strong>全连接层</strong></span><span style="color:#0d0016;"><strong>，隐藏层中的全部单元对最终预测和重新定位做出相似的贡献，将所有输入缩放到一个图层效果很好</strong>。但是，类似贡献的假设在</span><span style="color:#b95514;"><strong>卷积神经网络</strong></span><span style="color:#0d0016;">不再适用，</span><span style="color:#1a439c;"><strong>大量的隐藏单位的感受野位于图像边界附近很少被打开，因此来自同一层内其他隐藏单元的统计数据有很大不同</strong></span><span style="color:#0d0016;">。有学者认为认为需要进一步的研究使卷积网络中的层归一化工作取得好的效果。</span></p> 
<p><span style="color:#0d0016;">总体来说，LN 较 BN 简单，它也是通过减少 ICS 来加速神经网络的训练。</span><span style="color:#be191c;"><strong>LN 在训练和测试时没有区别</strong></span><span style="color:#0d0016;">，只需要对当前隐藏层计算均值和方差而不需要保存每层的移动平均和方差用于测试且不受批次大小的限制，可以通过在线学习的方式一条一条的输入训练数据。</span></p> 
<p id="61"><span style="color:#0d0016;"><strong>优点：</strong>批量较小时，效果好；适用于自然语言处理任务。</span></p> 
<p id="62"><span style="color:#0d0016;"><strong>缺点：</strong>批量较大时，效果不如BN。</span></p> 
<hr> 
<h2><strong><span style="color:#ff9900;">2. 代码展示</span></strong></h2> 
<p><span style="color:#0d0016;">构造一个输入 shape=[B,C,H*W] 的张量，对每个样本在 [C, H*W] 这两个维度上做 LN</span></p> 
<p class="img-center"><img alt="" height="167" src="https://images2.imgbox.com/66/de/t94qr9Vg_o.png" width="393"></p> 
<pre><code class="language-python">import torch
from torch import nn

class LN(nn.Module):
    # 初始化
    def __init__(self, normalized_shape,  # 在哪个维度上做LN
                 eps:float = 1e-5, # 防止分母为0
                 elementwise_affine:bool = True):  # 是否使用可学习的缩放因子和偏移因子
        super(LN, self).__init__()
        # 需要对哪个维度的特征做LN, torch.size查看维度
        self.normalized_shape = normalized_shape  # [c,w*h]
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        # 构造可训练的缩放因子和偏置
        if self.elementwise_affine:  
            self.gain = nn.Parameter(torch.ones(normalized_shape))  # [c,w*h]
            self.bias = nn.Parameter(torch.zeros(normalized_shape))  # [c,w*h]

    # 前向传播
    def forward(self, x: torch.Tensor): # [b,c,w*h]
        # 需要做LN的维度和输入特征图对应维度的shape相同
        assert self.normalized_shape == x.shape[-len(self.normalized_shape):]  # [-2:]
        # 需要做LN的维度索引
        dims = [-(i+1) for i in range(len(self.normalized_shape))]  # [b,c,w*h]维度上取[-1,-2]维度，即[c,w*h]
        # 计算特征图对应维度的均值和方差
        mean = x.mean(dim=dims, keepdims=True)  # [b,1,1]
        mean_x2 = (x**2).mean(dim=dims, keepdims=True)  # [b,1,1]
        var = mean_x2 - mean**2  # [b,c,1,1]
        x_norm = (x-mean) / torch.sqrt(var+self.eps)  # [b,c,w*h]
        # 线性变换
        if self.elementwise_affine:
            x_norm = self.gain * x_norm + self.bias  # [b,c,w*h]
        return x_norm

# ------------------------------- #
# 验证
# ------------------------------- #

if __name__ == '__main__':

    x = torch.linspace(0, 23, 24, dtype=torch.float32)  # 构造输入层
    x = x.reshape([2,3,2*2])  # [b,c,w*h]
    # 实例化
    ln = LN(x.shape[1:])
    # 前向传播
    x = ln(x)
    print(x.shape)</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/248d575b0dce90a3f356c486c2886a3d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">OPC UA服务端(Prosys OPC UA Simulation Server)和客户端(OPC UA Explorer)工具使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/10646b705cc73828132232caf7e683ce/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">利用pom依赖上传jar包到nexus私服</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>