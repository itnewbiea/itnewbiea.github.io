<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>进阶的爬虫系列 ——贴吧爬取术 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="进阶的爬虫系列 ——贴吧爬取术" />
<meta property="og:description" content="进阶的爬虫系列
——不得不说的贴吧爬取术
感谢各位能点开我的这篇博文，才开始写，这个算是很简单的爬虫，文中如有错误和不足欢迎各位大神多多包涵指正，大家的建议是我不断前行的动力，废话不多说我们直接进入主题。
目标：爬取贴吧数据 步骤： 首先我们进入百度贴吧的页面，通过进入不同的贴吧以及翻页解析其url的变化规律 通过观察我们可以看出 “kw=”的后面是接的贴吧的名字，而“pn=”的后面是接的页数，从0开始，每翻一页pn对应的数值会加50。发现规律以后我们就可以通过url中贴吧名字及页数这两个点用循环遍历爬取页面数据。
然后我们明确业务逻辑: 定义一个类——&gt;初始化——&gt;构造url列表——&gt;遍历，发送请求，获取响应——&gt;保存 根据业务逻辑构建初始代码如下：
# coding=utf-8
class Tieba:
def __init__(self,tieba_name):
self.url_temp =&#34;https://tieba.baidu.com/f?kw=&#34; &#43; tieba_name &#43; &#34;&amp;ie=utf-8&amp;pn={}&#34;
self.headers = {
&#34;User-Agent&#34;:&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36&#34;
}
def get_url_list(self):
# 构造url列表
pass
def parse_url(self,url):
# 发送请求，获取响应
pass
def save_html(self,html_str,page_num):
# 保存html字符串
pass
def run(self):#实现主要逻辑
#1.构造url列表
#2.遍历，发送请求，获取响应
#3.保存
pass
if __name__ == &#39;__main__&#39;:
tieba = Tieba(&#39;李毅&#39;)
tieba.run()
1.初始化url路径，和请求头" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/46e947457c76cab0a9c5a27b1795e0ac/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-09-07T22:02:26+08:00" />
<meta property="article:modified_time" content="2019-09-07T22:02:26+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">进阶的爬虫系列 ——贴吧爬取术</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p> </p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">进阶的爬虫系列<br> ——不得不说的贴吧爬取术<br> 感谢各位能点开我的这篇博文，才开始写，这个算是很简单的爬虫，文中如有错误和不足欢迎各位大神多多包涵指正，大家的建议是我不断前行的动力，废话不多说我们直接进入主题。</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">目标：爬取贴吧数据 <br> 步骤： <br> 首先我们进入百度贴吧的页面，通过进入不同的贴吧以及翻页解析其url的变化规律 </a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">通过观察我们可以看出 “kw=”的后面是接的贴吧的名字，而“pn=”的后面是接的页数，从0开始，每翻一页pn对应的数值会加50。发现规律以后我们就可以通过url中贴吧名字及页数这两个点用循环遍历爬取页面数据。</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">然后我们明确业务逻辑: <br> 定义一个类——&gt;初始化——&gt;构造url列表——&gt;遍历，发送请求，获取响应——&gt;保存 <br> 根据业务逻辑构建初始代码如下：</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988"># coding=utf-8</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">class Tieba:<br>     def __init__(self,tieba_name):</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">        self.url_temp ="https://tieba.baidu.com/f?kw=" + tieba_name + "&amp;ie=utf-8&amp;pn={}"<br>         self.headers = {<!-- --><br>             "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"<br>         }<br>     def get_url_list(self):<br>         # 构造url列表<br>         pass</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    def parse_url(self,url):<br>         # 发送请求，获取响应<br>         pass</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    def save_html(self,html_str,page_num):<br>         # 保存html字符串<br>         pass</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    def run(self):#实现主要逻辑<br>         #1.构造url列表<br>         #2.遍历，发送请求，获取响应<br>         #3.保存<br>         pass</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">if __name__ == '__main__':</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    tieba = Tieba('李毅')<br>     tieba.run()<br><br> 1.初始化url路径，和请求头</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    def __init__(self,tieba_name):<br>         self.tieba_name = tieba_name<br>         self.url_temp ="https://tieba.baidu.com/f?kw=" + tieba_name + "&amp;ie=utf-8&amp;pn={}"<br>         self.headers = {<!-- --><br>             "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"<br>         }<br><br> 2.构造url列表</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    def get_url_list(self):<br>         # 构造url列表<br>         return [self.url_temp.format(i*50)for i in range(100)]<br><br> 这边是以列表的方式将遍历出来的100页的url的路径保存，通过调用get_url_list这个方法使用</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">3.发送请求，获取响应</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    def parse_url(self,url):<br>         # 发送请求，获取响应<br>         print(url)<br>         response = requests.get(url,headers = self.headers)<br>         return response.content.decode()<br><br> 4.将爬取的页面以吧名+第几页命名，并保存，保存的时候一定要加encoding=’utf-8’，不然会因为编码问题报错</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    def save_html(self,html_str,page_num):<br>         file_path = "{}-第{}页.html".format(self.tieba_name,page_num)<br>         with open(file_path,"w",encoding='utf-8') as f:<br>             f.write(html_str)<br><br> 5 将定义的方法根据业务逻辑，开始调用</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    def run(self):#实现主要逻辑<br>         #1.构造url列表<br>         url_list = self.get_url_list()</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">        #2.遍历，发送请求，获取响应<br>         for url in url_list:<br>             html_str = self.parse_url(url)</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">            #3.保存<br>             page_num = url_list.index(url) + 1 #页码数<br>             self.save_html(html_str,page_num)<br><br> 6.运行，这里传的是李毅吧，大家也可以传入其他吧名，都可以爬取</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">if __name__ == '__main__':</a></p> 
<p><a href="https://blog.csdn.net/ppter_zhang/article/details/80427988">    tieba = Tieba('李毅')<br>     tieba.run()<br><br> 最后超级感谢大家的耐心观看<br> ————————————————<br> 版权声明：本文为CSDN博主「ppter_zhang」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。<br> 原文链接：https://blog.csdn.net/ppter_zhang/article/details/80427988</a></p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6099c0a105b4918da61f1eaff301264a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">[转]计算机四级网络工程师思维导图--常考重点</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bd452a5ad54ed9448322cd074be07830/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mongoose 增删改查 --------删</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>