<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度有趣 | 30 快速图像风格迁移 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度有趣 | 30 快速图像风格迁移" />
<meta property="og:description" content="简介 使用TensorFlow实现快速图像风格迁移（Fast Neural Style Transfer）
原理 在之前介绍的图像风格迁移中，我们根据内容图片和风格图片优化输入图片，使得内容损失函数和风格损失函数尽可能小
和DeepDream一样，属于网络参数不变，根据损失函数调整输入数据，因此每生成一张图片都相当于训练一个模型，需要很长时间
训练模型需要很长时间，而使用训练好的模型进行推断则很快
使用快速图像风格迁移可大大缩短生成一张迁移图片所需的时间，其模型结构如下，包括转换网络和损失网络
风格图片是固定的，而内容图片是可变的输入，因此以上模型用于将任意图片快速转换为指定风格的图片
转换网络：参数需要训练，将内容图片转换成迁移图片损失网络：计算迁移图片和风格图片之间的风格损失，以及迁移图片和原始内容图片之间的内容损失 经过训练后，转换网络所生成的迁移图片，在内容上和输入的内容图片相似，在风格上和指定的风格图片相似
进行推断时，仅使用转换网络，输入内容图片，即可得到对应的迁移图片
如果有多个风格图片，对每个风格分别训练一个模型即可
实现 基于以下两个项目进行修改，github.com/lengstrom/f…、github.com/hzy46/fast-…
依然通过之前用过的imagenet-vgg-verydeep-19.mat计算内容损失函数和风格损失函数
需要一些图片作为输入的内容图片，对图片具体内容没有任何要求，也不需要任何标注，这里选择使用MSCOCO数据集的train2014部分，cocodataset.org/#download，共82612张图片
加载库
# -*- coding: utf-8 -*- import tensorflow as tf import numpy as np import cv2 from imageio import imread, imsave import scipy.io import os import glob from tqdm import tqdm import matplotlib.pyplot as plt %matplotlib inline 复制代码 查看风格图片，共10张
style_images = glob.glob(&#39;styles/*.jpg&#39;) print(style_images) 复制代码 加载内容图片，去掉黑白图片，处理成指定大小，暂时不进行归一化，像素值范围为0至255之间
def resize_and_crop(image, image_size): h = image." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/8e868c10192e3974723d2baccb708e12/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-09-21T12:13:49+08:00" />
<meta property="article:modified_time" content="2018-09-21T12:13:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度有趣 | 30 快速图像风格迁移</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="article-content"> 
 <h4 class="heading">简介</h4> 
 <p>使用TensorFlow实现快速图像风格迁移（Fast Neural Style Transfer）</p> 
 <h4 class="heading">原理</h4> 
 <p>在之前介绍的图像风格迁移中，我们根据内容图片和风格图片优化输入图片，使得内容损失函数和风格损失函数尽可能小</p> 
 <p>和DeepDream一样，属于网络参数不变，根据损失函数调整输入数据，因此每生成一张图片都相当于训练一个模型，需要很长时间</p> 
 <p>训练模型需要很长时间，而使用训练好的模型进行推断则很快</p> 
 <p>使用快速图像风格迁移可大大缩短生成一张迁移图片所需的时间，其模型结构如下，包括转换网络和损失网络</p> 
 <p></p> 
 <figure> 
  <figcaption></figcaption> 
 </figure> 
 <p></p> 
 <p>风格图片是固定的，而内容图片是可变的输入，因此以上模型用于将任意图片快速转换为指定风格的图片</p> 
 <ul><li>转换网络：参数需要训练，将内容图片转换成迁移图片</li><li>损失网络：计算迁移图片和风格图片之间的风格损失，以及迁移图片和原始内容图片之间的内容损失</li></ul> 
 <p>经过训练后，转换网络所生成的迁移图片，在内容上和输入的内容图片相似，在风格上和指定的风格图片相似</p> 
 <p>进行推断时，仅使用转换网络，输入内容图片，即可得到对应的迁移图片</p> 
 <p>如果有多个风格图片，对每个风格分别训练一个模型即可</p> 
 <h4 class="heading">实现</h4> 
 <p>基于以下两个项目进行修改，<a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Flengstrom%2Ffast-style-transfer" rel="nofollow">github.com/lengstrom/f…</a>、<a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fhzy46%2Ffast-neural-style-tensorflow" rel="nofollow">github.com/hzy46/fast-…</a></p> 
 <p>依然通过之前用过的<code>imagenet-vgg-verydeep-19.mat</code>计算内容损失函数和风格损失函数</p> 
 <p>需要一些图片作为输入的内容图片，对图片具体内容没有任何要求，也不需要任何标注，这里选择使用MSCOCO数据集的train2014部分，<a href="https://link.juejin.im?target=http%3A%2F%2Fcocodataset.org%2F%23download" rel="nofollow">cocodataset.org/#download</a>，共82612张图片</p> 
 <p>加载库</p> 
 <pre><code class="hljs bash copyable"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>

import tensorflow as tf
import numpy as np
import cv2
from imageio import imread, imsave
import scipy.io
import os
import glob
from tqdm import tqdm
import matplotlib.pyplot as plt
%matplotlib inline
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>查看风格图片，共10张</p> 
 <pre><code class="hljs bash copyable">style_images = glob.glob(<span class="hljs-string">'styles/*.jpg'</span>)
<span class="hljs-built_in">print</span>(style_images)
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>加载内容图片，去掉黑白图片，处理成指定大小，暂时不进行归一化，像素值范围为0至255之间</p> 
 <pre><code class="hljs bash copyable">def resize_and_crop(image, image_size):
    h = image.shape[0]
    w = image.shape[1]
    <span class="hljs-keyword">if</span> h &gt; w:
        image = image[h // 2 - w // 2: h // 2 + w // 2, :, :]
    <span class="hljs-keyword">else</span>:
        image = image[:, w // 2 - h // 2: w // 2 + h // 2, :]    
    image = cv2.resize(image, (image_size, image_size))
    <span class="hljs-built_in">return</span> image

X_data = []
image_size = 256
paths = glob.glob(<span class="hljs-string">'train2014/*.jpg'</span>)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(range(len(paths))):
    path = paths[i]
    image = imread(path)
    <span class="hljs-keyword">if</span> len(image.shape) &lt; 3:
        <span class="hljs-built_in">continue</span>
    X_data.append(resize_and_crop(image, image_size))
X_data = np.array(X_data)
<span class="hljs-built_in">print</span>(X_data.shape)
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>加载vgg19模型，并定义一个函数，对于给定的输入，返回vgg19各个层的输出值，就像在GAN中那样，通过<code>variable_scope</code>重用实现网络的重用</p> 
 <pre><code class="hljs bash copyable">vgg = scipy.io.loadmat(<span class="hljs-string">'imagenet-vgg-verydeep-19.mat'</span>)
vgg_layers = vgg[<span class="hljs-string">'layers'</span>]

def vgg_endpoints(inputs, reuse=None):
    with tf.variable_scope(<span class="hljs-string">'endpoints'</span>, reuse=reuse):
        def _weights(layer, expected_layer_name):
            W = vgg_layers[0][layer][0][0][2][0][0]
            b = vgg_layers[0][layer][0][0][2][0][1]
            layer_name = vgg_layers[0][layer][0][0][0][0]
            assert layer_name == expected_layer_name
            <span class="hljs-built_in">return</span> W, b

        def _conv2d_relu(prev_layer, layer, layer_name):
            W, b = _weights(layer, layer_name)
            W = tf.constant(W)
            b = tf.constant(np.reshape(b, (b.size)))
            <span class="hljs-built_in">return</span> tf.nn.relu(tf.nn.conv2d(prev_layer, filter=W, strides=[1, 1, 1, 1], padding=<span class="hljs-string">'SAME'</span>) + b)

        def _avgpool(prev_layer):
            <span class="hljs-built_in">return</span> tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=<span class="hljs-string">'SAME'</span>)

        graph = {}
        graph[<span class="hljs-string">'conv1_1'</span>]  = _conv2d_relu(inputs, 0, <span class="hljs-string">'conv1_1'</span>)
        graph[<span class="hljs-string">'conv1_2'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv1_1'</span>], 2, <span class="hljs-string">'conv1_2'</span>)
        graph[<span class="hljs-string">'avgpool1'</span>] = _avgpool(graph[<span class="hljs-string">'conv1_2'</span>])
        graph[<span class="hljs-string">'conv2_1'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'avgpool1'</span>], 5, <span class="hljs-string">'conv2_1'</span>)
        graph[<span class="hljs-string">'conv2_2'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv2_1'</span>], 7, <span class="hljs-string">'conv2_2'</span>)
        graph[<span class="hljs-string">'avgpool2'</span>] = _avgpool(graph[<span class="hljs-string">'conv2_2'</span>])
        graph[<span class="hljs-string">'conv3_1'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'avgpool2'</span>], 10, <span class="hljs-string">'conv3_1'</span>)
        graph[<span class="hljs-string">'conv3_2'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv3_1'</span>], 12, <span class="hljs-string">'conv3_2'</span>)
        graph[<span class="hljs-string">'conv3_3'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv3_2'</span>], 14, <span class="hljs-string">'conv3_3'</span>)
        graph[<span class="hljs-string">'conv3_4'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv3_3'</span>], 16, <span class="hljs-string">'conv3_4'</span>)
        graph[<span class="hljs-string">'avgpool3'</span>] = _avgpool(graph[<span class="hljs-string">'conv3_4'</span>])
        graph[<span class="hljs-string">'conv4_1'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'avgpool3'</span>], 19, <span class="hljs-string">'conv4_1'</span>)
        graph[<span class="hljs-string">'conv4_2'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv4_1'</span>], 21, <span class="hljs-string">'conv4_2'</span>)
        graph[<span class="hljs-string">'conv4_3'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv4_2'</span>], 23, <span class="hljs-string">'conv4_3'</span>)
        graph[<span class="hljs-string">'conv4_4'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv4_3'</span>], 25, <span class="hljs-string">'conv4_4'</span>)
        graph[<span class="hljs-string">'avgpool4'</span>] = _avgpool(graph[<span class="hljs-string">'conv4_4'</span>])
        graph[<span class="hljs-string">'conv5_1'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'avgpool4'</span>], 28, <span class="hljs-string">'conv5_1'</span>)
        graph[<span class="hljs-string">'conv5_2'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv5_1'</span>], 30, <span class="hljs-string">'conv5_2'</span>)
        graph[<span class="hljs-string">'conv5_3'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv5_2'</span>], 32, <span class="hljs-string">'conv5_3'</span>)
        graph[<span class="hljs-string">'conv5_4'</span>]  = _conv2d_relu(graph[<span class="hljs-string">'conv5_3'</span>], 34, <span class="hljs-string">'conv5_4'</span>)
        graph[<span class="hljs-string">'avgpool5'</span>] = _avgpool(graph[<span class="hljs-string">'conv5_4'</span>])

        <span class="hljs-built_in">return</span> graph
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>选择一张风格图，减去通道颜色均值后，得到风格图片在vgg19各个层的输出值，计算四个风格层对应的Gram矩阵</p> 
 <pre><code class="hljs bash copyable">style_index = 1
X_style_data = resize_and_crop(imread(style_images[style_index]), image_size)
X_style_data = np.expand_dims(X_style_data, 0)
<span class="hljs-built_in">print</span>(X_style_data.shape)

MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))

X_style = tf.placeholder(dtype=tf.float32, shape=X_style_data.shape, name=<span class="hljs-string">'X_style'</span>)
style_endpoints = vgg_endpoints(X_style - MEAN_VALUES)
STYLE_LAYERS = [<span class="hljs-string">'conv1_2'</span>, <span class="hljs-string">'conv2_2'</span>, <span class="hljs-string">'conv3_3'</span>, <span class="hljs-string">'conv4_3'</span>]
style_features = {}

sess = tf.Session()
<span class="hljs-keyword">for</span> layer_name <span class="hljs-keyword">in</span> STYLE_LAYERS:
    features = sess.run(style_endpoints[layer_name], feed_dict={X_style: X_style_data})
    features = np.reshape(features, (-1, features.shape[3]))
    gram = np.matmul(features.T, features) / features.size
    style_features[layer_name] = gram
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>定义转换网络，典型的卷积、残差、逆卷积结构，内容图片输入之前也需要减去通道颜色均值</p> 
 <pre><code class="hljs bash copyable">batch_size = 4
X = tf.placeholder(dtype=tf.float32, shape=[None, None, None, 3], name=<span class="hljs-string">'X'</span>)
k_initializer = tf.truncated_normal_initializer(0, 0.1)

def relu(x):
    <span class="hljs-built_in">return</span> tf.nn.relu(x)

def conv2d(inputs, filters, kernel_size, strides):
    p = int(kernel_size / 2)
    h0 = tf.pad(inputs, [[0, 0], [p, p], [p, p], [0, 0]], mode=<span class="hljs-string">'reflect'</span>)
    <span class="hljs-built_in">return</span> tf.layers.conv2d(inputs=h0, filters=filters, kernel_size=kernel_size, strides=strides, padding=<span class="hljs-string">'valid'</span>, kernel_initializer=k_initializer)

def deconv2d(inputs, filters, kernel_size, strides):
    shape = tf.shape(inputs)
    height, width = shape[1], shape[2]
    h0 = tf.image.resize_images(inputs, [height * strides * 2, width * strides * 2], tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    <span class="hljs-built_in">return</span> conv2d(h0, filters, kernel_size, strides)
    
def instance_norm(inputs):
    <span class="hljs-built_in">return</span> tf.contrib.layers.instance_norm(inputs)

def residual(inputs, filters, kernel_size):
    h0 = relu(conv2d(inputs, filters, kernel_size, 1))
    h0 = conv2d(h0, filters, kernel_size, 1)
    <span class="hljs-built_in">return</span> tf.add(inputs, h0)

with tf.variable_scope(<span class="hljs-string">'transformer'</span>, reuse=None):
    h0 = tf.pad(X - MEAN_VALUES, [[0, 0], [10, 10], [10, 10], [0, 0]], mode=<span class="hljs-string">'reflect'</span>)
    h0 = relu(instance_norm(conv2d(h0, 32, 9, 1)))
    h0 = relu(instance_norm(conv2d(h0, 64, 3, 2)))
    h0 = relu(instance_norm(conv2d(h0, 128, 3, 2)))

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(5):
        h0 = residual(h0, 128, 3)

    h0 = relu(instance_norm(deconv2d(h0, 64, 3, 2)))
    h0 = relu(instance_norm(deconv2d(h0, 32, 3, 2)))
    h0 = tf.nn.tanh(instance_norm(conv2d(h0, 3, 9, 1)))
    h0 = (h0 + 1) / 2 * 255.
    shape = tf.shape(h0)
    g = tf.slice(h0, [0, 10, 10, 0], [-1, shape[1] - 20, shape[2] - 20, -1], name=<span class="hljs-string">'g'</span>)
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>将转换网络的输出即迁移图片，以及原始内容图片都输入到vgg19，得到各自对应层的输出，计算内容损失函数</p> 
 <pre><code class="hljs bash copyable">CONTENT_LAYER = <span class="hljs-string">'conv3_3'</span>
content_endpoints = vgg_endpoints(X - MEAN_VALUES, True)
g_endpoints = vgg_endpoints(g - MEAN_VALUES, True)

def get_content_loss(endpoints_x, endpoints_y, layer_name):
    x = endpoints_x[layer_name]
    y = endpoints_y[layer_name]
    <span class="hljs-built_in">return</span> 2 * tf.nn.l2_loss(x - y) / tf.to_float(tf.size(x))

content_loss = get_content_loss(content_endpoints, g_endpoints, CONTENT_LAYER)
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>根据迁移图片和风格图片在指定风格层的输出，计算风格损失函数</p> 
 <pre><code class="hljs bash copyable">style_loss = []
<span class="hljs-keyword">for</span> layer_name <span class="hljs-keyword">in</span> STYLE_LAYERS:
    layer = g_endpoints[layer_name]
    shape = tf.shape(layer)
    bs, height, width, channel = shape[0], shape[1], shape[2], shape[3]
    
    features = tf.reshape(layer, (bs, height * width, channel))
    gram = tf.matmul(tf.transpose(features, (0, 2, 1)), features) / tf.to_float(height * width * channel)
    
    style_gram = style_features[layer_name]
    style_loss.append(2 * tf.nn.l2_loss(gram - style_gram) / tf.to_float(tf.size(layer)))

style_loss = tf.reduce_sum(style_loss)
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>计算全变差正则，得到总的损失函数</p> 
 <pre><code class="hljs bash copyable">def get_total_variation_loss(inputs):
    h = inputs[:, :-1, :, :] - inputs[:, 1:, :, :]
    w = inputs[:, :, :-1, :] - inputs[:, :, 1:, :]
    <span class="hljs-built_in">return</span> tf.nn.l2_loss(h) / tf.to_float(tf.size(h)) + tf.nn.l2_loss(w) / tf.to_float(tf.size(w)) 

total_variation_loss = get_total_variation_loss(g)

content_weight = 1
style_weight = 250
total_variation_weight = 0.01

loss = content_weight * content_loss + style_weight * style_loss + total_variation_weight * total_variation_loss
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>定义优化器，通过调整转换网络中的参数降低总损失</p> 
 <pre><code class="hljs bash copyable">vars_t = [var <span class="hljs-keyword">for</span> var <span class="hljs-keyword">in</span> tf.trainable_variables() <span class="hljs-keyword">if</span> var.name.startswith(<span class="hljs-string">'transformer'</span>)]
optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss, var_list=vars_t)
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>训练模型，每轮训练结束后，用一张测试图片进行测试，并且将一些tensor的值写入events文件，便于使用tensorboard查看</p> 
 <pre><code class="hljs bash copyable">style_name = style_images[style_index]
style_name = style_name[style_name.find(<span class="hljs-string">'/'</span>) + 1:].rstrip(<span class="hljs-string">'.jpg'</span>)
OUTPUT_DIR = <span class="hljs-string">'samples_%s'</span> % style_name
<span class="hljs-keyword">if</span> not os.path.exists(OUTPUT_DIR):
    os.mkdir(OUTPUT_DIR)

tf.summary.scalar(<span class="hljs-string">'losses/content_loss'</span>, content_loss)
tf.summary.scalar(<span class="hljs-string">'losses/style_loss'</span>, style_loss)
tf.summary.scalar(<span class="hljs-string">'losses/total_variation_loss'</span>, total_variation_loss)
tf.summary.scalar(<span class="hljs-string">'losses/loss'</span>, loss)
tf.summary.scalar(<span class="hljs-string">'weighted_losses/weighted_content_loss'</span>, content_weight * content_loss)
tf.summary.scalar(<span class="hljs-string">'weighted_losses/weighted_style_loss'</span>, style_weight * style_loss)
tf.summary.scalar(<span class="hljs-string">'weighted_losses/weighted_total_variation_loss'</span>, total_variation_weight * total_variation_loss)
tf.summary.image(<span class="hljs-string">'transformed'</span>, g)
tf.summary.image(<span class="hljs-string">'origin'</span>, X)
summary = tf.summary.merge_all()
writer = tf.summary.FileWriter(OUTPUT_DIR)

sess.run(tf.global_variables_initializer())
losses = []
epochs = 2

X_sample = imread(<span class="hljs-string">'sjtu.jpg'</span>)
h_sample = X_sample.shape[0]
w_sample = X_sample.shape[1]

<span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> range(epochs):
    data_index = np.arange(X_data.shape[0])
    np.random.shuffle(data_index)
    X_data = X_data[data_index]
    
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(range(X_data.shape[0] // batch_size)):
        X_batch = X_data[i * batch_size: i * batch_size + batch_size]
        ls_, _ = sess.run([loss, optimizer], feed_dict={X: X_batch})
        losses.append(ls_)
        
        <span class="hljs-keyword">if</span> i &gt; 0 and i % 20 == 0:
            writer.add_summary(sess.run(summary, feed_dict={X: X_batch}), e * X_data.shape[0] // batch_size + i)
            writer.flush()
        
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'Epoch %d Loss %f'</span> % (e, np.mean(losses)))
    losses = []

    gen_img = sess.run(g, feed_dict={X: [X_sample]})[0]
    gen_img = np.clip(gen_img, 0, 255)
    result = np.zeros((h_sample, w_sample * 2, 3))
    result[:, :w_sample, :] = X_sample / 255.
    result[:, w_sample:, :] = gen_img[:h_sample, :w_sample, :] / 255.
    plt.axis(<span class="hljs-string">'off'</span>)
    plt.imshow(result)
    plt.show()
    imsave(os.path.join(OUTPUT_DIR, <span class="hljs-string">'sample_%d.jpg'</span> % e), result)
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>保存模型</p> 
 <pre><code class="hljs bash copyable">saver = tf.train.Saver()
saver.save(sess, os.path.join(OUTPUT_DIR, <span class="hljs-string">'fast_style_transfer'</span>))
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>测试图片依旧是之前用过的交大庙门</p> 
 <p></p> 
 <figure> 
  <figcaption></figcaption> 
 </figure> 
 <p></p> 
 <p>风格迁移结果</p> 
 <p></p> 
 <figure> 
  <figcaption></figcaption> 
 </figure> 
 <p></p> 
 <p>训练过程中可以使用tensorboard查看训练过程</p> 
 <pre><code class="hljs bash copyable">tensorboard --logdir=samples_starry
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p></p> 
 <figure> 
  <figcaption></figcaption> 
 </figure> 
 <p></p> 
 <p></p> 
 <figure> 
  <figcaption></figcaption> 
 </figure> 
 <p></p> 
 <p>在单机上使用以下代码即可快速完成风格迁移，在CPU上也只需要10秒左右</p> 
 <pre><code class="hljs bash copyable"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>

import tensorflow as tf
import numpy as np
from imageio import imread, imsave
import os
import time

def the_current_time():
    <span class="hljs-built_in">print</span>(time.strftime(<span class="hljs-string">"%Y-%m-%d %H:%M:%S"</span>, time.localtime(int(time.time()))))

style = <span class="hljs-string">'wave'</span>
model = <span class="hljs-string">'samples_%s'</span> % style
content_image = <span class="hljs-string">'sjtu.jpg'</span>
result_image = <span class="hljs-string">'sjtu_%s.jpg'</span> % style
X_image = imread(content_image)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

saver = tf.train.import_meta_graph(os.path.join(model, <span class="hljs-string">'fast_style_transfer.meta'</span>))
saver.restore(sess, tf.train.latest_checkpoint(model))

graph = tf.get_default_graph()
X = graph.get_tensor_by_name(<span class="hljs-string">'X:0'</span>)
g = graph.get_tensor_by_name(<span class="hljs-string">'transformer/g:0'</span>)

the_current_time()

gen_img = sess.run(g, feed_dict={X: [X_image]})[0]
gen_img = np.clip(gen_img, 0, 255) / 255.
imsave(result_image, gen_img)

the_current_time()
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>对于其他风格图片，用相同方法训练对应模型即可</p> 
 <p></p> 
 <figure> 
  <figcaption></figcaption> 
 </figure> 
 <p></p> 
 <h4 class="heading">参考</h4> 
 <ul><li>Perceptual Losses for Real-Time Style Transfer and Super-Resolution：<a href="https://link.juejin.im?target=https%3A%2F%2Farxiv.org%2Fabs%2F1603.08155v1" rel="nofollow">arxiv.org/abs/1603.08…</a></li><li>Fast Style Transfer in TensorFlow：<a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Flengstrom%2Ffast-style-transfer" rel="nofollow">github.com/lengstrom/f…</a></li><li>A Tensorflow Implementation for Fast Neural Style：<a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fhzy46%2Ffast-neural-style-tensorflow" rel="nofollow">github.com/hzy46/fast-…</a></li></ul> 
 <h4 class="heading">视频讲解课程</h4> 
 <p><a href="https://link.juejin.im?target=https%3A%2F%2Fstudy.163.com%2Fcourse%2FcourseMain.htm%3FcourseId%3D1004777011%26amp%3Bshare%3D2%26amp%3BshareId%3D1022262032" rel="nofollow">深度有趣（一）</a></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4b27b11df68ce3906827b16d697a240e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">bugku 过狗一句话 writeup</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0a28db315613690dd8e860f1754144c9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">El表达式获取request的attribute跟parameter的值</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>