<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>用Python获取最新的省、市、县 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="用Python获取最新的省、市、县" />
<meta property="og:description" content="因为数据库的数据信息建于12年，很多地区三级关系和名称都跟不上了，让我手动一个个添加又觉得麻烦，怎么说我也是一个码农而不是文员，上代码：
#! /usr/bin/env python
# coding=utf-8
import requests
from lxml import etree
import sys
import xlwt
import re
import copy
import json
from lxml.html import fromstring,tostring
import HTMLParser
import re
#初始化
reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)
#根据网页源码，匹配并返回源码内指定的整个a标签
def get_content(data,str,num=999):
content = []
data1 = etree.HTML(data).xpath(str)
i = 0
for x in data1:
i &#43;= 1
#如果大于指定的数，就跳出循环
if i&gt;num:
continue;
content.append(HTMLParser.HTMLParser().unescape(tostring(x)))
#删除第一次获取数据时，多余的第一第二个数据
if num == 36:
del content[0],content[0]
return content
#根据网站首页的编码，获取各个省的链接，并且返回三级地区的所有a标签
def get_url(url):" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/5b0c1aa96ea76af848d441295d1fa678/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-23T17:51:16+08:00" />
<meta property="article:modified_time" content="2022-11-23T17:51:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">用Python获取最新的省、市、县</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>因为数据库的数据信息建于12年，很多地区三级关系和名称都跟不上了，让我手动一个个添加又觉得麻烦，怎么说我也是一个码农而不是文员，上代码：</p> 
<blockquote> 
 <p>#! /usr/bin/env python<br> # coding=utf-8<br> import requests<br> from lxml import etree<br> import sys<br> import xlwt<br> import re<br> import copy<br> import json<br> from lxml.html import fromstring,tostring<br> import HTMLParser<br> import re</p> 
 <p>#初始化<br> reload(sys)<br> sys.setdefaultencoding('utf8')</p> 
 <p>#根据网页源码，匹配并返回源码内指定的整个a标签<br> def get_content(data,str,num=999):</p> 
 <p>    content = []<br>     data1 = etree.HTML(data).xpath(str)<br>     i = 0<br>     for x in data1:<br>         i += 1<br>         #如果大于指定的数，就跳出循环<br>         if i&gt;num:<br>             continue;<br>         content.append(HTMLParser.HTMLParser().unescape(tostring(x)))</p> 
 <p>    #删除第一次获取数据时，多余的第一第二个数据<br>     if num == 36:<br>         del content[0],content[0]</p> 
 <p>    return content</p> 
 <p><br> #根据网站首页的编码，获取各个省的链接，并且返回三级地区的所有a标签<br> def get_url(url):<br>     #获取数据<br>     data = requests.get(url,stream=True).content<br>     #获取整个a标签，注意匹配法则<br>     contents = get_content(data,"//div/ul/li/a",36)<br>     # print contents<br>     # exit()<br>     #获取一级地区编码，用于拼接链接<br>     data1 = etree.HTML(data).xpath("//div/ul/li/a/@href")<br>     i = 0<br>     key = []<br>     for x in data1:<br>         i += 1<br>         if i&gt;36:<br>             continue;<br>         #匹配值符合地区编码规则的内容写入<br>         if len(x) == 18:<br>             key.append(x[2:14])  #获取地区链接</p> 
 <p>    #根据省编码获取三级编码和地区名称<br>     # url2s = url+key[0]+'.htm'<br>     # data2s = requests.get(url2s,stream=True).content #调用拼接的链接 获取数据<br>     # content2 = get_content(data2s,"//div/table/tr/td/a")<br>     # contents += content2<br>     for htm in key:<br>         url2s = url+htm+'.htm'<br>         print url2s<br>         data2s = requests.get(url2s,stream=True).content #调用拼接的链接 获取数据<br>         content2 = get_content(data2s,"//div/table/tr/td/a")<br>         contents += content2</p> 
 <p>    return contents</p> 
 <p>#根据a标签list数据，进行正则表达式匹配获取需要的地区编码和地区名称，并写入文件存储<br> def set_data(data):<br>     dictData = {}<br>     for x in data:<br>         key = re.search(r"\d+\d+\d+", x, re.M)<br>         val = re.search(r"[^\x00-\xff]+", x, re.M)<br>         #生成字典<br>         dictData[key.group()] = val.group()<br>         #print key.group()+"-"+val.group()<br>     #对字典进行排序，排序后得到的是list类型的数据，而不是字典<br>     NewData = sorted(dictData.items(), key=lambda e:e[0])<br>     # 如果filename不存在会自动创建， 'w'表示写数据，写之前会清空文件中的原有数据！<br>     filename = "area.txt"<br>     with open(filename,'w') as f:<br>             for d in NewData:<br>                 # print json.dumps(d, ensure_ascii=False, encoding='UTF-8')<br>                 print d[0]+"-"+d[1]<br>                 f.write(d[0]+"-"+d[1]+"\n")</p> 
 <p><br> #执行处理方法<br> url = 'http://www.xzqy.net/'<br> #测试数据<br> dataGG = [u'&lt;a href="./110000000000.htm"&gt;\u5317\u4eac&lt;/a&gt;', u'&lt;a href="./120000000000.htm"&gt;\u5929\u6d25&lt;/a&gt;',<br>  u'&lt;a href="./310000000000.htm"&gt;\u4e0a\u6d77&lt;/a&gt;', u'&lt;a href="./500000000000.htm"&gt;\u91cd\u5e86&lt;/a&gt;',<br>  u'&lt;a href="./370000000000.htm"&gt;\u5c71\u4e1c&lt;/a&gt;', u'&lt;a href="./140000000000.htm"&gt;\u5c71\u897f&lt;/a&gt;',<br>  u'&lt;a href="./130000000000.htm"&gt;\u6cb3\u5317&lt;/a&gt;', u'&lt;a href="./410000000000.htm"&gt;\u6cb3\u5357&lt;/a&gt;',<br>  u'&lt;a href="./430000000000.htm"&gt;\u6e56\u5357&lt;/a&gt;', u'&lt;a href="./420000000000.htm"&gt;\u6e56\u5317&lt;/a&gt;',<br>  u'&lt;a href="./320000000000.htm"&gt;\u6c5f\u82cf&lt;/a&gt;', u'&lt;a href="./330000000000.htm"&gt;\u6d59\u6c5f&lt;/a&gt;',<br>  u'&lt;a href="./340000000000.htm"&gt;\u5b89\u5fbd&lt;/a&gt;', u'&lt;a href="./350000000000.htm"&gt;\u798f\u5efa&lt;/a&gt;',<br>  u'&lt;a href="./220000000000.htm"&gt;\u5409\u6797&lt;/a&gt;', u'&lt;a href="./210000000000.htm"&gt;\u8fbd\u5b81&lt;/a&gt;',<br>  u'&lt;a href="./230000000000.htm"&gt;\u9ed1\u9f99\u6c5f&lt;/a&gt;', u'&lt;a href="./360000000000.htm"&gt;\u6c5f\u897f&lt;/a&gt;',<br>  u'&lt;a href="./610000000000.htm"&gt;\u9655\u897f&lt;/a&gt;', u'&lt;a href="./460000000000.htm"&gt;\u6d77\u5357&lt;/a&gt;',<br>  u'&lt;a href="./510000000000.htm"&gt;\u56db\u5ddd&lt;/a&gt;', u'&lt;a href="./520000000000.htm"&gt;\u8d35\u5dde&lt;/a&gt;',<br>  u'&lt;a href="./530000000000.htm"&gt;\u4e91\u5357&lt;/a&gt;', u'&lt;a href="./620000000000.htm"&gt;\u7518\u8083&lt;/a&gt;',<br>  u'&lt;a href="./810000000000.htm"&gt;\u9999\u6e2f&lt;/a&gt;', u'&lt;a href="./820000000000.htm"&gt;\u6fb3\u95e8&lt;/a&gt;',<br>  u'&lt;a href="./710000000000.htm"&gt;\u53f0\u6e7e&lt;/a&gt;', u'&lt;a href="./440000000000.htm"&gt;\u5e7f\u4e1c&lt;/a&gt;',<br>  u'&lt;a href="./450000000000.htm"&gt;\u5e7f\u897f&lt;/a&gt;', u'&lt;a href="./540000000000.htm"&gt;\u897f\u85cf&lt;/a&gt;',<br>  u'&lt;a href="./630000000000.htm"&gt;\u9752\u6d77&lt;/a&gt;', u'&lt;a href="./640000000000.htm"&gt;\u5b81\u590f&lt;/a&gt;',<br>  u'&lt;a href="./650000000000.htm"&gt;\u65b0\u7586&lt;/a&gt;', u'&lt;a href="./150000000000.htm"&gt;\u5185\u8499\u53e4&lt;/a&gt;']</p> 
 <p>dataAll = get_url(url)<br> set_data(dataAll)<br> exit()</p> 
</blockquote> 
<p> 这个爬虫是第二版，第一版因为效率问题让我很不满意，所有遗留了很多引用包，可以忽略。</p> 
<p>在这次编码过程，也学习了很多Python的内容：<br> 1，字典是关联数组，不能使用下标来调用值，排序后返回的是二维的list数据类型<br> 2，用xpath获取整个a标签时，需要xpath参数的定位规则<br> 3，数据在控制台输出的时候，中文显示的是十六进制，查看是否是因为 整个list输出的原因，如二维list循环输出的是一维list，所有还需要进行更精确的指定到中文的节点，即：list显示十六进制，要精确到末尾接单才会正常显示。当然也可以用json模块的方法进行处理输出，注释代码内有写。</p> 
<p>因为兴趣而随便写写，觉得好玩，便分享出来</p> 
<p style="text-align:center;"> <img alt="" src="https://images2.imgbox.com/83/9b/IkVnsKvd_o.png"></p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6838641219508cd8a6250fb6fc76d8a8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【STM32CubeIDE】STM32CubeIDE_快速入门</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1ccab1dbabefe6985e28c1a1bc9802cf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">二分法查找</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>