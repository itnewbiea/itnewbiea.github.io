<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>yolov5 6.1 关于 tensorrt 加速的使用以及问题说明 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="yolov5 6.1 关于 tensorrt 加速的使用以及问题说明" />
<meta property="og:description" content="文章目录 1. 参考连接2. 使用说明2.1 导出加速模型2.1 使用加速模型2.2 加速参数对比 3. 问题说明3.1 在 Tensorrt 8.4.1.5 版本上使用 export.py 导出失败的问题3.2 把模型文件由 best.pt 更换成加速后的 best.engine 后，执行推理时标注的类别名不正确的问题3.3 导出加速模型和使用加速模型推理时 batch_size 不一致的问题3.3.1 为啥使用 PyTorch 的 pt 模型不需要匹配 batch_size，而使用 Tensorrt 的 engine 模型需要呢（代码层面）？3.3.2 为什么 pt 模型不需要检测，而 engine 模型需要检测张量形状匹配呢？（原理层面） 3.4 导出过程中出现了 Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes 提示信息的问题3.5 高版本 tensorrt 导出的模型文件移植到低版本上报出属性错误 &#39;NoneType&#39; object has no attribute &#39;num_bindings&#39; 的问题3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/3f8d3539ff7781f003ca1d708b9997a1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-26T09:37:47+08:00" />
<meta property="article:modified_time" content="2023-07-26T09:37:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">yolov5 6.1 关于 tensorrt 加速的使用以及问题说明</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1__1" rel="nofollow">1. 参考连接</a></li><li><a href="#2__7" rel="nofollow">2. 使用说明</a></li><li><ul><li><a href="#21__8" rel="nofollow">2.1 导出加速模型</a></li><li><a href="#21__28" rel="nofollow">2.1 使用加速模型</a></li><li><a href="#22__33" rel="nofollow">2.2 加速参数对比</a></li></ul> 
  </li><li><a href="#3__48" rel="nofollow">3. 问题说明</a></li><li><ul><li><a href="#31__Tensorrt_8415__exportpy__49" rel="nofollow">3.1 在 Tensorrt 8.4.1.5 版本上使用 export.py 导出失败的问题</a></li><li><a href="#32__bestpt__bestengine__94" rel="nofollow">3.2 把模型文件由 best.pt 更换成加速后的 best.engine 后，执行推理时标注的类别名不正确的问题</a></li><li><a href="#33__batch_size__100" rel="nofollow">3.3 导出加速模型和使用加速模型推理时 batch_size 不一致的问题</a></li><li><ul><li><a href="#331__PyTorch__pt__batch_size_Tensorrt__engine__105" rel="nofollow">3.3.1 为啥使用 PyTorch 的 pt 模型不需要匹配 batch_size，而使用 Tensorrt 的 engine 模型需要呢（代码层面）？</a></li><li><a href="#332__pt__engine__129" rel="nofollow">3.3.2 为什么 pt 模型不需要检测，而 engine 模型需要检测张量形状匹配呢？（原理层面）</a></li></ul> 
   </li><li><a href="#34__Some_tactics_do_not_have_sufficient_workspace_memory_to_run_Increasing_workspace_size_will_enable_more_tactics_please_check_verbose_output_for_requested_sizes__138" rel="nofollow">3.4 导出过程中出现了 Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes 提示信息的问题</a></li><li><a href="#35__tensorrt__NoneType_object_has_no_attribute_num_bindings__159" rel="nofollow">3.5 高版本 tensorrt 导出的模型文件移植到低版本上报出属性错误 'NoneType' object has no attribute 'num_bindings' 的问题</a></li><li><a href="#36__7130_Tensorrt__onnx_not_found_and_is_required_by_YOLOv5_attempting_autoupdate__Failed_building_wheel_for_onnx__170" rel="nofollow">3.6 初次在 7.1.3.0 Tensorrt 上运行导出加速模型时提示 onnx not found and is required by YOLOv5, attempting auto-update 并报错 Failed building wheel for onnx 的问题</a></li><li><a href="#37__pt__engine__200" rel="nofollow">3.7 推理时指定的长方形图片输入尺寸在使用 pt 模型时正常而使用 engine 加速模型时报出输入尺寸和模型尺寸不匹配的问题（不建议“导出”与“训练”指定不同的图片尺寸，该问题旨在进一步熟悉来龙去脉）</a></li></ul> 
  </li><li><a href="#4__317" rel="nofollow">4 相关说明</a></li><li><ul><li><a href="#41_stride__318" rel="nofollow">4.1 stride 说明</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="1__1"></a>1. 参考连接</h2> 
<ul><li><a href="https://zstar.blog.csdn.net/article/details/129992962" rel="nofollow">【目标检测】YOLOv5多进程/多线程推理加速实验</a></li><li><a href="https://zstar.blog.csdn.net/article/details/130056931" rel="nofollow">【目标检测】YOLOv5推理加速实验：TensorRT加速</a></li><li><a href="https://blog.csdn.net/qq1198768105/article/details/127486657">【目标检测】使用TensorRT加速YOLOv5</a></li></ul> 
<h2><a id="2__7"></a>2. 使用说明</h2> 
<h3><a id="21__8"></a>2.1 导出加速模型</h3> 
<pre><code class="prism language-bash">python3 export.py <span class="token parameter variable">--weights</span> best.pt <span class="token parameter variable">--data</span> dataset/dataset.yaml <span class="token parameter variable">--include</span> engine  <span class="token parameter variable">--workspace</span> <span class="token number">16</span> <span class="token parameter variable">--half</span> <span class="token parameter variable">--int8</span> <span class="token parameter variable">--device</span> <span class="token number">0</span>
</code></pre> 
<table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>–weights best.pt</td><td>指定使用的待加速的模型文件为 best.pt</td></tr><tr><td>–data dataset/dataset.yaml</td><td>指定的数据集配置文件的路径和名称</td></tr><tr><td>–include engine</td><td>指定要导出的模型格式为 “engine”，即使用 TensorRT 引擎</td></tr><tr><td>–workspace 16</td><td>设置 TensorRT 库的工作空间内存，根据实际情况调整，单位：GB</td></tr><tr><td>–half</td><td>选项用于执行混合精度训练，它使用半精度浮点数（FP16）来加速训练过程。半精度浮点数在计算时使用更少的存储空间和计算资源，从而提高模型的训练速度。虽然使用半精度浮点数可以加快训练速度，但由于其较低的精度，可能会对模型的精度产生一定影响。因此，在使用 --half 选项时需要仔细权衡速度和精度之间的平衡。</td></tr><tr><td>–int8</td><td>选项用于执行整数量化（quantization），它将模型参数和激活值转换为 8 位整数。整数量化可以显著减少模型的存储需求和计算复杂性，使得模型可以在更有限的资源下进行推理。整数量化适用于部署到一些专用硬件和嵌入式设备上，如 FPGA 或边缘设备。需要注意的是，整数量化可能会对模型的精度产生更大的影响，因为在将浮点数转换为整数时会存在信息损失。</td></tr><tr><td>–device 0</td><td>指定脚本要在第 0 号设备上运行，通常表示第一个可用的 GPU，默认是 cpu</td></tr></tbody></table> 
<p>运行命令后，需等待一段时间，如果运行无误，将在本目录下生成 best.engine 的加速模型</p> 
<p>以下是启用 --half 和 --int8 参数来推导加速模型，过程长达 2000 秒</p> 
<p><img src="https://images2.imgbox.com/79/17/spC0jEhX_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/96/76/wOebcrNW_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="21__28"></a>2.1 使用加速模型</h3> 
<p>使用很简单，就把原先推理时使用的 best.pt 模型替换成 best.engine 即可，以及用命令参数指定数据集配置文件，如下：</p> 
<pre><code class="prism language-bash">python3 detect.py <span class="token parameter variable">--weights</span> best.engine <span class="token parameter variable">--data</span> dataset/dataset.yaml <span class="token parameter variable">--source</span> picture_path
</code></pre> 
<h3><a id="22__33"></a>2.2 加速参数对比</h3> 
<p>以下是做的一些不同参数的加速测试，加速模型的提速效果还是可以的。</p> 
<table><thead><tr><th>export.py 参数</th><th>detect.py 参数</th><th>detect 时间</th></tr></thead><tbody><tr><td>-</td><td>–weights best.pt</td><td>Speed: 1.7ms pre-process, 37.3ms inference, 3.0ms NMS per image at shape (1, 3, 640, 640)</td></tr><tr><td>-</td><td>–weights best.pt --half</td><td>Speed: 1.6ms pre-process, 36.0ms inference, 3.3ms NMS per image at shape (1, 3, 640, 640)</td></tr><tr><td>-</td><td>–weights <strong>best.engine</strong></td><td>Speed: 1.6ms pre-process, 19.6ms inference, 3.0ms NMS per image at shape (1, 3, 640, 640)</td></tr><tr><td>–half --int8</td><td>–weights <strong>best.engine</strong> --half</td><td>Speed: 1.9ms pre-process, 16.8ms inference, 4.0ms NMS per image at shape (1, 3, 640, 640)</td></tr><tr><td>–half --int8 --imgsz 384 640</td><td>–weights <strong>best.engine</strong> --half --imgsz 384 640</td><td>Speed: 1.7ms pre-process, 14.5ms inference, 3.4ms NMS per image at shape (1, 3, 384, 640)</td></tr><tr><td>–half --int8 --imgsz 320 640</td><td>–weights <strong>best.engine</strong> --half --imgsz 320 640</td><td>Speed: 1.9ms pre-process, 13.0ms inference, 3.7ms NMS per image at shape (1, 3, 320, 640)</td></tr><tr><td>–half --int8 --imgsz 320 320</td><td>–weights <strong>best.engine</strong> --half --imgsz 320 320</td><td>Speed: 1.5ms pre-process, 6.9ms inference, 3.3ms NMS per image at shape (1, 3, 320, 320)</td></tr></tbody></table> 
<p>说明一下：</p> 
<ul><li><strong>export.py 参数</strong>：导出 tensorrt 加速模型时使用的参数，- 横杠表示没有相关参数</li><li><strong>detect.py 参数</strong>：推理时使用的参数，- 横杠表示没有相关参数</li></ul> 
<h2><a id="3__48"></a>3. 问题说明</h2> 
<h3><a id="31__Tensorrt_8415__exportpy__49"></a>3.1 在 Tensorrt 8.4.1.5 版本上使用 export.py 导出失败的问题</h3> 
<p>按照如下命令导出加速模型：</p> 
<pre><code class="prism language-bash"> python3 export.py <span class="token parameter variable">--weights</span> best.pt <span class="token parameter variable">--data</span> dataset/dataset.yaml <span class="token parameter variable">--include</span> engine <span class="token parameter variable">--device</span> <span class="token number">0</span>
</code></pre> 
<p>运行结果如下：</p> 
<p><img src="https://images2.imgbox.com/9b/19/havr4FUv_o.png" alt="在这里插入图片描述"></p> 
<p>报错信息如下：</p> 
<ul><li><strong>ONNX: export failure: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument other in method wrapper__equal)（导出失败，期望所有计算都在同一个设备上，但发现计算存在于 cpu 和 gpu 上）</strong></li><li><strong>TensorRT: export failure: failed to export ONNX file: best.onnx（导出 best.onnx 文件失败）</strong></li></ul> 
<p>ONNX：ONNX（Open Neural Network Exchange）是一个开放的跨平台的深度学习模型交换格式，允许在不同的深度学习框架之间无缝转换和部署模型。</p> 
<p><strong>根据以上错误信息的提示，可以先不指定 GPU ，使用 CPU 运行生成 best.onnx，但同时会在生成 best.engine 时报错终止，因为 Tensorrt 必须使用 GPU</strong></p> 
<pre><code class="prism language-bash">python3 export.py <span class="token parameter variable">--weights</span> best.pt <span class="token parameter variable">--data</span> dataset/dataset.yaml <span class="token parameter variable">--include</span> engine
</code></pre> 
<p>运行结果如下：</p> 
<p><img src="https://images2.imgbox.com/4e/65/qOuIx4o6_o.png" alt="在这里插入图片描述"></p> 
<p>根据以上提示说明导出 best.engine 必须在 GPU 上才行，再次执行如下命令：</p> 
<pre><code class="prism language-bash">python3 export.py <span class="token parameter variable">--weights</span> best.pt <span class="token parameter variable">--data</span> dataset/dataset.yaml <span class="token parameter variable">--include</span> engine <span class="token parameter variable">--device</span> <span class="token number">0</span>
</code></pre> 
<p>运行结果如下：</p> 
<p><img src="https://images2.imgbox.com/21/59/Bl4yAUOO_o.png" alt="在这里插入图片描述"></p> 
<p>虽然还是会报出导出 best.onnx 文件失败，但由于之前使用 cpu 已成功导出该文件，所以后续 best.onnx 文件转换成 best.engine 文件的处理得以继续进行。</p> 
<p>最终运行结果如下：</p> 
<p><img src="https://images2.imgbox.com/9d/9d/V1qFbbuW_o.png" alt="在这里插入图片描述"></p> 
<p>导出成功，解决了一开始执行导出指令总是失败的问题，终端还显示了转换所需时间，以及使用示例。其实只要详细查看报错信息，对于问题的解决还是有很大帮助的。</p> 
<p><strong>但是在另外一台 tensorrt 7.1.3.0 版本的设备上却没有出现以上问题，暂不知道原因所在</strong></p> 
<h3><a id="32__bestpt__bestengine__94"></a>3.2 把模型文件由 best.pt 更换成加速后的 best.engine 后，执行推理时标注的类别名不正确的问题</h3> 
<p><strong>这是因为 .pt 模型文件中包含了完整的模型定义，包括类别信息和权重。而使用 TensorRT 加速模型（.engine文件）进行推理时，文件中并不包含类别信息，因此需要提供数据集配置文件来指定类别信息。</strong></p> 
<p>所以在使用加速模型文件 best.engine 时，需要在 detect.py 文件中把默认的 coco128.yaml 数据集配置文件更换成自己的配置文件，或者在调用 detect.py 时使用 --data 参数指定数据集配置文件<br> <img src="https://images2.imgbox.com/9d/61/P9lfXR1P_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="33__batch_size__100"></a>3.3 导出加速模型和使用加速模型推理时 batch_size 不一致的问题</h3> 
<p>不断尝试以不同参数导出加速模型时，结果出现了如下报错：<br> <img src="https://images2.imgbox.com/88/da/lW3BLZw3_o.png" alt="在这里插入图片描述"><br> 从报错情况来看，是 batch_size 不一致问题，回顾发现是本次导出加速模型时，指定了 --batch_size 为 32 了，在导出加速模型时不使用 --batch_size 重新导出即可，因为默认就是 1</p> 
<h4><a id="331__PyTorch__pt__batch_size_Tensorrt__engine__105"></a>3.3.1 为啥使用 PyTorch 的 pt 模型不需要匹配 batch_size，而使用 Tensorrt 的 engine 模型需要呢（代码层面）？</h4> 
<p>从以上图片报错信息跳转到 common.py 文件的 418 行来寻找初步的答案</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> im<span class="token punctuation">,</span> augment<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> visualize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># YOLOv5 MultiBackend inference</span>
        b<span class="token punctuation">,</span> ch<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> im<span class="token punctuation">.</span>shape  <span class="token comment"># batch, channel, height, width</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>pt <span class="token keyword">or</span> self<span class="token punctuation">.</span>jit<span class="token punctuation">:</span>  <span class="token comment"># PyTorch</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>im<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>jit <span class="token keyword">else</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>im<span class="token punctuation">,</span> augment<span class="token operator">=</span>augment<span class="token punctuation">,</span> visualize<span class="token operator">=</span>visualize<span class="token punctuation">)</span>
            <span class="token keyword">return</span> y <span class="token keyword">if</span> val <span class="token keyword">else</span> y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
		<span class="token comment"># 此处省略一部分代码</span>
		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>engine<span class="token punctuation">:</span>  <span class="token comment"># TensorRT</span>
        	<span class="token comment"># 与上面的 PyTorch 模型增加了如下 assert 语句，即检测张量形状是否相等，不相等则抛出异常</span>
            <span class="token keyword">assert</span> im<span class="token punctuation">.</span>shape <span class="token operator">==</span> self<span class="token punctuation">.</span>bindings<span class="token punctuation">[</span><span class="token string">'images'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">,</span> <span class="token punctuation">(</span>im<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bindings<span class="token punctuation">[</span><span class="token string">'images'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>binding_addrs<span class="token punctuation">[</span><span class="token string">'images'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>im<span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>context<span class="token punctuation">.</span>execute_v2<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>binding_addrs<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            y <span class="token operator">=</span> self<span class="token punctuation">.</span>bindings<span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
		<span class="token comment"># 此处省略一部分代码</span>
		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<h4><a id="332__pt__engine__129"></a>3.3.2 为什么 pt 模型不需要检测，而 engine 模型需要检测张量形状匹配呢？（原理层面）</h4> 
<p>使用 TensorRT 加速模型的主要目的是提高推理性能，以获得更快的推理速度和更高的吞吐量。TensorRT 通过对模型进行优化和运行时的高效计算，实现了对推理过程的加速。</p> 
<p>当使用 TensorRT 加速模型时，需要将模型转换为 TensorRT 的 engine 格式，这个 engine 是针对特定硬件和指定的输入尺寸进行了优化。在转换为 engine 时，需要指定输入的 batch_size，这是因为 TensorRT 在进行网络结构优化和内存分配时会基于指定的 batch_size 进行操作。</p> 
<p>而在使用原始的 pt 模型进行推理时，模型会直接在 PyTorch 框架下执行。在这种情况下，会根据实际的推理需求，使用不同的 batch_size 进行推理，而不需要与训练时使用的 batch_size 匹配。因为 PyTorch 框架允许在推理过程中灵活地调整 batch_size，模型的网络结构和内存分配会根据实际的 batch_size 进行动态调整。</p> 
<p>不仅仅张量中的 batch_size 要匹配，其他参数诸如图片通道数、图片尺寸都需要匹配才能够正确加载和执行推理。</p> 
<h3><a id="34__Some_tactics_do_not_have_sufficient_workspace_memory_to_run_Increasing_workspace_size_will_enable_more_tactics_please_check_verbose_output_for_requested_sizes__138"></a>3.4 导出过程中出现了 Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes 提示信息的问题</h3> 
<p><img src="https://images2.imgbox.com/d7/a7/yodUdXAf_o.png" alt="在这里插入图片描述"><br> 通常是由 TensorRT 库的工作空间内存不足引起的，TensorRT 在编译和优化模型时使用工作空间内存，以支持不同的优化策略和算法。如果工作空间内存不足，可能无法运行某些优化策略，从而报出该警告信息</p> 
<p>查看 export.py 的 parse_opt 函数得知：</p> 
<pre><code class="prism language-bash">parser.add_argument<span class="token punctuation">(</span><span class="token string">'--workspace'</span>, <span class="token assign-left variable">type</span><span class="token operator">=</span>int, <span class="token assign-left variable">default</span><span class="token operator">=</span><span class="token number">4</span>, <span class="token assign-left variable">help</span><span class="token operator">=</span><span class="token string">'TensorRT: workspace size (GB)'</span><span class="token punctuation">)</span>
</code></pre> 
<p>由此看来默认的 4GB 少了，可通过 --workspace 参数来增加工作空间<br> GPU 机器有 32 GB 内存，没有指定该参数时，一半内存都没有使用到。<br> 一开始使用 --workspace 8，可仍旧报出警告，最终改成 --workspace 16</p> 
<pre><code class="prism language-bash">python3 export.py <span class="token parameter variable">--weights</span> best.pt <span class="token parameter variable">--data</span> ./dataset/dataset.yaml <span class="token parameter variable">--include</span> engine <span class="token parameter variable">--half</span> <span class="token parameter variable">--int8</span> <span class="token parameter variable">--device</span> <span class="token number">0</span> <span class="token parameter variable">--workspace</span> <span class="token number">16</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/e0/02/E35XWvkT_o.png" alt="在这里插入图片描述"><br> 使用 jtop 指令查看，内存使用量上来了。<br> <img src="https://images2.imgbox.com/6f/cb/KOHTkt9m_o.png" alt="在这里插入图片描述"><br> 最终生成的结果如下，导出时间也有所下降：<br> <img src="https://images2.imgbox.com/45/ee/0P2hfI1W_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="35__tensorrt__NoneType_object_has_no_attribute_num_bindings__159"></a>3.5 高版本 tensorrt 导出的模型文件移植到低版本上报出属性错误 ‘NoneType’ object has no attribute ‘num_bindings’ 的问题</h3> 
<p>博主有两台不同型号和系统的 GPU 机器，博主把在 8.4.1.5 版本的 tensorrt 上导出的加速模型直接拷贝到 7.1.3.0 版本的 tensorrt 上运行推理，结果报错如下：<br> <img src="https://images2.imgbox.com/fb/0f/sHqfRi67_o.png" alt="在这里插入图片描述"><br> 出现该问题由于版本不兼容导致的， 而且 tensorrt 依赖软硬件环境，最好不要拷贝 engine 加速模型在不同机器上使用，遵循谁使用谁导出的原则</p> 
<p>tensorrt 版本查询语句如下：</p> 
<pre><code class="prism language-bash">python <span class="token parameter variable">-c</span> <span class="token string">"import tensorrt;print(tensorrt.__version__)"</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/96/54/ajnwfPrG_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/4a/53/lJ0BXv1i_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="36__7130_Tensorrt__onnx_not_found_and_is_required_by_YOLOv5_attempting_autoupdate__Failed_building_wheel_for_onnx__170"></a>3.6 初次在 7.1.3.0 Tensorrt 上运行导出加速模型时提示 onnx not found and is required by YOLOv5, attempting auto-update 并报错 Failed building wheel for onnx 的问题</h3> 
<p>衔接上一个问题，在旧版本 tensorrt 机器上重新运行 tensorrt 加速模型的导出</p> 
<pre><code class="prism language-bash">python3 export.py <span class="token parameter variable">--weights</span> best.pt <span class="token parameter variable">--data</span> ./dataset/dataset.yaml <span class="token parameter variable">--include</span> engine <span class="token parameter variable">--half</span> <span class="token parameter variable">--int8</span> <span class="token parameter variable">--device</span> <span class="token number">0</span> <span class="token parameter variable">--workspace</span> <span class="token number">16</span>
</code></pre> 
<p>运行后提示 onnx not found and is required by YOLOv5, attempting auto-update… 后就报如下错误：</p> 
<p><img src="https://images2.imgbox.com/dc/7d/iossxv4d_o.png" alt="在这里插入图片描述"><br> 提示自动安装 onnx 时报错，根据提示说明缺少构建 ONNX 所需的一些依赖项或编译环境不完整导致的，</p> 
<p>这种情况大概率是由于网络不通畅导致的，开代理使用如下命令来处理：</p> 
<pre><code class="prism language-bash">conda <span class="token function">install</span> <span class="token parameter variable">-c</span> conda-forge onnx
</code></pre> 
<p><strong>conda-forge：</strong> 是一个社区驱动的 Conda 软件包源，它提供了大量的开源软件包供用户安装和使用。与默认的 Conda 软件包源相比，conda-forge 提供了更广泛的软件包选择和更新频率更高的版本更新。</p> 
<p>运行之后出现如下提示，提示将安装和更新以下包：</p> 
<p><img src="https://images2.imgbox.com/a7/f1/HAJj0GEj_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2c/c1/fc4tteim_o.png" alt="在这里插入图片描述"></p> 
<p>按照提示输入 Y 即可安装所有，安装完成后，通过 conda list onnx 查看安装结果</p> 
<p><img src="https://images2.imgbox.com/73/05/oBcam2rP_o.png" alt="在这里插入图片描述"><br> 安装成功，再次执行如下指令，即可生成加速模型了</p> 
<pre><code class="prism language-bash">python3 export.py <span class="token parameter variable">--weights</span> best.pt <span class="token parameter variable">--data</span> ./dataset/dataset.yaml <span class="token parameter variable">--include</span> engine <span class="token parameter variable">--half</span> <span class="token parameter variable">--int8</span> <span class="token parameter variable">--device</span> <span class="token number">0</span> <span class="token parameter variable">--workspace</span> <span class="token number">16</span>
</code></pre> 
<h3><a id="37__pt__engine__200"></a>3.7 推理时指定的长方形图片输入尺寸在使用 pt 模型时正常而使用 engine 加速模型时报出输入尺寸和模型尺寸不匹配的问题（不建议“导出”与“训练”指定不同的图片尺寸，该问题旨在进一步熟悉来龙去脉）</h3> 
<p>博主发现推理输入尺寸是 640 * 640，而博主的原始输入图片是 1080 * 1920，根据同比例缩放，应该是 360 * 640，所以博主尝试使用如下指令导出 tensorrt 加速模型</p> 
<pre><code class="prism language-bash">python3 export.py <span class="token parameter variable">--weights</span> best.pt <span class="token parameter variable">--data</span> ./dataset/dataset.yaml <span class="token parameter variable">--include</span> engine <span class="token parameter variable">--imgsz</span> <span class="token number">360</span> <span class="token number">640</span> <span class="token parameter variable">--half</span> <span class="token parameter variable">--int8</span> <span class="token parameter variable">--device</span> <span class="token number">0</span> <span class="token parameter variable">--workspace</span> <span class="token number">16</span>
</code></pre> 
<p>结果后续使用以上指令导出的加速模型进行推理时，发现推理过程输出的推理输入尺寸是 384 * 640，回溯导出加速模型的调试输出信息时发现如下提示</p> 
<p><img src="https://images2.imgbox.com/21/de/q8vm0L52_o.png" alt="在这里插入图片描述"></p> 
<p>说是 --imgsz 的参数值必须是 32 的倍数，已自动更新为 384（384 = 32 * 12），博主认为 384 比 352（352 = 32 * 11）与 360 的差值大，所以选择了 352，一是更符合原始图片 1080*1920 的固定缩放比，二是进一步降低了计算的像素数，所以最终选择了 352 * 640 的图片尺寸来导出加速模型</p> 
<p>但是当使用如下指令来使用 352 * 640 的加速模型来进行推理时结果却报错：</p> 
<pre><code class="prism language-bash">python3 detect.py <span class="token parameter variable">--weights</span> best.engine <span class="token parameter variable">--data</span> dataset/dataset.yaml <span class="token parameter variable">--source</span> xxx <span class="token parameter variable">--half</span> <span class="token parameter variable">--imgsz</span> <span class="token number">352</span> <span class="token number">640</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/6c/72/EgGh51rS_o.png" alt="在这里插入图片描述"></p> 
<p>从报错信息可以看出图片输入尺寸与加速模型图片尺寸不一致，加速模型使用的图片尺寸是 352 * 640，表明之前导出的加速模型是没有问题的，而明明指定了推理图片的输入尺寸是 352 * 640，为啥变成了 384 * 640 呢？</p> 
<p>其实上图中输出的调试信息已经有提示，就在图片中间位置：</p> 
<pre><code class="prism language-bash">WARNING: --img-size <span class="token punctuation">[</span><span class="token number">352</span>, <span class="token number">640</span><span class="token punctuation">]</span> must be multiple of max stride <span class="token number">64</span>, updating to <span class="token punctuation">[</span><span class="token number">384</span>, <span class="token number">640</span><span class="token punctuation">]</span>
</code></pre> 
<p>相关处理代码如下：</p> 
<p>detect.py</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">run</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
	<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
	<span class="token comment"># 此处省略一部分代码</span>
	<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
	<span class="token comment"># Load model</span>
    device <span class="token operator">=</span> select_device<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    model <span class="token operator">=</span> DetectMultiBackend<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dnn<span class="token operator">=</span>dnn<span class="token punctuation">,</span> data<span class="token operator">=</span>data<span class="token punctuation">)</span>
    stride<span class="token punctuation">,</span> names<span class="token punctuation">,</span> pt<span class="token punctuation">,</span> jit<span class="token punctuation">,</span> onnx<span class="token punctuation">,</span> engine <span class="token operator">=</span> model<span class="token punctuation">.</span>stride<span class="token punctuation">,</span> model<span class="token punctuation">.</span>names<span class="token punctuation">,</span> model<span class="token punctuation">.</span>pt<span class="token punctuation">,</span> model<span class="token punctuation">.</span>jit<span class="token punctuation">,</span> model<span class="token punctuation">.</span>onnx<span class="token punctuation">,</span> model<span class="token punctuation">.</span>engine
    imgsz <span class="token operator">=</span> check_img_size<span class="token punctuation">(</span>imgsz<span class="token punctuation">,</span> s<span class="token operator">=</span>stride<span class="token punctuation">)</span>  <span class="token comment"># check image size</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'imgsz:'</span><span class="token punctuation">,</span>imgsz<span class="token punctuation">,</span> stride<span class="token punctuation">)</span>
	<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
	<span class="token comment"># 此处省略一部分代码</span>
	<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<p>general.py</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">check_img_size</span><span class="token punctuation">(</span>imgsz<span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> floor<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Verify image size is a multiple of stride s in each dimension</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>imgsz<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># integer i.e. img_size=640</span>
        new_size <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>make_divisible<span class="token punctuation">(</span>imgsz<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> floor<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># list i.e. img_size=[640, 480]</span>
        new_size <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">max</span><span class="token punctuation">(</span>make_divisible<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> floor<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> imgsz<span class="token punctuation">]</span>
    <span class="token keyword">if</span> new_size <span class="token operator">!=</span> imgsz<span class="token punctuation">:</span>
        LOGGER<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'WARNING: --img-size </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>imgsz<span class="token punctuation">}</span></span><span class="token string"> must be multiple of max stride </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>s<span class="token punctuation">}</span></span><span class="token string">, updating to </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>new_size<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> new_size
</code></pre> 
<p>commom.py</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DetectMultiBackend</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># YOLOv5 MultiBackend class for python inference on various backends</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token string">'yolov5s.pt'</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dnn<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> data<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
		<span class="token comment"># 此处省略一部分代码</span>
		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        pt<span class="token punctuation">,</span> jit<span class="token punctuation">,</span> onnx<span class="token punctuation">,</span> xml<span class="token punctuation">,</span> engine<span class="token punctuation">,</span> coreml<span class="token punctuation">,</span> saved_model<span class="token punctuation">,</span> pb<span class="token punctuation">,</span> tflite<span class="token punctuation">,</span> edgetpu<span class="token punctuation">,</span> tfjs <span class="token operator">=</span> self<span class="token punctuation">.</span>model_type<span class="token punctuation">(</span>w<span class="token punctuation">)</span>  <span class="token comment"># get backend</span>
        <span class="token comment"># 定义 stride 的初始值为 64</span>
        stride<span class="token punctuation">,</span> names <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f'class</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token punctuation">}</span></span><span class="token string">'</span></span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment"># assign defaults</span>
        w <span class="token operator">=</span> attempt_download<span class="token punctuation">(</span>w<span class="token punctuation">)</span>  <span class="token comment"># download if not local</span>
        <span class="token keyword">if</span> data<span class="token punctuation">:</span>  <span class="token comment"># data.yaml path (optional)</span>
            <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">'ignore'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
                names <span class="token operator">=</span> yaml<span class="token punctuation">.</span>safe_load<span class="token punctuation">(</span>f<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'names'</span><span class="token punctuation">]</span>  <span class="token comment"># class names</span>

        <span class="token keyword">if</span> pt<span class="token punctuation">:</span>  <span class="token comment"># PyTorch</span>
            model <span class="token operator">=</span> attempt_load<span class="token punctuation">(</span>weights <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>weights<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span> <span class="token keyword">else</span> w<span class="token punctuation">,</span> map_location<span class="token operator">=</span>device<span class="token punctuation">)</span>
            <span class="token comment"># 根据模型 stride 值更新 stride</span>
            stride <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>stride<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>  <span class="token comment"># model stride</span>
            names <span class="token operator">=</span> model<span class="token punctuation">.</span>module<span class="token punctuation">.</span>names <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'module'</span><span class="token punctuation">)</span> <span class="token keyword">else</span> model<span class="token punctuation">.</span>names  <span class="token comment"># get class names</span>
            self<span class="token punctuation">.</span>model <span class="token operator">=</span> model  <span class="token comment"># explicitly assign for to(), cpu(), cuda(), half()</span>
		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
		<span class="token comment"># 此处省略一部分代码</span>
		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        <span class="token keyword">elif</span> engine<span class="token punctuation">:</span>  <span class="token comment"># TensorRT</span>
            LOGGER<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Loading </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>w<span class="token punctuation">}</span></span><span class="token string"> for TensorRT inference...'</span></span><span class="token punctuation">)</span>
            <span class="token keyword">import</span> tensorrt <span class="token keyword">as</span> trt  <span class="token comment"># https://developer.nvidia.com/nvidia-tensorrt-download</span>
            check_version<span class="token punctuation">(</span>trt<span class="token punctuation">.</span>__version__<span class="token punctuation">,</span> <span class="token string">'7.0.0'</span><span class="token punctuation">,</span> hard<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># require tensorrt&gt;=7.0.0</span>
            Binding <span class="token operator">=</span> namedtuple<span class="token punctuation">(</span><span class="token string">'Binding'</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">,</span> <span class="token string">'dtype'</span><span class="token punctuation">,</span> <span class="token string">'shape'</span><span class="token punctuation">,</span> <span class="token string">'data'</span><span class="token punctuation">,</span> <span class="token string">'ptr'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            logger <span class="token operator">=</span> trt<span class="token punctuation">.</span>Logger<span class="token punctuation">(</span>trt<span class="token punctuation">.</span>Logger<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>
            <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">,</span> trt<span class="token punctuation">.</span>Runtime<span class="token punctuation">(</span>logger<span class="token punctuation">)</span> <span class="token keyword">as</span> runtime<span class="token punctuation">:</span>
                model <span class="token operator">=</span> runtime<span class="token punctuation">.</span>deserialize_cuda_engine<span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            bindings <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> index <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>num_bindings<span class="token punctuation">)</span><span class="token punctuation">:</span>
                name <span class="token operator">=</span> model<span class="token punctuation">.</span>get_binding_name<span class="token punctuation">(</span>index<span class="token punctuation">)</span>
                dtype <span class="token operator">=</span> trt<span class="token punctuation">.</span>nptype<span class="token punctuation">(</span>model<span class="token punctuation">.</span>get_binding_dtype<span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">)</span>
                shape <span class="token operator">=</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>get_binding_shape<span class="token punctuation">(</span>index<span class="token punctuation">)</span><span class="token punctuation">)</span>
                data <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>shape<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>dtype<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
                bindings<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> Binding<span class="token punctuation">(</span>name<span class="token punctuation">,</span> dtype<span class="token punctuation">,</span> shape<span class="token punctuation">,</span> data<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            binding_addrs <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> d<span class="token punctuation">.</span>ptr<span class="token punctuation">)</span> <span class="token keyword">for</span> n<span class="token punctuation">,</span> d <span class="token keyword">in</span> bindings<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            context <span class="token operator">=</span> model<span class="token punctuation">.</span>create_execution_context<span class="token punctuation">(</span><span class="token punctuation">)</span>
            batch_size <span class="token operator">=</span> bindings<span class="token punctuation">[</span><span class="token string">'images'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
       <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
	   <span class="token comment"># 此处省略一部分代码</span>
	   <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
	  
	 <span class="token comment"># locals() 返回当前作用域中的所有局部变量和它们的值。返回类型是字典，其中键是变量名，值是变量的值。</span>
	 <span class="token comment"># self.__dict__ 字典，表示类的实例对象的属性。通过该字典我们可以访问和操作类的实例对象的属性</span>
	 self<span class="token punctuation">.</span>__dict__<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token builtin">locals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># assign all variables to self</span>
</code></pre> 
<p>原来是使用加速模型进行推理时，传入 check_img_size 的 stride 为 64 <strong>（较小的步幅可以保留更多的细节信息，但计算量较大；而较大的步幅可以加快计算速度，但可能会损失一些细节信息）</strong>，而指定的图片输入尺寸 352 不是 64 的倍数，通过此函数自动调整成 384 了，</p> 
<p>至于为什么传入的 stride 是 64，那是因为在 DetectMultiBackend 类的 <strong>init</strong> 函数中，当检测到是 tensorrt 的加速模型时，直接使用了 <strong>stride, names = 64, [f’class{i}’ for i in range(1000)] # assign defaults</strong> 语句定义的值 64</p> 
<p>而 best.pt 模型（PyTorch）是自适应输入图片尺寸的，所以不会报出图片尺寸不匹配的问题</p> 
<h2><a id="4__317"></a>4 相关说明</h2> 
<h3><a id="41_stride__318"></a>4.1 stride 说明</h3> 
<p>是指网络中卷积层的步幅（stride）参数。其作用是控制特征图（feature map）相对于输入图像的缩小比例。</p> 
<p>在 yolov5 中，模型使用了一系列的卷积层来逐步提取图像特征。每个卷积层通常都会使用一个固定的 stride 值来决定其输出特征图的尺寸和感受野。较小的 stride 值会导致输出特征图尺寸相对于输入图像尺寸较大，而较大的stride 值会导致输出特征图尺寸相对于输入图像尺寸较小。</p> 
<p><strong>改变特征图分辨率：</strong> 较大的 stride 值会导致特征图分辨率相对于输入图像降低。这可以帮助模型捕获更广阔的上下文信息，并在一定程度上减少计算量。</p> 
<p><strong>改变感受野：</strong> 较小的 stride 值可以增大卷积层的感受野（即每个像素在输入图像上所关注的区域），使模型能够获取更多的局部和全局上下文信息。</p> 
<p><strong>物体检测精度和定位准确性：</strong> 较小的 stride 值有助于提高物体检测的精度和定位准确性，因为模型可以更细致地检测和定位较小的目标物体。</p> 
<p>需要注意的是，较小的 stride 值会导致网络的计算和内存消耗增加，可能会减慢训练和推理速度。因此，在选择模型的 stride 值时，需要根据具体应用场景进行权衡，平衡精度和速度之间的关系。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/360317878860b385643db8e92c95f8ce/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【数据分析专栏之Python篇】二、Jupyer Notebook安装配置及基本使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dcf795af98da6034d84258c4f5b1dc1a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">el-table表格自动滚动</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>