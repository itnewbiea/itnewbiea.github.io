<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>什么是文本的词嵌入？ - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="什么是文本的词嵌入？" />
<meta property="og:description" content="前言 词嵌入是单词的一种数值化表示方式，一般情况下会将一个单词映射到一个高维的向量中（词向量）来代表这个单词。例如我们将： ‘机器学习’表示为 [1,2,3] ‘深度学习‘表示为[2,3,3] ‘英雄联盟‘表示为[9,1,3] 对于词向量，我们可以使用余弦相似度在计算机中来判断单词之间的距离： ‘机器学习’与‘深度学习‘的距离：
‘机器学习’与‘英雄联盟‘的距离：
通过这篇文章你可以学习到下面内容：
文本的词嵌入是什么，以及它与其他特征提取方法有什么区别。介绍3种主流的从文本数据中学习词嵌入的方法。怎么训练新的词嵌入以及如何在日常的NLP任务中使用预先训练好的词嵌入。 什么是词嵌入? 词嵌入是一种对文本算法学习后的表示形式，甚至，你可以理解为一个单词在算法中的储存形式。大家知道存入计算机的都是0101的数值化序列，这里也是同理，词嵌入就是将文本数值化以方便拟合算法。这种将单词或者文档数字化表示的方式被认为是深度学习在自然语言处理任务中最具有挑战性的问题之一。
使用密集和低维向量的一个好处是方便计算：大多数神经网络工具包不能很好地处理非常高维，稀疏的向量。......密集表示的主要好处是泛化能力，如果我们认为某些特征可能提供类似的线索，那么提供能够捕获这些相似性的表示是值得的。
— Page 92, Neural Network Methods in Natural Language Processing, 2017.
词嵌入实际上是一种将各个单词在预定的向量空间中表示为实值向量的一类技术。每个单词被映射成一个向量（初始随机化），并且这个向量可以通过神经网络的方式来学习更新。因此这项技术基本集中应用与深度学习领域。
这项技术的关键点在于如何用密集的分布式向量来表示每个单词。这样做的好处在于与one-hot这样的编码对比，使用词嵌入表示的单词向量往往只有几十或者几百个维度。极大的减少了计算和储存量。 将词汇表中的每个词与分布式词特征向量相关联...特征向量表示词的不同方面特征，每个词与向量空间中的点相关联。特征的数量......远小于词汇的大小
— A Neural Probabilistic Language Model, 2003.
这种分布式的词向量表示方式依赖于单词的使用习惯，这就使得具有相似使用方式的单词具有相似的表示形式。这个怎么理解呢？回想我们在看游戏直播，IG获得冠军时候的弹幕，很多主播（无论游戏还是娱乐）的直播间都在发‘IG牛逼’和 ‘77777’，所以我们算法能学习到，‘IG牛逼’和 ‘77777’是向量空间中很相近的2个词。
虽然看起来有些粗糙，但是这个方法背后有很深的语言学理论支撑。即Zellig Harris的“distributional hypothesis”。这个假设可以归纳为：具有相似语境的词语具有相似的含义。有关更深入的信息，请参阅Harris的1956年论文 “Distributional structure“。
这种使用语境定义单词的概念可以通过John Firth经常重复的谚语来概括：
You shall know a word by the company it keeps!
— Page 11, “A synopsis of linguistic theory 1930-1955“, in Studies in Linguistic Analysis 1930-1955, 1962." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/e8c12779d4d759fb9e3fe1b7f27b5ef2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-11-21T11:30:52+08:00" />
<meta property="article:modified_time" content="2018-11-21T11:30:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">什么是文本的词嵌入？</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>前言</h3> 
<p>    词嵌入是单词的一种数值化表示方式，一般情况下会将一个单词映射到一个高维的向量中（<span style="color:#f33b45;">词向量</span>）来代表这个单词。例如我们将：  </p> 
<ol><li> ‘<span style="color:#86ca5e;">机器学习</span>’表示为<span style="color:#ffbb66;"> </span><span style="color:#e579b6;">[1,2,3]   </span></li><li><span style="color:#e579b6;"> </span>‘<span style="color:#86ca5e;">深度学习</span>‘表示为<span style="color:#e579b6;">[2,3,3]  </span></li><li> ‘<span style="color:#86ca5e;">英雄联盟</span>‘表示为<span style="color:#e579b6;">[9,1,3]</span></li></ol> 
<p>   对于词向量，我们可以使用余弦相似度在计算机中来判断单词之间的距离： </p> 
<p>‘<span style="color:#86ca5e;">机器学习</span>’与‘<span style="color:#86ca5e;">深度学习</span>‘的距离：<img alt="cos(\Theta_1 )=\frac{1*2+2*3+3*3}{\sqrt{1^2+2^2+3^3}\sqrt{2^2+3^2+3^3}}=0.97" class="mathcode" src="https://images2.imgbox.com/25/d1/zcnF4H5P_o.gif"></p> 
<p>‘<span style="color:#86ca5e;">机器学习</span>’与‘<span style="color:#86ca5e;">英雄联盟</span>‘的距离：<img alt="cos(\Theta_2 )=\frac{1*9+2*1+3*3}{\sqrt{1^2+2^2+3^3}\sqrt{9^2+1^2+3^3}}=0.56" class="mathcode" src="https://images2.imgbox.com/d8/44/xsp6IivI_o.gif"></p> 
<p>通过这篇文章你可以学习到下面内容：</p> 
<ol><li>文本的词嵌入是什么，以及它与其他特征提取方法有什么区别。</li><li>介绍3种主流的从文本数据中学习词嵌入的方法。</li><li>怎么训练新的词嵌入以及如何在日常的NLP任务中使用预先训练好的词嵌入。  </li></ol> 
<h4> </h4> 
<h3>什么是词嵌入?</h3> 
<p>     词嵌入是一种对文本算法学习后的表示形式，甚至，你可以理解为一个单词在算法中的储存形式。大家知道存入计算机的都是0101的数值化序列，这里也是同理，词嵌入就是将文本数值化以方便拟合算法。这种将单词或者文档数字化表示的方式被认为是深度学习在自然语言处理任务中最具有挑战性的问题之一。</p> 
<p> </p> 
<blockquote> 
 <p style="margin-left:0px;"><span style="color:#555555;">使用密集和低维向量的一个好处是方便计算：大多数神经网络工具包不能很好地处理非常高维，稀疏的向量。......密集表示的主要好处是泛化能力，如果我们认为某些特征可能提供类似的线索，那么提供能够捕获这些相似性的表示是值得的。</span></p> 
</blockquote> 
<p> — Page 92, <a href="http://amzn.to/2wycQKA" rel="nofollow">Neural Network Methods in Natural Language Processing</a>, 2017.</p> 
<p>       词嵌入实际上是一种将各个单词在预定的向量空间中表示为实值向量的一类技术。每个单词被映射成一个向量（初始随机化），并且这个向量可以通过神经网络的方式来学习更新。因此这项技术基本集中应用与深度学习领域。</p> 
<p>     这项技术的关键点在于<span style="color:#e579b6;">如何用密集的分布式向量来表示每个单词。</span>这样做的好处在于与one-hot这样的编码对比，使用词嵌入表示的单词向量往往只有几十或者几百个维度。极大的减少了计算和储存量。 </p> 
<p> </p> 
<blockquote> 
 <p style="margin-left:0px;"><span style="color:#555555;">将词汇表中的每个词与分布式词特征向量相关联...特征向量表示词的不同方面特征，每个词与向量空间中的点相关联。特征的数量......远小于词汇的大小</span></p> 
</blockquote> 
<p>— <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="nofollow">A Neural Probabilistic Language Model</a>, 2003.</p> 
<p>      这种分布式的词向量表示方式依赖于单词的使用习惯，这就使得具有相似使用方式的单词具有相似的表示形式。这个怎么理解呢？回想我们在看游戏直播，IG获得冠军时候的弹幕，很多主播（无论游戏还是娱乐）的直播间都在发‘<span style="color:#86ca5e;">IG牛逼</span>’和 ‘<span style="color:#e579b6;">77777’</span>，所以我们算法能学习到，‘<span style="color:#86ca5e;">IG牛逼</span>’和 ‘<span style="color:#e579b6;">77777’</span>是向量空间中很相近的2个词。</p> 
<p>      虽然看起来有些粗糙，但是这个方法背后有很深的语言学理论支撑。即Zellig Harris的“<em>distributional hypothesis</em>”。这个假设可以归纳为：具有相似语境的词语具有相似的含义。有关更深入的信息，请参阅Harris的1956年论文 “<a href="http://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520" rel="nofollow">Distributional structure</a>“。</p> 
<p>这种使用语境定义单词的概念可以通过John Firth经常重复的谚语来概括：</p> 
<p> </p> 
<blockquote> 
 <p>You shall know a word by the company it keeps!</p> 
</blockquote> 
<p>— Page 11, “<a href="http://annabellelukin.edublogs.org/files/2013/08/Firth-JR-1962-A-Synopsis-of-Linguistic-Theory-wfihi5.pdf" rel="nofollow">A synopsis of linguistic theory 1930-1955</a>“, in Studies in Linguistic Analysis 1930-1955, 1962.</p> 
<p><span style="color:#f33b45;">小知识：（</span><a href="http://blog.sina.com.cn/s/blog_548d2b9d01018m55.html" rel="nofollow">链接</a>）</p> 
<blockquote> 
 <p>英语中有俗语You shall know a person by the company it keeps.</p> 
 <p>而英国语言学家 J. R. Firth在此基础上造出You shall know a word by the company it keeps.的语料库语言学名句。</p> 
 <p>类似的说法，在中国古已有之。</p> 
 <p><br> 其一：不知其人视其友 出自《孔子家语》（中华书局版，2009：136页）电子版下载<br> 子曰："商也好与贤己者处，赐也好说不若己者。不知其子视其父，不知其人视其友，不知其君视其所使，不知其地视其草木。故曰与善人居，如入芝兰之室，久而不闻其香，即与之化矣。与不善人居，如入鲍鱼之肆，久而不闻其臭，亦与之化矣。丹之所藏者赤，漆之所藏者黑，是以君子必慎其所与处者焉。"</p> 
 <p>其二：观人于其所友<br> 李敖引自中国古语，详细出处未考出。也或许为李敖所造。</p> 
</blockquote> 
<h3>词嵌入算法</h3> 
<p style="margin-left:0px;">词嵌入是从文本语料中学习到的一种将单词表示为预定义大小的实值向量形式。学习过程一般与某个神经网络的模型任务一同进行，比如文档分类。</p> 
<p style="margin-left:0px;">下面介绍3中词嵌入技术。</p> 
<h4 style="margin-left:0px;">1.Embedding Layer</h4> 
<p>       这里并没有专业的名次，使用就称之为Embedding Layer吧。Embedding Layer是特定的自然语言处理任务（例如语言建模或者文档分类）上的与神经网络模型共同学习更新的词嵌入方法。</p> 
<p>      使用Embedding Layer通常步骤一般是先预处理语料文本，将每个单词转化成one-hot形式的编码。而此单词对应的词向量其实是算法模型的其中一部分，词向量用预定义的维度来表示，大小我们随机初始化。在这里Embedding Layer其实就是神经网络的input layer，而词向量矩阵即是input layer 到 hidden layer中间的权值矩阵。</p> 
<blockquote> 
 <p>… when the input to a neural network contains symbolic categorical features (e.g. features that take one of k distinct symbols, such as words from a closed vocabulary), it is common to associate each possible feature value (i.e., each word in the vocabulary) with a d-dimensional vector for some d. These vectors are then considered parameters of the model, and are trained jointly with the other parameters.</p> 
</blockquote> 
<p>— Page 49, <a href="http://amzn.to/2wycQKA" rel="nofollow">Neural Network Methods in Natural Language Processing</a>, 2017.</p> 
<p>    牢骚一下： <span style="color:#7c79e5;">神经网络本来可解释性就不高，懂了也未必能理解其中参数的意味</span></p> 
<p>    这种将one-hot编码的单词映射到预定维度向量空间的方法，在处理句子时候，如果使用多层感知器模型，那么在作为模型的输入之前，需要将多个单词的向量连接起来（理解成CNN的全连接层吧），如果使用RNN来做的话，就不用这么考虑了，因为RNN本来就是以序列为输入的。</p> 
<p><span style="color:#555555;">       这种学习嵌入层的方法需要大量的训练数据并且可能很慢，但是确实能学习到针对特定文本数据和NLP任务的词嵌入。</span></p> 
<h4>2. Word2Vec</h4> 
<p style="margin-left:0px;">      Word2Vec是一种能有效从文本语料库中学习到独立词嵌入的统计方法。它是由<span style="color:#555555;">Tomas Mikolov等人2013年在谷歌开发的一个基于神经网络的词嵌入学习方法，并从那时起，开创了预训练单词嵌入的这种标准。</span></p> 
<p style="margin-left:0px;"><span style="color:#555555;">     这项工作涉及到语义基本的分析和单词向量化的数学探索。举个例子，在wrod2vec词向量空间中，“ <em>国王</em> ” -“ <em>男人</em> ”=“ <em>女王</em> ” -“ <em>女人</em>”，这就好像算法模型学习到了 国王对于女王的意义就好比男人对于女人的意义。</span></p> 
<blockquote> 
 <p style="margin-left:0px;">We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King – Man + Woman” results in a vector very close to “Queen.”</p> 
</blockquote> 
<p>— <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf" rel="nofollow">Linguistic Regularities in Continuous Space Word Representations</a>, 2013.</p> 
<ul><li>Continuous Bag-of-Words, or CBOW model.</li><li>Continuous Skip-Gram Model. </li></ul> 
<p>关于算法原理参数解释可以参考我之前的文章：<a href="https://blog.csdn.net/qq_41664845/article/details/82971728">【word2vec】算法原理 公式推导</a></p> 
<h4 style="margin-left:0px;"><span style="color:#222222;">3. GloVe</span></h4> 
<p>GloVe算法是对于word2vec方法的扩展，更为有效。它由斯坦福大学Pennington等人开发。</p> 
<p>传统经典的词向量空间模型会使用诸如LSA（潜在语义分析）之类的矩阵分解技术来生成单词的词向量表示形式。这种技术在全文检索统计方面做的很好，但是不如word2vec可以捕获单词的语义。</p> 
<p>GloVe是一种将矩阵分解技术（如LSA）的全局统计与word2vec中的基于上下文的学习相结合的方法。GloVe不是使用窗口来定义局部上下文，而是使用整个文本语料库中的统计信息构造显式的单词上下文或单词共现矩阵。这样的词嵌入方法可能会决定学习模型的好坏。</p> 
<blockquote> 
 <p>GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.</p> 
</blockquote> 
<p>— <a href="https://nlp.stanford.edu/pubs/glove.pdf" rel="nofollow">GloVe: Global Vectors for Word Representation</a>, 2014.</p> 
<h4> </h4> 
<h3>使用词嵌入</h3> 
<p>在自然语言处理项目中使用单词嵌入时，你可以选择下面几种方式。</p> 
<h4> </h4> 
<h4>自己学一个词嵌入</h4> 
<p>你可以通过你要解决的NLP问题自己学习一个词嵌入方法。当然这需要大量的文本数据来确保你的学习的词嵌入是有用的。例如几百万或者几十亿个单词。</p> 
<p>另外，训练词嵌入的时候，你也有2个选项：</p> 
<ol><li><strong>Learn it Standalone，</strong>训练模型学习词嵌入，训练好的结果保存下来，作为另一个模型的一部分。之后还可以作为其他任务的词嵌入。</li><li><strong>Learn Jointly</strong>, 将训练词嵌入作为一个大型任务模型的其中一部分，让训练词嵌入与NLP任务模型一同训练更新。但是这样往往是训练好的词嵌入只能提供给这一个模型使用</li></ol> 
<p> </p> 
<h4>使用别人训练好的词嵌入</h4> 
<p>研究人员通常可以免费提供经过预先训练的单词嵌入，这样你就可以在自己的学术或商业项目中下载并使用它们。</p> 
<p>比如最近腾讯就开源出了800W的词嵌入向量模型。不过15G的词嵌入文件，真不是一般人玩得起来的。这里提供一个地址：</p> 
<p><a href="https://github.com/cliuxinxin/TX-WORD2VEC-SMALL">https://github.com/cliuxinxin/TX-WORD2VEC-SMALL</a> 这个git主人，对我的学习指导很多，希望大家star一下。</p> 
<p>另外，使用已经训练好的词嵌入的时候，你也有2个选项：</p> 
<ol><li><strong>Static，</strong>直接套用</li><li><strong>Updated</strong>,作为初始权值.用自己的数据.更新一下</li></ol> 
<h4> </h4> 
<p>对于使用训练好的还是自己训练在业内目前应该还没有个标准，使用在实际工作中，你需要做一些不同的尝试来发掘能提升项目能力的一些方法。加油~</p> 
<h4> </h4> 
<h3>参考：</h3> 
<h4><a href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="nofollow">What Are Word Embeddings for Text?</a></h4> 
<p>by <a href="https://machinelearningmastery.com/author/jasonb/" rel="nofollow">Jason Brownlee</a> on October 11, 2017 in <a href="https://machinelearningmastery.com/category/natural-language-processing/" rel="nofollow">Deep Learning for Natural Language Processing</a></p> 
<p> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d4cd6af905dec1c0f7a7c052e6aa607d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Maven-编写测试代码</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d2ae7e62770bb5317fd769a48f948bd7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">CIO访谈实录丨渤海人寿携手SmartX超融合大幅提升开发测试效率</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>