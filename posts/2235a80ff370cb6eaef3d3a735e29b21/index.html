<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>[Paper Note] A Deep Journey into Super-resolution：A Survery - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="[Paper Note] A Deep Journey into Super-resolution：A Survery" />
<meta property="og:description" content="A Deep Journey into Super-resolution: A Survey Saeed Anwar, Salman Khan, and Nick Barnes
Abstract
基于深度卷积网络的超分辨率是一个快速发展的领域，具有许多实际应用。在本文中，我们在三个经典数据集和三个最近引入的具有挑战性的数据集上对超过30个最先进的超分辨率卷积神经网络（CNN）进行了比较，并以单图像超分辨率为基准来检验。我们提出了对基于深度学习的超分辨率网络的分类法，将现有方法分为九类，包括线性，残差，多分支，递归，渐进，基于注意力和对抗性设计。我们还比较了网络结构复杂性，内存占用，模型输入和输出，学习细节，网络损失类型和一些重要对结构差异（例如，深度，跳过连接，过滤器）。通过广泛的评估，显示出过去几年中模型准确性的一致和快速增长，以及模型复杂性和可以使用的大规模数据集的相应提升。还观察到，被确定为基准的开创性方法已经明显优于当前的竞争者。尽管近年来取得了进展，但我们发现了现有技术的一些缺点，并为解决这些开放性问题提供了未来的研究方向
Index Terms—Super-resolution (SR), High-resolution (HR), Deep learning, Convolutional neural networks (CNNs), Generative adversarial networks (GANs), Survey.
1.Introduction 近几年来，图像超分辨率(SR)得到了越来越多的研究界的关注.超分辨率的目的是将具有粗细节的低分辨率图像转换为具有高视觉质量和好的细节的高分辨率的图像， 使用更高的视觉质量和精细的细节处理高分辨率图像。图像超分辨率也被冠以其他名称，例如图像缩放、插值、上采样、缩放和扩大。生成分辨率更高的光栅图的过程可以使用单个图像或多个图像来执行。本文主要关注单幅图像的超分辨率，因为单图像的超分辨率更具挑战性，并且多图像的超分辨率直接建立在单图像的基础之上.
高分辨率图像提升了场景和组成对象的重建细节，对于许多设备来说都是至关重要的，例如大型计算机的显示、高清电视机和手持设备(移动电话、平板电脑、相机等)。此外，超分辨率在许多其他领域也有着重要的应用，例如场景中的物体检测[1]（特别是小的物体）[2])，监视视频中的人脸识别[3]、医学图像[4]、提高遥感图像的可解释性[5]、天文图像[6]和法医鉴定[7]。
超分辨率是一个经典的问题，由于多种原因，它仍然被认为是计算机视觉中一个具有挑战性和开放性的研究问题。首先，SR是一个不适定的逆问题，例如 欠定问题。对于相同的低分辨率图像，并不存在唯一，而是存在多个解。为了限定解空间，通常需要可靠的先验信息。第二，问题的复杂性随着上采样参数的增加而增加.在较多参数情况下，缺失的场景细节的恢复将更复杂，并且因此会经常导致重建出错误信息。此外，输出结果的质量无法直接衡量，即定量指标(例如PSNR “Peak Signal to Noise Ratio” ，SSIM “Structural Similarity index” )与人类的感知的联并不强。
超分辨率方法可大致分为两类：传统方法和基于深度学习的方法。传统的算法已经有几十年了，但现在效果远不如深度学习的方法。因此，许多新算法都是用数据驱动的深度学习模型来重建所需的细节，以获得精确的超分辨率。深度学习是机器学习的一个分支，机器学习的目的是直接从数据中自动学习输入和输出之间的关系。除了SR领域，深度学习算法在其他人工智能[8]的子领域上也显示出了良好的结果，例如，物体分类[9]和检测[10]、自然语言处理[11]，[12]、图像处理[13]，[14]和音频信号处理[15]。鉴于这些原因，在本综述中，我们主要关注SR的深度学习算法，对于传统方法则只是讲述简单的背景（第2节）。
**我们的贡献：**在这个论述中，我们的重点是单张(自然)图像超分辨率的深度学习方法。我们的贡献分五部分：1)我们对图像超分辨率的最新技术作了全面的回顾。2)根据SR算法的结构差异，提出了一种新的SR算法分类方法。3)对参数个数、算法设置，训练细节和重要的结构创新进行了综合分析，这些都会使得性能显著改进。4)在六个公开的SISR的数据集上对算法进行了系统的评价。5)我们讨论了挑战性问题，并对未来可能的研究方向提供了见解。
2.Background 假设以 y y y 表示低分辨率(LR)图像，并以 x x x 表示相应的高分辨率(HR)图像，则退化过程如下：
(1) y = Φ ( x ; θ η ) \mathbf{y}=\mathbf{\Phi}\left(\mathbf{x} ; \theta_{\eta}\right)\tag{1} y=Φ(x;θη​)(1)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/2235a80ff370cb6eaef3d3a735e29b21/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-06-19T23:50:45+08:00" />
<meta property="article:modified_time" content="2019-06-19T23:50:45+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">[Paper Note] A Deep Journey into Super-resolution：A Survery</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="A_Deep_Journey_into_Superresolution_A_Survey_0"></a>A Deep Journey into Super-resolution: A Survey</h2> 
<p>Saeed Anwar, Salman Khan, and Nick Barnes</p> 
<p><strong>Abstract</strong><br> 基于深度卷积网络的超分辨率是一个快速发展的领域，具有许多实际应用。在本文中，我们在三个经典数据集和三个最近引入的具有挑战性的数据集上对超过30个最先进的超分辨率卷积神经网络（CNN）进行了比较，并以单图像超分辨率为基准来检验。我们提出了对基于深度学习的超分辨率网络的分类法，将现有方法分为九类，包括线性，残差，多分支，递归，渐进，基于注意力和对抗性设计。我们还比较了网络结构复杂性，内存占用，模型输入和输出，学习细节，网络损失类型和一些重要对结构差异（例如，深度，跳过连接，过滤器）。通过广泛的评估，显示出过去几年中模型准确性的一致和快速增长，以及模型复杂性和可以使用的大规模数据集的相应提升。还观察到，被确定为基准的开创性方法已经明显优于当前的竞争者。尽管近年来取得了进展，但我们发现了现有技术的一些缺点，并为解决这些开放性问题提供了未来的研究方向<br> Index Terms—Super-resolution (SR), High-resolution (HR), Deep learning, Convolutional neural networks (CNNs), Generative adversarial networks (GANs), Survey.</p> 
<h3><a id="1Introduction_7"></a>1.Introduction</h3> 
<p>近几年来，图像超分辨率(SR)得到了越来越多的研究界的关注.超分辨率的目的是将具有粗细节的低分辨率图像转换为具有高视觉质量和好的细节的高分辨率的图像， 使用更高的视觉质量和精细的细节处理高分辨率图像。图像超分辨率也被冠以其他名称，例如图像缩放、插值、上采样、缩放和扩大。生成分辨率更高的光栅图的过程可以使用单个图像或多个图像来执行。本文主要关注单幅图像的超分辨率，因为单图像的超分辨率更具挑战性，并且多图像的超分辨率直接建立在单图像的基础之上.</p> 
<p>高分辨率图像提升了场景和组成对象的重建细节，对于许多设备来说都是至关重要的，例如大型计算机的显示、高清电视机和手持设备(移动电话、平板电脑、相机等)。此外，超分辨率在许多其他领域也有着重要的应用，例如场景中的物体检测[1]（特别是小的物体）[2])，监视视频中的人脸识别[3]、医学图像[4]、提高遥感图像的可解释性[5]、天文图像[6]和法医鉴定[7]。</p> 
<p>超分辨率是一个经典的问题，由于多种原因，它仍然被认为是计算机视觉中一个具有挑战性和开放性的研究问题。首先，SR是一个不适定的逆问题，例如 欠定问题。对于相同的低分辨率图像，并不存在唯一，而是存在多个解。为了限定解空间，通常需要可靠的先验信息。第二，问题的复杂性随着上采样参数的增加而增加.在较多参数情况下，缺失的场景细节的恢复将更复杂，并且因此会经常导致重建出错误信息。此外，输出结果的质量无法直接衡量，即定量指标(例如PSNR “<strong>P</strong>eak <strong>S</strong>ignal to <strong>N</strong>oise <strong>R</strong>atio” ，SSIM “<strong>S</strong>tructural <strong>Sim</strong>ilarity index” )与人类的感知的联并不强。</p> 
<p>超分辨率方法可大致分为两类：传统方法和基于深度学习的方法。传统的算法已经有几十年了，但现在效果远不如深度学习的方法。因此，许多新算法都是用数据驱动的深度学习模型来重建所需的细节，以获得精确的超分辨率。深度学习是机器学习的一个分支，机器学习的目的是直接从数据中自动学习输入和输出之间的关系。除了SR领域，深度学习算法在其他人工智能[8]的子领域上也显示出了良好的结果，例如，物体分类[9]和检测[10]、自然语言处理[11]，[12]、图像处理[13]，[14]和音频信号处理[15]。鉴于这些原因，在本综述中，我们主要关注SR的深度学习算法，对于传统方法则只是讲述简单的背景（第2节）。</p> 
<p>**我们的贡献：**在这个论述中，我们的重点是单张(自然)图像超分辨率的深度学习方法。我们的贡献分五部分：1)我们对图像超分辨率的最新技术作了全面的回顾。2)根据SR算法的结构差异，提出了一种新的SR算法分类方法。3)对参数个数、算法设置，训练细节和重要的结构创新进行了综合分析，这些都会使得性能显著改进。4)在六个公开的SISR的数据集上对算法进行了系统的评价。5)我们讨论了挑战性问题，并对未来可能的研究方向提供了见解。</p> 
<h3><a id="2Background_18"></a>2.Background</h3> 
<p>假设以 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         y 
        
       
      
        y 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span></span></span></span></span> 表示低分辨率(LR)图像，并以 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">x</span></span></span></span></span> 表示相应的高分辨率(HR)图像，则退化过程如下：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            (1) 
           
          
          
           
           
             y 
            
           
             = 
            
           
             Φ 
            
            
            
              ( 
             
            
              x 
             
            
              ; 
             
             
             
               θ 
              
             
               η 
              
             
            
              ) 
             
            
           
          
         
        
       
         \mathbf{y}=\mathbf{\Phi}\left(\mathbf{x} ; \theta_{\eta}\right)\tag{1} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.63888em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.01597em;">y</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathbf">Φ</span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">η</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span><span class="tag"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></span><br> 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Φ 
        
       
      
        \Phi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord">Φ</span></span></span></span></span>是退化函数，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          θ 
         
        
          n 
         
        
       
      
        \theta_n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>表示退化系数（例如缩放参数，噪声等）。在现实世界中，只有<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         y 
        
       
      
        y 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span></span></span></span></span>是已知的，其余的关于退化过程或者退化系数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          θ 
         
        
          n 
         
        
       
      
        \theta_n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的信息都没有。超分辨率旨在消除退化效果，并复原出真实图像<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">x</span></span></span></span></span>的近似解<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          ^ 
         
        
       
      
        \hat{x} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathit">x</span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.22222em;">^</span></span></span></span></span></span></span></span></span></span>，<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            (2) 
           
          
          
           
            
            
              x 
             
            
              ^ 
             
            
           
             = 
            
            
            
              Φ 
             
             
             
               − 
              
             
               1 
              
             
            
            
            
              ( 
             
            
              y 
             
            
              , 
             
             
             
               θ 
              
             
               ζ 
              
             
            
              ) 
             
            
           
          
         
        
       
         \hat{\mathbf{x}}=\Phi^{-1}\left(\mathbf{y}, \theta_{\zeta}\right) \tag{2} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.70788em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.70788em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">x</span></span></span></span><span class="" style="top: -3.01344em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.22222em;">^</span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.15022em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.864108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathbf" style="margin-right: 0.01597em;">y</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.07378em;">ζ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span><span class="tag"><span class="strut" style="height: 1.15022em; vertical-align: -0.286108em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></span><br> 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          θ 
         
        
          ζ 
         
        
       
      
        \theta_\zeta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.980548em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.07378em;">ζ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是函数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Φ 
         
         
         
           − 
          
         
           1 
          
         
        
       
      
        \Phi^{-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>的参数，退化过程是未知的并且十分复杂。这个过程可能被多种因素影响，例如噪音（传感器和斑点）、压缩、模糊（失焦和移动）以及一些其他的情况。因此，大多数研究方法都倾向于下面的退化模型而不是方程（1）<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            (3) 
           
          
          
           
           
             y 
            
           
             = 
            
           
             ( 
            
           
             x 
            
           
             ⊗ 
            
           
             k 
            
           
             ) 
            
            
            
              ↓ 
             
            
              s 
             
            
           
             + 
            
           
             n 
            
           
          
         
        
       
         \mathbf{y}=(\mathbf{x} \otimes \mathbf{k}) \downarrow_{s}+\mathbf{n} \tag{3} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.63888em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.01597em;">y</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathbf">k</span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel"><span class="mrel">↓</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord">+</span><span class="mord"><span class="mord mathbf">n</span></span></span><span class="tag"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">3</span></span><span class="mord">)</span></span></span></span></span></span></span><br> 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         k 
        
       
      
        k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03148em;">k</span></span></span></span></span>是模糊核，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         ⨂ 
        
       
         k 
        
       
      
        x\bigotimes k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00001em; vertical-align: -0.25001em;"></span><span class="mord mathit">x</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">⨂</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.03148em;">k</span></span></span></span></span>是高分辨率图像和模糊核之间的卷积操作，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ↓ 
        
       
      
        \downarrow 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mrel">↓</span></span></span></span></span>是下采样操作，缩放参数为s。变量 n 表示标准差（噪音等级）为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        \sigma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>的加性高斯白噪声。在图像超分辨率中，目标是最小化与模型相关联的数据保真项,模型为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         y 
        
       
         = 
        
       
         x 
        
       
         ⨂ 
        
       
         k 
        
       
         + 
        
       
         n 
        
       
      
        y=x \bigotimes k + n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.00001em; vertical-align: -0.25001em;"></span><span class="mord mathit">x</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">⨂</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.03148em;">k</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">n</span></span></span></span></span>，如<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            (4) 
           
          
          
           
           
             J 
            
            
            
              ( 
             
             
             
               x 
              
             
               ^ 
              
             
            
              , 
             
             
             
               θ 
              
             
               ζ 
              
             
            
              , 
             
            
              k 
             
            
              ) 
             
            
           
             = 
            
            
             
              
              
                ∥ 
               
              
                x 
               
              
                ⊗ 
               
              
                k 
               
              
                − 
               
              
                y 
               
              
                ∥ 
               
              
             
               ⎵ 
              
             
            
               data fidelity term  
             
            
           
             + 
            
           
             α 
            
            
             
              
              
                Ψ 
               
               
               
                 ( 
                
               
                 x 
                
               
                 , 
                
                
                
                  θ 
                 
                
                  ζ 
                 
                
               
                 ) 
                
               
              
             
               ⎵ 
              
             
            
               regularizer  
             
            
           
          
         
        
       
         J\left(\hat{\mathbf{x}}, \theta_{\zeta}, \mathbf{k}\right)=\underbrace{\|\mathbf{x} \otimes \mathbf{k}-\mathbf{y}\|}_{\text { data fidelity term }}+\alpha \underbrace{\Psi\left(\mathbf{x}, \theta_{\zeta}\right)}_{\text { regularizer }} \tag{4} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord mathit" style="margin-right: 0.09618em;">J</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.70788em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">x</span></span></span></span><span class="" style="top: -3.01344em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.22222em;">^</span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.07378em;">ζ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathbf">k</span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.47022em; vertical-align: -1.72022em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.75em;"><span class="" style="top: -1.41589em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight"> data fidelity term </span></span></span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.75em;"><span class="svg-align" style="top: -2.102em;"><span class="pstrut" style="height: 3em;"></span><span class="stretchy" style="height: 0.548em; min-width: 1.6em;"><span class="brace-left" style="height: 0.548em;"> 
                  <svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMinYMin slice"> 
                   <path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path> 
                  </svg></span><span class="brace-center" style="height: 0.548em;"> 
                  <svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMidYMin slice"> 
                   <path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path> 
                  </svg></span><span class="brace-right" style="height: 0.548em;"> 
                  <svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMaxYMin slice"> 
                   <path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path> 
                  </svg></span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">∥</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathbf">k</span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.01597em;">y</span></span><span class="mord">∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.898em;"><span class=""></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.72022em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 2.50632em; vertical-align: -1.75632em;"></span><span class="mord mathit" style="margin-right: 0.0037em;">α</span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.75em;"><span class="" style="top: -1.37978em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight"> regularizer </span></span></span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord munder"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.75em;"><span class="svg-align" style="top: -2.06589em;"><span class="pstrut" style="height: 3em;"></span><span class="stretchy" style="height: 0.548em; min-width: 1.6em;"><span class="brace-left" style="height: 0.548em;"> 
                  <svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMinYMin slice"> 
                   <path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13
 35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688
 0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7
-331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path> 
                  </svg></span><span class="brace-center" style="height: 0.548em;"> 
                  <svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMidYMin slice"> 
                   <path d="M199572 214
c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14
 53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3
 11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0
-5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path> 
                  </svg></span><span class="brace-right" style="height: 0.548em;"> 
                  <svg width="400em" height="0.548em" viewbox="0 0 400000 548" preserveaspectratio="xMaxYMin slice"> 
                   <path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3
 28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237
-174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path> 
                  </svg></span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">Ψ</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.07378em;">ζ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.934108em;"><span class=""></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.75632em;"><span class=""></span></span></span></span></span></span><span class="tag"><span class="strut" style="height: 2.50632em; vertical-align: -1.75632em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">4</span></span><span class="mord">)</span></span></span></span></span></span></span><br> 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.0037em;">α</span></span></span></span></span>是数据保真项和图像先验<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Ψ 
        
       
         ( 
        
       
         ⋅ 
        
       
         ) 
        
       
      
        \Psi(·) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">Ψ</span><span class="mopen">(</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mclose">)</span></span></span></span></span>的平衡因子。据Yang等人所说[16]，根据先验图像，超分辨率方法大致可分为：预测方法[17]、基于边的方法[18]、统计方法[19]、基于图像片(patch)的方法[20]、[21]、[22]和深度学习方法[23]。在本文中，我们的重点是应用深度神经网络学习这个先验的方法。</p> 
<h3><a id="3_Single_Image_SuperResolution_37"></a>3. Single Image Super-Resolution</h3> 
<p><img src="https://images2.imgbox.com/da/55/wZV4Db8U_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/be/c9/sEufO2G4_o.png" alt="在这里插入图片描述"><br> SISR问题得到了广泛的研究，许多文献都采用了不用的基于深度学习的技术。根据模型特点，我们将现有的方法分为九组。Fig.1 显示了本文献中使用的总体分类，其中，我们首先讨论了最早最简单的网络设计——线性网络。</p> 
<h4><a id="31_Linear_networks_41"></a>3.1 Linear networks</h4> 
<p>线性网络有一个简单的结构，只有一条信号流路径，没有任何跳过连接或多分支。在这样的网络设计中，多个卷积层相互叠加，输入依次从初始层流到后面的层。线性网络与上采样操作（早期上采样或后期上采样）执行的方式不同。一些线性网络学习重现残差图像，即LR和HR图像之间的差值[24]，[25]，[26]。由于在这种情况下，网络体系结构是线性的，所以我们将它们归类为线性网络。线性网络与残差网络(Sec 3.2)相反，残差网络在设计中有跳过链接。我们在两个子分类中详细阐述了值得注意的线性网络设计。</p> 
<h5><a id="311_Early_Upsampling_Designs_43"></a>3.1.1 Early Upsampling Designs</h5> 
<p>早期上采样的结构是线性网络，其首先对LR输入进行上采样，使其与期望的HR输出大小相匹配，然后学习层次特征表示来生成输出。一种常见的上采样操作是双三次插值，这是一种计算量很大的运算。基于这条技术线路的一项开创性工作是SRCNN，我们接下来将对此进行解释。</p> 
<p><strong>SRCNN</strong> Super Resolution CNN 超分辨率卷积神经网络，简称SRCNN[23]，[27]是第一次只使用卷积层来做超分辨率的成功的尝试，被认为是基于深度学习的SR的开创性工作，激发了后续在这个方向上的尝试。SRCNN结构简单明了，它只由卷积层组成，除最后一层外每一层后面都跟着整流线性单元(Relu)非线性。共有三个卷积层和两个ReLU层，线性地叠加在一起。虽然卷积层是相同的，作者将卷积层按功能命名。第一个卷积层叫作图像片(patch)提取层或特征提取层，其从输入图像中得到feature map。第二层为非线性映射层，将feature map映射成高维特征向量。最后一层聚合feature map来输出最终的高分辨率图像。SRCNN的结构如 Fig.2 所示。<br> 训练数据集是通过从HR图像中提取大小为32×32的不重叠的稠密图像块来合成。低分辨率的输入图像片首先先下采样，然后用双三次插值进行上采样，使其具有与高分辨率输出图像相同的大小。SRCNN是一种端到端可训练的网络，最大限度地减少了输出的重构高分辨率图像与真实高分辨率图像的差值，损失函数使用的是均方误差MSE。</p> 
<p><strong>VDSR</strong> Very Deep Super-Resolution 与SR-CNN[23]和FSRCNN[28]中使用的浅层网络结构不同，深度超分辨率(VDSR)[24]基于[29]中提出的深度CNN体系结构。这种架构通常被称为VGG-NET，并在所有层都是用3x3大小的卷积核。为了避免深层网络(尤其是20层权重层)的收敛缓慢问题，提出了两种有效的策略。首先，不直接生成HR图像，而是学习残差映射，通过映射来产生HR和LR图像差。因此，它提供了一个更容易实现的目标，而且络只关注高频信息。其次，梯度剪裁至[θ，θ]范围，这样可以用很高的学习率来加快训练过程。结果证明更深层次的网络可以提供更好的上下文信息和更好的泛化表示能力，可以用来解决多尺度的超分辨率问题。</p> 
<p><strong>DnCNN</strong> Denoising CNN [25]学习直接预测高频残差，而不是超分辨图像。残差图像是LR和HR图像的差。DnCNN结构非常简单，类似SRCNN，只堆叠卷积层，然后是批标准化和ReLU层。DnCNN的体系结构如 Fig.2 所示。</p> 
<p>虽然这两种模型都能获得良好的结果，但它们的性能在很大程度上取决于噪声估计的准确性，并且是不知道图像底层结构和纹理的。此外，由于每个卷积层之后都有批标准化操作，所以计算量很大。</p> 
<p><strong>IRCNN</strong> Image Restore CNN[26]提出了一组基于CNN的去噪器，可以联合起来用于一些低级视觉任务，例如图像去噪、去模糊和超分辨率。该技术旨在将高性能的有区分度的CNN网络与基于模型的优化方法结合起来，以便在图像恢复任务上获得更好的泛化能力。特别是半二次方分割技术，这项技术用来分离正则化和保真项。之后，由于CNN具有优秀的建模能力和测试时间效率，因此利用CNN分别对去噪先验进行学习。CNN去噪器由7个扩展(膨胀)卷积层组成，每个层都有批标准化层和ReLU非线性层。扩展卷积操作通过更大的感受野来帮助建模更大的上下文信息。为了加快学习过程，残差图像学习规则与之前的VDSR[24],DRCN[31],DRRN[32]结构相同。作者还提出了使用小尺寸样本训练和零填充，以避免卷积操作的边界问题。</p> 
<p>一组25个去噪器，训练的噪声等级属于[0,50]，这些噪声集成在一起专门用来做图像恢复任务的训练。所提出的方法在图像去噪、去模糊和超分辨率方面都有很好的效果。</p> 
<h5><a id="312__Late_Upsampling_Designs_58"></a>3.1.2 Late Upsampling Designs</h5> 
<p>正如我们在前面的例子中所看到的，线性网络通常对输入图像进行早期上采样。这种操作计算量很大，因为后期的网络结构会因为处理较大的输入而成倍增长。为了解决这一问题，提出后上采样(post-upsampling)网络，后上采样网络先学习低分辨率输入，然后对网络输出附近的特征进行上采样。这种策略的优点是低内存占用，并且高效。我们将在下面讨论这些设计。</p> 
<p><strong>FSRCNN</strong> Fast Super-Resolution CNN (FSRCNN)[28]比SRCNN[27]有更高速度和更好的质量。目的是达到实时计算速度(24 fps)，SRCNN是1.3fps。FSRCNN[28]架构简单，由四个卷积层和一个反卷积层组成。FSRCNN[28]的体系结构如 Fig.2 所示。</p> 
<p>尽管前四层都实现卷积运算，但FSRCNN[28]根据功能命名了各层，即特征提取层、收缩层、非线性映射层和扩展层。特征提取步骤类似于SRCNN[27]，唯一的区别在于输入大小和过滤器大小。SRCNN[27]的输入是一个上采样的双三次图像片，而FSRCNN[28]的输入是没有经过上采样的原始图像片。第二个卷积层被命名为收缩层，因为它通过采用更小的滤波器尺寸(f=1)来减少特征维数(参数个数)，从而提升了计算效率。第三层是非线性映射层，作者认为这在SRCNN[27]和FSRCNN[28]中都是关键的一步，因为它有助于学习非线性函数并且会因此对效果产生很大的影响。通过实验,非线性映射层中的滤波器的大小设置为3,通道数量和之前的层一样。最后，扩展层，是收缩步骤的反运算。通过扩展来增加维数。该层使性能提高了0.3dB。</p> 
<p>网络的最后一部分是上采样和聚集反卷积层，它是卷积的逆过程。在卷积运算中，图像与卷积核按照一定步长进行卷积操作，输出是输入的大小乘以步长的倒数，即1/stride。然而，滤波器在反卷积层中的作用和卷积层正好相反，这里的步长可看作是上采样参数。类似地，另一个与SRCNN[27]的微妙的区别是在每个卷积层之后使用带参数的ReLU(PReLU)[33]而不是ReLU。 <em>ps.PReLU 是 分段为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
          和 
         
        
          α 
         
        
          ∗ 
         
        
          x 
         
        
       
         x 和 \alpha * x 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.46528em; vertical-align: 0em;"></span><span class="mord mathit">x</span><span class="mord cjk_fallback">和</span><span class="mord mathit" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">x</span></span></span></span></span></em></p> 
<p>FSRCNN[28]使用与SRCNN[27]相同的损失函数，即均方误差。在训练方面，[28]使用了91-image 数据集[34]，以及从互联网上收集的100幅图像。在数据增强方面，通过例如旋转、翻转和缩放等操作，使图像的数量增加了19倍。</p> 
<p><strong>ESPCN</strong> Efficient sub-pixel CNN (ESPCN)[35]是一种快速的SR方法，可以在实时情况下对图像和视频进行操作。如上所述，传统SR技术首先通过双三次插值将LR图像映射到更高的分辨率，然后在高维空间中学习SR模型。ESPCN指出，这套流程需要很高的计算要求，并建议在LR空间中进行特征提取。在提取特征后，ESPCN在其末端使用子像素卷积层来聚合LR特征，并同时将其映射到高维空间，在高维空间中重建HR图像。在LR空间中进行特征处理大大减少了内存和计算量。</p> 
<p>本工作中使用的子像素卷积运算实质上类似于卷积转置或反卷积运算[36]，其中使用分数核步长来增加输入特征图的空间分辨率。另外使用一个单独的扩展核来映射每个特征图，从而为LR到HR映射的建模提供了更大的灵活性。使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失来训练整体的网络。ESPCN在单个GPU上对1080p的视频处理，具备有竞争力的SR性能和较高的实时处理速度。</p> 
<h4><a id="32_Residual_Networks_73"></a>3.2 Residual Networks</h4> 
<p>与线性网络相比，残差学习在网络设计中使用跳过连接来避免梯度消失问题，从而可以设计非常深的网络。[9]首先阐述了它对图像分类问题的意义。最近，一些网络[37]，[38]都利用残差学习来提高SR性能。在这种方法中，算法学习残差，也就是输入和真实值之间的高频。根据这些网络中使用的阶段数，我们将现有的残差学习方法分为单阶段[37]、[38]和多阶段网络[39]、[40]、[41]。</p> 
<h5><a id="321_Singlestage_Residual_Nets_75"></a>3.2.1 Single-stage Residual Nets</h5> 
<p><strong>EDSR</strong> Enhanced Deep Super-Resolution (EDSR)[37]修改了最初用于图像分类的ResNet体系结构[9],使其能够处理SR任务。具体来说，从每个残差块中移除了批标准化层，移除了ReLU激活（残余块外部），这种改动证明是有效的。与VDSR类似，还扩展了单一尺度方法以处理多个尺度。提出的多尺度深度SR(MDSR)结构能够通过大量的共享参数来减少参数的数量。特定尺度的层仅在输入和输出块附近并行地应用，以学习与尺度相关的表示。训练使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失。数据增强(旋转和翻转)被用于创建“自集成”，即经过变换的输入穿过整个网络，加以反向变换和平均，合在一起创建单个输出。作者指出，这种自集合方案不需要学习多个单独的模型，但产生的结果可与传统的基于集合的模型相比较。与以前的一些结构，例如SR-CNN、VDSR和其他基于ResNet的近似结构(SR-GAN[42])相比，EDSR和MDSR在量化方法(Peak Signal to Noise Ratio 峰值信噪比)方面有更好的性能。<br> <strong>CARN</strong> Cascading residual network(CARN)[38]使用残差块[43]学习低分辨率输入与高分辨率输出之间的关系。模型的不同之处在于局部和全局级联模块的存在。中间层的特征被级联并收敛到1×1的卷积层上。局部级联中都是简单的残差块，除此以外局部级联连接与全局级联连接相同。由于多级别表示和快捷（跳跃）连接，使得信息传播效率更高。CARN模型结构如 Fig.2 所示。</p> 
<p>这个模型使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         64 
        
       
         × 
        
       
         64 
        
       
      
        64\times64 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">6</span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">6</span><span class="mord">4</span></span></span></span></span>的图像片进行训练，数据来自Yang等人的BSD[44]和有数据增强的DIV2K数据集[45]，采用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失。优化器是Adam[46]，初始学习率是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
         
         
           − 
          
         
           4 
          
         
        
       
      
        10^{-4} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></span>，每经过<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         4 
        
       
         × 
        
       
         1 
        
        
        
          0 
         
        
          5 
         
        
       
      
        4\times10^5 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span>步学习率减半。</p> 
<h5><a id="322_MultiStage_Residual_Nets_80"></a>3.2.2 Multi-Stage Residual Nets</h5> 
<p>多阶段结构由多个子网组成，并且一般是按顺序训练[39]，[40]。第一个子网通常对粗特征进行预测，然后其他子网改进初始预测。在本段还还包括了编码器-解码器的设计(例如[41])，它首先使用编码器对输入进行下采样，然后通过解码器执行上采样(因此是两个不同的阶段)。下面的结构通过各种不同的阶段进行了图像超分辨率。</p> 
<p><strong>FormResNet</strong> Formesnet由[39]提出，其建立在DnCNN上，如 Fig.2 所示。该模型由两个网络组成，它们都类似于DnCNN，不同的是损失层。第一个网络，被称为“Formatting layer”，包含欧几里德和感知损失。像BM3D这样的经典算法也可以代替这个格式化层。第二个网络“DiffResNet”类似于DnCNN，其输入来自于第一个网络。第一个网络消除了均匀区域中的高频损坏，而第二个网络学习结构化区域。FormResNet对DnCNN的结果做了很小的提升。</p> 
<p><strong>BTSRN</strong> BTSRN是用于图像超分辨率的平衡的两阶段残差网络[40]的代表。该网络由低分辨率阶段和高分辨率阶段组成。在低分辨率阶段，feature map具有较小的大小，与输入相同。使用反卷积和最近邻上采样对feature map进行上采样。然后将上采样后的feature map喂到高分辨率阶段。在低分辨率和高分辨率级中，都使用了一种称为投影卷积的残差块[9]变体。残差块由1×1卷积层组成并作为特征图的映射，以此来减小3×3卷积特征的输入大小。低分辨率阶段有6个残差块，高分辨率阶段由4个残差块组成。<br> 作为2017年NTIRE挑战赛的参赛模型[45]，该模型接受了来自DIV2K数据集[45]，800个训练图像和100个验证图像，共900个图像的训练。在训练过程中，图像被裁剪为108×108大小的图像片，并通过翻转和旋转操作进行增强。初始学习率设定为0.001，在每次迭代后指数下降0.6倍，使用Adam进行优化。 残差块由128个特征图作为输入，64个作为输出。预测值和真实值之间的差距通过<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>距离来衡量。</p> 
<p><strong>REDNet</strong> 最近，由于UNet[47]的成功，[41]提出了一种使用编码器(基于卷积层)和解码器(基于反卷积层)的超分辨率算法。RedNet[41]Residual Encoder Decoder Network主要由卷积层和对称的反卷积层组成。在每个卷积层和反卷积层之后都有一个ReLU层。卷积层用来提取特征并保留对象结构和消除退化。另一方面，反卷积层重构了图像中缺失的细节。此外，在卷积层和对称的反卷积层之间增加了跳跃连接。在进行非线性校正之前，将卷积层的特征图与镜像反卷积层的输出相加。输入是双三次插值图像，最后的反卷积层的输出是高分辨率图像。网络是端到端可训练的，通过最小化系统输出和真实值之间的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>范数实现收敛。结构如 Fig.2。</p> 
<p>作者提出了RedNet体系结构的三个变体，其总体结构保持不变，但卷积层和反卷积层的数目发生了一些变化。最佳执行体系结构有30层权重层，每个层64个feature map。此外，Berkeley Segmentation Dataset(BSD)[44]中的亮度通道被用来生成训练图像集。按规则步长提取<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         50 
        
       
         × 
        
       
         50 
        
       
      
        50\times50 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">5</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">5</span><span class="mord">0</span></span></span></span></span>大小的图像块，作为真实值。通过对真实值进行进行下采样，再对其用双三次插值上采样到原始大小，形成输入。网络通过从91 images[34]中提取图像片进行训练，使用均方误差(MSE)损失函数。输入大小<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         9 
        
       
         × 
        
       
         9 
        
       
      
        9\times9 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">9</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">9</span></span></span></span></span>，输出大小<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         5 
        
       
         × 
        
       
         5 
        
       
      
        5\times5 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">5</span></span></span></span></span>。这些图像片通过均值和方差标准化，这些均值和方差随后添加到相应的复原得到的高分辨率输出中。核的大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         5 
        
       
         × 
        
       
         5 
        
       
      
        5\times5 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">5</span></span></span></span></span>，有128个特征通道。</p> 
<h4><a id="33_Recursive_networks_91"></a>3.3 Recursive networks</h4> 
<p>顾名思义，递归网络[31]、[32]、[48]使用递归连接的卷积层或递归链接的单元。这种设计的主要目标是逐步将更困难的SR问题分解成一组简单的问题，这些简单的问题更容易解决。基本结构如 Fig.2 所示，并且在下文中提出了更多的递归模型的细节。</p> 
<h5><a id="331_DRCN_93"></a>3.3.1 DRCN</h5> 
<p>如名字所示，Deep Recursive Convolutional Network (DRCN）多次叠加相同的卷积层。这种技术的一个优点是对于更多的递归，参数的数量保持不变。DRCN[31]由三个较小的网络组成，即嵌入网、推理网和重建网。</p> 
<p>第一个子网称为嵌入网络，将输入(灰度或彩色图像)转换为特征图。第二个网络(推理网)执行超分辨率分析，通过递归地使用由卷积和ReLU组成的单个层来分析图像区域。每次递归后，感受野的大小都会增加。推理网的输出是高分辨率的特征图，通过重构网络将其转换为灰度或彩色。</p> 
<h5><a id="332_DRRN_98"></a>3.3.2 DRRN</h5> 
<p>Deep Recursive Residual Network (DRRN)[32]提出了一个深度CNN模型，但具有保守的参数复杂程度。与以前的模型VDSR[24]、REDNet[41]和DRCN[31]相比，该模型更深，具有多达52个卷积层。同时，相对于REDNet、DRCN和VDSR，DRRN分别将网络复杂度降低了14、6和2倍。这是通过将残差图像学习[49]与网络层的小块之间的局部特征连接结合起来实现的。作者强调，这种并行信息流实现了更深的网络结构的稳定训练。</p> 
<p>与DRCN[31]类似，DRRN利用递归学习，即多次复制一个基本的跳跃连接块来实现多路径网络区域(见Fig.2)。因为重复部分之间的参数是共享的，所以内存开销和计算复杂度都得到了显著的降低。最终结构通过堆叠多个递归部分获得。DRCN使用标准的SGD优化器[49]进行参数训练。损失层基于MSE损失，类似于其他流行的结构。这个提出的构作出了与以前的方法一致的改进，它支持更深的递归结构和残差学习。</p> 
<h5><a id="333_MemNet_102"></a>3.3.3 MemNet</h5> 
<p>Tai等人提出了一种新的图像超分辨率的持久记忆网络(简称MemNet)[48]。.MemNet可以同SRCNN[27]一样分解为三个部分。第一部分称为特征提取块，它从输入图像中提取特征。这部分与[27]、[28]、[35]等早期设计相同。第二部分由堆叠在一起的一系列记忆块组成，是记忆网络中的关键部分。如Fig.2 所示，记忆块由一个递归单元和一个门单元组成。递归部分类似于ResNet[43]，由两个具有预激活机制的卷积层和与门单元的紧密连接组成。每个门单元是一个1×1卷积核大小的卷积层。</p> 
<p>Memnet[48]采用MSE损失函数。实验设置与VDSR[24]相同，使用了200张来自BSD[44]的图片和来自Yang等人的91图。网络由六个记忆块组成，每个记忆块有六次递归。MemNet中的总层数为80。MemNet还用于其他图像恢复任务，例如图像去噪和JPEG去块，并在这些任务中显示出了不错的结果。</p> 
<h4><a id="34_Progressive_reconstruction_designs_106"></a>3.4 Progressive reconstruction designs</h4> 
<p>通常情况下，CNN算法可以在一步内预测输出，但是对于大规模参数来说，这可能是不可行的。为了处理大数目的参数，一些算法[50]、[51]对输出进行了多步预测，即2×然后4×等。</p> 
<h5><a id="341_SCN_108"></a>3.4.1 SCN</h5> 
<p>Wang等人[50]提出了一种将稀疏编码[52]的优点与深度神经网络的领域知识相结合的方案。通过这种结合，它致力于获得一个紧凑的模型和性能提升。所提出的sparse coding-based network(SCN)[50]模拟了Learned Iterative Shrinkage and Thresholding Algorithm(LISTA)来建立多层神经网络。</p> 
<p>类似于SRCNN[23]，第一个卷积层从低分辨率图像中提取特征，然后将其送到LISTA网络。为了获得每个特征的稀疏编码，LISTA网络由有限数量的重复阶段组成。LISTA级由两个线性层和一个具有激活函数的非线性层组成，激活函数的阈值是在训练期间学习/更新的。为了简化训练，作者将非线性神经元分解为两个线性缩放层和一个单位阈值神经元。这两个缩放层是对角矩阵，它们之间是相互的。例如，如果存在成倍增大的层，则在阈值单元之后对其进行相应的缩小。在LISTA网络之后，通过在连续的线性层上将稀疏码和高分辨率字典相乘，重建原始高分辨率图像片。最后一步，再次使用线性层，将高分辨率图像片放置在图像的原始位置，以获得高分辨率的输出。</p> 
<h5><a id="342_LapSRN_112"></a>3.4.2 LapSRN</h5> 
<p>Deep Laplacian pyramid super-resolution network (LapSRN)[51]采用金字塔框架。LapSRN由三个子网络组成，它们将残差图像逐步预测到8倍大小。每个子网络的残差图像被添加到低分辨率输入图像中，以获得高分辨率图像。第一个子网络的输出为是2倍残差，第二个子网络提供4倍大小的残差，最后一个子网给出8倍残差图像。将这些残差图像添加到相应尺寸的上采样图像中，以获得最终的超分辨率图像。作者将残差预测分支定义为特征提取，同时将双三次图像和残差的和称为图像重建分支。LapSRN网络结构如Fig.2。由三种类型组成，即卷积层、leaky ReLU层和反卷积层。按照CNN的惯例，卷积层在Leaky ReLU(允许负斜率为0.2)之前，反卷积层在在子网末端，以增加相应尺寸的残差图像的大小。</p> 
<p>LapSRN使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失函数的的一个可微变体，称为Charbonnier，它可以处理异常值。每个子网络都使用该损失，类似于一种多损耗结构。卷积层过滤器大小<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         3 
        
       
         × 
        
       
         3 
        
       
      
        3\times3 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span></span></span></span></span>，反卷积层过滤器大小<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         4 
        
       
         × 
        
       
         4 
        
       
      
        4\times4 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">4</span></span></span></span></span>，每个都有64个通道。训练数据类似SRCNN[27]，Yang等人的91图和BSD数据集中的200图。<br> LapSRN模型使用三种不同的模型执行2×、4×和8×的超分辨率。他们还提出了一个单模型，称为多尺度(MS)LapSRN，它联合在一起学习如何处理多种SR尺度[53]。有趣的是，单个MSLapSRN模型的性能优于从三个不同模型得到的结果。一个解释是，单个模型利用了公共的尺度特征，有助于实现更精确的结果。</p> 
<h4><a id="35_Densely_Connected_Networks_117"></a>3.5 Densely Connected Networks</h4> 
<p>在用于图像分类的DenseNet[54]体构获得成功后，受到启发提出了基于稠密连接的CNN的超分辨率算法，以提高性能。这种设计的主要动机是将网络深度上可获得的层次线索结合起来，以获得高度的灵活性和更丰富的特征表示。下面讨论一些流行的模型。</p> 
<h5><a id="351_SRDenseNet_119"></a>3.5.1 SR-DenseNet</h5> 
<p>网络体系结构[55]基于DenseNet[54]，DenseNet使用层间的密集连接，即一个层直接操作所有之前层的输出。这样的从低级到高级特征层的信息流避免了消失梯度问题，能够学习精简模型并加快训练过程。对于网络的后面部分，SR-DenseNet使用几个反卷积层来扩大输入尺寸。作者提出了SR-DenseNet的三种变体。(1)稠密块顺序排列，然后是反卷积层。在这种方式下只有高级特征被用来重建最终的SR图像。(2)在最终重建前，结合从初始层获得的低级特征。为此，需要使用跳跃连接来结合低级别和高级别的特征。(3)通过利用低层特征和稠密块之间的多个跳跃连接将所有特征结合起来，以获得直接的信息流，从而获得更好地HR重建结果。由于互补特征是在网络中的多个阶段编码的，所以将所有feature map结合一起的方式比SR-DenseNet的其他变体有更好的效果。整个模型训练使用均方误差（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失）。总的来说，SR-DenseNet模型证明，与不使用层间稠密连接的模型相比，使用以后性能得到提高。</p> 
<h5><a id="352_RDN_121"></a>3.5.2 RDN</h5> 
<p>顾名思义，Residual Dense Network[56] (RDN)将残差跳跃连接（受SR-ResNet的启发）与稠密连接（受SR-DenseNet的启发）结合起来。主要想法是分层特征表示应完全用来学习局部模式。为此，在两种不同范围内引入残差连接，局部和全局。在局部范围，提出了一种新的残差稠密块(RDB)，其中每个块的输入(前一个块的图像或输出)被转发到所有有RDB的层，并添加到块的输出，从而使得每个块更多地关注残差模式。由于稠密的连接会快速地导致高维输出，因此在每个RDB中采用了一种使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
       
         × 
        
       
         1 
        
       
      
        1\times1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>卷积的局部特征融合方法来减少维数。在全局范围，将多个RDB的输出融合在一起(通过级联和1×1卷积操作)，并进行全局残差学习，以结合来自多个块的特征。残差连接有助于稳定网络训练，并提升了SR-DenseNet[55]的性能。</p> 
<p>相对于SR-DenseNet中使用的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失，RDN使用了<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失函数，并证明了其收敛性有所提升。对每个批次随机选取的32×32的图像片对网络进行训练。将翻转和旋转等数据增强方法作为一种正则化措施使用。作者还试验了在LR图像中存在的不同形式的退化(例如噪声和伪影)。本网络显示了良好的抵抗退化的弹性，并恢复了大量的增强SR图像。</p> 
<h5><a id="353_DDBPN_125"></a>3.5.3 D-DBPN</h5> 
<p>Dense deep back-projection network。用于超分辨率的稠密深度反向投影网络[57]从传统的SR方法(例如，[17])中汲取灵感，这些传统方法迭代地执行反向投影来学习LR和HR图像之间的反馈误差信号。其动机是，只用前馈方法不是最佳的对从LR到HR的图像映射进行建模的方法，而反馈机制可以极大地帮助获得更好的结果。为此目的，所提出的结构包括一系列的上、下采样层，它们之间紧密相连。通过这种方式，将来自网络中不同深度的HR图像组合在一起，以实现最终的输出。</p> 
<p>上、下采样块的结构如Fig.2所示。简洁起见，本文给出了前几层单连接的更简单的情况，并将读取器定向到[57]以获得完整的稠密连接块。该设计的一个重要特征是把输入特征图的上采样输出和残差信号结合起来。在上采样特征图中显式地添加残差信号提供了误差反馈，并迫使网络将注意力放在精细的细节上。使用标准的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失函数训练网络。D-DBPN对于4倍的SR有相对较高的计算复杂度，具有千万级的参数。不过，也推出了复杂度较低的版本，但是会导致性能略有下降。</p> 
<h4><a id="36_Multibranch_designs_129"></a>3.6 Multi-branch designs</h4> 
<p>与单流程(线性)和跳跃连接的设计不同，多分支网络的目标是在多个上下文尺度上获得不同特征集合，然后融合这些互补信息以获得更好的HR重建。该设计还可以实现多个信号流，从而在训练中的前向和后向过程中更好交换信息。多分支设计在其他几个计算机视觉任务中也越来越普遍。我们在下面一节中解释多分支网络。</p> 
<h5><a id="361_CNF_131"></a>3.6.1 CNF</h5> 
<p>Ren等人[58]提出了融合多卷积神经网络用来解决图像超分辨率问题。作者将他们的CNN网络称为上下文网络融合(CNF)，其中每个SRCNN[27]都是由不同的层数构成的。然后，每个SRCNN[27]的输出通过一个卷积层，最后通过求和池化融合。</p> 
<p>使用从开放图像数据集[59]，[60]收集的2,000万个图像训练模型，每个图像大小仅为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         33 
        
       
         × 
        
       
         33 
        
       
      
        33\times33 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span><span class="mord">3</span></span></span></span></span>像素，只有亮度通道。首先，每个SRCNN分别被训练50轮，学习率为1e-4。然后对融合网络进行十轮训练，学习率不变。这种渐进学习策略类似于课程学习，它从简单的任务开始，然后转移到更复杂的任务，联合优化多个子网以获得更好的SR。损失是均方误差。</p> 
<h5><a id="362_CMSC_135"></a>3.6.2 CMSC</h5> 
<p>Cascaded multi-scale cross-network, 简称CMSC[61]，由特征提取层、级联子网和重构网络组成。特征提取层和SRCNN[27]、FSRCNN[28]中的特征提取层功能一样。每个子网由合并运行块(merge-and-run MR)组成。每个MR块由两个平行分支组成，每个分支具有两个卷积层。每个分支的残差连接被累积在一起，然后分别添加到两个分支的输出中，如Fig.2 所示。CMSC的每个子网由4个MR块组成，它们各自具有的感受野，大小分别为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         3 
        
       
         × 
        
       
         3 
        
       
      
        3\times3 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span></span></span></span></span>、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         5 
        
       
         × 
        
       
         5 
        
       
      
        5\times5 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">5</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         7 
        
       
         × 
        
       
         7 
        
       
      
        7\times7 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">7</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">7</span></span></span></span></span>，用于在多尺度上获取上下文信息。此外，MR块中的每个卷积层后都跟着批标准化和Leaky ReLU[62]。最后的重建层生成最终输出。</p> 
<p>使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失函数是，它使用一个平衡项将中间输出与最终输出相结合。输入采用双三次插值的上采样，图像大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         41 
        
       
         × 
        
       
         41 
        
       
      
        41\times41 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">4</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">4</span><span class="mord">1</span></span></span></span></span>。该模型使用与VDSR[24]相似的291幅图像进行训练，初始学习速率为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
         
         
           − 
          
         
           1 
          
         
        
       
      
        10^{-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>，共50轮，每10轮学习率降低10倍。CMSC性能不如EDSR[37]及其变体MDSR[37]。</p> 
<h5><a id="363_IDN_140"></a>3.6.3 IDN</h5> 
<p>The Information Distillation Network (IDN)[63]由三部分组成：特征提取、多层信息蒸馏和重建。特征提取块由两个卷积层组成，用于特征提取。蒸馏块由两部分组成，一个是增强单元，另一个是压缩单元。<br> 增强单元有六个卷积层，每层都有leaky ReLU。第三个卷积层的输出被切片，一半与块的输入连接，另一半作为第四卷积层的输入。级联组件的输出与增强块的输出相加。总共使用了四个增强块。压缩单元的实现通过在每个增强块后使用1×1卷积层来实现。重建部分是一个核大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         17 
        
       
         × 
        
       
         17 
        
       
      
        17\times17 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mord">7</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">7</span></span></span></span></span>的反卷积层。</p> 
<p>首先使用绝对值平均误差损失对网络进行训练，然后利用均方误差损失对网络进行精细调整。训练的图像与[48]相同。输入大小是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         26 
        
       
         × 
        
       
         26 
        
       
      
        26\times26 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">6</span></span></span></span></span>。使用Adam46]作为优化器，初始学习速率为1e-4，共<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
        
          5 
         
        
       
      
        10^5 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span>次迭代。</p> 
<h4><a id="37_Attentionbased_Networks_145"></a>3.7 Attention-based Networks</h4> 
<p>前面讨论的网络设计考虑所有空间位置和信道对于超分辨率问题有一致的重要性。在一些情况下，在给定的层中选择性地处理部分特征可能会更有帮助。基于注意力的模型[64]，[65]允许这种特定选择的灵活性，并认为并不是所有的特性都是超分辨率所必需的，其重要程度不同。基于注意力的模型与深层网络相结合，已经显示出对SR的显著改进。下面是使用注意机制的CNN模型。</p> 
<h5><a id="371_SelNet_147"></a>3.7.1 SelNet</h5> 
<p>Choi和Kim[64]提出了一种新的用于图像超分辨率网络的选择单元，称为SelNet。选择单元充当卷积层之间的门，只允许通过从特征图中选定的值。该选择单元由一个恒等映射和一个ReLU级联、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
       
         × 
        
       
         1 
        
       
      
        1\times1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>卷积和一个sigmoid层组成。SelNet总共有22个卷积层，在每个卷积层之后都有选择单元。类似于VDSR[24]，在SelNet[64]中也采用了残差学习和梯度转换（梯度裁剪的一种），以实现更快的学习。</p> 
<p>从DIV2K数据集[45]中剪裁出大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         120 
        
       
         × 
        
       
         120 
        
       
      
        120\times120 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mord">2</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">2</span><span class="mord">0</span></span></span></span></span>的低分辨率图片作为网络输入。学习率为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
         
         
           − 
          
         
           1 
          
         
        
       
      
        10^{-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>，共训练50轮，使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失。</p> 
<h5><a id="372_RCAN_151"></a>3.7.2 RCAN</h5> 
<p>Residual Channel Attention Network (Rcan)[65]是最近提出的一种用于单图像超分辨率的深度CNN结构。这种结构的主要亮点包括：(a)递归残差设计，残差连接在全局残差网络的每个块内均存在，(B)每个局部残差块都有一个通道注意机制，使得滤波器激活从<span class="katex--inline">KaTeX parse error: Expected 'EOF', got '\timesw' at position 2: h\̲t̲i̲m̲e̲s̲w̲\timesc</span>折叠到一个<span class="katex--inline">KaTeX parse error: Expected 'EOF', got '\timesc' at position 9: 1\times1\̲t̲i̲m̲e̲s̲c̲</span>维的向量(通过瓶颈后)，这个向量作为所有通道图的选择注意力。第一个亮点允许多条信息流从初始层传到最终层。第二个贡献是使网络能够专注于对最终任务更为重要的有选择性的特征，并有效地对特征图之间的关系进行建模。</p> 
<p>网络训练使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失函数。据观察，递归残差样式的结构可以使非常深的网络具有更好的收敛性。此外，与IRCNN[26]、VDSR[24]和RDN[56]等现代方法相比，它具有更好的性能。这展现了通道注意力机制[66]对于低水平视觉任务的高效性。尽管如此，该框架的一个缺点是与LapSRN[51]、MemNet[48]和VDSR[24]相比，它的计算复杂度较高(对于4×SR需要约1500万个参数)。</p> 
<h5><a id="373_SRRAM_155"></a>3.7.3 SRRAM</h5> 
<p>A Residual Attention Module for SR (SRRAM) 。这项最近的工作[67]重点关注用于单图像超分辨率的注意力块。他们利用通用的SR结构来评估一系列注意力机制，以比较其性能和各自的优点/缺点、SRRAM[67]的结构类似于RCAN[65]，这两种方法都受到EDSR[37]的启发。SRRAM可分为特征提取、特征扩大和特征重建三个部分。第一部分和最后一部分都类似于先前讨论的方法[23]，[28]。特征扩大部分则是由残差注意模块(RAM)组成的。RAM是SRRAM的一个基本单元，它由残差块、空间注意力和信道注意力组成，用于学习信道间和信道内的依赖关系。</p> 
<p>模型训练的数据是从DIV2K数据集中随机裁剪的图像片，大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         48 
        
       
         × 
        
       
         48 
        
       
      
        48\times48 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">4</span><span class="mord">8</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">4</span><span class="mord">8</span></span></span></span></span>，带有数据增强。过滤器大小<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         3 
        
       
         × 
        
       
         3 
        
       
      
        3\times3 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span></span></span></span></span>，feature map 64个.使用Adam优化器和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失，初始学习率固定为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
         
         
           − 
          
         
           4 
          
         
        
       
      
        10^{-4} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></span>，最终模型使用了共计64个RAM块。</p> 
<h4><a id="38_Multipledegradation_handling_networks_159"></a>3.8 Multiple-degradation handling networks</h4> 
<p>目前讨论的超分辨率网络(例如，[23]，[24])考虑了双三次退化。然而，在现实中这可能不是一种可行的假设，因为可能同时发生多次退化。为了处理这些真实世界的场景，本文提出了以下方法.</p> 
<h5><a id="381_ZSSR_161"></a>3.8.1 ZSSR</h5> 
<p>全称Zero-Shot Super-Resolution[68]，它跟随经典方法的脚步，利用深度神经网络进行内部图像统计，实现图像的超分辨率。ZSSR[68]的结构简单，使用测试图像的下采样版本进行训练。目的是从LR图像中预测测试图像。一旦网络学习了LR测试图像和测试图像之间的关系，就可以使用相同的网络，以测试图像作为输入，预测SR图像。因此，它不需要对特定退化的图像进行训练，并且可以在推理过程中实时学习针对特定图像的网络。ZSSR[68]共有8个卷积层，每层跟着由64个通道组成的ReLU层。与[24]、[37]相似，ZSSR[68]使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>范式学习残差图像。</p> 
<h5><a id="382_SRMD_163"></a>3.8.2 SRMD</h5> 
<p>Super-resolution network for multiple degradations (SRMD)[69]处理级联的低分辨率图及其退化图。SRMD的结构类似于[23]、[25]、[26]。首先，将滤波器大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         3 
        
       
         × 
        
       
         3 
        
       
      
        3\times3 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span></span></span></span></span>的卷积层级联后用于特征提取，然后依次是卷积、ReLU和批标准化层。此外，类似于[35]，卷积运算用于提取HR子图，最后，将多个HR子图像转换为最终的单个HR输出。SRMD直接学习HR图像而不是图像的残差。作者还介绍了一种名为SRMDNF的变体，它从无噪音的退化中学习。在SRMDNF网络中，卷积层中的第一噪声级别的图的连接被移除，剩余结构与SRMD相同。SRMD的网络结构如Fig.2 所示。</p> 
<p>与多尺度训练相比，作者为每个上采样尺度训练了单个模型。采用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失，训练的图像片大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         40 
        
       
         × 
        
       
         40 
        
       
      
        40\times40 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">4</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">4</span><span class="mord">0</span></span></span></span></span>。卷积层个数固定为12层，每层有128个特征图。训练数据集共5944张图，来自BSD [44], DIV2K [45] 和 Waterloo [70] 数据集。初始学习率固定为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
         
         
           − 
          
         
           3 
          
         
        
       
      
        10^{-3} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span></span>，随后下降到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
         
         
           − 
          
         
           5 
          
         
        
       
      
        10^{-5} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span>。学习率降低的标准基于迭代之间的误差变化。SRMD及其变体都无法打破较早的SR网络的PSNR(峰值信噪比)记录，例如EDSR[37]、MDSR[37]和CMSC[61]。不过它的联合处理多重退化的能力较为独特。</p> 
<h4><a id="39_GAN_Models_167"></a>3.9 GAN Models</h4> 
<p>Generative Adversarial Networks (GaN)[71]，[72]采用博弈论方法，其中模型分两个部分，即生成器和判别器。生成器创建图像，让生成器无法识别是真实的HR图像还是人工的超分辨率图像。这样，就可以生成具有更好的感知质量的HR图像。相应的PSNR值被普遍降低，这就说明了在SR文献中流行的定量测量方法并不完全与感知上的效果相关。基于GAN的超分辨方法[42]、[73]见下文。</p> 
<h5><a id="391_SRGAN_169"></a>3.9.1 SRGAN</h5> 
<p>单幅图的大幅度上采样超分辨率问题具有很大的挑战性。SRGAN[42]建议使用对抗目标函数，以生成接近自然图像的超分辨率输出。 他们工作的主要亮点是提出了一个多任务损失公式，公式由三个主要部分组成：(1)编码像素级别相似性的均方误差损失，(2)在高级图像表示(例如，深层网络特征)上定义的距离度量的感知相似性度量，和(3)一种对抗性损失，它平衡了生成器和鉴别器之间的最小-最大博弈(标准GAN目标[71])。该框架基本可得到与高维图像相似的输出。为了进行量化，引入一个新的评分，平均意见评分(MOS)，由人类根据每一幅超分辨率图像的质量好坏进行评分。由于其的模型通常学习去优化直接依赖数据的度量(如像素误差)，所以[42]在感知质量度量上远远超过竞争对手。</p> 
<h5><a id="392_EnhanceNet_171"></a>3.9.2 EnhanceNet</h5> 
<p>这种网络设计的重点是在超分辨率图像中创建可信的纹理细节[73]。常规图像质量度量(如PSNR)的一个关键问题是它们不符合图像的感知质量。这会导致图像过于平滑，没有清晰的纹理。为了克服这个问题，除了常规像素级的MSE损失之外，EnhanceNet还使用了另外两个损失项：(a)在预先训练的网络[74]的中间特征表示上定义了感知损失函数，形式为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>距离。(b)对于低分辨率和高分辨率图像的纹理匹配采用了纹理匹配损失，并量化为根据深层特征计算出的Gram矩阵之间的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失。整个网络结构都是对抗训练的，SR网络的目标是欺骗鉴别器网络。</p> 
<p>增强网使用的结构基于全卷积网络[75]和残差学习原理[24]。结果表明，虽然仅使用像素级损失就能获得最佳的PSNR，但附加的损失项和对抗性的训练可以得到更真实、感知效果更好的输出。缺点是，当对高频纹理区域进行超分辨率时，对抗性训练可能会产生可见的伪影。</p> 
<h5><a id="393_SRFeat_175"></a>3.9.3 SRFeat</h5> 
<p>SRFeat[77]是另一种基于GAN的超分辨率算法，其具有特征识别的功能。这项工作更关注输入图像的感知真实性，通过使用额外的鉴别器帮助生成器生成高频结构特征而不是噪声伪影。这一需求是通过区分合成(机器生成)的和真实的图像特征来实现的。该网络采用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         9 
        
       
         × 
        
       
         9 
        
       
      
        9\times9 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">9</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">9</span></span></span></span></span>的卷积层进行特征提取。然后，使用与[9]类似的带有长距离跳跃连接的残差块，残差快有<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
       
         × 
        
       
         1 
        
       
      
        1\times1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>的卷积。特征图通过像素混叠层进行上采样，以达到所期望的输出大小。作者使用了16个残差块和两种不同的特征图，即64和128。该模型将感知(对抗性损失)和像素级损失(<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>)函数相结合，通过Adam优化器进行优化[46]。系统输入的分辨率为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         74 
        
       
         × 
        
       
         74 
        
       
      
        74\times74 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">7</span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">7</span><span class="mord">4</span></span></span></span></span>，输出<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         296 
        
       
         × 
        
       
         296 
        
       
      
        296\times296 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">2</span><span class="mord">9</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord">9</span><span class="mord">6</span></span></span></span></span>的图像。该网络使用来自ImageNet[78]的120000张图像对生成器进行预训练，然后使用增强后的DIV2K数据集[45]进行微调，学习率在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
         
         
           − 
          
         
           4 
          
         
        
       
      
        10^{-4} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
        
        
          0 
         
         
         
           − 
          
         
           6 
          
         
        
       
      
        10^{-6} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">6</span></span></span></span></span></span></span></span></span></span></span></span></span>之间。</p> 
<h5><a id="394_ESRGAN_177"></a>3.9.4 ESRGAN</h5> 
<p>Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN)[76]在SRGAN[42]的基础上，去除批标准化并合并稠密块。每个稠密块的输入也连接到相应块的输出，使得每个密集块上都有一个残差连接。ESRGAN还有一个全局残差连接，以加强残差学习。此外，作者还采用了一种称为Relativistic GAN的增强了的鉴别器[79]。</p> 
<p>用来自DIV2K[45]和Flicker2K数据集[45]的3450幅图像进行训练，数据集都经过了数据增强，首先使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失，然后使用根据感知损失训练过的模型进行训练。训练的图像片大小设置为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         128 
        
       
         × 
        
       
         128 
        
       
      
        128\times128 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord">2</span><span class="mord">8</span></span></span></span></span>，网络深度为23块。每块包含五个卷积层，每个层都产生64个特征图。然而，与RCAN[65]相比，虽然视觉结果更好，但在量化的度量方式下稍差。</p> 
<h3><a id="4EXPERIMENTAL_EVALUATION_182"></a>4.EXPERIMENTAL EVALUATION</h3> 
<p><img src="https://images2.imgbox.com/1b/7e/gH6G3aHC_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/19/d9/lHkVBEwa_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ea/f6/vnjFz2uF_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/95/8b/8w2520oF_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/22/48/QbMU2fTk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/eb/fc/mt1E6kwk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d1/fa/tDnF274l_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3e/17/mGGqnzEK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/63/a7/RTdeS5YD_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="41_Dataset_192"></a>4.1 Dataset</h4> 
<p>我们在公开的基准数据集上将现有的算法进行了比较，数据集包括Set5[80]、Set14[81]、BSD100[82]、Urban100[83]、DIV2K[45]和Manga109[84]。一些数据集中的代表性图片见Fig.3。</p> 
<ul><li><strong>Set5</strong>[80]是一个经典的数据集，只包含婴儿、鸟类、蝴蝶、头部和女性五张测试图。</li><li><strong>Set14</strong>[81]比Set5[80]包含更多的类别；然而，图像的数量仍然很低，只有14幅测试图像。</li><li><strong>BSD100</strong>[82]是Martin等人提出的另一个具有100幅测试图像的经典数据集。数据集由各种各样的图像组成，从自然图像到特定物体，如植物、人、食物等。</li><li><strong>Urban100</strong>[83]是较新的数据集，由Huang等人提出。图像数目与BSD 100[82]相同，但是构成完全不同。照片的重点是人造建筑，即城市场景。</li><li><strong>DIV2K</strong>[45]是一个用于NIRTE挑战的数据集。图像质量为2K分辨率，由800幅用于训练的图像和各100幅用于测试和验证的图像组成。由于测试集是不公开的，所以所有算法的结果都是在验证图像集上的。</li><li><strong>Manga109</strong>[84]是评估超分辨率算法的最新补充。该数据集是一本漫画中的109幅测试图像的集合。这些漫画由日本的专业画家完成，在1970s至2010s之间只供商业使用。</li></ul> 
<h4><a id="42_Quantitative_Measures_200"></a>4.2 Quantitative Measures</h4> 
<p>第三节详细介绍的算法分别从峰值信噪比(PSNR)和结构相似性指数(SSIM)[85]两方面对算法进行衡量。Table2给出了超分辨率算法的2×、3×和4×的结果。目前，RCAN[65]在2×和3×上的PSNR和SSIM性能较好，ESRGAN[76]则在4×上较好。然而，很难说一个算法完全获胜，因为涉及到许多因素，例如网络复杂性、网络深度、训练数据、用于训练的图像大小、特征图的数量等。一个可能的进行公平比较的方法是保持所有参数保持一致。</p> 
<p>在图6中，我们给出了几种旨在提高图像PSNR的最先进算法之间的视觉比较。此外，图7显示了基于GAN的算法的输出，这些算法是感知驱动的，目的是提高视觉质量。可以注意到，图7中的输出通常更清晰，但相对于优化像素级损失度量的方法，更清晰的图像反而PSNR值相对较低。</p> 
<h4><a id="43_Number_of_parameters_204"></a>4.3 Number of parameters</h4> 
<p>表1显示了不同SR算法的参数比较。直接重建的方法从LR到HR空间进行一步上采样，而递进重建则是通过多次上采样预测出HR图像。深度表示从输入到输出的4×SR的最长路径上的卷积层和转置层的数目。全局残差学习(GRL)是指网络学习真实HR图像和上采样(即使用双三次插值或滤波器)LR图像之间的差异。局部残差学习(LRL)代表中间卷积层之间的局部跳跃连接。可以看到,与在网络[27]、[37]、[37]中执行先上采样的方法相比，执行延迟上采样[28]、[35]的方法具有相当低的计算成本[37],[37],[65].</p> 
<h4><a id="44_Choice_of_network_loss_206"></a>4.4 Choice of network loss</h4> 
<p>在用于图像的超分辨率卷积神经网络中，最常用的损失是均方误差<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>或绝对值平均误差<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>。而生成性对抗性网络(GANs)除了像MSE这样的像素级损失之外，还使用感知损失(对抗性损失)。从Table.1可以明显看出，最初的CNN方法是使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失尽心个训练的；然而，最近出现了使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>损失的趋势，绝对值平均误差(<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>)比<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>更加鲁棒。原因是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>更在意错误的预测，而<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>考虑更均匀的误差分布。</p> 
<h4><a id="45_Network_depth_208"></a>4.5 Network depth</h4> 
<p>与SRCNN[23]中的说法相反，SRCNN认为网络深度太大有时会降低质量，而VDSR[24]最初证明，使用更深的网络有助于提高PSNR和图像质量。EDSR[37]进一步证实了这一说法，EDSR中卷积层的数目几乎是VDSR[24]的四倍。最近，RCAN[65]使用了400多个卷积层来提高图像质量。目前一批CNN[32]，[38]正在加入更多的卷积层以搭建更深的网络，以此提高图像质量和数量，自SRCNN开始以来，这种趋势在深度学习SR中一直保持着主导地位。</p> 
<h4><a id="46_Skip_Connections_210"></a>4.6 Skip Connections</h4> 
<p>总的来说，跳跃连接在改进SR结果中起着至关重要的作用。这些连接主要分为四种类型：全局连接、本地连接、递归连接和稠密连接。最初，VDSR[24]使用全局残差学习(GRL)，并显示出比SRCNN[23]更大的性能改进。然后，DRRN[32]和DRCN[31]证明了递归连接的有效性。最近，EDSR[37]和RCAN[65]采用了局部残差学习(LRL)，即局部连接的同时也保持全局残差学习(GRL)。类似的，RDN[56]和ESRGAN[76]结合了稠密连接和全局连接。现代CNN正在创新各种方法来改进和引入不同层或模块之间的其他类型的连接。在Table.1中，我们显示了使用跳跃链接的方法。</p> 
<h3><a id="5_FUTURE_DIRECTIONSOPEN_PROBLEMS_213"></a>5 FUTURE DIRECTIONS/OPEN PROBLEMS</h3> 
<p>尽管深度网络在超分辨率任务上表现出了优异的性能，但仍有几个开放的研究问题。我们概述了以下一些未来研究方向。</p> 
<p><strong>加入先验</strong>：目前SR的深度学习网络是数据驱动模型，端到端可训。虽然这种方法在通常情况下显示出了很好的效果，当某一类退化发生时，比如大量的训练数据确实(例如，在医学成像中)，它被证明不是最优的。在这种情况下，如果传感器、成像对象/场景、获取条件等信息是已知的，就可以利用有价值的的先验信息来获得高分辨率图像。最近围绕这一方向的工作已经提出了基于深度网络[86]和稀疏编码[87]的先验方法，以获得更好的超分辨率。</p> 
<p><strong>目标函数和度量</strong>：现有的SR方法主要使用像素级的误差度量，例如<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          1 
         
        
       
      
        l_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>距离和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          2 
         
        
       
      
        l_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.01968em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>距离，或者是两者的结合。由于这些度量只包括局部像素级的信息，因此产生的图像并不能总是有良好的感知结果。例如，具有较高PSNR和SSIM值的图像在感知质量较低的情况下给出了过于平滑的图像[88]。针对这一问题，文献中提出了若干衡量感知损失的方法。传统的感知指标是固定的，例如SSIM[85]，多尺度SSIM[89]，而最近的方法是模拟人类对图像的感知，例如LPIPS[90]和PieAPP[91]。每种方法都有各自的失败案例。因此，没有一个通用的感知度量，可以在所有条件下都能最优，并完美地量化图像质量。因此，新目标函数的挖掘是一个开放的研究问题。</p> 
<p><strong>统一的解决方案的需求</strong>：在现实生活中，两种或两种以上的退化往往同时发生。在这种情况下，一个重要的考虑是如何联合的恢复出有高分辨率、低噪音和增强细节的图像。目前的SR模型通常仅处理了一种情况，当其他退化情况出现时就会对模型造成影响。此外，针对具体问题的模型在结构、损失函数和训练细节方面存在差异。设计一个能够同时在多个低级视觉任务中表现优秀的的统一模型[69]是一个挑战。</p> 
<p><strong>无监督图像SR</strong>：本篇综述中讨论的模型一般考虑LR-HR图像对，以此来学习超分辨率映射函数。一个有趣的方向是探索在没有相应的HR图像的情况下如何进行SR。这个问题的一个解决方案是Zero-shot SR[68]，它在给定图像的进一步下采样版本上学习SR模型。但是，当输入图像的分辨率已经很低时，该解决方案就不可行。无监督图像SR旨在通过从未配对的LR-HR图像集中学习一个函数来解决这个问题[92]。这样的功能对于现实生活中的设置非常有用，因为在一些情况下获得相匹配的HR图像并不容易。</p> 
<p><strong>更高的SR比率</strong>：现有的超分辨率模型一般不会处理极端的超分辨率问题，但这对于人群场景中清晰分别出人脸等情况是非常有用的。很少有模型的SR率高于8×(例如16×和32×)[51]。在如此极端的上采样条件下，如何在图像中保持精确的局部细节变得具有挑战性。此外，一个开放的问题是如何在这些超分辨率图像中仍保持高的感知质量。</p> 
<p><strong>任意的SR比率</strong>：在实际场景中，对于给定的输入，通常不知道哪一个上采样倍数是最优的。当数据集中的所有图像都不知道下采样的倍数时，这将成为训练中的一大挑战，因为单一模型很难涵盖不同级别的细节。在这种情况下，在训练和用特定的SR模型进行推理之前，先确定退化的程度是很重要的。</p> 
<p><strong>真实情况与人工退化</strong>：现有的SR算法大多采用双三次插值来生成LR图像。在真实场景中遇到的实际的LR图像的分布与使用双三次插值生成的图像的分布完全不同。因此，用人为退化的图像训练的SR模型在实际场景中并不具备很好的泛化能力。最近为解决这一问题所做的一项工作是训练一个GAN来模拟现实世界的退化[93]。</p> 
<h3><a id="6_CONCLUSION_230"></a>6 CONCLUSION</h3> 
<p>单图像超分辨率是一个具有挑战性的研究问题，具有重要的现实生活应用。深度学习方法的巨大成功使得基于深度卷积神经网络的图像超分辨率技术得到了迅速的发展。人们在网络结构和学习策略等方面，提出了不同的方法，并获得了令人振奋的创新。这篇综述对现有的基于深度学习的超分辨率方法进行了全面的分析。我们注意到，近年来，随着网络复杂程度逐渐增加，超分辨率性能得到了极大的提高。值得注意的是，现有的方法仍因为一些缺陷而在应用于关键的真实世界场景的过程中受到限制(例如，不充分的度量、模型复杂度高、无法处理现实生活中的退化情况)。我们希望本篇综述可以吸引更多力量，从而解决这些关键问题。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6449f45a4272581ff5e8cdc4f4b37ca9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">微信公众号开发引入jssdk，分享配置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fe219e7c85ec31cedb88319b4fcd0319/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解决“NOT FOUND  The requested URL was not found on this server”</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>