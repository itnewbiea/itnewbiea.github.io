<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>27报网MM爬虫福利一 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="27报网MM爬虫福利一" />
<meta property="og:description" content="27报 http://www.126z.net/z/2019/11/28/41365.html
刚学爬虫花了4个小时写的，每一步备注的都很清楚，喜欢的朋友自己可以研究研究......
目标网站：http://www.126z.net
环境：Python3.x
相关第三方模块：requests、lxml
Re：各位在测试时只需要打开终端，使用 python xxx.py 运行即可。
源码如下:
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import time
import requests
from lxml import etree
class MeiZiSpider(object):
&#34;&#34;&#34;27 bao website MM spider&#34;&#34;&#34;
def __init__(self):
self.base_url = &#39;https://www.126z.net/meinv/list_1.html&#39;
self.headers = {&#39;User-Agent&#39;: &#39;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#39;}
# 构建url的前缀
self.url_prefix = &#39;https://www.126z.net&#39;
# 构建第二层url的前缀
self.inner_url_prefix = &#39;https://www.126z.net/meinv/&#39;
# 构建点开每个MM内部的所有url
self.inner_url = &#39;https://www.126z.net/meinv/56870_{}.html&#39;
# 文件保存的路径
self.path = os." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/fab4c95f7c616c6b58f5a9e44bb83d62/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-11-28T20:13:17+08:00" />
<meta property="article:modified_time" content="2019-11-28T20:13:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">27报网MM爬虫福利一</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><a href="http://www.126z.net/z/2019/11/28/41365.html" rel="nofollow">27报</a> http://www.126z.net/z/2019/11/28/41365.html</p> 
<p>刚学爬虫花了4个小时写的，每一步备注的都很清楚，喜欢的朋友自己可以研究研究......</p> 
<p> </p> 
<p>目标网站：http://www.126z.net</p> 
<p> </p> 
<p>环境：Python3.x</p> 
<p> </p> 
<p>相关第三方模块：requests、lxml</p> 
<p> </p> 
<p>Re：各位在测试时只需要打开终端，使用 python xxx.py 运行即可。</p> 
<p> </p> 
<p>源码如下:</p> 
<p>#!/usr/bin/env python</p> 
<p># -*- coding: utf-8 -*-</p> 
<p>import os</p> 
<p>import time</p> 
<p> </p> 
<p>import requests</p> 
<p>from lxml import etree</p> 
<p> </p> 
<p> </p> 
<p>class MeiZiSpider(object):</p> 
<p>    """27 bao website MM spider"""</p> 
<p> </p> 
<p>    def __init__(self):</p> 
<p>        self.base_url = 'https://www.126z.net/meinv/list_1.html'</p> 
<p>        self.headers = {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)'}</p> 
<p> </p> 
<p>        # 构建url的前缀</p> 
<p>        self.url_prefix = 'https://www.126z.net'</p> 
<p> </p> 
<p>        # 构建第二层url的前缀</p> 
<p>        self.inner_url_prefix = 'https://www.126z.net/meinv/'</p> 
<p> </p> 
<p>        # 构建点开每个MM内部的所有url</p> 
<p>        self.inner_url = 'https://www.126z.net/meinv/56870_{}.html'</p> 
<p> </p> 
<p>        # 文件保存的路径</p> 
<p>        self.path = os.getcwd() + '/27baoMM/'</p> 
<p> </p> 
<p>    def send_first_req(self):</p> 
<p>        """</p> 
<p>        发送第一层请求，返回首页响应的html内容</p> 
<p>        :return: 第一层url的html_str</p> 
<p>        """</p> 
<p> </p> 
<p>        html_str = requests.get(self.base_url, headers=self.headers).content.decode()</p> 
<p> </p> 
<p>        return html_str</p> 
<p> </p> 
<p>    def get_total_page(self):</p> 
<p>        """</p> 
<p>        获取网站MM图片总页数</p> 
<p>        :return: 图片总页数</p> 
<p>        """</p> 
<p> </p> 
<p>        # 模拟发送请求, 返回响应数据(便于提取总页数)</p> 
<p>        html_str = self.send_first_req()</p> 
<p> </p> 
<p>        # 将返回的 html字符串 转换为 element对象</p> 
<p>        element_obj = etree.HTML(html_str)</p> 
<p> </p> 
<p>        # 提取末页的url, 再切片提取最大页数</p> 
<p>        total_page = element_obj.xpath('//*[@id="pages"]/a[9]/@href')[0].split('_')[-1].split('.')[0]</p> 
<p>        print('========该网站MM图片目前共有 {} 页========'.format(total_page))</p> 
<p> </p> 
<p>        return total_page</p> 
<p> </p> 
<p>    def get_each_girl_url(self, each_page_url):</p> 
<p>        """</p> 
<p>        第一层解析 ---&gt; 获取每一页每个MM(第二层)的url</p> 
<p>        :param each_page_url: 每一页的url</p> 
<p>        :return: 每一页每个MM(第二层)的url和title</p> 
<p>        """</p> 
<p> </p> 
<p>        # 用于存放拼接后每个MM的url</p> 
<p>        girl_url_list = []</p> 
<p> </p> 
<p>        # 模拟发送请求, 返回响应数据(便于提取url)</p> 
<p>        html_str = self.send_request(each_page_url)</p> 
<p> </p> 
<p>        # 将返回的 html字符串 转换为 element对象</p> 
<p>        element_obj = etree.HTML(html_str)</p> 
<p> </p> 
<p>        # 提取所有url后缀  ---&gt; 列表</p> 
<p>        url_postfix = element_obj.xpath('/html/body/div[2]/div[2]/ul/li/a/@href')</p> 
<p> </p> 
<p>        # 提取每个妹妹的标题, 便于后面命名</p> 
<p>        url_title_li = element_obj.xpath('/html/body/div[2]/div[2]/ul/li/p/a/text()')</p> 
<p> </p> 
<p>        # 遍历提取的url列表, 拼接完整的url, 并保存列表中</p> 
<p>        for url in url_postfix:</p> 
<p>            # 添加拼接后的url到列表</p> 
<p>            girl_url_list.append(self.url_prefix + url)</p> 
<p>        # print(girl_url_list)</p> 
<p> </p> 
<p>        title_and_url_li = list(zip(url_title_li, girl_url_list))</p> 
<p>        # print(title_and_url_li)</p> 
<p> </p> 
<p>        return title_and_url_li</p> 
<p> </p> 
<p>    def send_request(self, url):</p> 
<p>        """</p> 
<p>        发送第二层请求，返回响应的html内容</p> 
<p>        :param url: 每个MM(第二层)的url</p> 
<p>        :return: html_str</p> 
<p>        """</p> 
<p>        try:</p> 
<p>            response = requests.get(url, headers=self.headers, timeout=5)</p> 
<p>        except:</p> 
<p>            return None</p> 
<p>        html_str = response.content</p> 
<p> </p> 
<p>        return html_str</p> 
<p> </p> 
<p>    def parse_data(self, html_str):</p> 
<p>        """</p> 
<p>        第二层解析 ---&gt; 获取当前的每个url</p> 
<p>        :param html_str: 第二层响应的html内容</p> 
<p>        :return: 具体每张图片的url</p> 
<p>        """</p> 
<p> </p> 
<p>        # 将返回的 html字符串 转换为 element对象</p> 
<p>        element_obj = etree.HTML(html_str)</p> 
<p>        each_image = element_obj.xpath('/html/body/div[3]/center/img/@src')[0]</p> 
<p>        # print(each_image)</p> 
<p>        return each_image</p> 
<p> </p> 
<p>    def save_file(self, image_data, title, image_name):</p> 
<p>        """</p> 
<p>        保存图片</p> 
<p>        :param image_data: 具体每张图片的数据</p> 
<p>        :param title: 具体每个MM的标题</p> 
<p>        :param image_name: 具体每张图片的名字</p> 
<p>        :return:</p> 
<p>        """</p> 
<p> </p> 
<p>        # 如果目录不存在, 就新建</p> 
<p>        if not os.path.exists(self.path + title):</p> 
<p>            os.makedirs(self.path + title)</p> 
<p>        # 切换到新建目录下</p> 
<p>        os.chdir(self.path + title)</p> 
<p> </p> 
<p>        with open(image_name, 'wb') as f:</p> 
<p>            f.write(image_data)</p> 
<p> </p> 
<p>    def download(self):</p> 
<p>        total_page = self.get_total_page()</p> 
<p>        # 开循环, 爬取每个页面的每个妹妹的所有图片</p> 
<p>        for page in range(1, int(total_page) + 1):</p> 
<p>            print('\n正在爬取第 {} 页...\n'.format(page))</p> 
<p>            # 构建每一页的url</p> 
<p>            each_page_url = 'https://www.126z.net/meinv/list_{}.html'.format(page)</p> 
<p>            title_and_url_li = self.get_each_girl_url(each_page_url)</p> 
<p> </p> 
<p>            # 遍历获取每个妹妹的总url</p> 
<p>            for girl_url in title_and_url_li:</p> 
<p>                # print(girl_url[1])</p> 
<p> </p> 
<p>                html_str = self.send_request(girl_url[1])</p> 
<p>                print('正在爬取的地址: {}'.format(girl_url[1]))</p> 
<p> </p> 
<p>                # 获取目前爬取的MM内部总页数</p> 
<p>                inner_total_page = etree.HTML(html_str).xpath('//*[@id="pages"]/a[last()-1]/text()')[0]</p> 
<p>                print('\n目前爬取的MM图片共有 {} 张\n'.format(inner_total_page))</p> 
<p> </p> 
<p>                # 循环并构造每一张图片的url, 请求, 解析, 并保存</p> 
<p>                image_data = None</p> 
<p>                for inner_page in range(1, int(inner_total_page) + 1):</p> 
<p>                    print('正在爬取 {} 第 {} 张图片...'.format(girl_url[0], inner_page))</p> 
<p>                    # 构造第二页以后的每个url</p> 
<p>                    each_image_url = girl_url[1].split('.html')[0] + '_{}.html'</p> 
<p>                    if inner_page == 1:</p> 
<p>                        # 获取第一页的url, 因为第一页和其他页的规律不一样，所以这里单独出来</p> 
<p>                        each_image_url = self.parse_data(html_str)</p> 
<p>                        # html_str = self.send_request(each_image_url)</p> 
<p>                        image_data = self.send_request(each_image_url)</p> 
<p>                    else:</p> 
<p>                        each_image_url = each_image_url.format(inner_page)</p> 
<p>                        # print(each_image_url)</p> 
<p>                        html_str = self.send_request(each_image_url)</p> 
<p>                        image = self.parse_data(html_str)</p> 
<p>                        image_data = self.send_request(image)</p> 
<p> </p> 
<p>                    # 如果请求图片的url在5秒之内无响应, 立即终止当前妹妹所有url的循环</p> 
<p>                    if image_data is None:</p> 
<p>                        print('当前MM的url无响应，正在跳转到下一个MM的url\n')</p> 
<p>                        break</p> 
<p>                    # 拼接保存图片的名称</p> 
<p>                    image_name = str(inner_page) + '.jpg'</p> 
<p>                    try:</p> 
<p>                        # 切换图片保存目录</p> 
<p>                        os.chdir(self.path + girl_url[0])</p> 
<p>                        # 判断文件是否存在, 存在则不再保存, 跳过</p> 
<p>                        if os.path.exists(image_name):</p> 
<p>                            print('第 {} 张已保存, 跳过...'.format(inner_page))</p> 
<p>                            continue</p> 
<p>                    except Exception as e:</p> 
<p>                        print('目录 "{}"：不存在, 正在新建...'.format(self.path + girl_url[0]))</p> 
<p> </p> 
<p>                    # 保存图片到本地</p> 
<p>                    self.save_file(image_data, girl_url[0], image_name)</p> 
<p>                # print(image_list)</p> 
<p>                if image_data:</p> 
<p>                    print('\n{} 的所有图片已全部爬取完毕...\n'.format(girl_url[0]))</p> 
<p> </p> 
<p>                # 休眠5秒中, 避免请求一直处于长连接导致ip被封</p> 
<p>                time.sleep(10)</p> 
<p> </p> 
<p> </p> 
<p>if __name__ == '__main__':</p> 
<p>    MeiZiSpider().download()</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/662e653bed043ecbc534f00964d0e4da/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">访问laravel项目时候出现vendor/autoload.php): failed to open stream: No such file or directory</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6759d2498fc15cd071f7a1fb405ad3f5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">OpenWrt 4G路由器DIY</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>