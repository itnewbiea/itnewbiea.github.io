<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>3. Flink集群配置文件以及日志系统概述 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="3. Flink集群配置文件以及日志系统概述" />
<meta property="og:description" content="文章目录 1. 常用Scope区别2. [Flink集群搭建](https://www.jianshu.com/p/c47e8f438291)2.1. 独立集群2.2. yarn集群2.3. [Flink 1.10.0 分布式高可用集群搭建](https://blog.csdn.net/RivenDong/article/details/104416464)2.4. 配置文件 3. [SLF4J和Logback和Log4j和Logging的区别与联系](https://www.cnblogs.com/suger43894/p/9543685.html)3.1. ==一个著名的日志系统是怎么设计出来的==3.2. 在standalone集群模式下运行案例遇到的一个问题 4. [Flink DataStream API 编程指南](https://blog.csdn.net/lixgjob/article/details/86594601)4.1. Flink程序剖析4.2. 第一个Flink程序（WordCount） 5. [Flink任务提交流程（Standalone和Yarn）](https://blog.csdn.net/u013337425/article/details/102696305)6. 寄语：天行健，君子以自强不息 1. 常用Scope区别 compile
默认scope为compile，表示为当前依赖参与项目的编译、测试和运行阶段，属于强依赖。打包之时，会打到包里去。 test
该依赖仅仅参与测试相关的内容，包括测试用例的编译和执行，比如定性的Junit runtime
依赖仅参与运行周期中的使用。一般这种类库都是接口与实现相分离的类库，比如JDBC类库，在编译之时仅依赖相关的接口，在具体的运行之时，才需要具体的mysql、oracle等等数据的驱动程序。此类的驱动都是为runtime的类库。 provided
该依赖在打包过程中，不需要打进去，这个由运行的环境来提供，比如tomcat或者基础类库等等，事实上，该依赖可以参与编译、测试和运行等周期，与compile等同。区别在于打包阶段进行了exclude操作。 2. Flink集群搭建 要求： Java 1.8.x或更高版本ssh免密登录相同的目录结构 2.1. 独立集群 JAVA_HOME 配置
conf/flink-conf.yaml 通过 env.java.home键设置此变量env.java.home: /usr/lib/jdk1.8.0_162 Flink配置
选择一个master结点， 配置conf/flink-conf.yaml
jobmanager.rpc.address: 192.168.1.27 conf / slaves 配置从结点
其他重要配置
官网配置参数
分发安装包
scp -r flink hadoop@192.168.1.28:$PWD scp -r flink hadoop@192.168.1.29:$PWD scp -r flink hadoop@192." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/7acb06818ffc158a52ec0cb8113c6a6c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-02T17:29:57+08:00" />
<meta property="article:modified_time" content="2020-03-02T17:29:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">3. Flink集群配置文件以及日志系统概述</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1_Scope_1" rel="nofollow">1. 常用Scope区别</a></li><li><a href="#2_Flinkhttpswwwjianshucompc47e8f438291_13" rel="nofollow">2. [Flink集群搭建](https://www.jianshu.com/p/c47e8f438291)</a></li><li><ul><li><a href="#21__19" rel="nofollow">2.1. 独立集群</a></li><li><a href="#22_yarn_57" rel="nofollow">2.2. yarn集群</a></li><li><a href="#23_Flink_1100_httpsblogcsdnnetRivenDongarticledetails104416464_94" rel="nofollow">2.3. [Flink 1.10.0 分布式高可用集群搭建](https://blog.csdn.net/RivenDong/article/details/104416464)</a></li><li><a href="#24__107" rel="nofollow">2.4. 配置文件</a></li></ul> 
  </li><li><a href="#3_SLF4JLogbackLog4jLogginghttpswwwcnblogscomsuger43894p9543685html_369" rel="nofollow">3. [SLF4J和Logback和Log4j和Logging的区别与联系](https://www.cnblogs.com/suger43894/p/9543685.html)</a></li><li><ul><li><a href="#31__370" rel="nofollow">3.1. ==一个著名的日志系统是怎么设计出来的==</a></li><li><a href="#32_standalone_379" rel="nofollow">3.2. 在standalone集群模式下运行案例遇到的一个问题</a></li></ul> 
  </li><li><a href="#4_Flink_DataStream_API_httpsblogcsdnnetlixgjobarticledetails86594601_381" rel="nofollow">4. [Flink DataStream API 编程指南](https://blog.csdn.net/lixgjob/article/details/86594601)</a></li><li><ul><li><a href="#41_Flink_382" rel="nofollow">4.1. Flink程序剖析</a></li><li><a href="#42_FlinkWordCount_390" rel="nofollow">4.2. 第一个Flink程序（WordCount）</a></li></ul> 
  </li><li><a href="#5_FlinkStandaloneYarnhttpsblogcsdnnetu013337425articledetails102696305_455" rel="nofollow">5. [Flink任务提交流程（Standalone和Yarn）](https://blog.csdn.net/u013337425/article/details/102696305)</a></li><li><a href="#6__457" rel="nofollow">6. 寄语：天行健，君子以自强不息</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1_Scope_1"></a>1. 常用Scope区别</h2> 
<ul><li> <p><code>compile</code></p> 
  <ul><li><code>默认</code>scope为compile，表示为当前依赖参与项目的<code>编译</code>、<code>测试</code>和<code>运行</code>阶段，属于强依赖。打包之时，<code>会打到包里</code>去。</li></ul> </li><li> <p><code>test</code></p> 
  <ul><li>该依赖仅仅参与测试相关的内容，包括测试用例的编译和执行，比如定性的Junit</li></ul> </li><li> <p><code>runtime</code></p> 
  <ul><li>依赖仅参与运行周期中的使用。一般这种类库都是接口与实现相分离的类库，比如JDBC类库，在编译之时仅依赖相关的接口，在具体的运行之时，才需要具体的mysql、oracle等等数据的驱动程序。此类的驱动都是为runtime的类库。</li></ul> </li><li> <p><code>provided</code></p> 
  <ul><li>该依赖在打包过程中，不需要打进去，这个由运行的环境来提供，比如tomcat或者基础类库等等，事实上，该依赖可以参与<code>编译</code>、<code>测试</code>和<code>运行</code>等周期，与compile等同。区别在于打包阶段进行了exclude操作。</li></ul> </li></ul> 
<h2><a id="2_Flinkhttpswwwjianshucompc47e8f438291_13"></a>2. <a href="https://www.jianshu.com/p/c47e8f438291" rel="nofollow">Flink集群搭建</a></h2> 
<ul><li><strong>要求：</strong> 
  <ul><li><code>Java 1.8.x或更高版本</code></li><li><code>ssh免密登录</code></li><li><code>相同的目录结构</code></li></ul> </li></ul> 
<h3><a id="21__19"></a>2.1. 独立集群</h3> 
<ul><li> <p><code>JAVA_HOME 配置</code></p> 
  <ul><li>conf/flink-conf.yaml 通过 <code>env.java.home</code>键设置此变量<pre><code>env.java.home: /usr/lib/jdk1.8.0_162
</code></pre> </li></ul> </li><li> <p><code>Flink配置</code></p> 
  <ul><li> <p><code>选择一个master结点</code>， 配置conf/flink-conf.yaml</p> <pre><code>jobmanager.rpc.address: 192.168.1.27
</code></pre> </li><li> <p><code>conf / slaves 配置从结点</code><br> <img src="https://images2.imgbox.com/ed/48/4tk8mP1S_o.png" alt="在这里插入图片描述"></p> </li><li> <p><code>其他重要配置</code><br> <img src="https://images2.imgbox.com/d2/bc/CRSO6E9Y_o.png" alt="在这里插入图片描述"></p> </li><li> <p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/ops/config.html" rel="nofollow">官网配置参数</a></p> </li></ul> </li><li> <p><code>分发安装包</code></p> <pre><code>scp -r flink hadoop@192.168.1.28:$PWD
scp -r flink hadoop@192.168.1.29:$PWD
scp -r flink hadoop@192.168.1.30:$PWD
</code></pre> </li><li> <p><code>启动Flink集群</code></p> <pre><code>${FLINK_HOME}/bin/start-cluster.sh 
</code></pre> </li><li> <p><code>将JobManager / TaskManager实例添加到集群</code><br> <img src="https://images2.imgbox.com/34/e5/dYNYGJO3_o.png" alt="在这里插入图片描述"></p> </li><li> <p>Flink UI界面<br> <img src="https://images2.imgbox.com/c5/43/ffzRzcts_o.png" alt="在这里插入图片描述"></p> </li></ul> 
<h3><a id="22_yarn_57"></a>2.2. yarn集群</h3> 
<ul><li> <p><mark>设置Hadoop环境变量</mark></p> <pre><code>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
</code></pre> </li><li> <p><mark>以集群模式提交任务，每次都会新建flink集群</mark></p> <pre><code>./flink run -m yarn-cluster -p 1 -yjm 1024 -ytm 1024m /home/hadoop/fanjh/FirstFlinkDemo-assembly-1.0.jar 19168.1.27 9999
</code></pre> </li><li> <p><mark>启动Session flink集群，提交任务</mark></p> 
  <ul><li> <p>查看使用方法</p> <pre><code>$FLINK_HOME/bin/yarn-session.sh -h
</code></pre> </li><li> <p><code>在YARN上启动一个长期运行的Flink集群的会话</code></p> <pre><code>bin/yarn-session.sh -jm 1024m -tm 1024m -s 4
</code></pre> </li><li> <p><code>会话开启后，在YARN上提交Flink作业</code></p> <pre><code>bin/flink run /home/hadoop/fanjh/FirstFlinkDemo-assembly-1.0.jar 192.168.1.27 9999
</code></pre> </li></ul> </li><li> <p><mark>使用yarn 工具 来停止yarn session</mark></p> <pre><code>yarn application -kill &lt;applicationId&gt; 
</code></pre> </li></ul> 
<h3><a id="23_Flink_1100_httpsblogcsdnnetRivenDongarticledetails104416464_94"></a>2.3. <a href="https://blog.csdn.net/RivenDong/article/details/104416464">Flink 1.10.0 分布式高可用集群搭建</a></h3> 
<ul><li><mark>独立集群高可用</mark> 
  <ul><li> <p><code>配置masters文件conf/masters，并分发到每台机器</code><br> <img src="https://images2.imgbox.com/1c/b2/6bQtS8Mj_o.png" alt="在这里插入图片描述"></p> </li><li> <p><code>配置flink-conf.yam，并分发到每台机器</code></p> <pre><code>high-availability: zookeeper
high-availability.storageDir: hdfs:///flink/ha/
high-availability.zookeeper.quorum: baojiabei03:2181,baojiabei04:2181,baojiabei05:2181
high-availability.zookeeper.path.root: /flink
</code></pre> </li></ul> </li></ul> 
<h3><a id="24__107"></a>2.4. 配置文件</h3> 
<pre><code class="prism language-bash"><span class="token comment">################################################################################</span>
<span class="token comment">#  Licensed to the Apache Software Foundation (ASF) under one</span>
<span class="token comment">#  or more contributor license agreements.  See the NOTICE file</span>
<span class="token comment">#  distributed with this work for additional information</span>
<span class="token comment">#  regarding copyright ownership.  The ASF licenses this file</span>
<span class="token comment">#  to you under the Apache License, Version 2.0 (the</span>
<span class="token comment">#  "License"); you may not use this file except in compliance</span>
<span class="token comment">#  with the License.  You may obtain a copy of the License at</span>
<span class="token comment">#</span>
<span class="token comment">#      http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="token comment">#</span>
<span class="token comment">#  Unless required by applicable law or agreed to in writing, software</span>
<span class="token comment">#  distributed under the License is distributed on an "AS IS" BASIS,</span>
<span class="token comment">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="token comment">#  See the License for the specific language governing permissions and</span>
<span class="token comment"># limitations under the License.</span>
<span class="token comment">################################################################################</span>


<span class="token comment">#==============================================================================</span>
<span class="token comment"># Common</span>
<span class="token comment">#==============================================================================</span>

<span class="token comment"># The external address of the host on which the JobManager runs and can be</span>
<span class="token comment"># reached by the TaskManagers and any clients which want to connect. This setting</span>
<span class="token comment"># is only used in Standalone mode and may be overwritten on the JobManager side</span>
<span class="token comment"># by specifying the --host &lt;hostname&gt; parameter of the bin/jobmanager.sh executable.</span>
<span class="token comment"># In high availability mode, if you use the bin/start-cluster.sh script and setup</span>
<span class="token comment"># the conf/masters file, this will be taken care of automatically. Yarn/Mesos</span>
<span class="token comment"># automatically configure the host name based on the hostname of the node where the</span>
<span class="token comment"># JobManager runs.</span>

jobmanager.rpc.address: 192.168.1.27

<span class="token comment"># The RPC port where the JobManager is reachable.</span>

jobmanager.rpc.port: 6123


<span class="token comment"># The heap size for the JobManager JVM</span>

jobmanager.heap.size: 1024m


<span class="token comment"># The total process memory size for the TaskManager.</span>
<span class="token comment">#</span>
<span class="token comment"># Note this accounts for all memory usage within the TaskManager process, including JVM metaspace and other overhead.</span>

taskmanager.memory.process.size: 1024m

<span class="token comment"># To exclude JVM metaspace and overhead, please, use total Flink memory size instead of 'taskmanager.memory.process.size'.</span>
<span class="token comment"># It is not recommended to set both 'taskmanager.memory.process.size' and Flink memory.</span>
<span class="token comment">#</span>
<span class="token comment"># taskmanager.memory.flink.size: 1280m</span>

<span class="token comment"># The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.</span>

taskmanager.numberOfTaskSlots: 4

<span class="token comment"># The parallelism used for programs that did not specify and other parallelism.</span>

parallelism.default: 8

<span class="token comment"># The default file system scheme and authority.</span>
<span class="token comment"># </span>
<span class="token comment"># By default file paths without scheme are interpreted relative to the local</span>
<span class="token comment"># root file system 'file:///'. Use this to override the default and interpret</span>
<span class="token comment"># relative paths relative to a different file system,</span>
<span class="token comment"># for example 'hdfs://mynamenode:12345'</span>
<span class="token comment">#</span>
<span class="token comment"># fs.default-scheme</span>
env.java.home: /usr/lib/jdk1.8.0_162

<span class="token comment">#==============================================================================</span>
<span class="token comment"># High Availability</span>
<span class="token comment">#==============================================================================</span>

<span class="token comment"># The high-availability mode. Possible options are 'NONE' or 'zookeeper'.</span>
<span class="token comment">#</span>
high-availability: zookeeper

<span class="token comment"># The path where metadata for master recovery is persisted. While ZooKeeper stores</span>
<span class="token comment"># the small ground truth for checkpoint and leader election, this location stores</span>
<span class="token comment"># the larger objects, like persisted dataflow graphs.</span>
<span class="token comment"># </span>
<span class="token comment"># Must be a durable file system that is accessible from all nodes</span>
<span class="token comment"># (like HDFS, S3, Ceph, nfs, ...) </span>
<span class="token comment">#</span>
high-availability.storageDir: hdfs://cluster/flink/ha/
yarn.application-attempts: 10

<span class="token comment"># The list of ZooKeeper quorum peers that coordinate the high-availability</span>
<span class="token comment"># setup. This must be a list of the form:</span>
<span class="token comment"># "host1:clientPort,host2:clientPort,..." (default clientPort: 2181)</span>
<span class="token comment">#</span>
high-availability.zookeeper.quorum: 192.168.1.23:2181,192.168.1.24:2181,192.168.1.25:2181
high-availability.zookeeper.path.root: /flink

<span class="token comment"># ACL options are based on https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_BuiltinACLSchemes</span>
<span class="token comment"># It can be either "creator" (ZOO_CREATE_ALL_ACL) or "open" (ZOO_OPEN_ACL_UNSAFE)</span>
<span class="token comment"># The default value is "open" and it can be changed to "creator" if ZK security is enabled</span>
<span class="token comment">#</span>
<span class="token comment"># high-availability.zookeeper.client.acl: open</span>

<span class="token comment">#==============================================================================</span>
<span class="token comment"># Fault tolerance and checkpointing</span>
<span class="token comment">#==============================================================================</span>

<span class="token comment"># The backend that will be used to store operator state checkpoints if</span>
<span class="token comment"># checkpointing is enabled.</span>
<span class="token comment">#</span>
<span class="token comment"># Supported backends are 'jobmanager', 'filesystem', 'rocksdb', or the</span>
<span class="token comment"># &lt;class-name-of-factory&gt;.</span>
<span class="token comment">#</span>
state.backend: filesystem

<span class="token comment"># Directory for checkpoints filesystem, when using any of the default bundled</span>
<span class="token comment"># state backends.</span>
<span class="token comment">#</span>
state.checkpoints.dir: hdfs://cluster/flink/flink-checkpoints

<span class="token comment"># Default target directory for savepoints, optional.</span>
<span class="token comment">#</span>
state.savepoints.dir: hdfs://cluster/flink/flink-checkpoints

<span class="token comment"># Flag to enable/disable incremental checkpoints for backends that</span>
<span class="token comment"># support incremental checkpoints (like the RocksDB state backend). </span>
<span class="token comment">#</span>
<span class="token comment"># state.backend.incremental: false</span>

<span class="token comment"># The failover strategy, i.e., how the job computation recovers from task failures.</span>
<span class="token comment"># Only restart tasks that may have been affected by the task failure, which typically includes</span>
<span class="token comment"># downstream tasks and potentially upstream tasks if their produced data is no longer available for consumption.</span>

jobmanager.execution.failover-strategy: region

<span class="token comment">#==============================================================================</span>
<span class="token comment"># Rest &amp; web frontend</span>
<span class="token comment">#==============================================================================</span>

<span class="token comment"># The port to which the REST client connects to. If rest.bind-port has</span>
<span class="token comment"># not been specified, then the server will bind to this port as well.</span>
<span class="token comment">#</span>
rest.port: 8083

<span class="token comment"># The address to which the REST client will connect to</span>
<span class="token comment">#</span>
<span class="token comment">#rest.address: 0.0.0.0</span>

<span class="token comment"># Port range for the REST and web server to bind to.</span>
<span class="token comment">#</span>
<span class="token comment">#rest.bind-port: 8080-8090</span>

<span class="token comment"># The address that the REST &amp; web server binds to</span>
<span class="token comment">#</span>
<span class="token comment">#rest.bind-address: 0.0.0.0</span>

<span class="token comment"># Flag to specify whether job submission is enabled from the web-based</span>
<span class="token comment"># runtime monitor. Uncomment to disable.</span>

web.submit.enable: <span class="token boolean">true</span>

<span class="token comment">#==============================================================================</span>
<span class="token comment"># Advanced</span>
<span class="token comment">#==============================================================================</span>

<span class="token comment"># Override the directories for temporary files. If not specified, the</span>
<span class="token comment"># system-specific Java temporary directory (java.io.tmpdir property) is taken.</span>
<span class="token comment">#</span>
<span class="token comment"># For framework setups on Yarn or Mesos, Flink will automatically pick up the</span>
<span class="token comment"># containers' temp directories without any need for configuration.</span>
<span class="token comment">#</span>
<span class="token comment"># Add a delimited list for multiple directories, using the system directory</span>
<span class="token comment"># delimiter (colon ':' on unix) or a comma, e.g.:</span>
<span class="token comment">#     /data1/tmp:/data2/tmp:/data3/tmp</span>
<span class="token comment">#</span>
<span class="token comment"># Note: Each directory entry is read from and written to by a different I/O</span>
<span class="token comment"># thread. You can include the same directory multiple times in order to create</span>
<span class="token comment"># multiple I/O threads against that directory. This is for example relevant for</span>
<span class="token comment"># high-throughput RAIDs.</span>
<span class="token comment">#</span>
<span class="token comment"># io.tmp.dirs: /tmp</span>

<span class="token comment"># The classloading resolve order. Possible values are 'child-first' (Flink's default)</span>
<span class="token comment"># and 'parent-first' (Java's default).</span>
<span class="token comment">#</span>
<span class="token comment"># Child first classloading allows users to use different dependency/library</span>
<span class="token comment"># versions in their application than those in the classpath. Switching back</span>
<span class="token comment"># to 'parent-first' may help with debugging dependency issues.</span>
<span class="token comment">#</span>
<span class="token comment"># classloader.resolve-order: child-first</span>

<span class="token comment"># The amount of memory going to the network stack. These numbers usually need </span>
<span class="token comment"># no tuning. Adjusting them may be necessary in case of an "Insufficient number</span>
<span class="token comment"># of network buffers" error. The default min is 64MB, the default max is 1GB.</span>
<span class="token comment"># </span>
<span class="token comment"># taskmanager.memory.network.fraction: 0.1</span>
<span class="token comment"># taskmanager.memory.network.min: 64mb</span>
<span class="token comment"># taskmanager.memory.network.max: 1gb</span>

<span class="token comment">#==============================================================================</span>
<span class="token comment"># Flink Cluster Security Configuration</span>
<span class="token comment">#==============================================================================</span>

<span class="token comment"># Kerberos authentication for various components - Hadoop, ZooKeeper, and connectors -</span>
<span class="token comment"># may be enabled in four steps:</span>
<span class="token comment"># 1. configure the local krb5.conf file</span>
<span class="token comment"># 2. provide Kerberos credentials (either a keytab or a ticket cache w/ kinit)</span>
<span class="token comment"># 3. make the credentials available to various JAAS login contexts</span>
<span class="token comment"># 4. configure the connector to use JAAS/SASL</span>

<span class="token comment"># The below configure how Kerberos credentials are provided. A keytab will be used instead of</span>
<span class="token comment"># a ticket cache if the keytab path and principal are set.</span>

<span class="token comment"># security.kerberos.login.use-ticket-cache: true</span>
<span class="token comment"># security.kerberos.login.keytab: /path/to/kerberos/keytab</span>
<span class="token comment"># security.kerberos.login.principal: flink-user</span>

<span class="token comment"># The configuration below defines which JAAS login contexts</span>

<span class="token comment"># security.kerberos.login.contexts: Client,KafkaClient</span>

<span class="token comment">#==============================================================================</span>
<span class="token comment"># ZK Security Configuration</span>
<span class="token comment">#==============================================================================</span>

<span class="token comment"># Below configurations are applicable if ZK ensemble is configured for security</span>

<span class="token comment"># Override below configuration to provide custom ZK service name if configured</span>
<span class="token comment"># zookeeper.sasl.service-name: zookeeper</span>

<span class="token comment"># The configuration below must match one of the values set in "security.kerberos.login.contexts"</span>
<span class="token comment"># zookeeper.sasl.login-context-name: Client</span>

<span class="token comment">#==============================================================================</span>
<span class="token comment"># HistoryServer</span>
<span class="token comment">#==============================================================================</span>

<span class="token comment"># The HistoryServer is started and stopped via bin/historyserver.sh (start|stop)</span>

<span class="token comment"># Directory to upload completed jobs to. Add this directory to the list of</span>
<span class="token comment"># monitored directories of the HistoryServer as well (see below).</span>
jobmanager.archive.fs.dir: hdfs://cluster/flink/flink-completed-jobs/

<span class="token comment"># The address under which the web-based HistoryServer listens.</span>
historyserver.web.address: 192.168.1.29

<span class="token comment"># The port under which the web-based HistoryServer listens.</span>
historyserver.web.port: 8082

<span class="token comment"># Comma separated list of directories to monitor for completed jobs.</span>
historyserver.archive.fs.dir: hdfs://cluster/flink/flink-completed-jobs/

<span class="token comment"># Interval in milliseconds for refreshing the monitored directories.</span>
historyserver.archive.fs.refresh-interval: 10000


</code></pre> 
<h2><a id="3_SLF4JLogbackLog4jLogginghttpswwwcnblogscomsuger43894p9543685html_369"></a>3. <a href="https://www.cnblogs.com/suger43894/p/9543685.html" rel="nofollow">SLF4J和Logback和Log4j和Logging的区别与联系</a></h2> 
<h3><a id="31__370"></a>3.1. <mark>一个著名的日志系统是怎么设计出来的</mark></h3> 
<ul><li>日志消息除了能打印到控制台， 还可以输出到文件，甚至可以通过邮件发送出去(<code>例如生成环境出错的消息</code>)</li><li>日志内容应该可以做格式化， 例如变成纯文本，XML, HTML格式等等</li><li>对于不同的Java class，不同的 package ， 还有不同级别的日志，应该可以灵活地输出到不同的文件中 
  <ul><li>例如对于com.foo 这个package，所有的日志都输出到 foo.log 文件中</li><li>对于com.bar 这个package ，所有文件都输出到bar. log文件中</li><li><code>对于所有的ERROR级别的日志，都输出到 errors.log文件中</code></li></ul> </li><li>能对日志进行分级， 有些日志纯属debug ， 在本机或者测试环境使用， 方便程序员的调试， 生产环境完全不需要。有些日志是描述错误(error)的， 在生产环境下出错的话必须要记录下来，帮助后续的分析。<br> <img src="https://images2.imgbox.com/81/e2/vPVn8o1q_o.png" alt="在这里插入图片描述"></li></ul> 
<h3><a id="32_standalone_379"></a>3.2. 在standalone集群模式下运行案例遇到的一个问题</h3> 
<ul><li><code>在运行状态下，清空log文件夹下的内容之后，以后就不产生日志文件和输出文件了，重启之后恢复正常？？</code></li></ul> 
<h2><a id="4_Flink_DataStream_API_httpsblogcsdnnetlixgjobarticledetails86594601_381"></a>4. <a href="https://blog.csdn.net/lixgjob/article/details/86594601">Flink DataStream API 编程指南</a></h2> 
<h3><a id="41_Flink_382"></a>4.1. Flink程序剖析</h3> 
<p><code>Flink 程序看起来像是转换数据集合的规范化程序。每个程序由一些基本的部分组成</code></p> 
<ul><li>获取执行环境</li><li>加载/创建初始数据</li><li>指定对数据的转换操作</li><li>指定计算结果存放的位置</li><li>触发程序执行</li></ul> 
<h3><a id="42_FlinkWordCount_390"></a>4.2. 第一个Flink程序（WordCount）</h3> 
<ul><li> <p>整体结构<br> <img src="https://images2.imgbox.com/22/0c/vI3QUBwx_o.png" alt="在这里插入图片描述"></p> </li><li> <p>build.sbt</p> <pre><code>ThisBuild / resolvers ++= Seq(
    "Apache Development Snapshot Repository" at "https://repository.apache.org/content/repositories/snapshots/",
    Resolver.mavenLocal
)

name := "FirstFlinkDemo"

version := "1.0"

organization := "com.xiaofan"

ThisBuild / scalaVersion := "2.12.6"

val flinkVersion = "1.10.0"

val flinkDependencies = Seq(
  "org.apache.flink" %% "flink-scala" % flinkVersion % "provided",
  "org.apache.flink" %% "flink-streaming-scala" % flinkVersion % "provided",
  "org.slf4j" % "slf4j-log4j12" % "1.8.0-beta4")

lazy val root = (project in file(".")).
  settings(
    libraryDependencies ++= flinkDependencies
  )

// 打包指定的主类
assembly / mainClass := Some("com.xiaofan.SocketTextStreamWordCount")

// make run command include the provided dependencies
Compile / run  := Defaults.runTask(Compile / fullClasspath,
                                   Compile / run / mainClass,
                                   Compile / run / runner
                                  ).evaluated

// stays inside the sbt console when we press "ctrl-c" while a Flink programme executes with "run" or "runMain"
Compile / run / fork := true
Global / cancelable := true

// exclude Scala library from assembly
assembly / assemblyOption  := (assembly / assemblyOption).value.copy(includeScala = false)

fork in run := true

</code></pre> </li><li> <p>mainRunner<br> <code>这个好像设置不设置都没有什么影响</code><br> <img src="https://images2.imgbox.com/b7/05/Ih5fvJhh_o.png" alt="在这里插入图片描述"></p> </li><li> <p>SocketTextStreamWordCount类<br> <img src="https://images2.imgbox.com/c2/f5/uLjVxyxL_o.png" alt="在这里插入图片描述"></p> </li><li> <p>本地运行结果<br> <img src="https://images2.imgbox.com/c0/61/LNiyr4LJ_o.png" alt="在这里插入图片描述"></p> </li><li> <p>打包提交的集群</p> 
  <ul><li> <p>修改代码<br> <img src="https://images2.imgbox.com/f6/e6/y0XwNSff_o.png" alt="在这里插入图片描述"></p> </li><li> <p>打fat包：<code>sbt clean assembly</code></p> </li><li> <p>提交到集群：<code>bin/flink run /home/hadoop/fanjh/FirstFlinkDemo-assembly-1.0.jar 192.168.1.27 9999</code></p> <p><img src="https://images2.imgbox.com/48/58/cr0QqLUh_o.png" alt="在这里插入图片描述"></p> </li></ul> </li></ul> 
<h2><a id="5_FlinkStandaloneYarnhttpsblogcsdnnetu013337425articledetails102696305_455"></a>5. <a href="https://blog.csdn.net/u013337425/article/details/102696305">Flink任务提交流程（Standalone和Yarn）</a></h2> 
<h2><a id="6__457"></a>6. 寄语：天行健，君子以自强不息</h2>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4b40dd06d81492775d6083aa0f737eea/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">javascript 递归调用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f74dcb26a5bdac12153e76ac1eb7d7b0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">STM32F103的Systick无法进入SysTick_Handler中断的问题解决</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>