<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Flash-Attention代码调用尝试 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Flash-Attention代码调用尝试" />
<meta property="og:description" content="Flash-Attention代码调用尝试 本文主要介绍通过如何通过源码方式使用flash-attention，以实现更自由的调用。
1.介绍 Flash-attention原理：
论文：
FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré
Paper: https://arxiv.org/abs/2205.14135
IEEE Spectrum article about our submission to the MLPerf 2.0 benchmark using FlashAttention. FlashAttention
FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
Tri Dao
Paper: https://tridao.me/publications/flash2/flash2.pdf
源码：
https://github.com/Dao-AILab/flash-attention
FlashAttention-2 硬件支持
Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/0e3caf160b71b95fbbb63a81ba7649a8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-30T10:00:16+08:00" />
<meta property="article:modified_time" content="2023-11-30T10:00:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Flash-Attention代码调用尝试</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="FlashAttention_0"></a>Flash-Attention代码调用尝试</h3> 
<p>本文主要介绍通过如何通过源码方式使用flash-attention，以实现更自由的调用。</p> 
<h4><a id="1_7"></a>1.介绍</h4> 
<p><strong>Flash-attention原理：</strong></p> 
<p><strong>论文：</strong><br> FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness<br> Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré<br> Paper: https://arxiv.org/abs/2205.14135<br> IEEE Spectrum article about our submission to the MLPerf 2.0 benchmark using FlashAttention. FlashAttention</p> 
<p>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning<br> Tri Dao</p> 
<p>Paper: https://tridao.me/publications/flash2/flash2.pdf</p> 
<p>源码：<br> https://github.com/Dao-AILab/flash-attention</p> 
<p><strong>FlashAttention-2 硬件支持</strong></p> 
<p>Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100).<br> Turing GPUs 只能使用FlashAttention 1.x.</p> 
<p>Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).<br> All head dimensions up to 256. Head dim &gt; 192 backward requires A100/A800 or H100/H800.</p> 
<p><strong>pytorch2.0版本已内置了flash-attention1</strong></p> 
<h4><a id="2_39"></a>2.环境配置</h4> 
<p>CUDA 11.6 and above<br> PyTorch 1.12 and above</p> 
<p>以及LLM基本运行环境<br> 我的环境是：<br> transformers 4.33.1<br> torch 2.0.1+cu118<br> torchaudio 2.0.2+cu118<br> torchvision 0.15.2+cu118<br> accelerate 0.22.0<br> sentencepiece 0.1.99</p> 
<p>install flash-attention<br> 在线安装：<br> pip install flash-attn --no-build-isolation</p> 
<p>源码编译安装：<br> python setup.py install</p> 
<h4><a id="3_61"></a>3.代码实现</h4> 
<p>模型chatglm2-6b</p> 
<h5><a id="31__67"></a>3.1 模型调用</h5> 
<p>模型加载时使用modeling_chatglm.py 而非transformers的AutoModel加载，因为要对modeling中的AttenCore进行修改</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer
<span class="token keyword">from</span> modeling_chatglm <span class="token keyword">import</span> ChatGLMModel<span class="token punctuation">,</span> ChatGLMForConditionalGeneration

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> ChatGLMForConditionalGeneration<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<h5><a id="32Attention_82"></a>3.2Attention实现源码修改</h5> 
<pre><code class="prism language-python"><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
FLASH_ATTN_FLAG<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"inference by Flash attention src:"</span><span class="token punctuation">,</span> FLASH_ATTN_FLAG<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

<span class="token keyword">class</span> <span class="token class-name">CoreAttention</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">,</span> attention_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
            pytorch_major_version <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>__version__<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

            <span class="token keyword">if</span> pytorch_major_version <span class="token operator">&gt;=</span> <span class="token number">2</span><span class="token punctuation">:</span>

                <span class="token keyword">if</span> FLASH_ATTN_FLAG<span class="token punctuation">:</span>
                    <span class="token keyword">from</span> flash_attn <span class="token keyword">import</span> flash_attn_qkvpacked_func<span class="token punctuation">,</span>flash_attn_func
                    query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer <span class="token operator">=</span> <span class="token punctuation">[</span>k<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token punctuation">[</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">]</span><span class="token punctuation">]</span>
                    dropout_p<span class="token operator">=</span><span class="token number">0.0</span>
                    softmax_scale<span class="token operator">=</span><span class="token number">0.0</span>                    
                    context_layer <span class="token operator">=</span> flash_attn_func<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">,</span> dropout_p<span class="token punctuation">,</span> causal<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                    context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
                <span class="token comment">#chatglm2-6b Official code</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer <span class="token operator">=</span> <span class="token punctuation">[</span>k<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token punctuation">[</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">]</span><span class="token punctuation">]</span>

                    <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> query_layer<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> key_layer<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                        context_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">,</span>
                                                                                        is_causal<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                            attention_mask <span class="token operator">=</span> <span class="token operator">~</span>attention_mask
                        context_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>query_layer<span class="token punctuation">,</span> key_layer<span class="token punctuation">,</span> value_layer<span class="token punctuation">,</span>
                                                                                        attention_mask<span class="token punctuation">)</span>
                    context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
                new_context_layer_shape <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size_per_partition<span class="token punctuation">,</span><span class="token punctuation">)</span>
                context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">*</span>new_context_layer_shape<span class="token punctuation">)</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

</code></pre> 
<h4><a id="4_124"></a>4.结果</h4> 
<table><thead><tr><th>优化方案</th><th>input tokens</th><th>speed</th><th>显存占用(mb)</th></tr></thead><tbody><tr><td>pytorch</td><td>1800</td><td>33.8</td><td>15472</td></tr><tr><td>pytorch2.0</td><td>1800</td><td>36.5</td><td>14200</td></tr><tr><td>flash attention2</td><td>1800</td><td>36.7</td><td>14200</td></tr><tr><td>pytorch</td><td>7000</td><td>18</td><td>37322</td></tr><tr><td>pytorch2.0</td><td>7000</td><td>29.9</td><td>17030</td></tr><tr><td>flash attention2</td><td>7000</td><td>34.2</td><td>17102</td></tr><tr><td>pytorch</td><td>20000</td><td>OOM</td><td>OOM</td></tr><tr><td>pytorch2.0</td><td>20000</td><td>13.5</td><td>24122</td></tr><tr><td>flash attention2</td><td>20000</td><td>18.6</td><td>24194</td></tr><tr><td>pytorch</td><td>32396</td><td>OOM</td><td>OOM</td></tr><tr><td>pytorch2.0</td><td>32396</td><td>8</td><td>30448</td></tr><tr><td>flash attention2</td><td>32396</td><td>14.1</td><td>30520</td></tr></tbody></table>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e7bae35d56c6c947ff4811e452d6ff96/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PLECS: Analysis tools 分析工具，伯德图 Bode 分析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7483a088e1a709f9621539626dfbef33/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python完美表白代码</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>