<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Sutton reinforcement learning _ Chapter 2 Multi-armed Bandits - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Sutton reinforcement learning _ Chapter 2 Multi-armed Bandits" />
<meta property="og:description" content="打算看英文版Sutton的《强化学习》，从第二章开始记录下对每一章的理解，对每一块的内容大致介绍，留个纪念。
这一章围绕着多臂赌博机问题，介绍了基本的强化学习算法（value based），并探讨了利用（exploit）和探索（explore）问题。
2.1 A k-armed Bandit Problem
有k个赌博机，每次的操作就是拉下其中一个控制杆，随后你会得到一个奖励。通过多次的选择，你要使得总收益最大化。其中每个赌博机的收益服从一个分布。k个动作的每一个被选择时都有一个期望奖励，称为这个动作的价值。在时刻的动作为，对应的奖励为。任一动作对应的价值，记为。如果知道每个动作的价值，那么每次选择价值最高的动作就行。假设不知道动作的价值，我们就要进行估计。我们对动作在时刻的价值的估计记为，我们期望它接近。
2.2 Action-value Methods
这是一个估计动作价值的方法，用这些动作价值的估计来选择。一种简单的方法就是通过计算实际收益的平均值来作为动作价值的估计。
其中，当为真时值为1，反之为0。当分母为0时，我们将定义为一个默认值，比如0。
最简单的的方案是选择具有最高估计值的动作，及贪心（greedy）。如果有多个贪心动作，则选择任意一个，即，其中，是使得最大的动。这种方案总是利用当前的知识最大化眼前的收益，只有利用（exploit）没有探索（explore）。一个简单的代替是，以的概率从所有动作中随机选一个动作，的概率用贪心策略，这样每一个动作就有可能被采样到。
2.3 The 10-armed Testbed
评估算法和的性能，这部分比较好理解。
2.4 Incremental Implementation
我们用观测到的奖励的样本均值来估计action-value，这里介绍了一种高效的方法来计算样本均值。表示某一动作第次被选中获得的奖励，表示该动作被选择n-1次后它的估计的action-value。
可以换一种方式来计算这个式子
一个使用以增量式计算的样本均值和动作选择的多臂赌博机算法如下
2.5 Tracking a Nonstationary Problem
前面讨论的问题前提是每个赌博机的奖励概率分布是固定的（stationary），如果奖励概率是不固定的（随着时间变化，unstationary）,那么取平均的方法就不适用。这时比较流行的方法是去固定步长，即，其中
2.6 Optimistic Initial Values
目前我们讨论的方法都在一定程度上依赖于初始动作的值，这些方法是有偏的（biased）。但实际中，这通常并不是一个问题。
初始动作的价值同时也提供了一种简单的探索(explore)方式，如果初始值设置的比较大，那么刚开始算法会倾向于探索每一个动作，会进行大量的探索(explore)，我们把这种初始值的设定称为乐观初始价值。它适用于固定奖励概率分布问题，而不适用与非固定的情况，因为它探索的动因是暂时的。
2.7 Upper-Confidence-Bound Action Selection
这里提出了一种根据动作的潜力来选择可能事实上是最优的动作，这要考虑它们的估计有多接近最大值，以及这些估计的不确定性。按如下公式选择动作
表示在时刻之前动作被选中的次数，如果他为0，那么被认为是最好的动作。这方法叫置信度上限（UCB，Upper-Confidence-Bound)），平方根项是对动作值估计的不确定性或方差的度量。最大值的大小是动作的可能真实值的上限，是置信水平。下图是UCB算法与的比较
2.8 Gradient Bandit Algorithms
本节中，针对每个动作，考虑一个数值化的偏好函数。偏好函数越大，动作就越频繁地被选择。
和表示在时刻选择动作的概率。
基于梯度上升，提出一中自然的学习方法。每个步骤中，在选择动作并获得后，偏好函数会更新
其中，是一个大于0的树，表示步长，是在时刻内所有收益的平均值。作为一个基准，收益高于它，那么未来选择动作的概率会增加，反之减少。基准项的作用可以用下图来表示
实际上梯度赌博机算法可以理解为梯度上升的随机近似，证明过程如下
至此证明了该算法的期望更新与期望奖励的梯度是相等的，因此该算法是随机梯度上升算法的一种。这保证了算法的收敛性。
对于基准项，只要要求它与所选的动作无关即可。可以为一个固定的数0或1000，该算法仍是梯度上升算法的特例。基准项不影响算法的更新，但它会影响更新值的方查，从而影响收敛速度。采用奖励的平均可能不是最好的，但是它简单，在实践中很有效。
2.9 Associative Search (Contextual Bandits)
以上所考虑的问题都是非关联的，没有必要将不同的动作与不同的情境联系起来。在一般的强化学习问题中，往往有不止一种情境，他们的目标是学习一种策略：一个从特定情境到最优动作的映射。
关于这个问题不是本章的重点，就不介绍了。
2.10 Summary
在这一章介绍了几种平衡exploit和explore的方法，比较了几种算法的性能，并在最后提及了更复杂的平衡exploit和explore的方法（贝叶斯方法）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/9c896ebfdb030156f6c83182a9f9ad54/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-19T16:46:39+08:00" />
<meta property="article:modified_time" content="2020-07-19T16:46:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Sutton reinforcement learning _ Chapter 2 Multi-armed Bandits</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>打算看英文版Sutton的《强化学习》，从第二章开始记录下对每一章的理解，对每一块的内容大致介绍，留个纪念。</p> 
<p>这一章围绕着多臂赌博机问题，介绍了基本的强化学习算法（value based），并探讨了利用（exploit）和探索（explore）问题。</p> 
<p>2.1 A k-armed Bandit Problem</p> 
<p><img alt="" src="https://images2.imgbox.com/d4/c9/krcLEVMp_o.png"></p> 
<p>有k个赌博机，每次的操作就是拉下其中一个控制杆，随后你会得到一个奖励。通过多次的选择，你要使得总收益最大化。其中每个赌博机的收益服从一个分布。k个动作的每一个被选择时都有一个期望奖励，称为这个动作的价值。在<img alt="t" class="mathcode" src="https://images2.imgbox.com/59/fd/1NuO0LGm_o.gif">时刻的动作为<img alt="A_t" class="mathcode" src="https://images2.imgbox.com/b3/82/tQW1vei0_o.gif">，对应的奖励为<img alt="R_t" class="mathcode" src="https://images2.imgbox.com/77/76/1TiT0TPK_o.gif">。任一动作<img alt="a" class="mathcode" src="https://images2.imgbox.com/3a/db/3dJJLJjI_o.gif">对应的价值，记为<img alt="q_* (a)\doteq \mathbb{E}[R_t|A_t=a]" class="mathcode" src="https://images2.imgbox.com/ef/be/DGGg5wCc_o.gif">。如果知道每个动作的价值，那么每次选择价值最高的动作就行。假设不知道动作的价值，我们就要进行估计。我们对动作<img alt="a" class="mathcode" src="https://images2.imgbox.com/6e/2a/yt3WKi8B_o.gif">在时刻<img alt="t" class="mathcode" src="https://images2.imgbox.com/5e/c2/8ZaE0StK_o.gif">的价值的估计记为<img alt="Q_t(a)" class="mathcode" src="https://images2.imgbox.com/e4/0b/gmyEsbdn_o.gif">，我们期望它接近<img alt="q_*(a)" class="mathcode" src="https://images2.imgbox.com/47/f1/hZQ2jX43_o.gif">。</p> 
<p>2.2 Action-value Methods</p> 
<p>这是一个估计动作价值的方法，用这些动作价值的估计来选择。一种简单的方法就是通过计算实际收益的平均值来作为动作价值的估计。<img alt="Q_t(a)=\frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}}=\frac{\sum _{i=1}^{t-1}R_i\cdot \mathbb{I}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{I}_{A_i=a}}" class="mathcode" src="https://images2.imgbox.com/4a/5f/lA4D0GrV_o.gif"></p> 
<p>其中<img alt="\mathbb{I}_{predicate}" class="mathcode" src="https://images2.imgbox.com/f5/e4/VBi0Bfn9_o.gif">，当<img alt="predicate" class="mathcode" src="https://images2.imgbox.com/a5/a3/1sUnNooi_o.gif">为真时值为1，反之为0。当分母为0时，我们将<img alt="Q_t(a)" class="mathcode" src="https://images2.imgbox.com/a3/79/q86g5YJI_o.gif">定义为一个默认值，比如0。</p> 
<p>最简单的的方案是选择具有最高估计值的动作，及贪心（greedy）。如果有多个贪心动作，则选择任意一个，即<img alt="A_t\doteq argmax{Q_t(a)}" class="mathcode" src="https://images2.imgbox.com/8c/78/bqTELEGH_o.gif">，其中，<img alt="argmax_a" class="mathcode" src="https://images2.imgbox.com/6c/4e/ObMwBXp5_o.gif">是使得<img alt="Q_t(a)" class="mathcode" src="https://images2.imgbox.com/e3/ba/86atj7IZ_o.gif">最大的动<img alt="a" class="mathcode" src="https://images2.imgbox.com/fd/05/ycSke4r5_o.gif">。这种方案总是利用当前的知识最大化眼前的收益，只有利用（exploit）没有探索（explore）。一个简单的代替是<img alt="\epsilon - greedy" class="mathcode" src="https://images2.imgbox.com/52/38/0284x9kv_o.gif">，以<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/73/a5/stBRvbtl_o.gif">的概率从所有动作中随机选一个动作，<img alt="1-\epsilon" class="mathcode" src="https://images2.imgbox.com/ce/8b/XWOJSzA4_o.gif">的概率用贪心策略，这样每一个动作就有可能被采样到。</p> 
<p>2.3 The 10-armed Testbed</p> 
<p> 评估<img alt="greedy" class="mathcode" src="https://images2.imgbox.com/c8/dc/9azT27ne_o.gif">算法和<img alt="\epsilon -greedy" class="mathcode" src="https://images2.imgbox.com/cb/c5/T6IKly3M_o.gif">的性能，这部分比较好理解。</p> 
<p><img alt="" height="798" src="https://images2.imgbox.com/e5/8e/qhVhi92F_o.png" width="933"></p> 
<p>2.4 Incremental Implementation</p> 
<p>我们用观测到的奖励的样本均值来估计action-value，这里介绍了一种高效的方法来计算样本均值。<img alt="R_i" class="mathcode" src="https://images2.imgbox.com/b5/64/1ei7B0zf_o.gif">表示某一动作第<img alt="i" class="mathcode" src="https://images2.imgbox.com/e2/7e/Fup2z0bH_o.gif">次被选中获得的奖励，<img alt="Q_n" class="mathcode" src="https://images2.imgbox.com/36/d4/0g0R6TMW_o.gif">表示该动作被选择n-1次后它的估计的action-value。<img alt="Q_n\doteq \frac{R_1+R_2+...+R_n-1}{n-1}" class="mathcode" src="https://images2.imgbox.com/5e/dd/088nNmiw_o.gif"></p> 
<p>可以换一种方式来计算这个式子</p> 
<p><img alt="Q_{n+1}=\frac{1}{n}\sum_{i=1}^{n}R_i=\frac{1}{n}(R_n+\sum_{i=1}^{n-1}R_i)=\frac{1}{n}(R_n+(n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i)=\frac{1}{n}(R_n+(n-1)Q_n)=\frac{1}{n}(R_n+nQ_n-Q_n)=Q_n+\frac{1}{n}[R_n-Q_n]" class="mathcode" src="https://images2.imgbox.com/62/b3/zbuPLDCT_o.gif"></p> 
<p>一个使用以增量式计算的样本均值和<img alt="\varepsilon -greedy" class="mathcode" src="https://images2.imgbox.com/e1/b9/OklEpGX8_o.gif">动作选择的多臂赌博机算法如下</p> 
<p><img alt="" height="356" src="https://images2.imgbox.com/c8/c2/jsU17qFv_o.png" width="913"></p> 
<p>2.5 Tracking a Nonstationary Problem</p> 
<p>前面讨论的问题前提是每个赌博机的奖励概率分布是固定的（stationary），如果奖励概率是不固定的（随着时间变化，unstationary）,那么取平均的方法就不适用。这时比较流行的方法是去固定步长，即<img alt="Q_{n+1}\doteq Q_n+\alpha [R_n-Q_n]" class="mathcode" src="https://images2.imgbox.com/cb/ef/7ZKajz79_o.gif">，其中<img alt="\alpha \in (0,1]" class="mathcode" src="https://images2.imgbox.com/f3/b2/2BVjvtAU_o.gif"></p> 
<p>2.6 Optimistic Initial Values</p> 
<p>目前我们讨论的方法都在一定程度上依赖于初始动作的值<img alt="Q_1(a)" class="mathcode" src="https://images2.imgbox.com/f1/70/oR3Lqibt_o.gif">，这些方法是有偏的（biased）。但实际中，这通常并不是一个问题。</p> 
<p>初始动作的价值同时也提供了一种简单的探索(explore)方式，如果初始值设置的比较大，那么刚开始算法会倾向于探索每一个动作，会进行大量的探索(explore)，我们把这种初始值的设定称为乐观初始价值。它适用于固定奖励概率分布问题，而不适用与非固定的情况，因为它探索的动因是暂时的。</p> 
<p>2.7 Upper-Confidence-Bound Action Selection</p> 
<p>这里提出了一种根据动作的潜力来选择可能事实上是最优的动作，这要考虑它们的估计有多接近最大值，以及这些估计的不确定性。按如下公式选择动作</p> 
<p><img alt="A_t\doteq argmax[{Q_t(a)+c\sqrt{\frac{lnt}{N_t(a)}}}]" class="mathcode" src="https://images2.imgbox.com/36/4d/ZxENyEhe_o.gif"></p> 
<p><img alt="N_t(a)" class="mathcode" src="https://images2.imgbox.com/e7/81/AuSGAl5Q_o.gif">表示在时刻<img alt="t" class="mathcode" src="https://images2.imgbox.com/4f/88/nk7k4qwR_o.gif">之前动作<img alt="a" class="mathcode" src="https://images2.imgbox.com/91/a2/pFIWvokj_o.gif">被选中的次数，如果他为0，那么<img alt="a" class="mathcode" src="https://images2.imgbox.com/51/a6/lfDLNW58_o.gif">被认为是最好的动作。这方法叫置信度上限（UCB，Upper-Confidence-Bound)），平方根项是对<img alt="a" class="mathcode" src="https://images2.imgbox.com/02/1f/82Iqu44L_o.gif">动作值估计的不确定性或方差的度量。最大值的大小是动作<img alt="a" class="mathcode" src="https://images2.imgbox.com/b9/84/xFrRiCNx_o.gif">的可能真实值的上限，<img alt="c" class="mathcode" src="https://images2.imgbox.com/e2/f4/42JuidT4_o.gif">是置信水平。下图是UCB算法与<img alt="\epsilon- greedy" class="mathcode" src="https://images2.imgbox.com/2e/18/gNN0d7lC_o.gif">的比较</p> 
<p><img alt="" height="470" src="https://images2.imgbox.com/2b/a2/Eo0vWAFJ_o.png" width="924"></p> 
<p>2.8 Gradient Bandit Algorithms</p> 
<p>本节中，针对每个动作<img alt="a" class="mathcode" src="https://images2.imgbox.com/42/57/HV5qKntq_o.gif">，考虑一个数值化的偏好函数<img alt="H_t(a)" class="mathcode" src="https://images2.imgbox.com/ae/d7/Pr4Gm6o4_o.gif">。偏好函数越大，动作就越频繁地被选择。</p> 
<p><img alt="Pr\left \{ A_t=a \right \}\doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}}\doteq \pi _t(a)" class="mathcode" src="https://images2.imgbox.com/08/71/7roKbcKs_o.gif"></p> 
<p><img alt="Pr\left \{ A_t=a \right \}" class="mathcode" src="https://images2.imgbox.com/d3/93/ePLWOsPJ_o.gif">和<img alt="\pi _t(a)" class="mathcode" src="https://images2.imgbox.com/db/32/vYYhsl5z_o.gif">表示在<img alt="t" class="mathcode" src="https://images2.imgbox.com/3a/6a/KrTMSKVn_o.gif">时刻选择动作<img alt="a" class="mathcode" src="https://images2.imgbox.com/e2/f6/AovPlzcs_o.gif">的概率。</p> 
<p>基于梯度上升，提出一中自然的学习方法。每个步骤中，在选择动作<img alt="A_t" class="mathcode" src="https://images2.imgbox.com/c4/bd/oZTh4yNs_o.gif">并获得<img alt="R_t" class="mathcode" src="https://images2.imgbox.com/5e/bf/9JKLnfks_o.gif">后，偏好函数会更新</p> 
<p><img alt="H_{t+1}(A_t)\doteq H_t(A_t)+\alpha (R_t-\bar{R_t})(1-\pi_t(A_t)),\text{and}\newline H_{t+1}(a)\doteq H_t(a)-\alpha (R_t-\bar{R_t})\pi_t(a),\text{for all }a\neq A_t," class="mathcode" src="https://images2.imgbox.com/1c/e1/t12wQ2hy_o.gif"></p> 
<p> 其中，<img alt="\alpha" class="mathcode" src="https://images2.imgbox.com/3f/cd/KENyaSBs_o.gif">是一个大于0的树，表示步长，<img alt="\bar{R_t}\in\mathbb{R}" class="mathcode" src="https://images2.imgbox.com/1b/86/tbfXE1rY_o.gif">是在时刻<img alt="t" class="mathcode" src="https://images2.imgbox.com/44/fc/iyvswxWw_o.gif">内所有收益的平均值。<img alt="\bar{R_t}" class="mathcode" src="https://images2.imgbox.com/e0/d8/Da5qexbs_o.gif">作为一个基准，收益高于它，那么未来选择动作<img alt="A_t" class="mathcode" src="https://images2.imgbox.com/a6/90/RmUX5cAl_o.gif">的概率会增加，反之减少。基准项的作用可以用下图来表示</p> 
<p><img alt="" height="464" src="https://images2.imgbox.com/fc/07/08AHOrmE_o.png" width="888"></p> 
<p>实际上梯度赌博机算法可以理解为梯度上升的随机近似，证明过程如下</p> 
<p><img alt="" height="591" src="https://images2.imgbox.com/0a/33/ORaQU2B5_o.png" width="893"></p> 
<p><img alt="" height="798" src="https://images2.imgbox.com/8a/13/uIiEHQyk_o.png" width="879"> </p> 
<p> <img alt="" height="443" src="https://images2.imgbox.com/97/09/qcoRmFoX_o.png" width="881"></p> 
<p> <img alt="" height="738" src="https://images2.imgbox.com/67/00/TIGfMLKH_o.png" width="867"></p> 
<p>至此证明了该算法的期望更新与期望奖励的梯度是相等的，因此该算法是随机梯度上升算法的一种。这保证了算法的收敛性。</p> 
<p>对于基准项，只要要求它与所选的动作无关即可。可以为一个固定的数0或1000，该算法仍是梯度上升算法的特例。基准项不影响算法的更新，但它会影响更新值的方查，从而影响收敛速度。采用奖励的平均可能不是最好的，但是它简单，在实践中很有效。</p> 
<p>2.9 Associative Search (Contextual Bandits)<br> 以上所考虑的问题都是非关联的，没有必要将不同的动作与不同的情境联系起来。在一般的强化学习问题中，往往有不止一种情境，他们的目标是学习一种策略：一个从特定情境到最优动作的映射。</p> 
<p>关于这个问题不是本章的重点，就不介绍了。</p> 
<p>2.10 Summary</p> 
<p>在这一章介绍了几种平衡exploit和explore的方法，比较了几种算法的性能，并在最后提及了更复杂的平衡exploit和explore的方法（贝叶斯方法）。</p> 
<p><img alt="" height="550" src="https://images2.imgbox.com/fd/f3/k0sEAzjd_o.png" width="898"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d52afc36987c89208faf777743c36ac2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">串口传输速率计算</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dc2c073d4f854d56701600547ba3c95d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">全网最全经典卷积神经网络架构汇总——深度学习之ILSVRC竞赛(ImageNet竞赛)优胜网络结构</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>