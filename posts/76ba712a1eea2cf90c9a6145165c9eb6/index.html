<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>视频生成: 基于Stable Diffusion的微调方法 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="视频生成: 基于Stable Diffusion的微调方法" />
<meta property="og:description" content="chatGPT带来了几个月的AIGC热度，文本图像生成模型大行其道，但AI在视频生成任务上尚没有较好的开源仓库，并受限于“缺那么几百块A100&#34;的资源问题，大多数人无法展开视频生成的研究。好在目前有不少针对视频生成的相关paper，也有不少开源实现，事实上缺的是一个完整的训练&#43;推理&#43;Pretrained模型，本文要解决的就是这个问题。
1. Stable Diffusion以及其中Unet结构，下图摘自论文: High-Resolution Image Synthesis with Latent Diffusion Models
为了支持视频生成，需要对Unet结构中的部分模块进行改造，包括2d卷积以及Self-Attention和Cross-Attention。在许多的Stable Diffusion开源实现中，Tune A Video这篇论文的代码较为干净简洁，在利用Stable Diffusion V1-4权重作为pretrained，参考Make A Video利用3d伪引入空间信息，并且保留Tune A Video中关于Sparse Cross Attention的修改。
2. 3d伪卷积引入时空相关信息，图片摘自Make A Video
代码实现引用lucidrains的make-a-video-pytorch，并且加入关于时空的Position Embedding部分。
3. Sparse Casual Attention 出于节省运算量的目的，当前帧跟第一帧和当前帧的前一帧做Cross Attention，这个只是运算上的调整，Cross Attention结构并无修改。
4. 3090如何训练
大多数论文，训练视频生成都是采用8张A100做微调，或者利用成百上千的GPU进行大规模训练。对于咱穷人来说，只有两块3090，训练方法分步骤进行:
a. 128x128
b. 256x256，batch size单卡为4，grad accumulation设置为100
5. 数据集
视频数据集webvid, hdvila100m
图片数据集laion400m
我简单实验下来，加上图片数据集混合训练文本生成效果会更好一些。
代码开源于: https://github.com/xuduo35/MakeLongVideo" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/76ba712a1eea2cf90c9a6145165c9eb6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-09T21:09:41+08:00" />
<meta property="article:modified_time" content="2023-04-09T21:09:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">视频生成: 基于Stable Diffusion的微调方法</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>    chatGPT带来了几个月的AIGC热度，文本图像生成模型大行其道，但AI在视频生成任务上尚没有较好的开源仓库，并受限于“缺那么几百块A100"的资源问题，大多数人无法展开视频生成的研究。好在目前有不少针对视频生成的相关paper，也有不少开源实现，事实上缺的是一个完整的训练+推理+Pretrained模型，本文要解决的就是这个问题。</p> 
<p>    1. Stable Diffusion以及其中Unet结构，下图摘自论文: High-Resolution Image Synthesis with Latent Diffusion Models</p> 
<p><img alt="" height="584" src="https://images2.imgbox.com/ec/2c/E1r3rnbY_o.png" width="910"></p> 
<p>    为了支持视频生成，需要对Unet结构中的部分模块进行改造，包括2d卷积以及Self-Attention和Cross-Attention。在许多的Stable Diffusion开源实现中，Tune A Video这篇论文的代码较为干净简洁，在利用Stable Diffusion V1-4权重作为pretrained，参考Make A Video利用3d伪引入空间信息，并且保留Tune A Video中关于Sparse Cross Attention的修改。</p> 
<p>    2. 3d伪卷积引入时空相关信息，图片摘自Make A Video</p> 
<p>    <img alt="" height="410" src="https://images2.imgbox.com/a4/75/Luf6yvqQ_o.png" width="811"></p> 
<p>    代码实现引用lucidrains的<a class="link-info" href="https://github.com/lucidrains/make-a-video-pytorch" title="make-a-video-pytorch">make-a-video-pytorch</a>，并且加入关于时空的Position Embedding部分。</p> 
<p>    3. Sparse Casual Attention </p> 
<p><img alt="" height="402" src="https://images2.imgbox.com/0f/a2/WodxC1Y1_o.png" width="886"></p> 
<p>    出于节省运算量的目的，当前帧跟第一帧和当前帧的前一帧做Cross Attention，这个只是运算上的调整，Cross Attention结构并无修改。</p> 
<p>    4. 3090如何训练</p> 
<p>    大多数论文，训练视频生成都是采用8张A100做微调，或者利用成百上千的GPU进行大规模训练。对于咱穷人来说，只有两块3090，训练方法分步骤进行:</p> 
<p>    a. 128x128</p> 
<p>    b. 256x256，batch size单卡为4，grad accumulation设置为100</p> 
<p>  5. 数据集</p> 
<p>  视频数据集webvid, hdvila100m</p> 
<p>  图片数据集laion400m</p> 
<p>  我简单实验下来，加上图片数据集混合训练文本生成效果会更好一些。</p> 
<p>  代码开源于: <a href="https://github.com/xuduo35/MakeLongVideo" title="https://github.com/xuduo35/MakeLongVideo">https://github.com/xuduo35/MakeLongVideo</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/31db49e91c7cc5bdc11c2c283cbdb617/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据库作业</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ca31236791670ff528ba776171589ea1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【个人学习】使用torch实现xml文件返回的图片</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>