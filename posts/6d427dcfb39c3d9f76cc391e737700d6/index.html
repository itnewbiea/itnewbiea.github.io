<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Megatron-LM源码系列(六)：Distributed-Optimizer分布式优化器实现Part1 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Megatron-LM源码系列(六)：Distributed-Optimizer分布式优化器实现Part1" />
<meta property="og:description" content="1. 使用说明 在megatron中指定--use-distributed-optimizer就能开启分布式优化器, 参数定义在megatron/arguments.py中。分布式优化器的思路是将训练中的优化器状态均匀地分布到不同数据并行的rank结点上，相当于开启ZERO-1的训练。
group.add_argument(&#39;--use-distributed-optimizer&#39;, action=&#39;store_true&#39;, help=&#39;Use distributed optimizer.&#39;) 在使用--use-distributed-optimizer, 同时会check两个参数 args.DDP_impl == &#39;local&#39;(默认开启)和args.use_contiguous_buffers_in_local_ddp(默认开启)。
# If we use the distributed optimizer, we need to have local DDP # and we should make sure use-contiguous-buffers-in-local-ddp is on. if args.use_distributed_optimizer: assert args.DDP_impl == &#39;local&#39; assert args.use_contiguous_buffers_in_local_ddp 分布式优化器节省的理论显存值依赖参数类型和梯度类型，以下是每一个parameter对应占用的理论字节数(d表示数据并行的size大小，也就是一个数据并行中的卡数, 等于 T P × P P TP \times PP TP×PP )：
训练数据类型Non-distributed optim（单位Byte）Distributed optim（单位Byte）float16 param, float16 grads204 &#43; 16/dfloat16 param, fp32 grads186 &#43; 12/dfp32 param, fp32 grads168 &#43; 8/d 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/6d427dcfb39c3d9f76cc391e737700d6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-31T14:13:27+08:00" />
<meta property="article:modified_time" content="2023-12-31T14:13:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Megatron-LM源码系列(六)：Distributed-Optimizer分布式优化器实现Part1</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="1__0"></a>1. 使用说明</h3> 
<p>在megatron中指定<code>--use-distributed-optimizer</code>就能开启分布式优化器, 参数定义在<code>megatron/arguments.py</code>中。分布式优化器的思路是将训练中的优化器状态均匀地分布到不同数据并行的rank结点上，相当于开启<code>ZERO-1</code>的训练。</p> 
<pre><code class="prism language-python">    group<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--use-distributed-optimizer'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                       <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'Use distributed optimizer.'</span><span class="token punctuation">)</span>
</code></pre> 
<p>在使用<code>--use-distributed-optimizer</code>, 同时会check两个参数 <code>args.DDP_impl == 'local'</code>(默认开启)和<code>args.use_contiguous_buffers_in_local_ddp</code>(默认开启)。</p> 
<pre><code class="prism language-python">    <span class="token comment"># If we use the distributed optimizer, we need to have local DDP</span>
    <span class="token comment"># and we should make sure use-contiguous-buffers-in-local-ddp is on.</span>
    <span class="token keyword">if</span> args<span class="token punctuation">.</span>use_distributed_optimizer<span class="token punctuation">:</span>
        <span class="token keyword">assert</span> args<span class="token punctuation">.</span>DDP_impl <span class="token operator">==</span> <span class="token string">'local'</span>
        <span class="token keyword">assert</span> args<span class="token punctuation">.</span>use_contiguous_buffers_in_local_ddp
</code></pre> 
<p>分布式优化器节省的理论显存值依赖参数类型和梯度类型，以下是每一个parameter对应占用的理论字节数(<code>d</code>表示数据并行的size大小，也就是一个数据并行中的卡数, 等于 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         T 
        
       
         P 
        
       
         × 
        
       
         P 
        
       
         P 
        
       
      
        TP \times PP 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">TP</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">PP</span></span></span></span></span> )：</p> 
<table><thead><tr><th>训练数据类型</th><th>Non-distributed optim（单位Byte）</th><th>Distributed optim（单位Byte）</th></tr></thead><tbody><tr><td>float16 param, float16 grads</td><td>20</td><td>4 + 16/d</td></tr><tr><td>float16 param, fp32 grads</td><td>18</td><td>6 + 12/d</td></tr><tr><td>fp32 param, fp32 grads</td><td>16</td><td>8 + 8/d</td></tr></tbody></table> 
 
<h3><a id="2__28"></a>2. 实现介绍</h3> 
<ul><li> <p>Distributed-Optimizer分布式优化器的主要实现是通过连续的<code>grad buffer</code>来进行的，<code>grad buffer</code>中用于模型状态和优化器状态之间进行parameter参数和grad梯度的通信。<code>grad buffer</code>中使用reduce-scatter和all-gather进行通信。</p> </li><li> <p>数据流如下：<br> <img src="https://images2.imgbox.com/5d/25/SgQIPomt_o.png" alt="在这里插入图片描述"></p> 
  <ol><li>在每个dp的rank上计算完grad后，组成待更新的grad buffer数组</li><li>更新的时候通过reduce-scatter将grad buffer切分到各个rank上</li><li>在每个rank上完成优化器的step操作</li><li>最后将所有结果执行allgather操作得到更新后的grad buffer。</li></ol> </li><li> <p>以fp16类型grad为例，grad buffer分片说明如下：<br> <img src="https://images2.imgbox.com/27/69/UvrjgMHg_o.png" alt="在这里插入图片描述"></p> 
  <ul><li>一共有4个参数，分别用绿/黄/蓝/红表示；总参数大小为16个fp16类型数据</li><li>按DP中rank的个数对总数据均匀切分</li><li>如果参数过大，每个rank可能会只包含部分参数的数据，所以要考虑参数的偏移</li><li>每个DP rank中的每个param参数都对应有3个偏移，一个是world_index表示总的数据偏移，一个是local_index表示在当前rank中的数据偏移，一个是param_index相对于param来说，表示当前rank结点存的数据的偏移。</li><li>以黄色参数Param1为例，在rank0存了Param1的一个元素，rank1存了Param1的4个元素；world_index来说rank0上黄色部分的元素是总数据的[3,4], rank1上黄色部分的4个元素是总数据的[4,8]; local_index来说在rank0上表示[3,4]，rank1表示当前结点全部的4个元素，范围也就是[0,4];param_index来说，对于rank0上的Param1的param_index就是[0,1]，在rank2上的param_index就是[1,5];</li></ul> </li><li> <p>关键步骤详解：</p> 
  <ol><li>上图中每个方块看成是一个grad buffer中的一个fp16类型元素，在反向结束以后，grad buffer中有16个fp16类型的元素</li><li>在每一个DP rank上调用reduce-scatter操作</li><li>每个DP rank的grad buffer中都有4个fp16类型元素经过了reduce-scatter操作更新，没更新的12个fp16类型元素等待后续垃圾回收</li><li>每个DP rank从grad buffer中拷贝更新后的4个fp16类型元素到fp32类型的main grad buffer中，准备开始后续的更新操作，例如 
    <ul><li>DP rank0拷贝[0:4]个元素</li><li>DP rank1拷贝[4:8]个元素</li><li>DP rank2拷贝[8:12]个元素</li><li>DP rank3拷贝[12:16]个元素</li></ul> </li><li>执行Optimizer.step(), step()操作必须通过fp32类型来进行计算</li><li>每个DP rank从main grad buffer中拷贝step()更新后的4个fp32类型元素到fp16类型的grad buffer中</li><li>执行allgather操作, 这样每个grad buffer就都是最新更新后的数据了</li><li>基于grad buffer来更新各个模型的fp16类型的参数</li><li>开始进行下一轮的更新</li></ol> </li></ul> 
<h3><a id="3__64"></a>3. 源码实现</h3> 
<h4><a id="31__65"></a>3.1 程序入口</h4> 
<ul><li>初始化的入口在文件<code>megatron/training.py</code>的<code>get_model</code>函数中，在创建<code>LocalDDP</code>的实例中会传入<code>args.use_contiguous_buffers_in_local_ddp</code>。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>distributed <span class="token keyword">import</span> DistributedDataParallel <span class="token keyword">as</span> torchDDP

<span class="token keyword">def</span> <span class="token function">get_model</span><span class="token punctuation">(</span>model_provider_func<span class="token punctuation">,</span> model_type<span class="token operator">=</span>ModelType<span class="token punctuation">.</span>encoder_or_decoder<span class="token punctuation">,</span> wrap_with_ddp<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">if</span> wrap_with_ddp<span class="token punctuation">:</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>DDP_impl <span class="token operator">==</span> <span class="token string">'torch'</span><span class="token punctuation">:</span>
            <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        <span class="token keyword">elif</span> args<span class="token punctuation">.</span>DDP_impl <span class="token operator">==</span> <span class="token string">'local'</span><span class="token punctuation">:</span>
            model <span class="token operator">=</span> <span class="token punctuation">[</span>LocalDDP<span class="token punctuation">(</span>model_module<span class="token punctuation">,</span>
                              args<span class="token punctuation">.</span>accumulate_allreduce_grads_in_fp32<span class="token punctuation">,</span>
                              args<span class="token punctuation">.</span>use_contiguous_buffers_in_local_ddp<span class="token punctuation">)</span>
                     <span class="token keyword">for</span> model_module <span class="token keyword">in</span> model<span class="token punctuation">]</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<ul><li>训练的入口定义在<code>train_step</code>函数中, 基本流程如下：</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train_step</span><span class="token punctuation">(</span>forward_step_func<span class="token punctuation">,</span> data_iterator<span class="token punctuation">,</span>
               model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> opt_param_scheduler<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    <span class="token comment"># 清除grad</span>
    <span class="token keyword">if</span> args<span class="token punctuation">.</span>DDP_impl <span class="token operator">==</span> <span class="token string">'local'</span> <span class="token keyword">and</span> args<span class="token punctuation">.</span>use_contiguous_buffers_in_local_ddp<span class="token punctuation">:</span>
        <span class="token keyword">for</span> partition <span class="token keyword">in</span> model<span class="token punctuation">:</span>
            partition<span class="token punctuation">.</span>zero_grad_buffer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    <span class="token comment"># 执行前反向计算</span>
    losses_reduced <span class="token operator">=</span> forward_backward_func<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    <span class="token comment"># 对梯度执行Reduce-Scatter操作</span>
    optimizer<span class="token punctuation">.</span>reduce_model_grads<span class="token punctuation">(</span>args<span class="token punctuation">,</span> timers<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    <span class="token comment"># 更新梯度</span>
    timers<span class="token punctuation">(</span><span class="token string">'optimizer'</span><span class="token punctuation">,</span> log_level<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>start<span class="token punctuation">(</span>barrier<span class="token operator">=</span>args<span class="token punctuation">.</span>barrier_with_L1_time<span class="token punctuation">)</span>
    update_successful<span class="token punctuation">,</span> grad_norm<span class="token punctuation">,</span> num_zeros_in_grad <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span>args<span class="token punctuation">,</span> timers<span class="token punctuation">)</span>
    timers<span class="token punctuation">(</span><span class="token string">'optimizer'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    <span class="token comment"># 对更新后的param执行gather操作</span>
    <span class="token keyword">if</span> update_successful<span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>gather_model_params<span class="token punctuation">(</span>args<span class="token punctuation">,</span> timers<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    <span class="token comment"># 通过scheduler更新学习率</span>
    <span class="token keyword">if</span> update_successful<span class="token punctuation">:</span>
        increment <span class="token operator">=</span> get_num_microbatches<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> \
                    args<span class="token punctuation">.</span>micro_batch_size <span class="token operator">*</span> \
                    args<span class="token punctuation">.</span>data_parallel_size
        opt_param_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>increment<span class="token operator">=</span>increment<span class="token punctuation">)</span>
        skipped_iter <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        skipped_iter <span class="token operator">=</span> <span class="token number">1</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<h4><a id="32_grad_bufferDistributedDataParallel_129"></a>3.2 grad buffer初始化（DistributedDataParallel类）</h4> 
<ul><li><code>grad buffer</code>初始化是在类<code>DistributedDataParallel</code>的init函数中, 源码定义在<code>megatron/optimizer/distrib_optimizer.py</code>文件中。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DistributedDataParallel</span><span class="token punctuation">(</span>DistributedDataParallelBase<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">,</span>
                 accumulate_allreduce_grads_in_fp32<span class="token punctuation">,</span>
                 use_contiguous_buffers<span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre> 
<ul><li>创建grad buffer和index map</li></ul> 
<pre><code class="prism language-python">            self<span class="token punctuation">.</span>_grad_buffers <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
            self<span class="token punctuation">.</span>_grad_buffer_param_index_map <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
            data_parallel_world_size <span class="token operator">=</span> mpu<span class="token punctuation">.</span>get_data_parallel_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>按类型分别计算每个类型元素的个数，使用type_num_elements map进行存储，key是元素类型，value是类型出现的元素个数</li></ul> 
<pre><code class="prism language-python">            <span class="token comment"># First calculate total number of elements per type.</span>
            type_num_elements <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
            <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                    dtype <span class="token operator">=</span> _get_buffer_type<span class="token punctuation">(</span>param<span class="token punctuation">)</span>
                    type_num_elements<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span> <span class="token operator">=</span> type_num_elements<span class="token punctuation">.</span>get<span class="token punctuation">(</span>dtype<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> \
                                               <span class="token operator">+</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>nelement<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>实际开始分配grad buffer, 为了支持被DP并行数正好切分，需要先对每个类型出现的个数进行padding操作；然后通过<code>MemoryBuffer</code>进行存储的分配</li></ul> 
<pre><code class="prism language-python">            <span class="token comment"># Allocate the buffer.</span>
            <span class="token keyword">for</span> dtype<span class="token punctuation">,</span> num_elements <span class="token keyword">in</span> type_num_elements<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

                <span class="token comment"># If using distributed optimizer, pad memory buffer to be</span>
                <span class="token comment"># multiple of data_parallel_world_size. (This padding is done</span>
                <span class="token comment"># due to a constraint with the reduce_scatter op, which requires</span>
                <span class="token comment"># all tensors have equal size. See: optimizer.py.)</span>
                num_elements_padded <span class="token operator">=</span> data_parallel_world_size <span class="token operator">*</span> \
                    <span class="token builtin">int</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>num_elements <span class="token operator">/</span> data_parallel_world_size<span class="token punctuation">)</span><span class="token punctuation">)</span>

                <span class="token comment"># Allocate grad buffer.</span>
                self<span class="token punctuation">.</span>_grad_buffers<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span> <span class="token operator">=</span> MemoryBuffer<span class="token punctuation">(</span>num_elements<span class="token punctuation">,</span>
                                                         num_elements_padded<span class="token punctuation">,</span>
                                                         dtype<span class="token punctuation">)</span>
</code></pre> 
<ul><li>从grad buffer中给每一个param参数分配对应的main_grad空间，在分配main_grad时根据每个param参数的类型从对应的<code>self._grad_buffers[dtype]</code>中得到跟<code>param.data.shape</code>一样的tensor，这里的tensor与grad buffer共享存储。同时grad buffer的分配是按倒序来分配的，比如<code>self.module.parameters()</code>中有三个参数分别是<code>[p1, p2, p3]</code>, 在grad buffer中存储则是<code>[p3_grad, p2_grad, p1_grad]</code>。<code>_grad_buffer_param_index_map</code>用来记录每个param的梯度在grad buffer中存储的起始和结束位置。</li></ul> 
<pre><code class="prism language-python">            <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
            <span class="token comment"># Assume the back prop order is reverse the params order,</span>
            <span class="token comment"># store the start index for the gradients.</span>
            <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                    dtype <span class="token operator">=</span> _get_buffer_type<span class="token punctuation">(</span>param<span class="token punctuation">)</span>
                    type_num_elements<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span> <span class="token operator">-=</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>nelement<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token comment"># get的第二个参数是start_index，这里的start_index是从grad_buffer从大到小来算的</span>
                    param<span class="token punctuation">.</span>main_grad <span class="token operator">=</span> self<span class="token punctuation">.</span>_grad_buffers<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span>
                        param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> type_num_elements<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span><span class="token punctuation">)</span>
                    <span class="token keyword">if</span> dtype <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>_grad_buffer_param_index_map<span class="token punctuation">:</span>
                        self<span class="token punctuation">.</span>_grad_buffer_param_index_map<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
                    self<span class="token punctuation">.</span>_grad_buffer_param_index_map<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span><span class="token punctuation">[</span>param<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>
                        type_num_elements<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span><span class="token punctuation">,</span>
                        type_num_elements<span class="token punctuation">[</span>dtype<span class="token punctuation">]</span> <span class="token operator">+</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>nelement<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token punctuation">)</span>
</code></pre> 
<ul><li>遍历每一个参数，对于每一个参数的grad_fn的下一个function累加grad_acc函数进行改写，由于param本身没有<code>grad_fn</code>，通过trick方式使用<code>param.expand_as</code>给param加上了<code>grad_fn</code>函数。</li></ul> 
<pre><code class="prism language-python">            <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
            <span class="token comment"># Backward hook.</span>
            <span class="token comment"># Accumalation function for the gradients. We need</span>
            <span class="token comment"># to store them so they don't go out of scope.</span>
            self<span class="token punctuation">.</span>grad_accs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token comment"># Loop over all the parameters in the model.</span>
            <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                    <span class="token comment"># 使用expand_as使param具有grad_fn.</span>
                    param_tmp <span class="token operator">=</span> param<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>param<span class="token punctuation">)</span>
                    <span class="token comment"># 获取梯度累加函数，并注册hook改写</span>
                    grad_acc <span class="token operator">=</span> param_tmp<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                    grad_acc<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_make_param_hook<span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">)</span>
                    self<span class="token punctuation">.</span>grad_accs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>grad_acc<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">_make_param_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> param<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Create the all-reduce hook for backprop."""</span>
        <span class="token comment"># Hook used for back-prop.</span>
        <span class="token keyword">def</span> <span class="token function">param_hook</span><span class="token punctuation">(</span><span class="token operator">*</span>unused<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># Add the gradient to the buffer.</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token comment"># The gradient function of linear layers is fused with GEMMs</span>
                param<span class="token punctuation">.</span>main_grad<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
                <span class="token comment"># Now we can deallocate grad memory.</span>
                param<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">return</span> param_hook
</code></pre> 
<h3><a id="4__230"></a>4. 参考</h3> 
<ul><li><a href="https://www.mltalks.com/posts/3749485165/" rel="nofollow">Megatron-LM源码系列(六)：Distributed-Optimizer分布式优化器实现Part1</a></li></ul>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/788088e4974bd3fb9c80c5f93f593b59/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">RK3568笔记七：yolov5-seg实例分割测试验证</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7ddd4e1edfd52a604b8b077cbba7a96e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用Vue.js实现手机系统检测和页面响应</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>