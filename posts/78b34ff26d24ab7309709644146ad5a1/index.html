<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>End-to-End Object Detection with Transformers - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="End-to-End Object Detection with Transformers" />
<meta property="og:description" content="本文参考：端到端目标检测DETR
论文：DETR
代码：代码
目录
DETR
1.1 前言
1.1.1 研究动机：端到端目标检测的意义
1.1.2 简介
1.2 相关工作
1.3 算法
1.3.1 目标函数
1.3.2 模型结构
1.3.3 代码
1.4 实验
1.1 前言
1.1.1 研究动机：端到端目标检测的意义
DETR（DEtection TRansformer）是2020.5发布在Arxiv上的一篇论文，可以说是近年来目标检测领域的一个里程碑式的工作。从论文题目就可以看出，DETR其最大创新点有两个：end-to-end（端到端）和 引入Transformer。
目标检测任务，一直都是比图片分类复杂很多，因为需要预测出图片中物体的位置和类别。以往的主流的目标检测方法都不是端到端的目标检测，因为：会加入很多的先验知识，预先生成一些锚框。比如one-stage方法（YOLO系列）中的Anchor模板；two-stage（R-CNN系列）中的proposal。这些方法都不是直接预测物体，而是利用anchor/proposal去做近似，最后都会生成大大小小很多的预测框，必须在后处理时使用NMS去除这些冗余的框。
正是因为需要很多的人工干预、先验知识（Anchor）还有NMS，所以整个检测框架非常复杂，难调参难优化，并且部署困难（NMS需要的算子普通的库不一定支持，即不是所有硬件都支持）。所以说，一个端到端的目标检测是大家一直以来梦寐以求的。
1.1.2 简介
1.DETR如何做到end-to-end
DETR利用Transformer这种全局建模的能力，直接把目标检测视为集合预测问题（即给定一张图像，预测图像中感兴趣物体的集合）。然后使用可学习的object query替代了生成anchor的机制；使用了新的目标函数，并利用二分图匹配的方式，强制模型对每个物体只生成一个预测框，从而替代了NMS这一步。
DETR把之前不可学习的东西（anchor、NMS）变成可学的东西，删掉了这些依赖先验知识的部分，从而得到了一个简单有效的端到端的网络。所以DETR不需要费尽心思的设计anchor，不需要NMS后处理，也就没有那么多超参需要调，也不需要复杂的算子。
除了端到端这一点，DETR使用了 Transformer Encoder-Decoder 的架构。相比于原始的 Transformer，DETR是并行预测的（in parallel），即所有预测框是一起出框的。
2.简单架构
整个模型前向流程如上，训练分四个步骤：
（1）使用CNN网络提取图片特征
（2）全局建模：图片特征拉成一维，输入Transformer Encoder中进行全局建模，进一步通过自注意力学习全局特征。 之所以使用Transformer Encoder，是因为Transformer中的自注意力机制，使得图片中的每个点（特征）都能和图片中所有其他特征做交互了，这样模型就能大致知道哪块区域是一个物体，哪块区域又是另一个物体，从而能够尽量保证每个物体只出一个预测框。所以说这种全局特征非常有利于移除冗余的框。
（3）通过Transformer Decoder 生成N个预测框set of box prediction（默认取N=100，也就是一张图固定生成100个预测框）。
（4）计算二分图匹配损失（bipartite matching loss），选出最优预测框，然后计算最优框的损失。 计算N个预测框与所有GT box（真实框）的matching
loss，然后通过二分图匹配算法来选出与每个物体最匹配的预测框。比如上图中有两个物体，那么最后只有两个框和它们是最匹配的，归为前景；剩下98个都被标记为背景（no object）。最后和之前的目标检测算法一样，计算这两个框的分类损失和回归损失。
推理时，前三步是一样的。通过decoder生成N个预测框后，设置一个置信度阈值进行过滤，得到最终的预测框。（比如设阈值=0.7，表示只输出置信度大于0.7的预测框，剩下都当做背景框）。
总的来说，Transformer Encoder全局建模，用于区分物体；Transformer Decoder用于描绘物体边界，将物体位置补充的更完整。
3.性能" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/78b34ff26d24ab7309709644146ad5a1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-09T12:57:11+08:00" />
<meta property="article:modified_time" content="2023-01-09T12:57:11+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">End-to-End Object Detection with Transformers</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/d7/a5/AuC8uyA7_o.png" alt="在这里插入图片描述"><br> 本文参考：<a href="https://blog.csdn.net/qq_56591814/article/details/127701119?spm=1001.2014.3001.5501">端到端目标检测DETR</a><br> 论文：<a href="https://arxiv.org/pdf/2005.12872.pdf" rel="nofollow">DETR</a><br> 代码：<a href="https://gitcode.net/mirrors/facebookresearch/detr?utm_source=csdn_github_accelerator" rel="nofollow">代码</a><br> <strong>目录</strong><br> <strong>DETR</strong><br>  1.1 前言<br>   1.1.1 研究动机：端到端目标检测的意义<br>   1.1.2 简介<br>  1.2 相关工作<br>  1.3 算法<br>   1.3.1 目标函数<br>   1.3.2 模型结构<br>   1.3.3 代码<br>  1.4 实验<br>  <br> <strong>1.1 前言<br> 1.1.1 研究动机：端到端目标检测的意义</strong><br>   DETR（DEtection TRansformer）是2020.5发布在Arxiv上的一篇论文，可以说是近年来目标检测领域的一个里程碑式的工作。从论文题目就可以看出，DETR其最大创新点有两个：end-to-end（端到端）和 引入Transformer。<br>   目标检测任务，一直都是比图片分类复杂很多，因为需要预测出图片中物体的位置和类别。以往的主流的目标检测方法都不是端到端的目标检测，因为：会加入很多的先验知识，预先生成一些锚框。比如one-stage方法（YOLO系列）中的Anchor模板；two-stage（R-CNN系列）中的proposal。这些方法都不是直接预测物体，而是利用anchor/proposal去做近似，最后都会生成大大小小很多的预测框，必须在后处理时使用NMS去除这些冗余的框。<br>   正是因为需要很多的人工干预、先验知识（Anchor）还有NMS，所以整个检测框架非常复杂，难调参难优化，并且部署困难（NMS需要的算子普通的库不一定支持，即不是所有硬件都支持）。所以说，一个端到端的目标检测是大家一直以来梦寐以求的。<br> <strong>1.1.2 简介<br> 1.DETR如何做到end-to-end</strong><br>   DETR利用Transformer这种全局建模的能力，直接把目标检测视为集合预测问题（即给定一张图像，预测图像中感兴趣物体的集合）。然后使用可学习的object query替代了生成anchor的机制；使用了新的目标函数，并利用二分图匹配的方式，强制模型对每个物体只生成一个预测框，从而替代了NMS这一步。<br>   DETR把之前不可学习的东西（anchor、NMS）变成可学的东西，删掉了这些依赖先验知识的部分，从而得到了一个简单有效的端到端的网络。所以DETR不需要费尽心思的设计anchor，不需要NMS后处理，也就没有那么多超参需要调，也不需要复杂的算子。<br>   除了端到端这一点，DETR使用了 Transformer Encoder-Decoder 的架构。相比于原始的 Transformer，DETR是并行预测的（in parallel），即所有预测框是一起出框的。<br> <strong>2.简单架构</strong><br> <img src="https://images2.imgbox.com/97/87/zIW1B9H7_o.png" alt="在这里插入图片描述"><br> 整个模型前向流程如上，训练分四个步骤：<br> （1）使用CNN网络提取图片特征<br> （2）全局建模：图片特征拉成一维，输入Transformer Encoder中进行全局建模，进一步通过自注意力学习全局特征。 之所以使用Transformer Encoder，是因为Transformer中的自注意力机制，使得图片中的每个点（特征）都能和图片中所有其他特征做交互了，这样模型就能大致知道哪块区域是一个物体，哪块区域又是另一个物体，从而能够尽量保证每个物体只出一个预测框。所以说这种全局特征非常有利于移除冗余的框。<br> （3）通过Transformer Decoder 生成N个预测框set of box prediction（默认取N=100，也就是一张图固定生成100个预测框）。<br> （4）计算二分图匹配损失（bipartite matching loss），选出最优预测框，然后计算最优框的损失。 计算N个预测框与所有GT box（真实框）的matching<br> loss，然后通过二分图匹配算法来选出与每个物体最匹配的预测框。比如上图中有两个物体，那么最后只有两个框和它们是最匹配的，归为前景；剩下98个都被标记为背景（no object）。最后和之前的目标检测算法一样，计算这两个框的分类损失和回归损失。<br>   推理时，前三步是一样的。通过decoder生成N个预测框后，设置一个置信度阈值进行过滤，得到最终的预测框。（比如设阈值=0.7，表示只输出置信度大于0.7的预测框，剩下都当做背景框）。<br>   总的来说，Transformer Encoder全局建模，用于区分物体；Transformer Decoder用于描绘物体边界，将物体位置补充的更完整。<br> <strong>3.性能</strong><br> 在摘要中，作者卖了一下DETR的优点：<br> （1）简单性：不仅框架简单，可以进行端到端的检测，而且只要硬件支持CNN和Transformer就一定可以支持DETR。在COCO数据集上的性能，和一个 训练好的Faster R-CNN baseline是差不多的，无论从内存、速度还是精度来说。<br> （2）迁移性好：DETR框架可以很容易的拓展到其它任务上，比如在全景分割上的效果就很好（加个分割头就行）。</p> 
<p>局限性：<br> （1）DETR对大物体检测效果不错，但是对小物体的检测效果不好（见实验4.1）。前者归功于transformer可以进行全局建模，这样无论多大的物体都可以检测，而不像anchor based方法检测大物体时会受限于anchor的尺寸。后者是因为作者只是使用了一个简单的结构，很多目标检测的针对性设计还没有使用，比如多尺度特征、针对性的检测头。<br> （2）训练太慢。<br> 为了达到好的效果，作者在COCO上训练了500epoch，而一般模型训练几十个epoch就行了。<br> 改进<br>   半年后提出的Deformable-DETR, 融入了多尺度特征，成功解决小物体检测效果不好的问题，还解决了训练慢的问题。<br>   另外DETR不仅是一个目标检测方法，还是一个拓展性很强的框架。其设计理论，就是适用于更多复杂的任务，使其更加的简单，甚至是使用一个框架解决所有问题。后续确实有一系列基于它的改进工作，比如Omni-DETR, up-DETR, PnP-DETR, SMAC-DETR, DAB-DETR, SAM-DETR, DN-DETR, OW-DETR, OV-DETR等等，将DETR应用在了目标追踪、视频领域的姿态预测、语义分割等多个视觉任务上。<br> <strong>1.2 相关工作</strong><br> 这一块介绍了三部分：<br> （1）介绍之前的集合预测工作<br> （2）如何使用Parallel Decoding让transformer可以并行预测<br> （3）目标检测<br>   集合预测：以前也有集合预测这一类的方法，也做了二分图匹配，也可以做到每个物体只得到一个预测框，而不需要NMS。但是这些方法性能低，要不就是为了提高性能加了很多人工干预，显得复杂。<br>   encoder-decoder：以前也有用encoder-decoder做检测，但都是17年以前的工作，用的是RNN的结构，效果和性能都不好（RNN自回归，效率慢）。<br>   所以对比以前的工作发现，能让DETR工作的好最主要的原因就是使用了Transformer。比如上面两点，都是backbone学的特征不够好，才需要使用很多人工干预，或者说模型效果性能都不好。 所以说DETR的成功，还是Transformer的成功。<br> <strong>1.3 算法<br> 1.3.1 目标函数</strong><br>   DETR 模型每次输出固定个数（N=100）的预测框，如何判断哪个预测框匹配哪个GT box呢？这就涉及到二分图匹配算法。<br>   在 scipy 库中，已经封装好了匈牙利算法，只需要将成本矩阵（cost matrix）输入进去就能够得到最优的排列。在 DETR 的官方代码中，也是调用的这个函数进行匹配（from scipy.optimize import linear_sum_assignment）。<br>   从N个预测框中，选出与M个GT Box最匹配的预测框，也可以转化为二分图匹配问题，这里需要填入矩阵的“成本”，就是每个预测框和GT Box的损失。对于目标检测问题，损失就是分类损失和边框损失组成。即：<br> <img src="https://images2.imgbox.com/07/ef/2YkQUncZ_o.png" alt="在这里插入图片描述"><br> 但是在DETR 中，损失函数有两点小改动：<br> （1）去掉分类损失中的log。对于前一项分类损失，通常目标检测方法计算损失时是需要加 log的，但是 DETR 中为了保证两部分损失的数值区间接近，便于优化，选择了去掉 了log；<br> （2）回归损失为L1 loss+GIOU。对于后一项回归损失，通常方法只计算一个 L1 loss（预测框和真实框坐标的L1 损失）。但是L1 loss和预测框的大小有关，框越大损失越大。 DETR 中，用 Transformer 提取的全局特征对大物体比较友好，经常出一些大框，这样就不利于优化，因此作者这里还添加了一个 GIoU Loss。<br> <strong>1.3.2 模型结构</strong><br>   作者在这部分给出了模型更详细的框架，如下图所示<br> <img src="https://images2.imgbox.com/eb/a1/QxbTOCok_o.png" alt="在这里插入图片描述"><br>   下面参考官网的一个demo，以输入尺寸3×800×1066为例进行前向过程：<br> （1）CNN提取特征（[800,1066,3]→[25,34,256]）<br>   backbone为ResNet-50，最后一个stage输出特征图为25×34×2048（32倍下采样），然后用1×1的卷积将通道数降为256；<br> （2）Transformer encoder 计算自注意力（[25,34,256]→[850,256]<br>   将上一步的特征拉直为850×256，并加上同样维度的位置编码（Transformer本身没有位置信息），然后输入的Transformer encoder进行自注意力计算，最终输出维度还是850×256；<br> （3）Transformer decoder解码，生成预测框<br>   decoder输入除了encoder部分最终输出的图像特征，还有前面提到的learned object query，其维度为100×256。在解码时，learned object query和全局图像特征不停地做across attention，最终输出100×256的自注意力结果。<br>   这里的object query即相当于之前的anchor/proposal，是一个硬性条件，告诉模型最后只得到100个输出。然后用这100个输出接FFN得到分类损失和回归损失。<br> （4）使用检测头输出预测框<br>   检测头就是目标检测中常用的全连接层（FFN），输出100个预测框（xcenter, ycenter, w, h ）和对应的类别。<br> （5）使用二分图匹配方式输出最终的预测框，然后计算预测框和真实框的损失，梯度回传，更新网络。</p> 
<p>除此之外还有部分细节：<br> （1）Transformer-encode/decoder都有6层<br> （2）除第一层外，每层Transformer encoder里都会先计算object query的self-attention，主要是为了移除冗余框。这些query交互之后，大概就知道每个query会出哪种框，互相之间不会再重复（见实验）。<br> （3）decoder加了auxiliary loss，即每层decoder输出的100×256维的结果，都加了FFN得到输出，然后去计算loss，这样模型收敛更快。（每层FFN共享参数）<br> <strong>1.3.3 伪代码</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">import</span> resnet50

<span class="token keyword">class</span> <span class="token class-name">DETR</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> nheads<span class="token punctuation">,</span>
        num_encoder_layers<span class="token punctuation">,</span> num_decoder_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># We take only convolutional layers from ResNet-50 model</span>
        self<span class="token punctuation">.</span>backbone <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token builtin">list</span><span class="token punctuation">(</span>resnet50<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 1×1卷积层将2048维特征降到256维</span>
        self<span class="token punctuation">.</span>transformer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Transformer<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> nheads<span class="token punctuation">,</span> num_encoder_layers<span class="token punctuation">,</span> num_decoder_layers<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_class <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> num_classes <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 类别FFN</span>
        self<span class="token punctuation">.</span>linear_bbox <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>                <span class="token comment"># 回归FFN</span>
        self<span class="token punctuation">.</span>query_pos <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># object query</span>
        <span class="token comment"># 下面两个是位置编码</span>
        self<span class="token punctuation">.</span>row_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span> hidden_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>col_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span> hidden_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>backbone<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        H<span class="token punctuation">,</span> W <span class="token operator">=</span> h<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        pos <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>col_embed<span class="token punctuation">[</span><span class="token punctuation">:</span>W<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>H<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
       					 self<span class="token punctuation">.</span>row_embed<span class="token punctuation">[</span><span class="token punctuation">:</span>H<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> W<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
       					 <span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 位置编码</span>
       					 
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>pos <span class="token operator">+</span> h<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>query_pos<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear_class<span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>linear_bbox<span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>


detr <span class="token operator">=</span> DETR<span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">91</span><span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> nheads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> num_encoder_layers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> num_decoder_layers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span>
detr<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">800</span><span class="token punctuation">,</span> <span class="token number">1200</span><span class="token punctuation">)</span>
logits<span class="token punctuation">,</span> bboxes <span class="token operator">=</span> detr<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>

</code></pre> 
<p><strong>1.4 实验<br> 1.4.1 对比 Faster RCNN</strong><br> <img src="https://images2.imgbox.com/16/b9/LesYC0Eb_o.png" alt="在这里插入图片描述"><br> （1）最上面一部分是 Detectron2 实现的 Faster RCNN ，但是本文中作者使用了很多trick<br> （2）中间部分是作者使用了GIoU loss、更强的数据增强策略、更长的训练时间来把上面三个模型重新训练了一次，这样更显公平。重新训练的模型以+表示，参数量等这些是一样的，但是普偏提了两个点<br> （3）下面部分是DETR模型，可以看到参数量、GFLOPS更小，但是推理更慢。模型比 Faster RCNN 精度高一点，主要是大物体检测提升6个点AP，小物体相比降低了4个点左右<br> （4）参数量、计算量和推理速度之间并没有必然的关系<br> （5）transformer encoder/decoder层数消融试验，结果是层数越多效果越好，但是考虑到计算量，作者最后选择6层。</p> 
<p>原文链接：https://blog.csdn.net/qq_56591814/article/details/127701119</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/396eb12437420010f0db1689885d10ab/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Pytorch—模型微调（fine-tune）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fc51b30d7c7ffd0826349568bb76eda9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">串口发送命令控制led灯的状态</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>