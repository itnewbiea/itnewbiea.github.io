<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AlexNet（ImageNet Classification with Deep Convolutional Neural Networks）网络详解 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="AlexNet（ImageNet Classification with Deep Convolutional Neural Networks）网络详解" />
<meta property="og:description" content="一.前言 AlexNet是2012年ImageNet竞赛冠军获得者Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，更多的更深的神经网路被提出，比如优秀的vgg,GoogleLeNet。其官方提供的数据模型，准确率达到57.1%,top 1-5 达到80.2%
注：其实在AlexNet网络问世之前，在进行图像识别，分割等工作时，我们采用手工提取特征或者是特征&#43;机器学习的方式，但是这样很难。因此有一种说法是我们能不能利用特征本身进行学习，那么特征本身就存在了层级关系（比如第一层是点，第二层是点与线组合的特征，第三层为局部特征，第四层…直到组合到一起到最后整个图像出来）那么这种特征学习的部分就是深度学习中的黑盒子，我们也不知道是如何进行的。
二. AlexNet的网络结构 Alexnet共有8层结构，前5层为卷积层，后三层为全连接层。从上图看，在网络设计上其实并非如上图所示，上图包含了GPU通信的部分。这是由当时GPU内存的限制引起的，作者使用两块GPU进行计算，因此分为了上下两部分
摘自原论文的结构图如下：
值得注意的一点：原图输入224 × 224，实际上进行了随机裁剪，实际大小为227 × 227。 但还有种说法就是原图就是224224，但是经过padding后将大小拓展成227227（源码好像是有padding操作，来自b站up主，下面详情分析是按照没有经过padding得来的） 对每层卷积进行分析：
注：经卷积后的矩阵尺寸大小计算公式为：N=（W-F&#43;2P）/S&#43;1
输入图片大小：W*W卷积核大小：F*Fstride：Spadding的像素数：P 三. AlexNet的亮点 首次使用GPU进行加速训练
使用Relu激活函数，而不是传统的Sigmoid激活函数或Tanh激活函数
Relu函数： f ( x ) = m a x ( 0 , x ) f(x)=max(0,x) f(x)=max(0,x)
ReLU 本质上是分段线性模型，前向计算非常简单，无需指数之类操作ReLU 的偏导也很简单，反向传播梯度，无需指数或者除法之类操作；ReLU 不容易发生梯度发散问题，Tanh 和Logistic 激活函数在两端的时候导数容易趋近于零，多级连乘后梯度更加约等于 0；ReLU 关闭了右边，从而会使得很多的隐层输出为 0，即网络变得稀疏，起到了类似 L1 的正则化作用，可以在一定程度上缓解过拟合。缺点：当然，ReLU 也是有缺点的，比如左边全部关了很容易导致某些隐藏节点永无翻身之日，所以后来又出现 pReLU、random ReLU等改进，而且 ReLU 会很容易改变数据的分布，因此 ReLU 后加 Batch Normalization 也是常用的改进的方法。 使用LRN局部响应归一化
局部响应归一化处理，实际就是利用临近的数据做归一化，该策略贡献了 1.2% 的准确率，该技术是深度学习训练时的一种提高准确度的技术方法，LRN 一般是在激活、池化后进行的一种处理方法。LRN 是对局部神经元的活动创建竞争机制，使得其中响应较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。
在全连接层的前两层使用Dropout随机失活神经元操作，以减少过拟合
四. 一些问题解答 AlexNet学习出来的特征是什么样子的？" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/e3cfdb92c8c8fd8b70f920afc6736059/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-17T20:54:47+08:00" />
<meta property="article:modified_time" content="2023-03-17T20:54:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AlexNet（ImageNet Classification with Deep Convolutional Neural Networks）网络详解</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>一.前言</h2> 
<p>AlexNet是2012年ImageNet竞赛冠军获得者Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，更多的更深的神经网路被提出，比如优秀的vgg,GoogleLeNet。其官方提供的数据模型，准确率达到57.1%,top 1-5 达到80.2%</p> 
<blockquote> 
 <p>注：其实在AlexNet网络问世之前，在进行图像识别，分割等工作时，我们采用手工提取特征或者是特征+机器学习的方式，但是这样很难。因此有一种说法是我们能不能利用特征本身进行学习，那么特征本身就存在了层级关系（比如第一层是点，第二层是点与线组合的特征，第三层为局部特征，第四层…直到组合到一起到最后整个图像出来）那么这种特征学习的部分就是深度学习中的<strong>黑盒子</strong>，我们也不知道是如何进行的。</p> 
</blockquote> 
<h2><a id="_AlexNet_4"></a>二. AlexNet的网络结构</h2> 
<ul><li>Alexnet共有8层结构，前5层为卷积层，后三层为全连接层。从上图看，在网络设计上其实并非如上图所示，上图包含了GPU通信的部分。这是由当时GPU内存的限制引起的，作者使用两块GPU进行计算，因此分为了上下两部分<br> 摘自<a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="nofollow">原论文</a>的结构图如下：<br> <img src="https://images2.imgbox.com/c3/7a/7b7Nh7SF_o.png" alt="在这里插入图片描述"></li><li>值得注意的一点：原图输入224 × 224，实际上进行了随机裁剪，实际大小为227 × 227。 但</li><li>还有种说法就是原图就是224<em>224，但是经过padding后将大小拓展成227</em>227（源码好像是有padding操作，来自b站up主，下面详情分析是按照没有经过padding得来的）</li></ul> 
<p>对每层卷积进行分析：<br> <img src="https://images2.imgbox.com/5f/77/wdUBE2zG_o.png" alt="-
-"></p> 
<blockquote> 
 <p>注：经卷积后的矩阵尺寸大小计算公式为：N=（W-F+2P）/S+1</p> 
 <ol><li>输入图片大小：W*W</li><li>卷积核大小：F*F</li><li>stride：S</li><li>padding的像素数：P</li></ol> 
</blockquote> 
<h2><a id="_AlexNet_21"></a>三. AlexNet的亮点</h2> 
<ul><li> <p>首次使用GPU进行加速训练</p> </li><li> <p>使用Relu激活函数，而不是传统的Sigmoid激活函数或Tanh激活函数<br> Relu函数：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           f 
          
         
           ( 
          
         
           x 
          
         
           ) 
          
         
           = 
          
         
           m 
          
         
           a 
          
         
           x 
          
         
           ( 
          
         
           0 
          
         
           , 
          
         
           x 
          
         
           ) 
          
         
        
          f(x)=max(0,x) 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p> 
  <ul><li>ReLU 本质上是分段线性模型，前向计算非常简单，无需指数之类操作</li><li>ReLU 的偏导也很简单，反向传播梯度，无需指数或者除法之类操作；</li><li>ReLU 不容易发生梯度发散问题，Tanh 和Logistic 激活函数在两端的时候导数容易趋近于零，多级连乘后梯度更加约等于 0；</li><li>ReLU 关闭了右边，从而会使得很多的隐层输出为 0，即网络变得稀疏，起到了类似 L1 的正则化作用，可以在一定程度上缓解过拟合。</li><li>缺点：当然，ReLU 也是有缺点的，比如左边全部关了很容易导致某些隐藏节点永无翻身之日，所以后来又出现 pReLU、random ReLU等改进，而且 ReLU 会很容易改变数据的分布，因此 ReLU 后加 Batch Normalization 也是常用的改进的方法。</li></ul> </li><li> <p>使用LRN局部响应归一化<br> 局部响应归一化处理，实际就是利用临近的数据做归一化，该策略贡献了 1.2% 的准确率，该技术是深度学习训练时的一种提高准确度的技术方法，LRN 一般是在激活、池化后进行的一种处理方法。LRN 是对局部神经元的活动创建竞争机制，使得其中响应较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p> </li><li> <p>在全连接层的前两层使用Dropout随机失活神经元操作，以减少过拟合<br> <img src="https://images2.imgbox.com/23/bc/T4J2gFuj_o.png" alt="在这里插入图片描述"></p> </li></ul> 
<h2><a id="__44"></a>四. 一些问题解答</h2> 
<p>AlexNet学习出来的特征是什么样子的？</p> 
<ul><li>第一层：都是一些填充的块状物和边界等特征</li><li>中间层：学习一些纹理特征</li><li>更高层：接近于分类器的层级，可以明显的看到物体的形状特征</li><li>最后一层：分类层，完全是物体的不同的姿态，根据不同的物体展现出不同姿态的特征了。</li><li>即无论对什么物体，学习过程都是：边缘→ \to→部分→ \to→整体</li></ul> 
<p><strong>附：为什么使用多层全连接：</strong></p> 
<p>全连接层在 CNN 中起到分类器的作用，前面的卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间，全连接层是将学到的特征映射映射到样本标记空间，就是矩阵乘法，再加上激活函数的非线性映射，多层全连接层理论上可以模拟任何非线性变换。但缺点也很明显: 无法保持空间结构。</p> 
<p>由于全连接网络的冗余（占整个我拿过来参数的 80%），近期一些好的网络模型使用全局平均池化（GAP）取代 FC 来融合学到的深度特征，最后使用 softmax 等损失函数作为网络目标函数来指导学习过程，用 GAP 替代 FC 的网络通常有较好的预测性能。</p> 
<p>全连接的一个作用是维度变换，尤其是可以把高维变到低维，同时把有用的信息保留下来。全连接另一个作用是隐含语义的表达 (embedding)，把原始特征映射到各个隐语义节点 (hidden node)。对于最后一层全连接而言，就是分类的显示表达。不同 channel 同一位置上的全连接等价与 1x1 的卷积。N 个节点的全连接可近似为 N 个模板卷积后的均值池化 (GAP)。</p> 
<p><code>GAP</code>：假如最后一层的数据是 10 个 6x6 的特征图，global average pooling 是将每个特征图计算所有像素点的均值，输出一个数据值，10 个特征图就会输出 10 个值，组成一个 1x10 的特征向量。</p> 
<ul><li>用特征图直接表示属于某类的置信率，比如有 10 个输出，就在最后输出 10 个特征图，每个特征图的值加起来求均值，然后将均值作为其属于某类的置信值，再输入 softmax 中，效果较好。</li><li>因为 FC 的参数众多，这么做就减少了参数的数量（在最近比较火的模型压缩中，这个优势可以很好的压缩模型的大小）。</li><li>因为减少了参数的数量，可以很好的减轻过拟合的发生。<br> <img src="https://images2.imgbox.com/c6/e2/JufPd0kJ_o.png" alt="在这里插入图片描述"></li></ul>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1727b9e069cc92634918cc55a12b8d56/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">人生有何意义？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/df6f8de8c23b0c7a5585ea9c7181709d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">爬虫实战（三）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>