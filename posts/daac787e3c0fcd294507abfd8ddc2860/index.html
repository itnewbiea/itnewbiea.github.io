<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Scrapy_settings配置文件设置 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Scrapy_settings配置文件设置" />
<meta property="og:description" content="文章目录 Scrapy_setting文件配置Scrapy常用参数 Scrapy_setting文件配置 代码未动，配置先行。本篇文章主要讲述一下Scrapy中的配置文件settings.py的参数含义
官文配置参数说明url：
https://docs.scrapy.org/en/latest/topics/settings.html
# Scrapy settings for ScrapyDemo project # 自动生成的配置，无需关注，不用修改 BOT_NAME = &#39;ScrapyDemo&#39; SPIDER_MODULES = [&#39;ScrapyDemo.spiders&#39;] NEWSPIDER_MODULE = &#39;ScrapyDemo.spiders&#39; # 这两个默认没有 # 日志保存，需要自己添加 # LOG_FILE=&#34;日志名.log&#34; # 日志输出等级 # LOG_LEVEL=&#34;WARNING&#34; &#39;&#39;&#39; LOG_LEVEL=&#34;WARNING&#34; 就只会WARNING等级之下的ERROR和CRITICAL 1.DEBUG 调试信息 2.INFO 一般信息 3.WARNING 警告 4.ERROR 普通错误 5.CRITICAL 严重错误 &#39;&#39;&#39; # 设置UA，但不常用，一般都是在MiddleWare中添加 USER_AGENT = &#39;ScrapyDemo (&#43;http://www.yourdomain.com)&#39; # 遵循robots.txt中的爬虫规则 ROBOTSTXT_OBEY = True # 需要改成Flase,要不然很多东西爬不了 # 对网站并发请求总数，默认16 CONCURRENT_REQUESTS = 32 # 相同网站两个请求之间的间隔时间，默认是0s。相当于time.sleep() DOWNLOAD_DELAY = 3 # 下面两个配置二选一，但其值不能大于CONCURRENT_REQUESTS，默认启用PER_DOMAIN # 对网站每个域名的最大并发请求，默认8 CONCURRENT_REQUESTS_PER_DOMAIN = 16 # 默认0，对网站每个IP的最大并发请求，会覆盖上面PER_DOMAIN配置， # 同时DOWNLOAD_DELAY也成了相同IP两个请求间的间隔了 CONCURRENT_REQUESTS_PER_IP = 16 # 禁用cookie，默认是True，启用 COOKIES_ENABLED = False # 请求头设置，这里基本上不用 DEFAULT_REQUEST_HEADERS = { # &#39;Accept&#39;: &#39;text/html,application/xhtml&#43;xml,application/xml;q=0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/daac787e3c0fcd294507abfd8ddc2860/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-12T20:59:26+08:00" />
<meta property="article:modified_time" content="2023-11-12T20:59:26+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Scrapy_settings配置文件设置</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Scrapy_setting_1" rel="nofollow">Scrapy_setting文件配置</a></li><li><a href="#Scrapy_121" rel="nofollow">Scrapy常用参数</a></li></ul> 
</div> 
<p></p> 
<h2><a id="Scrapy_setting_1"></a>Scrapy_setting文件配置</h2> 
<p>代码未动，配置先行。本篇文章主要讲述一下Scrapy中的配置文件settings.py的参数含义</p> 
<p>官文配置参数说明url：<br> https://docs.scrapy.org/en/latest/topics/settings.html</p> 
<pre><code class="prism language-python"><span class="token comment"># Scrapy settings for ScrapyDemo project</span>

<span class="token comment"># 自动生成的配置，无需关注，不用修改</span>
BOT_NAME <span class="token operator">=</span> <span class="token string">'ScrapyDemo'</span>
SPIDER_MODULES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'ScrapyDemo.spiders'</span><span class="token punctuation">]</span>
NEWSPIDER_MODULE <span class="token operator">=</span> <span class="token string">'ScrapyDemo.spiders'</span>

<span class="token comment"># 这两个默认没有</span>
<span class="token comment"># 日志保存，需要自己添加</span>
<span class="token comment"># LOG_FILE="日志名.log"</span>
<span class="token comment"># 日志输出等级</span>
<span class="token comment"># LOG_LEVEL="WARNING"</span>

<span class="token triple-quoted-string string">'''
LOG_LEVEL="WARNING"
就只会WARNING等级之下的ERROR和CRITICAL
1.DEBUG 调试信息
2.INFO 一般信息
3.WARNING 警告
4.ERROR 普通错误
5.CRITICAL 严重错误
'''</span>

<span class="token comment"># 设置UA，但不常用，一般都是在MiddleWare中添加</span>
USER_AGENT <span class="token operator">=</span> <span class="token string">'ScrapyDemo (+http://www.yourdomain.com)'</span>

<span class="token comment"># 遵循robots.txt中的爬虫规则</span>
ROBOTSTXT_OBEY <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># 需要改成Flase,要不然很多东西爬不了</span>

<span class="token comment"># 对网站并发请求总数，默认16</span>
CONCURRENT_REQUESTS <span class="token operator">=</span> <span class="token number">32</span>

<span class="token comment"># 相同网站两个请求之间的间隔时间，默认是0s。相当于time.sleep()</span>
DOWNLOAD_DELAY <span class="token operator">=</span> <span class="token number">3</span>
<span class="token comment"># 下面两个配置二选一，但其值不能大于CONCURRENT_REQUESTS，默认启用PER_DOMAIN</span>
<span class="token comment"># 对网站每个域名的最大并发请求，默认8</span>
CONCURRENT_REQUESTS_PER_DOMAIN <span class="token operator">=</span> <span class="token number">16</span>
<span class="token comment"># 默认0，对网站每个IP的最大并发请求，会覆盖上面PER_DOMAIN配置，</span>
<span class="token comment"># 同时DOWNLOAD_DELAY也成了相同IP两个请求间的间隔了</span>
CONCURRENT_REQUESTS_PER_IP <span class="token operator">=</span> <span class="token number">16</span>

<span class="token comment"># 禁用cookie，默认是True，启用</span>
COOKIES_ENABLED <span class="token operator">=</span> <span class="token boolean">False</span>

<span class="token comment"># 请求头设置，这里基本上不用</span>
DEFAULT_REQUEST_HEADERS <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
<span class="token comment">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span>
<span class="token comment">#   'Accept-Language': 'en',</span>
<span class="token comment">#}</span>

<span class="token comment"># 配置启用Spider MiddleWares，Key是class，Value是优先级</span>
SPIDER_MIDDLEWARES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">'ScrapyDemo.middlewares.ScrapydemoSpiderMiddleware'</span><span class="token punctuation">:</span> <span class="token number">543</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

<span class="token comment"># 配置启用Downloader MiddleWares</span>
DOWNLOADER_MIDDLEWARES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">'ScrapyDemo.middlewares.ScrapydemoDownloaderMiddleware'</span><span class="token punctuation">:</span> <span class="token number">543</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

<span class="token comment"># 配置并启用扩展，主要是一些状态监控</span>
EXTENSIONS <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">'scrapy.extensions.telnet.TelnetConsole'</span><span class="token punctuation">:</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

<span class="token comment"># 配置启用Pipeline用来持久化数据</span>
ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
   <span class="token string">'ScrapyDemo.pipelines.ScrapydemoPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

<span class="token comment"># 禁止控制台使用telnet连接scrapy获取状态，默认是启用。我们使用默认值即可</span>
TELNETCONSOLE_ENABLED <span class="token operator">=</span> <span class="token boolean">False</span>
<span class="token comment"># Telnet终端使用的端口范围。默认: [6023, 6073],如果设置为 None 或 0 ， 则使用动态分配的端口</span>
<span class="token comment"># TELNETCONSOLE_PORT</span>

<span class="token comment"># telnet账号，默认：scrapy</span>
TELNETCONSOLE_USERNAME <span class="token operator">=</span> <span class="token boolean">None</span>

<span class="token comment"># telnet密码：默认会自动生成一个密码</span>
TELNETCONSOLE_PASSWORD

<span class="token comment"># Telnet终端监听的接口(interface)。默认: '127.0.0.1'</span>
TELNETCONSOLE_HOST <span class="token operator">=</span> <span class="token string">'127.0.0.1'</span>


<span class="token comment"># AutoThrottle是限速节流算法</span>
<span class="token comment"># 让爬虫程序自适应download_delay和concurrent并发</span>
AUTOTHROTTLE_ENABLED <span class="token operator">=</span> <span class="token boolean">True</span>
<span class="token comment"># 爬虫程序启动时，开始对网站发起请求的延迟</span>
AUTOTHROTTLE_START_DELAY <span class="token operator">=</span> <span class="token number">5</span>
<span class="token comment"># 请求到响应的最大允许的延迟时间，必须大于download_delay</span>
AUTOTHROTTLE_MAX_DELAY <span class="token operator">=</span> <span class="token number">60</span>
<span class="token comment"># 并行发送到每个远程服务器的平均请求数，小于CONCURRENT_REQUESTS_PER_DOMAIN和CONCURRENT_REQUESTS_PER_IP</span>
AUTOTHROTTLE_TARGET_CONCURRENCY <span class="token operator">=</span> <span class="token number">1.0</span>
<span class="token comment"># 为每个响应启用显示限制统计信息</span>
AUTOTHROTTLE_DEBUG <span class="token operator">=</span> <span class="token boolean">False</span>

<span class="token comment"># HttpCache主要是将每次的请求和响应缓存到本地，可以离线进行处理</span>
<span class="token comment"># 配置启用HTTP Cache，默认不启用</span>
HTTPCACHE_ENABLED <span class="token operator">=</span> <span class="token boolean">True</span>
<span class="token comment"># 缓存的过期时间，0为永不过期</span>
HTTPCACHE_EXPIRATION_SECS <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># 缓存目录名称</span>
HTTPCACHE_DIR <span class="token operator">=</span> <span class="token string">'httpcache'</span>
<span class="token comment"># 设置不需要缓存的状态码请求</span>
HTTPCACHE_IGNORE_HTTP_CODES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token comment"># 此类将缓存保存到本地文件系统，还可以使用其他类保存到数据库</span>
HTTPCACHE_STORAGE <span class="token operator">=</span> <span class="token string">'scrapy.extensions.httpcache.FilesystemCacheStorage'</span>
</code></pre> 
<h2><a id="Scrapy_121"></a>Scrapy常用参数</h2> 
<pre><code class="prism language-python"><span class="token comment"># 日志输出等级</span>
<span class="token comment"># LOG_LEVEL="WARNING"</span>

<span class="token comment"># 遵循robots.txt中的爬虫规则</span>
ROBOTSTXT_OBEY <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># 需要改成Flase,要不然很多东西爬不了</span>

<span class="token comment"># 对网站并发请求总数，默认16</span>
CONCURRENT_REQUESTS <span class="token operator">=</span> <span class="token number">32</span>

<span class="token comment"># 相同网站两个请求之间的间隔时间，默认是0s。相当于time.sleep()</span>
DOWNLOAD_DELAY <span class="token operator">=</span> <span class="token number">3</span>

<span class="token comment"># 配置启用Downloader MiddleWares,用来设置UA,ip代理等</span>
<span class="token comment"># 值越小，优先级就越高</span>
DOWNLOADER_MIDDLEWARES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">'ScrapyDemo.middlewares.ScrapydemoDownloaderMiddleware'</span><span class="token punctuation">:</span> <span class="token number">543</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

<span class="token comment"># 配置启用Pipeline用来持久化数据，用来存储数据，Mysql,csv,mongodb等</span>
<span class="token comment"># 值越小，优先级就越高</span>
<span class="token comment"># 在这里就会优先运行'ScrapyDemo.pipelines.MysqlScrapydemoPipeline': 299，</span>
ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
   <span class="token string">'ScrapyDemo.pipelines.ScrapydemoPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
    <span class="token string">'ScrapyDemo.pipelines.MysqlScrapydemoPipeline'</span><span class="token punctuation">:</span> <span class="token number">299</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>其中这几个设置比较多<br> 日志一般还是要开启的，因为出错的话可以在日志中排错<br> 其次就是中间件，设置ip代理，ua<br> 最后就是管道文件的开启<br> 不开的话就不能进行文件的保存</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8af933614e73529ce30a21c75f441f8f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">MySQL数据库中的事务隔离与其他并发事务的讲解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/05668876d7b339c25159ebac5b1b0ddc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Qt之QStandardItemModel类】介绍</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>