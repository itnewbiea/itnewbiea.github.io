<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>文本处理 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="文本处理" />
<meta property="og:description" content="文本预处理 文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：
1.读入文本
2.分词
3.建立字典，将每个词映射到一个唯一的索引（index）
4.将文本从词的序列转换为索引的序列，方便输入模型
语言模型
一段自然语言文本可以看作是一个离散时间序列，给定一个长度为 T 的词的序列 w1,w2,…,wT ，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：
P(w1,w2,…,wT).
本节我们介绍基于统计的语言模型，主要是 n 元语法（ n -gram）。在后续内容中，我们将会介绍基于神经网络的语言模型。
语言模型
假设序列 w1,w2,…,wT 中的每个词是依次生成的，我们有
P(w1,w2,…,wT)=∏t=1TP(wt∣w1,…,wt−1)=P(w1)P(w2∣w1)⋯P(wT∣w1w2⋯wT−1) 例如，一段含有4个词的文本序列的概率
P(w1,w2,w3,w4)=P(w1)P(w2∣w1)P(w3∣w1,w2)P(w4∣w1,w2,w3).
语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目，词的概率可以通过该词在训练数据集中的相对词频来计算，例如， w1 的概率可以计算为：
P^(w1)=n(w1)n
其中 n(w1) 为语料库中以 w1 作为第一个词的文本的数量， n 为语料库中文本的总数量。
类似的，给定 w1 情况下， w2 的条件概率可以计算为：
P^(w2∣w1)=n(w1,w2)n(w1)
其中 n(w1,w2) 为语料库中以 w1 作为第一个词， w2 作为第二个词的文本的数量。
n元语法
序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。 n 元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面 n 个词相关，即 n 阶马尔可夫链（Markov chain of order n ），如果 n=1 ，那么有 P(w3∣w1,w2)=P(w3∣w2) 。基于 n−1 阶马尔可夫链，我们可以将语言模型改写为
P(w1,w2,…,wT)=∏t=1TP(wt∣wt−(n−1),…,wt−1)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/a8644fae08068d27c5f40ba9f2fe7d56/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-14T18:58:24+08:00" />
<meta property="article:modified_time" content="2020-02-14T18:58:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">文本处理</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>文本预处理</h3> 
<p>文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：</p> 
<p>1.读入文本<br> 2.分词<br> 3.建立字典，将每个词映射到一个唯一的索引（index）<br> 4.将文本从词的序列转换为索引的序列，方便输入模型<br> 语言模型<br> 一段自然语言文本可以看作是一个离散时间序列，给定一个长度为 T 的词的序列 w1,w2,…,wT ，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：</p> 
<p>P(w1,w2,…,wT).</p> 
<p>本节我们介绍基于统计的语言模型，主要是 n 元语法（ n -gram）。在后续内容中，我们将会介绍基于神经网络的语言模型。</p> 
<p>语言模型<br> 假设序列 w1,w2,…,wT 中的每个词是依次生成的，我们有</p> 
<pre><code> P(w1,w2,…,wT)=∏t=1TP(wt∣w1,…,wt−1)=P(w1)P(w2∣w1)⋯P(wT∣w1w2⋯wT−1)
</code></pre> 
<p>例如，一段含有4个词的文本序列的概率</p> 
<p>P(w1,w2,w3,w4)=P(w1)P(w2∣w1)P(w3∣w1,w2)P(w4∣w1,w2,w3).</p> 
<p>语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目，词的概率可以通过该词在训练数据集中的相对词频来计算，例如， w1 的概率可以计算为：</p> 
<p>P^(w1)=n(w1)n</p> 
<p>其中 n(w1) 为语料库中以 w1 作为第一个词的文本的数量， n 为语料库中文本的总数量。</p> 
<p>类似的，给定 w1 情况下， w2 的条件概率可以计算为：</p> 
<p>P^(w2∣w1)=n(w1,w2)n(w1)</p> 
<p>其中 n(w1,w2) 为语料库中以 w1 作为第一个词， w2 作为第二个词的文本的数量。</p> 
<p>n元语法<br> 序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。 n 元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面 n 个词相关，即 n 阶马尔可夫链（Markov chain of order n ），如果 n=1 ，那么有 P(w3∣w1,w2)=P(w3∣w2) 。基于 n−1 阶马尔可夫链，我们可以将语言模型改写为</p> 
<p>P(w1,w2,…,wT)=∏t=1TP(wt∣wt−(n−1),…,wt−1).</p> 
<p>以上也叫 n 元语法（ n -grams），它是基于 n−1 阶马尔可夫链的概率语言模型。例如，当 n=2 时，含有4个词的文本序列的概率就可以改写为：</p> 
<p>P(w1,w2,w3,w4)=P(w1)P(w2∣w1)P(w3∣w1,w2)P(w4∣w1,w2,w3)=P(w1)P(w2∣w1)P(w3∣w2)P(w4∣w3)</p> 
<p>当 n 分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列 w1,w2,w3,w4 在一元语法、二元语法和三元语法中的概率分别为</p> 
<pre><code>P(w1,w2,w3,w4)P(w1,w2,w3,w4)P(w1,w2,w3,w4)=P(w1)P(w2)P(w3)P(w4),=P(w1)P(w2∣w1)P(w3∣w2)P(w4∣w3),=P(w1)P(w2∣w1)P(w3∣w1,w2)P(w4∣w2,w3).
</code></pre> 
<p>当 n 较小时， n 元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当 n 较大时， n 元语法需要计算并存储大量的词频和多词相邻频率。</p> 
<p>思考： n 元语法可能有哪些缺陷？</p> 
<p>参数空间过大<br> 数据稀疏</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f85ab7265097ef2a859e4ab095155c83/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">STM32F103ZET6学习记录——在彩屏上显示RTC时钟的时间</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6aadf99eba85569f290d5ab394ee7cee/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">2020-02-15 Vue在使用extend动态挂载子组件时的双向传参问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>