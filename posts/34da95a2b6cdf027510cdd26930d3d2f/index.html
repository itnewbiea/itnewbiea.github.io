<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>java sql解析框架,Spark SQL源码剖析（一）SQL解析框架Catalyst流程概述， - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="java sql解析框架,Spark SQL源码剖析（一）SQL解析框架Catalyst流程概述，" />
<meta property="og:description" content="Spark SQL源码剖析(一)SQL解析框架Catalyst流程概述，
Spark SQL模块，主要就是处理跟SQL解析相关的一些内容，说得更通俗点就是怎么把一个SQL语句解析成Dataframe或者说RDD的任务。以Spark 2.4.3为例，Spark SQL这个大模块分为三个子模块，如下图所示
其中Catalyst可以说是Spark内部专门用来解析SQL的一个框架，在Hive中类似的框架是Calcite(将SQL解析成MapReduce任务)。Catalyst将SQL解析任务分成好几个阶段，这个在对应的论文中讲述得比较清楚，本系列很多内容也会参考论文，有兴趣阅读原论文的可以到这里看：Spark SQL: Relational Data Processing in Spark。
而Core模块其实就是Spark SQL主要解析的流程，当然这个过程中会去调用Catalyst的一些内容。这模块里面比较常用的类包括SparkSession，DataSet等。
至于hive模块，这个不用说，肯定跟hive有关的。这个模块在本系列基本不会涉及到，就不多介绍了。
值得一提的是，论文发表的时候还是在Spark1.x阶段，那个时候SQL解析成词法树用的是scala写的一个解析工具，到2.x阶段改为使用antlr4来做这部分工作(这应该算是最大的改变)。至于为什么要改，我猜是出于可读性和易用性方面的考虑，当然这个仅是个人猜测。
另外，这一系列会简单介绍一条SQL语句的处理流程，基于spark 2.4.3(sql这个模块在spark2.1后变化不大)。这一篇先从整体介绍Spark SQL出现的背景及解决问题，Dataframe API以及Catalyst的流程大概是怎么样，后面分阶段细说Catalyst的流程。
Spark SQL出现的背景及解决的问题
在最早的时候，大规模处理数据的技术是MapReduce，但这种框架执行效率太慢，进行一些关系型处理(如join)需要编写大量代码。后来hive这种框架可以让用户输入sql语句，自动进行优化并执行。
但在大型系统中，任然有两个主要问题，一个是ETL操作需要对接多个数据源。另一个是用户需要执行复杂分析，比如机器学习和图计算等。但传统的关系型处理系统中较难实现。
Spark SQL提供了两个子模块来解决这个问题，DataFrame API和Catalyst。
相比于RDD，Dataframe api提供了更加丰富的关系型api，并且能和RDD相互转换，后面Spark机器学习方面的工作重心，也从以RDD为基础的mllib转移到以Dataframe为基础的Spark ML(虽然Dataframe底层也是RDD)。
另一个就是Catalyst，通过它可以轻松为诸如机器学习之类的域添加数据源(比如json或通过case class自定义的类型)，优化规则和数据类型。
通过这两个模块，Spark SQL主要实现以下目标：
提供方便易用好的API，包括读取外部数据源，以及关系数据处理(用过的都知道)
使用已建立的DBMS技术提供高性能。
轻松支持新数据源，包括半结构化数据和外部数据库(比如MYSQL)。
图计算和机器学习方面的拓展
那下面就介绍Dataframe和Catalyst的流程，当然主要讨论的还是Catalyst。
统一API Dataframe
先来看看论文里面提供的一张图：
这张图可以说明很多，首先Spark的Dataframe API底层也是基于Spark的RDD。但与RDD不同的在于，Dataframe会持有schema(这个实在不好翻译，可以理解为数据的结构吧)，以及可以执行各种各样的关系型操作，比如Select，Filter，Join，Groupby等。从操作上来说，和pandas的Dataframe有点像(连名字都是一样的)。
同时因为是基于RDD的，所以很多RDD的特性Dataframe都能够享受到，比如说分布式计算中一致性，可靠性方面的保证，以及可以通过cache缓存数据，提高计算性能啊等等。
同时图中页展示了Dataframe可以通过JDBC链接外部数据库，通过控制台操作(spark-shell)，或者用户程序。说白了，就是Dataframe可以通过RDD转换而来，也可以通过外部数据表生成。
对了，这里顺便说一句，很多初次接触Spark SQL的童鞋可能会对Dataset和Dataframe这两个东西感到疑惑，在1.x时代它们确实有些差别，不过在spark2.x的时候，这两个API已经统一了。所以基本上Dataset和Dataframe可以看成是等价的东西。
最后还是结合代码做一下实际的展示吧，如下展示生成一个RDD，并且根据这个RDD生成对应的Dataframe，从中可以看出RDD和Dataframe的区别：
//生成RDD
scala&gt; val data = sc.parallelize(Array((1,2),(3,4)))
data: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at :24
scala&gt; data.foreach(println)
(1,2)
(3,4)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/34da95a2b6cdf027510cdd26930d3d2f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-03-13T19:43:06+08:00" />
<meta property="article:modified_time" content="2021-03-13T19:43:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">java sql解析框架,Spark SQL源码剖析（一）SQL解析框架Catalyst流程概述，</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size:16px;"> 
 <p>Spark SQL源码剖析(一)SQL解析框架Catalyst流程概述，</p> 
 <p>Spark SQL模块，主要就是处理跟SQL解析相关的一些内容，说得更通俗点就是怎么把一个SQL语句解析成Dataframe或者说RDD的任务。以Spark 2.4.3为例，Spark SQL这个大模块分为三个子模块，如下图所示</p> 
 <p align="center"><img src="https://images2.imgbox.com/89/43/x2pSYUaS_o.png" alt="8feca8d461633fc458e03a1ac271ddad.png"></p> 
 <p>其中Catalyst可以说是Spark内部专门用来解析SQL的一个框架，在Hive中类似的框架是Calcite(将SQL解析成MapReduce任务)。Catalyst将SQL解析任务分成好几个阶段，这个在对应的论文中讲述得比较清楚，本系列很多内容也会参考论文，有兴趣阅读原论文的可以到这里看：Spark SQL: Relational Data Processing in Spark。</p> 
 <p>而Core模块其实就是Spark SQL主要解析的流程，当然这个过程中会去调用Catalyst的一些内容。这模块里面比较常用的类包括SparkSession，DataSet等。</p> 
 <p>至于hive模块，这个不用说，肯定跟hive有关的。这个模块在本系列基本不会涉及到，就不多介绍了。</p> 
 <p>值得一提的是，论文发表的时候还是在Spark1.x阶段，那个时候SQL解析成词法树用的是scala写的一个解析工具，到2.x阶段改为使用antlr4来做这部分工作(这应该算是最大的改变)。至于为什么要改，我猜是出于可读性和易用性方面的考虑，当然这个仅是个人猜测。</p> 
 <p>另外，这一系列会简单介绍一条SQL语句的处理流程，基于spark 2.4.3(sql这个模块在spark2.1后变化不大)。这一篇先从整体介绍Spark SQL出现的背景及解决问题，Dataframe API以及Catalyst的流程大概是怎么样，后面分阶段细说Catalyst的流程。</p> 
 <p>Spark SQL出现的背景及解决的问题</p> 
 <p>在最早的时候，大规模处理数据的技术是MapReduce，但这种框架执行效率太慢，进行一些关系型处理(如join)需要编写大量代码。后来hive这种框架可以让用户输入sql语句，自动进行优化并执行。</p> 
 <p>但在大型系统中，任然有两个主要问题，一个是ETL操作需要对接多个数据源。另一个是用户需要执行复杂分析，比如机器学习和图计算等。但传统的关系型处理系统中较难实现。</p> 
 <p>Spark SQL提供了两个子模块来解决这个问题，DataFrame API和Catalyst。</p> 
 <p>相比于RDD，Dataframe api提供了更加丰富的关系型api，并且能和RDD相互转换，后面Spark机器学习方面的工作重心，也从以RDD为基础的mllib转移到以Dataframe为基础的Spark ML(虽然Dataframe底层也是RDD)。</p> 
 <p>另一个就是Catalyst，通过它可以轻松为诸如机器学习之类的域添加数据源(比如json或通过case class自定义的类型)，优化规则和数据类型。</p> 
 <p>通过这两个模块，Spark SQL主要实现以下目标：</p> 
 <p>提供方便易用好的API，包括读取外部数据源，以及关系数据处理(用过的都知道)</p> 
 <p>使用已建立的DBMS技术提供高性能。</p> 
 <p>轻松支持新数据源，包括半结构化数据和外部数据库(比如MYSQL)。</p> 
 <p>图计算和机器学习方面的拓展</p> 
 <p>那下面就介绍Dataframe和Catalyst的流程，当然主要讨论的还是Catalyst。</p> 
 <p>统一API Dataframe</p> 
 <p>先来看看论文里面提供的一张图：</p> 
 <p align="center"><img src="https://images2.imgbox.com/d1/d8/6pyuXFN4_o.png" alt="45a16de6e81d81f40a2df4ba39bac35e.png"></p> 
 <p>这张图可以说明很多，首先Spark的Dataframe API底层也是基于Spark的RDD。但与RDD不同的在于，Dataframe会持有schema(这个实在不好翻译，可以理解为数据的结构吧)，以及可以执行各种各样的关系型操作，比如Select，Filter，Join，Groupby等。从操作上来说，和pandas的Dataframe有点像(连名字都是一样的)。</p> 
 <p>同时因为是基于RDD的，所以很多RDD的特性Dataframe都能够享受到，比如说分布式计算中一致性，可靠性方面的保证，以及可以通过cache缓存数据，提高计算性能啊等等。</p> 
 <p>同时图中页展示了Dataframe可以通过JDBC链接外部数据库，通过控制台操作(spark-shell)，或者用户程序。说白了，就是Dataframe可以通过RDD转换而来，也可以通过外部数据表生成。</p> 
 <p>对了，这里顺便说一句，很多初次接触Spark SQL的童鞋可能会对Dataset和Dataframe这两个东西感到疑惑，在1.x时代它们确实有些差别，不过在spark2.x的时候，这两个API已经统一了。所以基本上Dataset和Dataframe可以看成是等价的东西。</p> 
 <p>最后还是结合代码做一下实际的展示吧，如下展示生成一个RDD，并且根据这个RDD生成对应的Dataframe，从中可以看出RDD和Dataframe的区别：</p> 
 <p>//生成RDD</p> 
 <p>scala&gt; val data = sc.parallelize(Array((1,2),(3,4)))</p> 
 <p>data: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at :24</p> 
 <p>scala&gt; data.foreach(println)</p> 
 <p>(1,2)</p> 
 <p>(3,4)</p> 
 <p>scala&gt; val df = data.toDF("fir","sec")</p> 
 <p>df: org.apache.spark.sql.DataFrame = [fir: int, sec: int]</p> 
 <p>scala&gt; df.show()</p> 
 <p>+---+---+</p> 
 <p>|fir|sec|</p> 
 <p>+---+---+</p> 
 <p>| 1| 2|</p> 
 <p>| 3| 4|</p> 
 <p>+---+---+</p> 
 <p>//跟RDD相比，多了schema</p> 
 <p>scala&gt; df.printSchema()</p> 
 <p>root</p> 
 <p>|-- fir: integer (nullable = false)</p> 
 <p>|-- sec: integer (nullable = false)</p> 
 <p>Catalyst流程解析</p> 
 <p>Catalyst在论文中被叫做优化器(Optimizer)，这部分是论文里面较为核心的内容，不过其实流程还是蛮好理解的，依旧贴下论文里面的图。</p> 
 <p align="center"><img src="https://images2.imgbox.com/df/a3/Kr7eO7Hq_o.png" alt="e3f77f9986fef0d7de275e8ae5c7948d.png"></p> 
 <p>主要流程大概可以分为以下几步：</p> 
 <p>提前说一下吧，上述流程多数是在org.apache.spark.sql.execution.QueryExecution这个类里面，这个贴一下简单的代码，看看就好，先不多做深究。后面的文章会详细介绍这里的内容。</p> 
 <p>class QueryExecution(val sparkSession: SparkSession, val logical: LogicalPlan) {<!-- --></p> 
 <p>......其他代码</p> 
 <p>//analyzer阶段</p> 
 <p>lazy val analyzed: LogicalPlan = {<!-- --></p> 
 <p>SparkSession.setActiveSession(sparkSession)</p> 
 <p>sparkSession.sessionState.analyzer.executeAndCheck(logical)</p> 
 <p>}</p> 
 <p>//optimizer阶段</p> 
 <p>lazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData)</p> 
 <p>//SparkPlan阶段</p> 
 <p>lazy val sparkPlan: SparkPlan = {<!-- --></p> 
 <p>SparkSession.setActiveSession(sparkSession)</p> 
 <p>// TODO: We use next(), i.e. take the first plan returned by the planner, here for now,</p> 
 <p>// but we will implement to choose the best plan.</p> 
 <p>planner.plan(ReturnAnswer(optimizedPlan)).next()</p> 
 <p>}</p> 
 <p>//prepareForExecution阶段</p> 
 <p>// executedPlan should not be used to initialize any SparkPlan. It should be</p> 
 <p>// only used for execution.</p> 
 <p>lazy val executedPlan: SparkPlan = prepareForExecution(sparkPlan)</p> 
 <p>//execute阶段</p> 
 <p>/** Internal version of the RDD. Avoids copies and has no schema */</p> 
 <p>lazy val toRdd: RDD[InternalRow] = executedPlan.execute()</p> 
 <p>......其他代码</p> 
 <p>}</p> 
 <p>值得一提的是每个阶段都使用了lazy懒加载，对这块感兴趣可以看看我之前的文章Scala函数式编程(六) 懒加载与Stream。</p> 
 <p>上述主要介绍Spark SQL模块内容，其出现的背景以及主要解决问题。而后简单介绍下Dataframe API的内容，以及Spark SQL解析SQL的内部框架Catalyst。后续主要会介绍Catalyst中各个步骤的流程，结合源码来做一些分析。</p> 
 <p>以上~</p> 
 <p>相关文章暂无相关文章</p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c2780f9bf379f4f028526c57484e6243/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android 实现搜索历史(1)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8180ab0af84205557df20edd32b2e175/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">KITTI数据集bin转化为bag</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>