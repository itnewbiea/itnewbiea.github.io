<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch之常用函数整理 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch之常用函数整理" />
<meta property="og:description" content="pytorch之常用函数整理 一、图像预处理函数1.1 torchvision.datasets.ImageFolder()函数 二、参数优化函数2.1 torch.optim.lr_scheduler.StepLR()函数2.2 optimizer.param_groups参数 三、模型保存与加载3.1 模型和模型参数保存torch.save()函数3.2 模型加载torch.load()函数3.3 模型参数加载model.load_state_dict()函数3.3.1 model.state_dict()函数和optimizer.state_dict()函数 3.4 总结模型参数保存与加载3.4.1 保存整个模型参数3.4.2 加载模型参数3.4.3 保存阶段性模型训练过程3.4.4 加载阶段性模型训练过程3.4.5 保存/加载多个模型到一个文件3.4.6 加载其他模型3.4.7 跨设备保存与加载模型3.4.7.1 GPU上保存，CPU上加载3.4.7.2 GPU上保存，GPU上加载3.4.7.3 CPU上保存，GPU上加载 四、其他函数4.1 torch.randperm函数 一、图像预处理函数 1.1 torchvision.datasets.ImageFolder()函数 ImageFolder函数假设所有的文件按文件夹保存，每个文件夹下存储同一类别的图片，文件夹名为类别名； ImageFolder(root, transform=None, target_transform=None, loader=default_loader) &#34;&#34;&#34; 参数解释： 1）root：图片存储根目录； 2）transform：对PIL Image进行的转换操作，原始图片作为输入，返回一个转换后的图片； 3）target_transform：对图片类别进行预处理的操作，输入为 target，输出对其的转换。如果不传该参数，即对 target 不做任何转换，返回的顺序索引 0,1, 2…； 4）loader：表示数据集加载方式，通常默认加载方式即可； 返回值： self.classes：用一个 list 保存类别名称； self.class_to_idx：类别对应的索引，与不做任何转换返回的 target 对应； self.imgs：保存(img-path, class) tuple的 list； &#34;&#34;&#34; from torchvision import transforms from torchvision.datasets import ImageFolder transform = transforms.Compose([ transforms.RandomResizedCrop(224), transforms." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/f4f8fff8b16a7dc77f620e8cb76885ef/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-13T16:17:23+08:00" />
<meta property="article:modified_time" content="2022-07-13T16:17:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch之常用函数整理</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>pytorch之常用函数整理</h4> 
 <ul><li><a href="#_2" rel="nofollow">一、图像预处理函数</a></li><li><ul><li><a href="#11_torchvisiondatasetsImageFolder_3" rel="nofollow">1.1 torchvision.datasets.ImageFolder()函数</a></li></ul> 
  </li><li><a href="#_50" rel="nofollow">二、参数优化函数</a></li><li><ul><li><a href="#21_torchoptimlr_schedulerStepLR_51" rel="nofollow">2.1 torch.optim.lr_scheduler.StepLR()函数</a></li><li><a href="#22_optimizerparam_groups_58" rel="nofollow">2.2 optimizer.param_groups参数</a></li></ul> 
  </li><li><a href="#_83" rel="nofollow">三、模型保存与加载</a></li><li><ul><li><a href="#31_torchsave_86" rel="nofollow">3.1 模型和模型参数保存torch.save()函数</a></li><li><a href="#32_torchload_102" rel="nofollow">3.2 模型加载torch.load()函数</a></li><li><a href="#33_modelload_state_dict_132" rel="nofollow">3.3 模型参数加载model.load_state_dict()函数</a></li><li><ul><li><a href="#331_modelstate_dictoptimizerstate_dict_144" rel="nofollow">3.3.1 model.state_dict()函数和optimizer.state_dict()函数</a></li></ul> 
   </li><li><a href="#34__238" rel="nofollow">3.4 总结模型参数保存与加载</a></li><li><ul><li><a href="#341__239" rel="nofollow">3.4.1 保存整个模型参数</a></li><li><a href="#342__243" rel="nofollow">3.4.2 加载模型参数</a></li><li><a href="#343__253" rel="nofollow">3.4.3 保存阶段性模型训练过程</a></li><li><a href="#344__263" rel="nofollow">3.4.4 加载阶段性模型训练过程</a></li><li><a href="#345__284" rel="nofollow">3.4.5 保存/加载多个模型到一个文件</a></li><li><a href="#346__315" rel="nofollow">3.4.6 加载其他模型</a></li><li><a href="#347__325" rel="nofollow">3.4.7 跨设备保存与加载模型</a></li><li><ul><li><a href="#3471_GPUCPU_326" rel="nofollow">3.4.7.1 GPU上保存，CPU上加载</a></li><li><a href="#3472_GPUGPU_353" rel="nofollow">3.4.7.2 GPU上保存，GPU上加载</a></li><li><a href="#3473_CPUGPU_364" rel="nofollow">3.4.7.3 CPU上保存，GPU上加载</a></li></ul> 
   </li></ul> 
  </li></ul> 
  </li><li><a href="#_378" rel="nofollow">四、其他函数</a></li><li><ul><li><a href="#41_torchrandperm_379" rel="nofollow">4.1 torch.randperm函数</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_2"></a>一、图像预处理函数</h2> 
<h3><a id="11_torchvisiondatasetsImageFolder_3"></a>1.1 torchvision.datasets.ImageFolder()函数</h3> 
<ul><li>ImageFolder函数假设所有的文件按文件夹保存，每个文件夹下存储同一类别的图片，文件夹名为类别名；</li></ul> 
<pre><code class="prism language-python">ImageFolder<span class="token punctuation">(</span>root<span class="token punctuation">,</span> transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> target_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> loader<span class="token operator">=</span>default_loader<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
参数解释：
1）root：图片存储根目录；
2）transform：对PIL Image进行的转换操作，原始图片作为输入，返回一个转换后的图片；
3）target_transform：对图片类别进行预处理的操作，输入为 target，输出对其的转换。如果不传该参数，即对 target 不做任何转换，返回的顺序索引 0,1, 2…；
4）loader：表示数据集加载方式，通常默认加载方式即可；
返回值：
self.classes：用一个 list 保存类别名称；
self.class_to_idx：类别对应的索引，与不做任何转换返回的 target 对应；
self.imgs：保存(img-path, class) tuple的 list；
"""</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/87/09/MSPiILqf_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> ImageFolder
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
     transforms<span class="token punctuation">.</span>RandomResizedCrop<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
     transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
     transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
     transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
dataset <span class="token operator">=</span> ImageFolder<span class="token punctuation">(</span><span class="token string">'data1/dogcat_2/'</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>

<span class="token comment"># 深度学习中图片数据一般保存成CxHxW，即通道数x图片高x图片宽</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>classes<span class="token punctuation">)</span>  <span class="token comment">#根据分的文件夹的名字来确定的类别</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>class_to_idx<span class="token punctuation">)</span> <span class="token comment">#按顺序为这些类别定义索引为0,1...</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>imgs<span class="token punctuation">)</span> <span class="token comment">#返回从所有文件夹中得到的图片的路径以及其类别</span>
<span class="token triple-quoted-string string">'''
输出：
['cat', 'dog']
{'cat': 0, 'dog': 1}
[('./data/train\\cat\\cat.12484.jpg', 0), 
 ('./data/train\\cat\\cat.12485.jpg', 0), 
 ('./data/train\\cat\\cat.12486.jpg', 0), 
 ('./data/train\\cat\\cat.12487.jpg', 0), 
 ('./data/train\\dog\\dog.12496.jpg', 1), 
 ('./data/train\\dog\\dog.12497.jpg', 1)，
 ('./data/train\\dog\\dog.12498.jpg', 1), 
 ('./data/train\\dog\\dog.12499.jpg', 1)]
'''</span>
</code></pre> 
<h2><a id="_50"></a>二、参数优化函数</h2> 
<h3><a id="21_torchoptimlr_schedulerStepLR_51"></a>2.1 torch.optim.lr_scheduler.StepLR()函数</h3> 
<ul><li>学习率衰减函数：每训练7个epoch，学习率衰减为原来的1/10</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch <span class="token keyword">import</span> optim
optimizer_ft<span class="token operator">=</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>params_to_update<span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">1e-2</span><span class="token punctuation">)</span>
scheduler<span class="token operator">=</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer_ft<span class="token punctuation">,</span>step_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token comment">#学习率衰减，每训练7个epoch，学习率衰减为原来的1/10</span>
</code></pre> 
<h3><a id="22_optimizerparam_groups_58"></a>2.2 optimizer.param_groups参数</h3> 
<ul><li>optimizer.param_groups： 是长度为1的list，其中的元素是1个字典；<br> 1）optimizer.param_groups[0]： 长度为7的字典，包括[‘amsgrad’, ‘params’, ‘lr’, ‘betas’, ‘weight_decay’, ‘eps’, ‘maximize’]这7个参数；</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
w1<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span>w1<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#结果</span>
<span class="token triple-quoted-string string">"""
[{'params': [tensor([[ 1.0950,  0.2128,  0.1464],
        [ 0.0240, -0.4230, -0.3268],
        [ 0.4877, -0.2145,  0.5996]], requires_grad=True)], 
  'lr': 0.001,
  'betas': (0.9, 0.999), 
  'eps': 1e-08, 
  'weight_decay': 0, 
  'amsgrad': False, 
  'maximize': False}]    
0.001
"""</span>
</code></pre> 
<h2><a id="_83"></a>三、模型保存与加载</h2> 
<ul><li>pytorch保存和加载模型后缀为：.pt和.pth</li></ul> 
<h3><a id="31_torchsave_86"></a>3.1 模型和模型参数保存torch.save()函数</h3> 
<ul><li><strong>torch.save：</strong> 保存一个序列化（serialized）的目标到磁盘。函数使用了Python的pickle程序用于序列化。模型（models），张量（tensors）和文件夹（dictionaries）都是可以用这个函数保存的目标类型。</li></ul> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">"""
torch.save(obj, f, pickle_module=&lt;module '...'&gt;, pickle_protocol=2)
参数：
obj：保存对象
f：类文件对象（必须实现写和刷新）或一个保存文件名的字符串
pickle_module：用于pickling源数据和对象模块
pickle_protocol：指定pickle_protocol可以覆盖默认参数
"""</span>
<span class="token comment">#保存整个模型</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span><span class="token string">'save.pt'</span><span class="token punctuation">)</span>
<span class="token comment">#只保存训练好的权重</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'save.pt'</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="32_torchload_102"></a>3.2 模型加载torch.load()函数</h3> 
<ul><li><strong>torch.load：</strong> 用来加载模型。torch.load() 使用 Python 的 解压工具（unpickling）来反序列化 pickled object 到对应存储设备上。首先在 CPU 上对压缩对象进行反序列化并且移动到它们保存的存储设备上，如果失败了（如：由于系统中没有相应的存储设备），就会抛出一个异常。用户可以通过 register_package 进行扩展，使用自己定义的标记和反序列化方法。</li></ul> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">"""
torch.load(f, map_location=None, pickle_module=&lt;module 'pickle' from '...'&gt;)
参数：
f：类文件对象（返回文件描述符）或一个保存文件名的字符串
map_location：一个函数或字典规定如何映射存储设备
pickle_module：用于unpickling元数据和对象的模块（必须匹配序列化文件时的pickle_module）
"""</span>
<span class="token comment">#加载模型</span>
torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'tensors.pt'</span><span class="token punctuation">)</span>
 
<span class="token comment"># 加载模型到CPU上 Load all tensors onto the CPU</span>
torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'tensors.pt'</span><span class="token punctuation">,</span> map_location<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token comment">#  加载模型到CPU上 Load all tensors onto the CPU, using a function</span>
torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'tensors.pt'</span><span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token keyword">lambda</span> storage<span class="token punctuation">,</span> loc<span class="token punctuation">:</span> storage<span class="token punctuation">)</span>
 
<span class="token comment"># 加载模型到第1个GPU上 Load all tensors onto GPU 1</span>
torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'tensors.pt'</span><span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token keyword">lambda</span> storage<span class="token punctuation">,</span> loc<span class="token punctuation">:</span> storage<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token comment"># 将模型从GPU1映射到GPU0上 Map tensors from GPU 1 to GPU 0</span>
torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'tensors.pt'</span><span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'cuda:1'</span><span class="token punctuation">:</span><span class="token string">'cuda:0'</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
 
<span class="token comment"># Load tensor from io.BytesIO object</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'tensor.pt'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    <span class="token builtin">buffer</span> <span class="token operator">=</span> io<span class="token punctuation">.</span>BytesIO<span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token builtin">buffer</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="33_modelload_state_dict_132"></a>3.3 模型参数加载model.load_state_dict()函数</h3> 
<ul><li><strong>model.load_state_dict</strong>：使用状态字典state_dict反序列化模型参数字典，用来加载模型参数。将state_dict中的parameters和buffers复制到model及其子节点中。</li></ul> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
state_dict：保存parameters和persistent buffers的字典
strict：可选参数，bool型。state_dict中的key是否和model.state_dict()返回的key一样。
"""</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span><span class="token string">'save.pt'</span><span class="token punctuation">)</span>
<span class="token comment">#model.load_state_dict()函数把加载的权重复制到模型的权重中去</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"save.pt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
</code></pre> 
<h4><a id="331_modelstate_dictoptimizerstate_dict_144"></a>3.3.1 model.state_dict()函数和optimizer.state_dict()函数</h4> 
<ul><li>state_dict其实是pytorch中模型的可学习参数（如weight和bias）python字典，模型的参数可通过model.parameters()获取。只有包含了可学参数层（卷积层、池化层）和已注册的命令（registered buffers，比如batchnorm的running_mean）才会进入state_dict中，优化目标torch.optim也有state_dict，其中包含的是优化器状态信息和使用到的超参数。</li><li>model.state_dict()函数</li></ul> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">"""
model.state_dict()
返回一个包含模型状态信息的字典。包含参数（weighs and biases）和持续的缓冲值（如：观测值的平均值）。
只有具有可更新参数的层才会被保存在模型的 state_dict 数据结构中
"""</span>
model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#结果</span>
<span class="token comment"># ['bias', 'weight']</span>
</code></pre> 
<ul><li>torch.optim.Optimizer.state_dict()函数</li></ul> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">"""
torch.optim.Optimizer.state_dict()
返回一个包含优化器状态信息的字典。包含两个 key：
state：字典，保存当前优化器的状态信息。不同优化器内容不同。
param_groups：字典，包含所有参数组（eg：超参数）。
"""</span>
<span class="token comment"># Author: Liuxin</span>
<span class="token comment"># Time: 2022/5/18</span>
<span class="token keyword">from</span> __future__ <span class="token keyword">import</span> print_function<span class="token punctuation">,</span> division

<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> lr_scheduler
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torchvision
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> models<span class="token punctuation">,</span> transforms
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> time
<span class="token keyword">import</span> os
<span class="token keyword">import</span> copy

<span class="token comment"># 定义模型</span>
<span class="token keyword">class</span> <span class="token class-name">TheModelClass</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TheModelClass<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


<span class="token comment"># 初始化模型</span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 初始化优化器</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

<span class="token comment"># 打印模型的 state_dict</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Model's state_dict:"</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> param_tensor <span class="token keyword">in</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>param_tensor<span class="token punctuation">,</span> <span class="token string">"\t"</span><span class="token punctuation">,</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>param_tensor<span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 打印优化器的 state_dict</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Optimizer's state_dict:"</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> var_name <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>var_name<span class="token punctuation">,</span> <span class="token string">"\t"</span><span class="token punctuation">,</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>var_name<span class="token punctuation">]</span><span class="token punctuation">)</span>
    
<span class="token comment">#结果</span>
Model's state_dict<span class="token punctuation">:</span>
conv1<span class="token punctuation">.</span>weight 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
conv1<span class="token punctuation">.</span>bias 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
conv2<span class="token punctuation">.</span>weight 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
conv2<span class="token punctuation">.</span>bias 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc1<span class="token punctuation">.</span>weight 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">400</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc1<span class="token punctuation">.</span>bias 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">120</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc2<span class="token punctuation">.</span>weight 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc2<span class="token punctuation">.</span>bias 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">84</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc3<span class="token punctuation">.</span>weight 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
fc3<span class="token punctuation">.</span>bias 	 torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

odict_keys<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'conv1.weight'</span><span class="token punctuation">,</span> <span class="token string">'conv1.bias'</span><span class="token punctuation">,</span> <span class="token string">'conv2.weight'</span><span class="token punctuation">,</span> <span class="token string">'conv2.bias'</span><span class="token punctuation">,</span> <span class="token string">'fc1.weight'</span><span class="token punctuation">,</span> <span class="token string">'fc1.bias'</span><span class="token punctuation">,</span> <span class="token string">'fc2.weight'</span><span class="token punctuation">,</span> <span class="token string">'fc2.bias'</span><span class="token punctuation">,</span> <span class="token string">'fc3.weight'</span><span class="token punctuation">,</span> <span class="token string">'fc3.bias'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

Optimizer's state_dict<span class="token punctuation">:</span>
state 	 <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
param_groups 	 <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token string">'maximize'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre> 
<h3><a id="34__238"></a>3.4 总结模型参数保存与加载</h3> 
<h4><a id="341__239"></a>3.4.1 保存整个模型参数</h4> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="342__243"></a>3.4.2 加载模型参数</h4> 
<pre><code class="prism language-python">model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>保存训练过程时，只需保存模型训练好的参数，使用torch.save()保存state_dict，能够方便模型的加载。因此推荐使用这种方式进行模型保存。</li><li>模型参数加载好后，要使用model.eval()来固定dropout和归一化层，否则每次预测结果会不同。</li><li>注意，load_state_dict()需要传入字典对象，因此需要先反序列化state_dict再传入load_state_dict()。</li></ul> 
<h4><a id="343__253"></a>3.4.3 保存阶段性模型训练过程</h4> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
            <span class="token string">'epoch'</span><span class="token punctuation">:</span> epoch<span class="token punctuation">,</span>
            <span class="token string">'model_state_dict'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'optimizer_state_dict'</span><span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'loss'</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span>
            <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="344__263"></a>3.4.4 加载阶段性模型训练过程</h4> 
<pre><code class="prism language-python">model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> TheOptimizerClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
            <span class="token string">'epoch'</span><span class="token punctuation">:</span> epoch<span class="token punctuation">,</span>
            <span class="token string">'model_state_dict'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'optimizer_state_dict'</span><span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'loss'</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span>
            <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'model_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
epoch <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span>
loss <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>

model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># - 或者 -</span>
model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="345__284"></a>3.4.5 保存/加载多个模型到一个文件</h4> 
<ul><li>保存</li></ul> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
            <span class="token string">'modelA_state_dict'</span><span class="token punctuation">:</span> modelA<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'modelB_state_dict'</span><span class="token punctuation">:</span> modelB<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'optimizerA_state_dict'</span><span class="token punctuation">:</span> optimizerA<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'optimizerB_state_dict'</span><span class="token punctuation">:</span> optimizerB<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
</code></pre> 
<ul><li>加载</li></ul> 
<pre><code class="prism language-python">modelA <span class="token operator">=</span> TheModelAClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
modelB <span class="token operator">=</span> TheModelBClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
optimizerA <span class="token operator">=</span> TheOptimizerAClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
optimizerB <span class="token operator">=</span> TheOptimizerBClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
 
checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>
modelA<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'modelA_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
modelB<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'modelB_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
optimizerA<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizerA_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
optimizerB<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizerB_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
 
modelA<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
modelB<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># - 或者 -</span>
modelA<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
modelB<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="346__315"></a>3.4.6 加载其他模型</h4> 
<ul><li>保存</li></ul> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>modelA<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
</code></pre> 
<ul><li>加载</li></ul> 
<pre><code class="prism language-python">modelB <span class="token operator">=</span> TheModelBClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
modelB<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="347__325"></a>3.4.7 跨设备保存与加载模型</h4> 
<h5><a id="3471_GPUCPU_326"></a>3.4.7.1 GPU上保存，CPU上加载</h5> 
<ul><li>当在CPU上加载一个GPU上训练的模型时，在torch.load()中指定map_location=torch.device(‘cpu’)，此时，map_location动态地将tensors的底层存储重新映射到CPU设备上。</li></ul> 
<pre><code class="prism language-python"><span class="token comment">#保存</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
<span class="token comment">#加载</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">,</span> map_location<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment">#上述代码只有在模型是在一块GPU上训练时才有效，如果模型在多个GPU上训练，那么在CPU上加载时，会得到类似如下错误：</span>

<span class="token comment">#KeyError: ‘unexpected key “module.conv1.weight” in state_dict’</span>
<span class="token comment">#原因是在使用多GPU训练并保存模型时，模型的参数名都带上了module前缀，因此可以在加载模型时，把key中的这个前缀去掉：</span>

<span class="token comment"># 原始通过DataParallel保存的文件</span>
state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'myfile.pth.tar'</span><span class="token punctuation">)</span>
<span class="token comment"># 创建一个不包含`module.`的新OrderedDict</span>
<span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict
new_state_dict <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> state_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> k<span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 去掉 `module.`</span>
    new_state_dict<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> v
<span class="token comment"># 加载参数</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>new_state_dict<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="3472_GPUGPU_353"></a>3.4.7.2 GPU上保存，GPU上加载</h5> 
<ul><li>在把GPU上训练的模型加载到GPU上时，只需要使用model.to(torch.device(‘cuda’))将初始化的模型转换为CUDA优化模型。同时确保在模型所有的输入上使用.to(torch.device(‘cuda’))。注意，调用my_tensor.to(device)会返回一份在GPU上的my_tensor的拷贝。不会覆盖原本的my_tensor，因此要记得手动将tensor重写：my_tensor = my_tensor.to(torch.device(‘cuda’))。</li></ul> 
<pre><code class="prism language-python"><span class="token comment">#保存</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
<span class="token comment">#加载</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 选择希望使用的GPU</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="3473_CPUGPU_364"></a>3.4.7.3 CPU上保存，GPU上加载</h5> 
<ul><li>在 GPU 上加载 CPU 训练保存的模型时，将 torch.load() 函数的 map_location 参数 设置为 cuda:device_id。这种方式将模型加载到指定设备。下一步，确保调用 model.to(torch.device(‘cuda’)) 将模型参数 tensor 转换为 cuda tensor。最后，确保模型输入使用 .to(torch.device(‘cuda’)) 为 cuda 优化模型准备数据。<br> 注意：调用 my_tensor.to(device) 会在 GPU 上返回 my_tensor 的新副本，不会覆盖 my_tensor。因此，使用 my_tensor = my_tensor.to(torch.device(‘cuda’)) 手动覆盖。</li></ul> 
<pre><code class="prism language-python"><span class="token comment">#保存</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
<span class="token comment">#加载</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># Choose whatever GPU device number you want</span>
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment"># Make sure to call input = input.to(device) on any input tensors that you feed to the model</span>
</code></pre> 
<p>参考网址：<a href="https://zhuanlan.zhihu.com/p/505487325" rel="nofollow">https://zhuanlan.zhihu.com/p/505487325</a></p> 
<h2><a id="_378"></a>四、其他函数</h2> 
<h3><a id="41_torchrandperm_379"></a>4.1 torch.randperm函数</h3> 
<ul><li>torch.randperm(n)：将0~n-1（包括0和n-1）随机打乱后获得的数字序列，函数名是random permutation缩写</li></ul> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token comment"># 结果：</span>
<span class="token comment">#tensor([2, 3, 6, 7, 8, 9, 1, 5, 0, 4])</span>
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1585d1e73e2b4f8741c6cce924958f7c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CentOS7通过yum安装httpd和php8.0</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f641c4a366fd95354bd5139c93dd6d11/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">TCP/IP协议学习</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>