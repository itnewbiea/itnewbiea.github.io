<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据采集工具Flume - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大数据采集工具Flume" />
<meta property="og:description" content="Flume 一.概述1.基础架构2.拓扑结构3.Agent内部原理 二.安装Flume三.入门案例1.监控端口2.实时监控单个追加文件3.读取目录新文件到HDFS4.实时监控多个追加文件 四.进阶案例1.Chanel Selector1.1 replicating1.2 multiplexing 2.Sink Processors2.1 failoversinkprocessor2.2 load_balance 3.聚合 五.Flume数据流监控 一.概述 Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单
Flume最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写入到HDFS或者Kafka，最主要的，flume是实时采集的
1.基础架构 Agent Agent 是一个 JVM 进程，主要有 3 个部分组成，Source、Channel、Sink，它以事件的形式将数据从源头送至目的
Source Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、sequence generator、syslog、http、legacy
doPut:将数据写进临时缓冲区putList
doCommit:检查channel内存队列是否足够合并
doRollback:内存队列不足，回滚数据
Channel Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个Sink 的读取操作
Flume 自带两种 Channel：Memory Channel 和 File Channel
Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/45838ba21b322f391d36137694e03c17/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-29T10:53:13+08:00" />
<meta property="article:modified_time" content="2022-07-29T10:53:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据采集工具Flume</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>Flume</h4> 
 <ul><li><ul><li><ul><li><a href="#_1" rel="nofollow">一.概述</a></li><li><ul><li><a href="#1_8" rel="nofollow">1.基础架构</a></li><li><a href="#2_44" rel="nofollow">2.拓扑结构</a></li><li><a href="#3Agent_70" rel="nofollow">3.Agent内部原理</a></li></ul> 
    </li><li><a href="#Flume_77" rel="nofollow">二.安装Flume</a></li><li><a href="#_103" rel="nofollow">三.入门案例</a></li><li><ul><li><a href="#1_104" rel="nofollow">1.监控端口</a></li><li><a href="#2_189" rel="nofollow">2.实时监控单个追加文件</a></li><li><a href="#3HDFS_297" rel="nofollow">3.读取目录新文件到HDFS</a></li><li><a href="#4_392" rel="nofollow">4.实时监控多个追加文件</a></li></ul> 
    </li><li><a href="#_481" rel="nofollow">四.进阶案例</a></li><li><ul><li><a href="#1Chanel_Selector_482" rel="nofollow">1.Chanel Selector</a></li><li><ul><li><a href="#11_replicating_483" rel="nofollow">1.1 replicating</a></li><li><a href="#12_multiplexing_637" rel="nofollow">1.2 multiplexing</a></li></ul> 
     </li><li><a href="#2Sink_Processors_766" rel="nofollow">2.Sink Processors</a></li><li><ul><li><a href="#21_failoversinkprocessor_772" rel="nofollow">2.1 failoversinkprocessor</a></li><li><a href="#22_load_balance_882" rel="nofollow">2.2 load_balance</a></li></ul> 
     </li><li><a href="#3_907" rel="nofollow">3.聚合</a></li></ul> 
    </li><li><a href="#Flume_1055" rel="nofollow">五.Flume数据流监控</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h4><a id="_1"></a>一.概述</h4> 
<p>Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量<code>日志采集、聚合和传输的系统</code>。Flume 基于流式架构，灵活简单</p> 
<p>Flume最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写入到HDFS或者Kafka，最主要的，flume是<code>实时采集的</code><br> <img src="https://images2.imgbox.com/0e/e6/BJDSiXYS_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="1_8"></a>1.基础架构</h5> 
<p><img src="https://images2.imgbox.com/26/03/SX9TQQ4Q_o.png" alt="在这里插入图片描述"></p> 
<ul><li>Agent</li></ul> 
<p>Agent 是一个 JVM 进程，主要有 3 个部分组成，Source、Channel、Sink，它以<code>事件</code>的形式将数据从源头送至目的</p> 
<ul><li>Source</li></ul> 
<p>Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、sequence generator、syslog、http、legacy</p> 
<p>doPut:将数据写进临时缓冲区putList<br> doCommit:检查channel内存队列是否足够合并<br> doRollback:内存队列不足，回滚数据</p> 
<ul><li>Channel</li></ul> 
<p>Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个Sink 的读取操作</p> 
<p>Flume 自带两种 Channel：Memory Channel 和 File Channel</p> 
<p>Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据</p> 
<ul><li>Sink</li></ul> 
<p>Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent，Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义</p> 
<p>doTake:先将数据取到临时缓冲区takeList<br> doCommit:提交到Sink，成功提交清空takeList<br> doRollback:发送出现异常，将takeList数据归还给内存队列</p> 
<ul><li>Event</li></ul> 
<p>传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构，Body 用来存放该条数据，形式为字节数组<br> <img src="https://images2.imgbox.com/8d/61/CKNYPMAC_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="2_44"></a>2.拓扑结构</h5> 
<p><strong>简单串联</strong></p> 
<p>这种模式是将多个 flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量， flume 数量过多不仅会影响传输速率，而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统</p> 
<p><img src="https://images2.imgbox.com/40/be/WSpuOzIj_o.png" alt="在这里插入图片描述"></p> 
<p><strong>复制和多路复用</strong></p> 
<p>单 source，多 channel、sink，Flume 支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地</p> 
<p><img src="https://images2.imgbox.com/60/dc/03scYvuh_o.png" alt="在这里插入图片描述"></p> 
<p><strong>负载均衡和故障转移</strong></p> 
<p>Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能</p> 
<p><img src="https://images2.imgbox.com/b2/6c/DdxEwrs1_o.png" alt="在这里插入图片描述"></p> 
<p><strong>聚合</strong></p> 
<p>这种模式是我们最常见的，也非常实用，日常 web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用 flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个 flume 采集日志，传送到一个集中收集日志</p> 
<p><img src="https://images2.imgbox.com/05/91/Jzna4HuH_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="3Agent_70"></a>3.Agent内部原理</h5> 
<p><img src="https://images2.imgbox.com/5f/45/N2XuCMP2_o.png" alt="在这里插入图片描述"></p> 
<p>注意，一个sink最多对应一个channel，而一个channel可以对应更多sink</p> 
<p>Sinkprocessor有三种，第一种defaultsinkprocessor，一个channel只能绑定一个sink，第二种loadbalancingsinkprocessor，负载均衡，多个sink轮流查看channel，第三种failoversinkprocessor，故障转移，对多个sink可以配置优先级，若优先级高的sink挂掉后，转移到优先级低的sink</p> 
<h4><a id="Flume_77"></a>二.安装Flume</h4> 
<p>官网下载安装包</p> 
<p>http://archive.apache.org/dist/flume/1.9.0/</p> 
<p><img src="https://images2.imgbox.com/9e/d7/7suPBEMD_o.png" alt="在这里插入图片描述"></p> 
<p>解压</p> 
<pre><code class="prism language-bash"><span class="token function">tar</span> -zxvf ./apache-flume-1.9.0-bin.tar.gz -C <span class="token builtin class-name">.</span>
</code></pre> 
<p>改名</p> 
<pre><code class="prism language-bash"><span class="token function">mv</span> ./apache-flume-1.9.0-bin/ ./flume-1.9
</code></pre> 
<p>将 lib 文件夹下的 guava-11.0.2.jar 删除以兼容 Hadoop 3.1.3</p> 
<pre><code class="prism language-bash"> <span class="token function">rm</span> /opt/module/flume-1.9/lib/guava-11.0.2.jar
</code></pre> 
<h4><a id="_103"></a>三.入门案例</h4> 
<h5><a id="1_104"></a>1.监控端口</h5> 
<p><strong>需求</strong></p> 
<p>使用 Flume 监听一个端口，收集该端口数据，并打印到控制台</p> 
<p><img src="https://images2.imgbox.com/50/1a/JhLFQeGF_o.png" alt="在这里插入图片描述"></p> 
<p><strong>实现步骤</strong></p> 
<p>（1）安装 netcat 工具</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> yum <span class="token function">install</span> -y <span class="token function">nc</span>
</code></pre> 
<p>（2）判断 44444 端口是否被占用</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">netstat</span> -nlp <span class="token operator">|</span> <span class="token function">grep</span> <span class="token number">44444</span>
</code></pre> 
<p>（3）创建 Flume Agent 配置文件 flume-netcat-logger.conf<br> （4）在 flume 目录下创建 job 文件夹并进入 job 文件夹<br> （5）在 job 文件夹下创建 Flume Agent 配置文件 flume-netcat-logger.conf</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> job
<span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> job/
<span class="token punctuation">[</span>gzhu@hadoop102 job<span class="token punctuation">]</span>$ <span class="token function">vim</span> flume-netcat-logger.conf
</code></pre> 
<p>（6）在 flume-netcat-logger.conf 文件中添加如下内容</p> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
<span class="token comment"># a1就是表示当前agent，单台机器里flume下a1要唯一</span>
a1.sources <span class="token operator">=</span> r1 <span class="token comment"># r1表示a1的输入源</span>
a1.sinks <span class="token operator">=</span> k1 <span class="token comment"># k1表示a1的输出目的地</span>
a1.channels <span class="token operator">=</span> c1 <span class="token comment"># c1表示a1的缓冲区channel</span>

<span class="token comment"># Describe/configure the source</span>
a1.sources.r1.type <span class="token operator">=</span> netcat  <span class="token comment"># source的类型</span>
a1.sources.r1.bind <span class="token operator">=</span> localhost
a1.sources.r1.port <span class="token operator">=</span> <span class="token number">44444</span>

<span class="token comment"># Describe the sink sink的类型 logger打印到控制台</span>
a1.sinks.k1.type <span class="token operator">=</span> logger

<span class="token comment"># Use a channel which buffers events in memory</span>
a1.channels.c1.type <span class="token operator">=</span> memory
a1.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span> <span class="token comment"># channel最多放1000个event</span>
a1.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span> <span class="token comment"># 一个事务的做多放100个event</span>

<span class="token comment"># Bind the source and sink to the channel</span>
a1.sources.r1.channels <span class="token operator">=</span> c1 <span class="token comment"># 一个source可以绑定多个channel</span>
a1.sinks.k1.channel <span class="token operator">=</span> c1 <span class="token comment"># 一个sink最多绑定一个channel</span>
</code></pre> 
<p>（7）先开启 flume 监听端口</p> 
<pre><code class="prism language-bash">bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger<span class="token operator">=</span>INFO,console
</code></pre> 
<p>参数说明：<br> -c：表示配置文件存储在 conf/目录<br> -n：表示给 agent 起名为 a1<br> -f：flume 本次启动读取的配置文件是在 job 文件夹下的 flume-telnet.conf文件<br> -Dflume.root.logger=INFO,console ：-D 表示 flume 运行时动态修改 flume.root.logger参数属性值，并将控制台日志打印级别设置为 INFO 级别。日志级别包括:log、info、warn、error</p> 
<p>（8）使用 netcat 工具向本机的 44444 端口发送内容</p> 
<pre><code class="prism language-bash"> <span class="token function">nc</span> localhost <span class="token number">44444</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/66/c9/W2KpOl7O_o.png" alt="在这里插入图片描述"></p> 
<p>（9）在 Flume 监听页面观察接收数据情况</p> 
<p><img src="https://images2.imgbox.com/b8/0e/4j09jUwx_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="2_189"></a>2.实时监控单个追加文件</h5> 
<p><strong>需求</strong></p> 
<p>实时监控日志，并上传到 HDFS 中<br> <img src="https://images2.imgbox.com/22/48/TvaT7ayY_o.png" alt="在这里插入图片描述"></p> 
<p><strong>实现步骤</strong></p> 
<p>（1）Flume 要想将数据输出到 HDFS，依赖 Hadoop 相关 jar 包，上传到/opt/module/flume-1.9/lib，以下jar包可在Hadoop的依赖下找到</p> 
<p><img src="https://images2.imgbox.com/b1/35/HFX2warT_o.png" alt="在这里插入图片描述"></p> 
<p><code>实现source是exec，代表执行一条命令，sink就是HDFS了</code></p> 
<p>（2）检查/etc/profile.d/my_env.sh 文件，确认 Hadoop 和 Java 环境变量配置正确</p> 
<pre><code class="prism language-bash"><span class="token comment">#JAVA_HOME</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/opt/module/jdk1.8.0_212
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$JAVA_HOME</span>/bin

<span class="token comment">#HADOOP_HOME</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_HOME</span><span class="token operator">=</span>/opt/module/hadoop-3.1.3
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$HADOOP_HOME</span>/bin
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$HADOOP_HOME</span>/sbin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_CONF_DIR</span><span class="token operator">=</span><span class="token variable">${HADOOP_HOME}</span>/etc/hadoop
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_CLASSPATH</span><span class="token operator">=</span><span class="token variable"><span class="token variable">`</span>hadoop classpath<span class="token variable">`</span></span>
</code></pre> 
<p>（2）创建 flume-file-hdfs.conf 文件</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 job<span class="token punctuation">]</span>$ <span class="token function">vim</span> flume-file-hdfs.conf
</code></pre> 
<p>注：要想读取 Linux 系统中的文件，就得按照 Linux 命令的规则执行命令。由于 Hive日志在 Linux 系统中所以读取文件的类型选择：exec 即 execute 执行的意思。表示执行Linux 命令来读取文件</p> 
<p>添加如下内容</p> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a2.sources <span class="token operator">=</span> r2
a2.sinks <span class="token operator">=</span> k2
a2.channels <span class="token operator">=</span> c2

<span class="token comment"># Describe/configure the source</span>
a2.sources.r2.type <span class="token operator">=</span> <span class="token builtin class-name">exec</span> <span class="token comment"># 执行命令 tail -f 循环读取日志文件 默认最后10行 可能会重复数据</span>
a2.sources.r2.command <span class="token operator">=</span> <span class="token function">tail</span> -F /opt/module/flume-1.9/exec.log

<span class="token comment"># Describe the sink</span>
a2.sinks.k2.type <span class="token operator">=</span> hdfs
a2.sinks.k2.hdfs.path <span class="token operator">=</span> hdfs://hadoop102:8020/flume/%Y%m%d/%H

<span class="token comment">#上传文件的前缀</span>
a2.sinks.k2.hdfs.filePrefix <span class="token operator">=</span> logs

<span class="token comment">#是否按照时间滚动文件夹</span>
a2.sinks.k2.hdfs.round <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#多少时间单位创建一个新的文件夹   1小时1次</span>
a2.sinks.k2.hdfs.roundValue <span class="token operator">=</span> <span class="token number">1</span>
<span class="token comment">#重新定义时间单位</span>
a2.sinks.k2.hdfs.roundUnit <span class="token operator">=</span> hour
<span class="token comment">#是否使用本地时间戳</span>
a2.sinks.k2.hdfs.useLocalTimeStamp <span class="token operator">=</span> <span class="token boolean">true</span>


<span class="token comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span>
a2.sinks.k2.hdfs.batchSize <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment">#设置文件类型，可支持压缩</span>
a2.sinks.k2.hdfs.fileType <span class="token operator">=</span> DataStream
<span class="token comment">#多久生成一个新的文件</span>
a2.sinks.k2.hdfs.rollInterval <span class="token operator">=</span> <span class="token number">60</span>
<span class="token comment">#设置每个文件的滚动大小</span>
a2.sinks.k2.hdfs.rollSize <span class="token operator">=</span> <span class="token number">134217700</span>
<span class="token comment">#文件的滚动与 Event 数量无关</span>
a2.sinks.k2.hdfs.rollCount <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># Use a channel which buffers events in memory</span>
a2.channels.c2.type <span class="token operator">=</span> memory
a2.channels.c2.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a2.channels.c2.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>

<span class="token comment"># Bind the source and sink to the channel</span>
a2.sources.r2.channels <span class="token operator">=</span> c2
a2.sinks.k2.channel <span class="token operator">=</span> c2
</code></pre> 
<p>（4）在flume-1.9文件下创建exec.log</p> 
<p>（5）运行 Flume</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ bin/flume-ng agent --conf conf/ --name  a2 --conf-file job/flume-file-hdfs.conf
</code></pre> 
<p>（5）开启 Hadoop<br> <img src="https://images2.imgbox.com/db/a7/RZXlUaZe_o.png" alt="在这里插入图片描述"></p> 
<p>（6）往文件中追加数据</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ <span class="token builtin class-name">echo</span> spark <span class="token operator">&gt;&gt;</span> exec.log
<span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ <span class="token builtin class-name">echo</span> hive <span class="token operator">&gt;&gt;</span> exec.log
</code></pre> 
<p>（7）在 HDFS 上查看文件<br> <img src="https://images2.imgbox.com/5f/6c/K7ERXH5v_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="3HDFS_297"></a>3.读取目录新文件到HDFS</h5> 
<p><strong>需求</strong></p> 
<p>使用Flume监听整个目录上传到HDFS</p> 
<p><img src="https://images2.imgbox.com/a1/95/goVn7s8p_o.png" alt="在这里插入图片描述"></p> 
<p><code>实现source是spooldir，sink是hdfs</code></p> 
<p><strong>实现步骤</strong></p> 
<p>（1）创建配置文件 flume-dir-hdfs.conf</p> 
<p>创建一个文件</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 job<span class="token punctuation">]</span>$ <span class="token function">vim</span> flume-dir-hdfs.conf
</code></pre> 
<p>添加如下内容</p> 
<pre><code class="prism language-bash">a3.sources <span class="token operator">=</span> r3
a3.sinks <span class="token operator">=</span> k3
a3.channels <span class="token operator">=</span> c3
<span class="token comment"># Describe/configure the source</span>
a3.sources.r3.type <span class="token operator">=</span> spooldir
a3.sources.r3.spoolDir <span class="token operator">=</span> /opt/module/flume-1.9/upload
a3.sources.r3.fileSuffix <span class="token operator">=</span> .COMPLETED
a3.sources.r3.fileHeader <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#忽略所有以.tmp 结尾的文件，不上传</span>
a3.sources.r3.ignorePattern <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">[</span>^ <span class="token punctuation">]</span>*<span class="token punctuation">\</span>.tmp<span class="token punctuation">)</span>
<span class="token comment"># Describe the sink</span>
a3.sinks.k3.type <span class="token operator">=</span> hdfs
a3.sinks.k3.hdfs.path <span class="token operator">=</span> hdfs://hadoop102:8020/flume/upload/%Y%m%d/%H
<span class="token comment">#上传文件的前缀</span>
a3.sinks.k3.hdfs.filePrefix <span class="token operator">=</span> upload- <span class="token comment">#是否按照时间滚动文件夹</span>
a3.sinks.k3.hdfs.round <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#多少时间单位创建一个新的文件夹</span>
a3.sinks.k3.hdfs.roundValue <span class="token operator">=</span> <span class="token number">1</span>
<span class="token comment">#重新定义时间单位</span>
a3.sinks.k3.hdfs.roundUnit <span class="token operator">=</span> hour
<span class="token comment">#是否使用本地时间戳</span>
a3.sinks.k3.hdfs.useLocalTimeStamp <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span>
a3.sinks.k3.hdfs.batchSize <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment">#设置文件类型，可支持压缩</span>
a3.sinks.k3.hdfs.fileType <span class="token operator">=</span> DataStream
<span class="token comment">#多久生成一个新的文件</span>
a3.sinks.k3.hdfs.rollInterval <span class="token operator">=</span> <span class="token number">60</span>
<span class="token comment">#设置每个文件的滚动大小大概是 128M</span>
a3.sinks.k3.hdfs.rollSize <span class="token operator">=</span> <span class="token number">134217700</span>
<span class="token comment">#文件的滚动与 Event 数量无关</span>
a3.sinks.k3.hdfs.rollCount <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># Use a channel which buffers events in memory</span>
a3.channels.c3.type <span class="token operator">=</span> memory
a3.channels.c3.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a3.channels.c3.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a3.sources.r3.channels <span class="token operator">=</span> c3
a3.sinks.k3.channel <span class="token operator">=</span> c3
</code></pre> 
<p>（2）启动监控文件夹命令</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf
</code></pre> 
<p>说明：在使用 Spooling Directory Source 时，不要在监控目录中创建并持续修改文件；上传完成的文件会以.COMPLETED 结尾；被监控文件夹每 500 毫秒扫描一次文件变动</p> 
<p>（3）向 upload 文件夹中添加文件</p> 
<p>在/opt/module/flume-1.9 目录下创建 upload 文件夹</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> upload
</code></pre> 
<p>向 upload 文件夹中添加文件</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 upload<span class="token punctuation">]</span>$ <span class="token function">touch</span> gzhu.log
<span class="token punctuation">[</span>gzhu@hadoop102 upload<span class="token punctuation">]</span>$ <span class="token function">touch</span> gzhu.tmp
<span class="token punctuation">[</span>gzhu@hadoop102 upload<span class="token punctuation">]</span>$ <span class="token function">touch</span> gzhu.txt
</code></pre> 
<p>（4）查看 HDFS 上的数据</p> 
<p><img src="https://images2.imgbox.com/8e/04/mZpykoTo_o.png" alt="在这里插入图片描述"></p> 
<p><code>注意，当我们监控这个目录时，文件一放进去，就立马被上传到了HDFS，并且已经上传了的文件会被标记成.COMPLETED，这样才不会导致重复上传，这样我们不能修改文件了，因为即使修改了，由于有后缀也不会上传了</code></p> 
<p><img src="https://images2.imgbox.com/62/6b/cNAMqV30_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="4_392"></a>4.实时监控多个追加文件</h5> 
<p>需求:使用 Flume 监听整个目录的实时追加文件，并上传至 HDFS<br> <img src="https://images2.imgbox.com/df/37/gmgz6QUL_o.png" alt="在这里插入图片描述"></p> 
<p><code>实现source是taildir（可以断点续传），实现sink是hdfs</code></p> 
<pre><code class="prism language-bash">/opt/module/flume-1.9/job

<span class="token function">vim</span> flume-taildir-hdfs.conf
</code></pre> 
<pre><code class="prism language-bash">a3.sources <span class="token operator">=</span> r3
a3.sinks <span class="token operator">=</span> k3
a3.channels <span class="token operator">=</span> c3
<span class="token comment"># Describe/configure the source</span>
a3.sources.r3.type <span class="token operator">=</span> TAILDIR
a3.sources.r3.positionFile <span class="token operator">=</span> /opt/module/flume-1.9/tail_dir.json
a3.sources.r3.filegroups <span class="token operator">=</span> f1 f2
a3.sources.r3.filegroups.f1 <span class="token operator">=</span> /opt/module/flume-1.9/files/.*file.*
a3.sources.r3.filegroups.f2 <span class="token operator">=</span> /opt/module/flume-1.9/files2/.*log.*
<span class="token comment"># Describe the sink</span>
a3.sinks.k3.type <span class="token operator">=</span> hdfs
a3.sinks.k3.hdfs.path <span class="token operator">=</span> hdfs://hadoop102:8020/flume/upload2/%Y%m%d/%H
<span class="token comment">#上传文件的前缀</span>
a3.sinks.k3.hdfs.filePrefix <span class="token operator">=</span> upload-

<span class="token comment">#是否按照时间滚动文件夹</span>
a3.sinks.k3.hdfs.round <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#多少时间单位创建一个新的文件夹</span>
a3.sinks.k3.hdfs.roundValue <span class="token operator">=</span> <span class="token number">1</span>
<span class="token comment">#重新定义时间单位</span>
a3.sinks.k3.hdfs.roundUnit <span class="token operator">=</span> hour
<span class="token comment">#是否使用本地时间戳</span>
a3.sinks.k3.hdfs.useLocalTimeStamp <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span>
a3.sinks.k3.hdfs.batchSize <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment">#设置文件类型，可支持压缩</span>
a3.sinks.k3.hdfs.fileType <span class="token operator">=</span> DataStream
<span class="token comment">#多久生成一个新的文件</span>
a3.sinks.k3.hdfs.rollInterval <span class="token operator">=</span> <span class="token number">60</span>
<span class="token comment">#设置每个文件的滚动大小大概是 128M`在这里插入代码片`</span>
a3.sinks.k3.hdfs.rollSize <span class="token operator">=</span> <span class="token number">134217700</span>
<span class="token comment">#文件的滚动与 Event 数量无关</span>
a3.sinks.k3.hdfs.rollCount <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># Use a channel which buffers events in memory</span>
a3.channels.c3.type <span class="token operator">=</span> memory
a3.channels.c3.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a3.channels.c3.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a3.sources.r3.channels <span class="token operator">=</span> c3
a3.sinks.k3.channel <span class="token operator">=</span> c3
</code></pre> 
<pre><code class="prism language-bash">bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-taildir-hdfs.conf
</code></pre> 
<p>在/opt/module/flume-1.9 目录下创建 files 文件夹</p> 
<pre><code class="prism language-bash"><span class="token function">mkdir</span> files

<span class="token function">mkdir</span> files2
</code></pre> 
<p>进入文件夹并创建文件，通过echo进行数据的追加</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> file1

<span class="token function">touch</span> file1.txt

<span class="token builtin class-name">echo</span> hello <span class="token operator">&gt;&gt;</span> file1
</code></pre> 
<p>tail_dir.json记录了每个文件追加到了哪个位置，因此可以准确的追加数据，但是，万一inode或者file有一个变化，那么就代表只是一个新的文件，flume认为该文件从来没有被上传过HDFS<br> <img src="https://images2.imgbox.com/50/7b/Qy9LnyQq_o.png" alt="在这里插入图片描述"></p> 
<p><mark>注意点</mark></p> 
<p>1.监控的多个文件，这些文件的追加内容会进入一个文件里（滚动时间范围内）</p> 
<p>2.假如我们监控Hive的日志，当天Hive的日志文件名字是hive.log，此时新增加的数据会被上传，但是当新的一天开始时，当天的hive.log会更名，会加上昨天的日期，例如，hive.log.2022-7-7，这样flume会认为这是一个新的文件，于是就导致了重复上传了</p> 
<p>我们就写死，只监控hive.log这个文件，但是也有问题，比如，在晚上11.30机器挂掉了，而我们第二天才修复，那么此时昨天那半小时的数据已经监控不到了，因为已经改名了，不是hive.log了，所以这样会丢数据</p> 
<p>解决办法有两种，第一种是找管控日志的人协商，生成日志时就带有日期，这样一天生成一本日志就好了。第二种办法就是修改flume的源码，只负责检测inode，这样文件改名后由于inode没变也不会重新上传了</p> 
<h4><a id="_481"></a>四.进阶案例</h4> 
<h5><a id="1Chanel_Selector_482"></a>1.Chanel Selector</h5> 
<h6><a id="11_replicating_483"></a>1.1 replicating</h6> 
<p><strong>需求</strong></p> 
<p>使用 Flume-1 监控文件变动，Flume-1 将变动内容传递给 Flume-2，Flume-2 负责存储到 HDFS。同时 Flume-1 将变动内容传递给 Flume-3，Flume-3 负责输出到 Local FileSystem，这是replicating<br> <img src="https://images2.imgbox.com/9d/86/zBgJ54Cp_o.png" alt="在这里插入图片描述"></p> 
<p>很明显，我们使用了3个agent，因此需要3个flume配置文件，我们统一将这个三个配置文件放在job目录下的group1文件夹下</p> 
<p><strong>第一个配置文件 flume-file-flume.conf</strong></p> 
<p>配置 1 个接收日志文件的 source 和两个 channel、两个 sink，分别输送给 flume-flume-hdfs 和 flume-flume-dir</p> 
<pre><code class="prism language-bash"><span class="token function">vim</span> flume-file-flume.conf
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a1.sources <span class="token operator">=</span> r1
a1.sinks <span class="token operator">=</span> k1 k2
a1.channels <span class="token operator">=</span> c1 c2
<span class="token comment"># 将数据流复制给所有 channel</span>
a1.sources.r1.selector.type <span class="token operator">=</span> replicating
<span class="token comment"># Describe/configure the source</span>
a1.sources.r1.type <span class="token operator">=</span> <span class="token builtin class-name">exec</span>
a1.sources.r1.command <span class="token operator">=</span> <span class="token function">tail</span> -F /opt/module/flume-1.9/files/exec.log
a1.sources.r1.shell <span class="token operator">=</span> /bin/bash -c
<span class="token comment"># Describe the sink</span>
<span class="token comment"># sink 端的 avro 是一个数据发送者</span>
a1.sinks.k1.type <span class="token operator">=</span> avro
a1.sinks.k1.hostname <span class="token operator">=</span> hadoop102
a1.sinks.k1.port <span class="token operator">=</span> <span class="token number">4141</span>
a1.sinks.k2.type <span class="token operator">=</span> avro
a1.sinks.k2.hostname <span class="token operator">=</span> hadoop102
a1.sinks.k2.port <span class="token operator">=</span> <span class="token number">4142</span>
<span class="token comment"># Describe the channel</span>
a1.channels.c1.type <span class="token operator">=</span> memory
a1.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a1.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
a1.channels.c2.type <span class="token operator">=</span> memory
a1.channels.c2.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a1.channels.c2.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a1.sources.r1.channels <span class="token operator">=</span> c1 c2
a1.sinks.k1.channel <span class="token operator">=</span> c1
a1.sinks.k2.channel <span class="token operator">=</span> c2
</code></pre> 
<p><strong>第二个配置文件 flume-flume-hdfs.conf</strong></p> 
<p>配置上级 Flume 输出的 Source，输出是到 HDFS 的 Sink</p> 
<pre><code class="prism language-bash"><span class="token function">vim</span> flume-flume-hdfs.conf
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a2.sources <span class="token operator">=</span> r1
a2.sinks <span class="token operator">=</span> k1
a2.channels <span class="token operator">=</span> c1
<span class="token comment"># Describe/configure the source</span>
<span class="token comment"># source 端的 avro 是一个数据接收服务</span>
a2.sources.r1.type <span class="token operator">=</span> avro
a2.sources.r1.bind <span class="token operator">=</span> hadoop102
a2.sources.r1.port <span class="token operator">=</span> <span class="token number">4141</span>
<span class="token comment"># Describe the sink</span>
a2.sinks.k1.type <span class="token operator">=</span> hdfs
a2.sinks.k1.hdfs.path <span class="token operator">=</span> hdfs://hadoop102:8020/flume2/%Y%m%d/%H
<span class="token comment">#上传文件的前缀</span>
a2.sinks.k1.hdfs.filePrefix <span class="token operator">=</span> flume2- 
<span class="token comment">#是否按照时间滚动文件夹</span>
a2.sinks.k1.hdfs.round <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#多少时间单位创建一个新的文件夹</span>
a2.sinks.k1.hdfs.roundValue <span class="token operator">=</span> <span class="token number">1</span>
<span class="token comment">#重新定义时间单位</span>
a2.sinks.k1.hdfs.roundUnit <span class="token operator">=</span> hour
<span class="token comment">#是否使用本地时间戳</span>
a2.sinks.k1.hdfs.useLocalTimeStamp <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span>
a2.sinks.k1.hdfs.batchSize <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment">#设置文件类型，可支持压缩</span>
a2.sinks.k1.hdfs.fileType <span class="token operator">=</span> DataStream
<span class="token comment">#多久生成一个新的文件</span>
a2.sinks.k1.hdfs.rollInterval <span class="token operator">=</span> <span class="token number">30</span>
<span class="token comment">#设置每个文件的滚动大小大概是 128M</span>
a2.sinks.k1.hdfs.rollSize <span class="token operator">=</span> <span class="token number">134217700</span> 
<span class="token comment">#文件的滚动与 Event 数量无关</span>
a2.sinks.k1.hdfs.rollCount <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># Describe the channel</span>
a2.channels.c1.type <span class="token operator">=</span> memory
a2.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a2.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a2.sources.r1.channels <span class="token operator">=</span> c1
a2.sinks.k1.channel <span class="token operator">=</span> c1
</code></pre> 
<p><strong>第三个配置文件 flume-flume-dir.conf</strong></p> 
<p>配置上级 Flume 输出的 Source，输出是到本地目录的 Sink</p> 
<pre><code class="prism language-bash"><span class="token function">vim</span> flume-flume-dir.conf
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a3.sources <span class="token operator">=</span> r1
a3.sinks <span class="token operator">=</span> k1
a3.channels <span class="token operator">=</span> c2
<span class="token comment"># Describe/configure the source</span>
a3.sources.r1.type <span class="token operator">=</span> avro
a3.sources.r1.bind <span class="token operator">=</span> hadoop102
a3.sources.r1.port <span class="token operator">=</span> <span class="token number">4142</span>
<span class="token comment"># Describe the sink</span>
a3.sinks.k1.type <span class="token operator">=</span> file_roll
a3.sinks.k1.sink.directory <span class="token operator">=</span> /opt/module/flume-1.9/data
<span class="token comment"># Describe the channel</span>
a3.channels.c2.type <span class="token operator">=</span> memory
a3.channels.c2.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a3.channels.c2.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a3.sources.r1.channels <span class="token operator">=</span> c2
a3.sinks.k1.channel <span class="token operator">=</span> c2
</code></pre> 
<p>提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录</p> 
<p><strong>执行配置文件</strong></p> 
<p>先启动服务端，再启动客户端</p> 
<pre><code class="prism language-bash">bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf
</code></pre> 
<pre><code class="prism language-bash">bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf
</code></pre> 
<pre><code class="prism language-bash">bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf
</code></pre> 
<p><strong>向exec.log追加数据</strong></p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">echo</span> My Name is Jack <span class="token operator">!</span> <span class="token operator">&gt;&gt;</span> exec.log
</code></pre> 
<h6><a id="12_multiplexing_637"></a>1.2 multiplexing</h6> 
<p>我们来看下官方的案例，一个source连接了4个channel，当我们选择器是multiplexing时，首先判断event中的头信息，我们知道，一个event是由头信息和body组成的，而头信息是一个K-V结构，那么下面这段代码应该清楚了，判断state，如果是CZ，发送c1这个channel，如果是US，发往c2和c3，否则发往c4，那现在问题来了，怎么向一个event里头信息里添加state-V这个数据呢？用拦截器</p> 
<pre><code class="prism language-bash">a1.sources <span class="token operator">=</span> r1
a1.channels <span class="token operator">=</span> c1 c2 c3 c4
a1.sources.r1.selector.type <span class="token operator">=</span> multiplexing
a1.sources.r1.selector.header <span class="token operator">=</span> state        <span class="token comment">#以每个Event的header中的state这个属性的值作为选择channel的依据</span>
a1.sources.r1.selector.mapping.CZ <span class="token operator">=</span> c1       <span class="token comment">#如果state=CZ，则选择c1这个channel</span>
a1.sources.r1.selector.mapping.US <span class="token operator">=</span> c2 c3    <span class="token comment">#如果state=US，则选择c2 和 c3 这两个channel</span>
a1.sources.r1.selector.default <span class="token operator">=</span> c4          <span class="token comment">#默认使用c4这个channel</span>
</code></pre> 
<p><mark>拦截器</mark></p> 
<p>我们想要实现拦截器，需要自定义实现拦截器类</p> 
<pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.flume<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>flume-ng-core<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>1.9.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<pre><code class="prism language-java"><span class="token comment">// 注意 Interceptor 接口是flume下的</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TypeInterceptor</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">private</span> <span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Event</span><span class="token punctuation">&gt;</span></span> addList<span class="token punctuation">;</span>

    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">initialize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        addList <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Event</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// 单个事件处理方法</span>
    <span class="token keyword">public</span> <span class="token class-name">Event</span> <span class="token function">intercept</span><span class="token punctuation">(</span><span class="token class-name">Event</span> event<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// 头信息是个map结构，我们首先获取头信息结构</span>
        <span class="token class-name">Map</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> headers <span class="token operator">=</span> event<span class="token punctuation">.</span><span class="token function">getHeaders</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// body默认是字节数组 我们转成字符串就行了</span>
        <span class="token class-name">String</span> body <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">String</span><span class="token punctuation">(</span>event<span class="token punctuation">.</span><span class="token function">getBody</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 我们拿到body也就是发送过来的数据了，再进行加工就可以了</span>
        <span class="token comment">// 我们这里假设  将点击行为 和 支付行为分开 将来我们区分的时候，就是根据某一个key的不同value值进行区分的！</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span>body<span class="token punctuation">.</span><span class="token function">contains</span><span class="token punctuation">(</span><span class="token string">"click"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
            <span class="token comment">// key随便起名字</span>
            headers<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"type"</span><span class="token punctuation">,</span><span class="token string">"click"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token punctuation">{<!-- --></span>
            headers<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"type"</span><span class="token punctuation">,</span><span class="token string">"pay"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        <span class="token comment">// event进行了加工，添加了头信息 返回就好了</span>
        <span class="token keyword">return</span> event<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// 多个事件的处理</span>
    <span class="token keyword">public</span> <span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Event</span><span class="token punctuation">&gt;</span></span> <span class="token function">intercept</span><span class="token punctuation">(</span><span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Event</span><span class="token punctuation">&gt;</span></span> list<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// 先清空我们定义的集合 因为不同批次的事件肯定不能再发一遍 清空 保证集合中只有本批次的事件</span>
        addList<span class="token punctuation">.</span><span class="token function">clear</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        list<span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span>event <span class="token operator">-&gt;</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// 单个事件的处理逻辑和上面是一样的，直接调用方法就好了</span>
            <span class="token class-name">Event</span> intercept <span class="token operator">=</span> <span class="token function">intercept</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">;</span>
            addList<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>intercept<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token keyword">return</span> addList<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token punctuation">}</span>

    <span class="token comment">// 这里我们要特别注意 官网的案例要求我们必须有一个Builder</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">Builder</span> <span class="token keyword">implements</span> <span class="token class-name">Interceptor<span class="token punctuation">.</span>Builder</span><span class="token punctuation">{<!-- --></span>

        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> <span class="token class-name">Interceptor</span> <span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">TypeInterceptor</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

        <span class="token annotation punctuation">@Override</span>
        <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span><span class="token class-name">Context</span> context<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>打包，将jar包放到flume的lib下</strong></p> 
<p>我们只需要将上面复制的案例改成多路复用就行了<br> <img src="https://images2.imgbox.com/e6/d7/ZHU5E3EM_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a1.sources <span class="token operator">=</span> r1
a1.sinks <span class="token operator">=</span> k1 k2
a1.channels <span class="token operator">=</span> c1 c2
<span class="token comment"># Describe/configure the source</span>
a1.sources.r1.type <span class="token operator">=</span> netcat
a1.sources.r1.bind <span class="token operator">=</span> localhost
a1.sources.r1.port <span class="token operator">=</span> <span class="token number">44444</span>
<span class="token comment"># 区别就在这里</span>
a1.sources.r1.interceptors <span class="token operator">=</span> i1
a1.sources.r1.interceptors.i1.type <span class="token operator">=</span> com.gzhu.interceptor.TypeInterceptor<span class="token variable">$Builder</span>
a1.sources.r1.selector.type <span class="token operator">=</span> multiplexing
a1.sources.r1.selector.header <span class="token operator">=</span> <span class="token builtin class-name">type</span>
a1.sources.r1.selector.mapping.click <span class="token operator">=</span> c1
a1.sources.r1.selector.mapping.pay <span class="token operator">=</span> c2
<span class="token comment"># Describe the sink</span>
a1.sinks.k1.type <span class="token operator">=</span> avro
a1.sinks.k1.hostname <span class="token operator">=</span> hadoop103
a1.sinks.k1.port <span class="token operator">=</span> <span class="token number">4141</span>
a1.sinks.k2.type<span class="token operator">=</span>avro
a1.sinks.k2.hostname <span class="token operator">=</span> hadoop104
a1.sinks.k2.port <span class="token operator">=</span> <span class="token number">4242</span>
<span class="token comment"># Use a channel which buffers events in memory</span>
a1.channels.c1.type <span class="token operator">=</span> memory
a1.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a1.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Use a channel which buffers events in memory</span>
a1.channels.c2.type <span class="token operator">=</span> memory
a1.channels.c2.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a1.channels.c2.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a1.sources.r1.channels <span class="token operator">=</span> c1 c2
a1.sinks.k1.channel <span class="token operator">=</span> c1
a1.sinks.k2.channel <span class="token operator">=</span> c2
</code></pre> 
<h5><a id="2Sink_Processors_766"></a>2.Sink Processors</h5> 
<p>你可以把多个sink分成一个组， 这时候Sink组逻辑处理器（Flume Sink Processors）可以对这同一个组里的几个sink进行负载均衡或者其中一个sink发生故障后将输出Event的任务转移到其他的sink上</p> 
<p>Sinkprocessor有三种，第一种defaultsinkprocessor，一个channel只能绑定一个sink，第二种loadbalancingsinkprocessor，负载均衡，多个sink轮流查看channel，第三种failoversinkprocessor，故障转移，对多个sink可以配置优先级，若优先级高的sink挂掉后，转移到优先级低的sink</p> 
<h6><a id="21_failoversinkprocessor_772"></a>2.1 failoversinkprocessor</h6> 
<p>我们假设flume-2的优先级高，它挂掉后将数据送到flume-3<br> <img src="https://images2.imgbox.com/40/a9/52NYu1lH_o.png" alt="在这里插入图片描述"><br> 在/opt/module/flume-1.9/job 目录下创建 group2 文件夹</p> 
<p><strong>第一个配置文件 flume-file-flume.conf</strong></p> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a1.sources <span class="token operator">=</span> r1
a1.channels <span class="token operator">=</span> c1
a1.sinkgroups <span class="token operator">=</span> g1
a1.sinks <span class="token operator">=</span> k1 k2
<span class="token comment"># Describe/configure the source</span>
a1.sources.r1.type <span class="token operator">=</span> <span class="token builtin class-name">exec</span>
a1.sources.r1.command <span class="token operator">=</span> <span class="token function">tail</span> -F /opt/module/flume-1.9/files/exec.log
a1.sources.r1.shell <span class="token operator">=</span> /bin/bash -c

a1.sinkgroups.g1.processor.type <span class="token operator">=</span> failover
a1.sinkgroups.g1.processor.priority.k1 <span class="token operator">=</span> <span class="token number">5</span>
a1.sinkgroups.g1.processor.priority.k2 <span class="token operator">=</span> <span class="token number">10</span>
a1.sinkgroups.g1.processor.maxpenalty <span class="token operator">=</span> <span class="token number">10000</span>
<span class="token comment"># Describe the sink</span>
a1.sinks.k1.type <span class="token operator">=</span> avro
a1.sinks.k1.hostname <span class="token operator">=</span> hadoop102
a1.sinks.k1.port <span class="token operator">=</span> <span class="token number">4141</span>
a1.sinks.k2.type <span class="token operator">=</span> avro
a1.sinks.k2.hostname <span class="token operator">=</span> hadoop102
a1.sinks.k2.port <span class="token operator">=</span> <span class="token number">4142</span>
<span class="token comment"># Describe the channel</span>
a1.channels.c1.type <span class="token operator">=</span> memory
a1.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a1.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a1.sources.r1.channels <span class="token operator">=</span> c1
a1.sinkgroups.g1.sinks <span class="token operator">=</span> k1 k2
a1.sinks.k1.channel <span class="token operator">=</span> c1
a1.sinks.k2.channel <span class="token operator">=</span> c1
</code></pre> 
<p><strong>第二个配置文件 flume-flume-console1.conf</strong></p> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a2.sources <span class="token operator">=</span> r1
a2.sinks <span class="token operator">=</span> k1
a2.channels <span class="token operator">=</span> c1
<span class="token comment"># Describe/configure the source</span>
a2.sources.r1.type <span class="token operator">=</span> avro
a2.sources.r1.bind <span class="token operator">=</span> hadoop102
a2.sources.r1.port <span class="token operator">=</span> <span class="token number">4141</span>
<span class="token comment"># Describe the sink</span>
a2.sinks.k1.type <span class="token operator">=</span> logger
<span class="token comment"># Describe the channel</span>
a2.channels.c1.type <span class="token operator">=</span> memory
a2.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a2.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a2.sources.r1.channels <span class="token operator">=</span> c1
a2.sinks.k1.channel <span class="token operator">=</span> c1
</code></pre> 
<p><strong>第三个配置文件 flume-flume-console2.conf</strong></p> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a3.sources <span class="token operator">=</span> r1
a3.sinks <span class="token operator">=</span> k1
a3.channels <span class="token operator">=</span> c2
<span class="token comment"># Describe/configure the source</span>
a3.sources.r1.type <span class="token operator">=</span> avro
a3.sources.r1.bind <span class="token operator">=</span> hadoop102
a3.sources.r1.port <span class="token operator">=</span> <span class="token number">4142</span>
<span class="token comment"># Describe the sink</span>
a3.sinks.k1.type <span class="token operator">=</span> logger
<span class="token comment"># Describe the channel</span>
a3.channels.c2.type <span class="token operator">=</span> memory
a3.channels.c2.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a3.channels.c2.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a3.sources.r1.channels <span class="token operator">=</span> c2
a3.sinks.k1.channel <span class="token operator">=</span> c2
</code></pre> 
<p><strong>执行配置文件</strong></p> 
<pre><code class="prism language-bash">bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger<span class="token operator">=</span>INFO,console

bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger<span class="token operator">=</span>INFO,console

bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-file-flume.conf
</code></pre> 
<p><strong>向exec.log追加数据</strong></p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">echo</span> zhang <span class="token operator">&gt;&gt;</span> exec.log
</code></pre> 
<p>可以看到数据到了console2，也就是4142端口，10&gt;5，优先级高<br> <img src="https://images2.imgbox.com/e3/ec/6VQaAV7L_o.png" alt="在这里插入图片描述"></p> 
<p>断开4142，再追加数据</p> 
<p><img src="https://images2.imgbox.com/d5/6e/f8j2Dkvk_o.png" alt="在这里插入图片描述"><br> 数据到了console1，也就是4141端口<br> <img src="https://images2.imgbox.com/08/0a/4vmgtpvO_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="22_load_balance_882"></a>2.2 load_balance</h6> 
<p>负载均衡Sink 选择器提供了在多个sink上进行负载均衡流量的功能。 它维护一个活动sink列表的索引来实现负载的分配。<code> 默认支持了轮询（round_robin）和随机（random）两种选择机制分配负载</code>。 默认是轮询，可以通过配置来更改。也可以从 AbstractSinkSelector 继承写一个自定义的选择器</p> 
<p>工作时，此选择器使用其配置的选择机制选择下一个sink并调用它。 如果所选sink无法正常工作，则处理器通过其配置的选择机制选择下一个可用sink。 此实现不会将失败的Sink列入黑名单，而是继续乐观地尝试每个可用的Sink</p> 
<p>如果所有sink调用都失败了，选择器会将故障抛给sink的运行器</p> 
<p>如果backoff设置为true则启用了退避机制，失败的sink会被放入黑名单，达到一定的超时时间后会自动从黑名单移除。 如从黑名单出来后sink仍然失败，则再次进入黑名单而且超时时间会翻倍，以避免在无响应的sink上浪费过长时间。 如果没有启用退避机制，在禁用此功能的情况下，发生sink传输失败后，会将本次负载传给下一个sink继续尝试，因此这种情况下是不均衡的</p> 
<p>我们只需要将上面flume-file-flume.conf改一点参数就可以了</p> 
<pre><code class="prism language-bash">a1.sinkgroups <span class="token operator">=</span> g1
a1.sinkgroups.g1.sinks <span class="token operator">=</span> k1 k2
a1.sinkgroups.g1.processor.type <span class="token operator">=</span> load_balance
a1.sinkgroups.g1.processor.backoff <span class="token operator">=</span> <span class="token boolean">true</span>
a1.sinkgroups.g1.processor.selector <span class="token operator">=</span> random <span class="token comment"># 也可以round_robin</span>
</code></pre> 
<p>a1.sinkgroups.g1.processor.backoff = true</p> 
<p>失败的sink是否成倍地增加退避它的时间。 如果设置为false，负载均衡在某一个sink发生异常后，下一次选择sink的时候仍然会将失败的这个sink加入候选队列； 如果设置为true，某个sink连续发生异常时会成倍地增加它的退避时间，在退避的时间内是无法参与负载均衡竞争的。退避机制只统计1个小时（可以设置）发生的异常，超过1个小时没有发生异常就会重新计算</p> 
<h5><a id="3_907"></a>3.聚合</h5> 
<p>采集多台服务器的日志文件，聚合到HDFS<br> <img src="https://images2.imgbox.com/06/97/eOAxjwpJ_o.png" alt="在这里插入图片描述"></p> 
<p>由于涉及了多台机器，首先要分发 Flume</p> 
<pre><code class="prism language-bash">xsync /opt/module/flume-1.9
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 job<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> group3

<span class="token punctuation">[</span>gzhu@hadoop103 job<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> group3

<span class="token punctuation">[</span>gzhu@hadoop104 job<span class="token punctuation">]</span>$ <span class="token function">mkdir</span> group3
</code></pre> 
<p><strong>hadoop102编写配置文件</strong></p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 job<span class="token punctuation">]</span>$ <span class="token builtin class-name">cd</span> group3

<span class="token function">vim</span> flume1-exec-flume.conf
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a1.sources <span class="token operator">=</span> r1
a1.sinks <span class="token operator">=</span> k1
a1.channels <span class="token operator">=</span> c1
<span class="token comment"># Describe/configure the source</span>
a1.sources.r1.type <span class="token operator">=</span> <span class="token builtin class-name">exec</span>
a1.sources.r1.command <span class="token operator">=</span> <span class="token function">tail</span> -F /opt/module/tmp/exec.log
a1.sources.r1.shell <span class="token operator">=</span> /bin/bash -c
<span class="token comment"># Describe the sink</span>
a1.sinks.k1.type <span class="token operator">=</span> avro
a1.sinks.k1.hostname <span class="token operator">=</span> hadoop104
a1.sinks.k1.port <span class="token operator">=</span> <span class="token number">4141</span>

<span class="token comment"># Describe the channel</span>
a1.channels.c1.type <span class="token operator">=</span> memory
a1.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a1.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a1.sources.r1.channels <span class="token operator">=</span> c1
a1.sinks.k1.channel <span class="token operator">=</span> c1
</code></pre> 
<p><strong>hadoop103编写配置文件</strong></p> 
<pre><code class="prism language-bash"><span class="token function">vim</span> flume2-exec-flume.conf
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a2.sources <span class="token operator">=</span> r1
a2.sinks <span class="token operator">=</span> k1
a2.channels <span class="token operator">=</span> c1
<span class="token comment"># Describe/configure the source</span>
a2.sources.r1.type <span class="token operator">=</span> <span class="token builtin class-name">exec</span>
a2.sources.r1.command <span class="token operator">=</span> <span class="token function">tail</span> -F /opt/module/tmp/input.log
a2.sources.r1.shell <span class="token operator">=</span> /bin/bash -c
<span class="token comment"># Describe the sink</span>
a2.sinks.k1.type <span class="token operator">=</span> avro
a2.sinks.k1.hostname <span class="token operator">=</span> hadoop104
a2.sinks.k1.port <span class="token operator">=</span> <span class="token number">4141</span>

<span class="token comment"># Describe the channel</span>
a2.channels.c1.type <span class="token operator">=</span> memory
a2.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a2.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a2.sources.r1.channels <span class="token operator">=</span> c1
a2.sinks.k1.channel <span class="token operator">=</span> c1
</code></pre> 
<p><strong>hadoop104收集</strong></p> 
<pre><code class="prism language-bash"><span class="token function">vim</span> flume3-flume-hdfs.conf
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># Name the components on this agent</span>
a3.sources <span class="token operator">=</span> r1
a3.sinks <span class="token operator">=</span> k1
a3.channels <span class="token operator">=</span> c1
<span class="token comment"># Describe/configure the source</span>
a3.sources.r1.type <span class="token operator">=</span> avro
a3.sources.r1.bind <span class="token operator">=</span> hadoop104
a3.sources.r1.port <span class="token operator">=</span> <span class="token number">4141</span>

a3.sinks.k1.type <span class="token operator">=</span> hdfs
a3.sinks.k1.hdfs.path <span class="token operator">=</span> hdfs://hadoop102:8020/flume4/upload/%Y%m%d/%H
<span class="token comment">#上传文件的前缀</span>
a3.sinks.k1.hdfs.filePrefix <span class="token operator">=</span> upload- 
<span class="token comment">#是否按照时间滚动文件夹</span>
a3.sinks.k1.hdfs.round <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#多少时间单位创建一个新的文件夹</span>
a3.sinks.k1.hdfs.roundValue <span class="token operator">=</span> <span class="token number">1</span>
<span class="token comment">#重新定义时间单位</span>
a3.sinks.k1.hdfs.roundUnit <span class="token operator">=</span> hour
<span class="token comment">#是否使用本地时间戳</span>
a3.sinks.k1.hdfs.useLocalTimeStamp <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token comment">#积攒多少个 Event 才 flush 到 HDFS 一次</span>
a3.sinks.k1.hdfs.batchSize <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment">#设置文件类型，可支持压缩</span>
a3.sinks.k1.hdfs.fileType <span class="token operator">=</span> DataStream
<span class="token comment">#多久生成一个新的文件</span>
a3.sinks.k1.hdfs.rollInterval <span class="token operator">=</span> <span class="token number">60</span>
<span class="token comment">#设置每个文件的滚动大小大概是 128M</span>
a3.sinks.k1.hdfs.rollSize <span class="token operator">=</span> <span class="token number">134217700</span>
<span class="token comment">#文件的滚动与 Event 数量无关</span>
a3.sinks.k1.hdfs.rollCount <span class="token operator">=</span> <span class="token number">0</span>


<span class="token comment"># Describe the channel</span>
a3.channels.c1.type <span class="token operator">=</span> memory
a3.channels.c1.capacity <span class="token operator">=</span> <span class="token number">1000</span>
a3.channels.c1.transactionCapacity <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># Bind the source and sink to the channel</span>
a3.sources.r1.channels <span class="token operator">=</span> c1
a3.sinks.k1.channel <span class="token operator">=</span> c1
</code></pre> 
<p><strong>分别启动</strong></p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop104 flume-1.9<span class="token punctuation">]</span>$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-hdfs.conf 


<span class="token punctuation">[</span>gzhu@hadoop103 flume-1.9<span class="token punctuation">]</span>$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume2-exec-flume.conf


<span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume1-exec-flume.conf
</code></pre> 
<p><strong>测试</strong></p> 
<p><img src="https://images2.imgbox.com/f8/30/5TAG5Ub5_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/fb/d1/Ei6yqseI_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/31/7e/GUVJnSUj_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Flume_1055"></a>五.Flume数据流监控</h4> 
<p>Ganglia 由 gmond、gmetad 和 gweb 三部分组成</p> 
<p>gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用 gmond，你可以很容易收集很多系统指标数据，如 CPU、内存、磁盘、网络和活跃进程的数据等</p> 
<p>gmetad（Ganglia Meta Daemon）整合所有信息，并将其以 RRD 格式存储至磁盘的服务</p> 
<p>gweb（Ganglia Web）Ganglia 可视化工具，gweb 是一种利用浏览器显示 gmetad 所存储数据的 PHP 前端。在 Web 界面中以图表方式展现集群的运行状态下收集的多种不同指标数据</p> 
<p>==安装 ganglia ==</p> 
<p>（1）规划</p> 
<pre><code class="prism language-bash">hadoop102: web gmetad gmod
hadoop103: gmod
hadoop104: gmod
</code></pre> 
<p>（2）在 102 103 104 分别安装 epel-release</p> 
<p>[gzhu@hadoop102 flume-1.9]$ sudo yum -y install epel-release</p> 
<p>（3）在 102 安装</p> 
<p>[gzhu@hadoop102 flume-1.9]$ sudo yum -y install ganglia-gmetad<br> [gzhu@hadoop102 flume-1.9]$ sudo yum -y install ganglia-web<br> [gzhu@hadoop102 flume-1.9]$ sudo yum -y install ganglia-gmond</p> 
<p>（4）在 103 和 104 安装<br> [gzhu@hadoop102 flume-1.9]$ sudo yum -y install ganglia-gmond</p> 
<p>（5）在 102 修改配置文件/etc/httpd/conf.d/ganglia.conf</p> 
<p>[gzhu@hadoop102 flume-1.9]$ sudo vim /etc/httpd/conf.d/ganglia.conf</p> 
<pre><code class="prism language-bash"><span class="token comment"># Ganglia monitoring system php web frontend</span>
<span class="token comment">#</span>
Alias /ganglia /usr/share/ganglia
<span class="token operator">&lt;</span>Location /ganglia<span class="token operator">&gt;</span>
 <span class="token comment"># Require local</span>
 <span class="token comment"># 通过 windows 访问 ganglia,需要配置 Linux 对应的主机(windows)ip 地址</span>
 Require <span class="token function">ip</span> windows主机ipv4，ipconfig查看
 <span class="token comment"># Require ip 10.1.2.3</span>
 <span class="token comment"># Require host example.org</span>
<span class="token operator">&lt;</span>/Location<span class="token operator">&gt;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/92/82/EgMn2da8_o.png" alt="在这里插入图片描述"></p> 
<p>（6）在 102 修改配置文件/etc/ganglia/gmetad.conf</p> 
<p>[gzhu@hadoop102 flume-1.9]$ sudo vim /etc/ganglia/gmetad.conf</p> 
<p>修改为：</p> 
<p>data_source “my cluster” hadoop102</p> 
<p><img src="https://images2.imgbox.com/6d/bb/T1idESkS_o.png" alt="在这里插入图片描述"><br> （7）在 <strong>102 103 104</strong> 修改配置文件/etc/ganglia/gmond.conf</p> 
<p>[gzhu@hadoop102 flume-1.9]$ sudo vim /etc/ganglia/gmond.conf</p> 
<p>修改为： 修改三个地方 name host bind</p> 
<pre><code>cluster {
 name = "my cluster"   # 修改1
 owner = "unspecified"
 latlong = "unspecified"
 url = "unspecified"
}
udp_send_channel {
 #bind_hostname = yes # Highly recommended, soon to be default.
 # This option tells gmond to use a source 
address
 # that resolves to the machine's hostname. 
Without
 # this, the metrics may appear to come from 
any
 # interface and the DNS names associated with
 # those IPs will be used to create the RRDs.
 # mcast_join = 239.2.11.71
 # 数据发送给 hadoop102
 host = hadoop102  # 修改2 注意每台机器不同
 port = 8649
 ttl = 1
}

udp_recv_channel {
 # mcast_join = 239.2.11.71
 port = 8649
# 接收来自任意连接的数据
 bind = 0.0.0.0  # 修改3
 retry_bind = true
 # Size of the UDP buffer. If you are handling lots of metrics 
you really
 # should bump it up to e.g. 10MB or even higher.
 # buffer = 10485760
}
</code></pre> 
<p>（8）在 102 修改配置文件/etc/selinux/config</p> 
<p>[gzhu@hadoop102 flume-1.9]$ sudo vim /etc/selinux/config</p> 
<p>修改为：</p> 
<pre><code class="prism language-bash"><span class="token comment"># This file controls the state of SELinux on the system.</span>
<span class="token comment"># SELINUX= can take one of these three values:</span>
<span class="token comment"># enforcing - SELinux security policy is enforced.</span>
<span class="token comment"># permissive - SELinux prints warnings instead of enforcing.</span>
<span class="token comment"># disabled - No SELinux policy is loaded.</span>
<span class="token assign-left variable">SELINUX</span><span class="token operator">=</span>disabled
<span class="token comment"># SELINUXTYPE= can take one of these two values:</span>
<span class="token comment"># targeted - Targeted processes are protected,</span>
<span class="token comment"># mls - Multi Level Security protection.</span>
<span class="token assign-left variable">SELINUXTYPE</span><span class="token operator">=</span>targeted
</code></pre> 
<p>临时生效</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> setenforce <span class="token number">0</span>
</code></pre> 
<p>（9）启动 ganglia （1）在 102 103 104 启动</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>gzhu@hadoop102 flume-1.9<span class="token punctuation">]</span>$ <span class="token function">sudo</span> systemctl start gmond
</code></pre> 
<p>在 102 启动</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> systemctl start httpd
<span class="token function">sudo</span> systemctl start gmetad
</code></pre> 
<p>（10）打开网页浏览 ganglia 页面<br> http://hadoop102/ganglia</p> 
<p>如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia 目录的权限：</p> 
<p>[gzhu@hadoop102 flume-1.9]$ sudo chmod -R 777 /var/lib/ganglia</p> 
<p><img src="https://images2.imgbox.com/9b/04/Ma7Afy4Y_o.png" alt="在这里插入图片描述"><br> 启动 Flume 任务 时加上参数</p> 
<pre><code class="prism language-bash">bin/flume-ng agent <span class="token punctuation">\</span>
-c conf/ <span class="token punctuation">\</span>
-n a1 <span class="token punctuation">\</span> 
-f job/flume-netcat-logger.conf <span class="token punctuation">\</span> 
-Dflume.root.logger<span class="token operator">=</span>INFO,console <span class="token punctuation">\</span> 
-Dflume.monitoring.type<span class="token operator">=</span>ganglia <span class="token punctuation">\</span> 
-Dflume.monitoring.hosts<span class="token operator">=</span>hadoop102:8649
</code></pre> 
<p><img src="https://images2.imgbox.com/f9/df/SqGbeCkM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/01/de/lOwCOc9N_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f3ecf78c03f897e3d196a9b59fd86d06/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">linux驱动学习笔记（1）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/86e14acec52e39a3d61f30a4ec9d3f78/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【openlayers】地图【二】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>