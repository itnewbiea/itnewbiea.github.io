<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Event-based vision - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Event-based vision" />
<meta property="og:description" content="Event-based vision 论文阅读笔记 一下内容仅仅是记录自己的论文阅读。
一、Event-based Vision: A Survey 该论文属于综述性文章，对基于事件视觉做了全面的阐述。论文主要介绍了事件相机的原理及事件的处理等等。
1.事件相机的原理 异步的监察每个像素感知器的亮度变化情况，从而产生一个事件，多个像素的变化就产生了事件流。（DVS128、DAVIS240、ATIS等），输出事件格式为AER格式。
2.事件相机与传统相机的区别 优点：
1)高时间分辨率：能够更快的捕捉亮度的变化，从而不会像传统相机那样产生运动模糊。
2)低延迟：APS相机成像都是按照固定的频率，一般1s/30fps,而事件相机的频率在1kHZ。
3)低能耗：由于事件相机是异步的对每个像素点进行监测，只有当强度变化超过预设阈值时才会产生事件流。故在无运动发生时将会一直处于待机状态。
4)高动态范围：事件相机能够在更大的动态范围内对运动做出响应（120dBvs60dB)。
5)低冗余：只输出动态变化信号，其背景等信息不会输出，故相比APS数据少了很多冗余数据。
3.事件的表示方式 1）一维：将事件作为一个一个单独的脉冲序列进行处理（event-by-event)，常采用基于概率模型的处理方式或SNN（脉冲神经网络进行处理）。
2）二维：二维的表示方式也是最常用的方式，因为它可以与我们传统的方式就行模型的学习。常用的处理方式有：堆积成帧（按事件个数、按时间间隔）、Time-Surface（时间平面）、Grid（随机采样生成图）。
3）三维：直接将T作为三维轴，然后使用PointNet等直接进行特征学习。
应用 由于时间相机只是一种样本的获取方式，所以只要对事件进行合理的处理，其可以应用到APS所能应用的所有的应用中。例如：目标检测、目标跟踪、姿态估计等。
二、A Low Power, Fully Event-Based Gesture Recognition System 主要贡献 1.该文第一次实现了基于事件的端到端的姿态识别系统。
2.发布了真实场景下的用于姿态识别的数据集（DvsGesture)
三、Real-Time 6DOF Pose Relocalization for Event Cameras withStacked Spatial LSTM Networks 主要贡献 1.提出了SP-LSTM 网络结构，按时间片的方式来集成时间帧，采用CNN提取深度特征信息，后采用LSTM。（CNN采用VGG16,LSTM2个隐藏层，加入dropout=0.5防止过拟合）。
2.使用对象损失函数，L(I) =‖ˆp−p‖2&#43;‖ˆq−q‖2 （ˆp、ˆq是从网络中预测的位置和方向在哪里）
3.提出了数据集划分的新方式，将一个时间的帧序列前70%作为训练集，后30%作为测试集。
code https://github.com/nqanh/pose_relocalization（仅采用6-DOF数据集的事件作为输入） 源码是kears，利用pytorch复现了下（误差在0.1左右），效果不是很好（容易过拟合）。
下面两篇发散论文是这篇论文网络结构的启发： ①、Image-based localization using LSTMs for structured feature correlation
主要贡献 1.利用CNN &#43; LSTM 对相机的位置进行回归预测。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/241846b3c8f8bd3aa9b795a71c0d5438/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-11T21:06:26+08:00" />
<meta property="article:modified_time" content="2020-11-11T21:06:26+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Event-based vision</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Eventbased_vision__0"></a>Event-based vision 论文阅读笔记</h2> 
<p>一下内容仅仅是记录自己的论文阅读。</p> 
<h3><a id="Eventbased_Vision_A_Survey_2"></a>一、Event-based Vision: A Survey</h3> 
<p>该论文属于综述性文章，对基于事件视觉做了全面的阐述。论文主要介绍了事件相机的原理及事件的处理等等。</p> 
<h4><a id="1_5"></a>1.事件相机的原理</h4> 
<p>异步的监察每个像素感知器的亮度变化情况，从而产生一个事件，多个像素的变化就产生了事件流。（DVS128、DAVIS240、ATIS等），输出事件格式为<a href="https://inivation.github.io/inivation-docs/Software%20user%20guides/AEDAT_file_formats.html#aedat-20" rel="nofollow">AER格式</a>。<br> <img src="https://images2.imgbox.com/c2/3c/BGMRMWXv_o.gif" alt="在这里插入图片描述"></p> 
<h4><a id="2_11"></a>2.事件相机与传统相机的区别</h4> 
<p>优点：<br> 1)高时间分辨率：能够更快的捕捉亮度的变化，从而不会像传统相机那样产生运动模糊。<br> 2)低延迟：APS相机成像都是按照固定的频率，一般1s/30fps,而事件相机的频率在1kHZ。<br> 3)低能耗：由于事件相机是异步的对每个像素点进行监测，只有当强度变化超过预设阈值时才会产生事件流。故在无运动发生时将会一直处于待机状态。<br> 4)高动态范围：事件相机能够在更大的动态范围内对运动做出响应（120dBvs60dB)。<br> 5)低冗余：只输出动态变化信号，其背景等信息不会输出，故相比APS数据少了很多冗余数据。</p> 
<p><img src="https://images2.imgbox.com/83/8e/aC4XwFPd_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="3_22"></a>3.事件的表示方式</h4> 
<p>1）一维：将事件作为一个一个单独的脉冲序列进行处理（event-by-event)，常采用基于概率模型的处理方式或SNN（脉冲神经网络进行处理）。<br> 2）二维：二维的表示方式也是最常用的方式，因为它可以与我们传统的方式就行模型的学习。常用的处理方式有：堆积成帧（按事件个数、按时间间隔）、Time-Surface（时间平面）、Grid（随机采样生成图）。<br> 3）三维：直接将T作为三维轴，然后使用PointNet等直接进行特征学习。</p> 
<h4><a id="_26"></a>应用</h4> 
<p>由于时间相机只是一种样本的获取方式，所以只要对事件进行合理的处理，其可以应用到APS所能应用的所有的应用中。例如：目标检测、目标跟踪、姿态估计等。</p> 
<h3><a id="A_Low_Power_Fully_EventBased_Gesture_Recognition_System_29"></a>二、A Low Power, Fully Event-Based Gesture Recognition System</h3> 
<h4><a id="_31"></a>主要贡献</h4> 
<p>1.该文第一次实现了基于事件的端到端的姿态识别系统。<br> 2.发布了真实场景下的用于姿态识别的数据集（<a href="http://research.ibm.com/dvsgesture/" rel="nofollow">DvsGesture</a>)</p> 
<h3><a id="RealTime_6DOF_Pose_Relocalization_for_Event_Cameras_withStacked_Spatial_LSTM_Networks_35"></a>三、Real-Time 6DOF Pose Relocalization for Event Cameras withStacked Spatial LSTM Networks</h3> 
<h4><a id="_36"></a>主要贡献</h4> 
<p>1.提出了SP-LSTM 网络结构，按时间片的方式来集成时间帧，采用CNN提取深度特征信息，后采用LSTM。（CNN采用VGG16,LSTM2个隐藏层，加入dropout=0.5防止过拟合）。<br> 2.使用对象损失函数，L(I) =‖ˆp−p‖2+‖ˆq−q‖2 （ˆp、ˆq是从网络中预测的位置和方向在哪里）<br> 3.提出了数据集划分的新方式，将一个时间的帧序列前70%作为训练集，后30%作为测试集。</p> 
<h4><a id="code_42"></a>code</h4> 
<p><a href="https://github.com/nqanh/pose_relocalization">https://github.com/nqanh/pose_relocalization</a>（仅采用6-DOF数据集的事件作为输入） <br> 源码是kears，利用pytorch复现了下（误差在0.1左右），效果不是很好（容易过拟合）。</p> 
<h4><a id="_46"></a>下面两篇发散论文是这篇论文网络结构的启发：</h4> 
<p>①、Image-based localization using LSTMs for structured feature correlation</p> 
<h5><a id="_49"></a>主要贡献</h5> 
<p>1.利用CNN + LSTM 对相机的位置进行回归预测。<br> <img src="https://images2.imgbox.com/a7/9a/aNlIamdC_o.png" alt="在这里插入图片描述"><br> CNN特征提取采用的是GoogleNet，直接将2048特征作为LSTM的输入，发现效果不是太理想，然后Reshape成（32*64），已上下左右四种切片方式输入到四个不同的LSTM中，然后将四个输出连接在一起送入全连接层进行回归预测。<br> 2.使用空间损失函数，<img src="https://images2.imgbox.com/33/40/hDNXBNyV_o.png" alt="在这里插入图片描述">（ˆp、ˆq是从网络中预测的位置和方向在哪里）<br> 3.与传统的SIFT方法进行对比，提出了6DOF的indoor和outdoor数据集。</p> 
<h3><a id="Focus_Is_All_You_Need_Loss_Functions_For_Eventbased_Vision_57"></a>四、Focus Is All You Need: Loss Functions For Event-based Vision</h3> 
<h4><a id="_58"></a>主要贡献</h4> 
<p>分析介绍了各种损失函数</p> 
<h3><a id="HOTS_A_Hierarchy_of_EventBasedTimeSurfaces_for_Pattern_Recognition_61"></a>五、HOTS: A Hierarchy of Event-BasedTime-Surfaces for Pattern Recognition</h3> 
<h4><a id="_62"></a>主要贡献</h4> 
<p>介绍了一种全新的事件处理方式，TS（Time-Surface）。</p> 
<h4><a id="TimeSurface_65"></a>Time-Surface原理</h4> 
<p><img src="https://images2.imgbox.com/8f/b1/UDyaYpbb_o.png" alt=""><br> <img src="https://images2.imgbox.com/fb/0f/vwIyOiw6_o.png" alt="在这里插入图片描述"><br> 简言之，就是对时空信息进行编码，就是在固定的事件领域内每个像素只记录一个事件。当有新的事件到达时，会利用一个衰减核函数对其空间领域内作比较，从而产生一个类似权重值的东西来记录当前到达的事件（包含两部分，正负极性事件分开）。</p> 
<h4><a id="HATS_Histograms_of_Averaged_Time_Surfaces_for_RobustEventbased_Object_Classification_69"></a>HATS: Histograms of Averaged Time Surfaces for RobustEvent-based Object Classification</h4> 
<p>该论文是基于TS思想做出的改进。</p> 
<h5><a id="_71"></a>主要贡献</h5> 
<p>1.改进了TS算法，由于原始的处理方法，对噪声事件没有做太多的处理，这就导致了算法对噪声非常的敏感，因此便提出了Local Memory Time Surface（他存储记忆了之前的事件，考虑时空窗口中所有过去事件的贡献，这样，考虑到计算的嘈杂事件的比率较小，结果更好地描述了事件基础流的真实动态。），然后通过对邻域中所有事件的时间面进行空间平均，可以进一步对时间面进行正则化。<br> 2.提出了真实场景的二分类数据集<a href="http://www.prophesee.ai/dataset-n-cars/" rel="nofollow">N-CARS</a> 。</p> 
<h3><a id="_EVFlowNet_SelfSupervised_Optical_FlowEstimation_for_Eventbased_Cameras_74"></a>六、 EV-FlowNet: Self-Supervised Optical FlowEstimation for Event-based Cameras</h3> 
<h4><a id="_75"></a>主要贡献</h4> 
<p>将事件表示为四个通道的img frame。前两个通道为像素的正负事件的计数（heatmap），后两个通道为像素的时间信息，弥补了缺失时间特征信息。</p> 
<h3><a id="EVGait_Eventbased_Robust_Gait_Recognition_using_Dynamic_Vision_Sensors_78"></a>七、EV-Gait: Event-based Robust Gait Recognition using Dynamic Vision Sensors</h3> 
<h4><a id="_79"></a>主要贡献</h4> 
<p>1.介绍了一种全新的噪声抑制方法。<br> 2.与其他各种先前提出的噪声处理方法做出了对比，其效果明显优于之前的工作。<br> 3.提出了一个用于分类的数据集DVS128-Gait.</p> 
<h4><a id="_84"></a>噪声抑制原理</h4> 
<p>基本原理就是根据运动的属性所引出，由于运动的非独立性（假如该点为运动点，那么该点附近的点也应该与它具有相同的运动属性（运动面）），基于这个假设，作者提出了采用最小二乘法的方式来拟合运动平面，从而判断该点是否为噪声点。<br> 假设具有同一运动事件的运动属性有如下运动平面：<br> <img src="https://images2.imgbox.com/87/60/nHbW6L0a_o.png" alt="在这里插入图片描述"><br> 然后采用最小二乘法的方法，对运动平面的参数进行拟合（a,b,c,d)。<br> <img src="https://images2.imgbox.com/23/d7/mAChBXk1_o.png" alt="在这里插入图片描述"><br> 拟合空间领域为（3，3），时间领域为（-1，1）。假设经过拟合后，平面存在，那么就有唯一的（a,b,c,d）。使得：<br> <img src="https://images2.imgbox.com/c1/7b/X3I6vkiw_o.png" alt="在这里插入图片描述"><br> 从而就可以判断该点在，x,y 方向上的运动。如果0&lt;|v|&lt; Vmax，那么该点就为事件事件，否则为噪声事件直接丢弃。</p> 
<h3><a id="Spacetime_Event_Clouds_for_Gesture_Recognition_from_RGB_Cameras_to_EventCameras_94"></a>八、Space-time Event Clouds for Gesture Recognition: from RGB Cameras to EventCameras</h3> 
<p>该论文提出了使用3D表示的方式处理事件流。利用PointNet来进行DvsGesture数据集的分类。</p> 
<h4><a id="_96"></a>主要贡献</h4> 
<p>1.将事件表示为（x,y,t）三维点云模型，利用PointNet进行分类。<br> 2.分别对采样事件（0.25，0.5，1.0)s 做出了实验对比（再从固定时间帧中随机抽取部分事件进行训练），将acc提到97.08%。</p> 
<h3><a id="Dynamic_Graph_CNN_for_EventCamera_BasedGesture_Recognition_100"></a>九、Dynamic Graph CNN for Event-Camera BasedGesture Recognition</h3> 
<p>该论文提出了使用3D表示的方式处理事件流。利用DGCNN 来进行DvsGesture和DHP19数据集的分类。</p> 
<h4><a id="_102"></a>主要贡献</h4> 
<p>1.为了解决PointNet和PointNet++只能针对每个点进行特征提取，从而失去了部分空间信息的缺陷，提出了EdgeConv的方式提取特征，从而提出了DGCNN 。<br> 2.在DvsGesture和DHP19数据集做了对比实验，acc分别为（98.56和95.94）。</p> 
<h3><a id="Eventbased_Gesture_Recognition_with_Dynamic_Background_Suppression_using_Smartphone_Computational_Capabilities_106"></a>十、Event-based Gesture Recognition with Dynamic Background Suppression using Smartphone Computational Capabilities</h3> 
<h4><a id="_107"></a>主要贡献</h4> 
<p>1.提出了一个动态背景的手势识别数据集<br> 2.提出了一种新的背景噪声抑制方式</p> 
<h4><a id="_110"></a>背景抑制原理</h4> 
<p>动态背景抑制（DBS）使用简单的思想，即物体离摄像机越近，它将产生的事件越多，因为其视在运动比远处的物体更重要。 通过此属性，可以将焦平面内的相对局部活动链接到深度。 低事件相对活性可以与背景相关联并因此被消除，而相对高活性区域可以与前景相对应。<br> 每个像素单元c，其中活动用Ac表示。对于属于c像素发出的每个传入的event ,ek=(xk,tk,pk)，我们可以应用其活动Ac 作以下更新：<br> <img src="https://images2.imgbox.com/66/0c/XGBmhtzQ_o.png" alt="在这里插入图片描述"><br> tk表示当前像素事件到达的时间，tc是当前像素的上一次事件的时间。Tb是衰减时间常数（自定义）。<br> 然后，我们可以计算所有单元的平均活动性A。 仅当满足以下条件时，才将传入的event ，ek = {xk，tk，pk）发送给机器学习模块。<br> <img src="https://images2.imgbox.com/ca/c7/u5OiNUeQ_o.png" alt="在这里插入图片描述"><br> α是过滤常量（自定义），AT是最低前景活动的阈值。为每个传入事件计算活动和阈值，从而在传入事件的时间解析中启用或禁用给定单元。<br> 实验中：tb=300μs，α=2，AT=5.</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d65f83835995ead9768005872ffd7d02/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">微积分(一)笔记1</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b89c980d9d9bf40785fff0f685fd9440/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何获取级联选择器选中的值</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>