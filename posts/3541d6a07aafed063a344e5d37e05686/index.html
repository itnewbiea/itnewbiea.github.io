<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>卷积和反卷积输出的计算公式 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="卷积和反卷积输出的计算公式" />
<meta property="og:description" content="参考：https://zhuanlan.zhihu.com/p/39240159
推荐：一个可视化卷积和反卷积的项目：https://github.com/vdumoulin/conv_arithmetic
1 反卷积计算 pytorch中反卷积的函数为：
class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0,output_padding=0,groups=1,bias=True, dilation=1) # 一般为以下 nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding) 参数的含义如下：
in_channels(int) – 输入信号的通道数out_channels(int) – 卷积产生的通道数kernel_size(int or tuple) - 卷积核的大小stride(int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。padding(int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*paddingoutput_padding(int or tuple, optional) - 输出边补充0的层数，高宽都增加paddinggroups(int, optional) – 从输入通道到输出通道的阻塞连接数bias(bool, optional) - 如果bias=True，添加偏置dilation(int or tuple, optional) – 卷积核元素之间的间距 而对于输入输出的计算，首先参数out_channels指定输出的通道数，即一定是output_size*output_size*out_channels，故主要计算输出的output_size，公式如下：
一般来说用以下公式：
例子：
# Generator Code # 生成器 反卷积 class Generator(nn.Module): def __init__(self, ngpu): super(Generator, self)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/3541d6a07aafed063a344e5d37e05686/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-23T20:39:28+08:00" />
<meta property="article:modified_time" content="2021-08-23T20:39:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">卷积和反卷积输出的计算公式</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>参考：<a href="https://zhuanlan.zhihu.com/p/39240159" rel="nofollow">https://zhuanlan.zhihu.com/p/39240159</a></p> 
<p>推荐：一个可视化卷积和反卷积的项目：<a href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a></p> 
<h2>1  反卷积计算</h2> 
<p>        pytorch中反卷积的函数为：</p> 
<pre><code class="language-python">class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0,output_padding=0,groups=1,bias=True, dilation=1)
# 一般为以下
nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)</code></pre> 
<p>        参数的含义如下：</p> 
<ul><li>in_channels(int) – 输入信号的通道数</li><li>out_channels(int) – 卷积产生的通道数</li><li>kernel_size(int or tuple) - 卷积核的大小</li><li>stride(int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。</li><li>padding(int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*padding</li><li>output_padding(int or tuple, optional) - 输出边补充0的层数，高宽都增加padding</li><li>groups(int, optional) – 从输入通道到输出通道的阻塞连接数</li><li>bias(bool, optional) - 如果bias=True，添加偏置</li><li>dilation(int or tuple, optional) – 卷积核元素之间的间距</li></ul> 
<p>        而对于输入输出的计算，首先参数out_channels指定输出的通道数，即一定是output_size*output_size*out_channels，故主要计算输出的output_size，公式如下：</p> 
<p>        <img alt="outputsize=(inputsize-1)*stride-2*padding+kernelsize+outputpadding" class="mathcode" src="https://images2.imgbox.com/cb/76/JUQHPCAE_o.png"></p> 
<p>        一般来说用以下公式：</p> 
<p>         <img alt="outputsize=(inputsize-1)*stride-2*padding+kernelsize" class="mathcode" src="https://images2.imgbox.com/c0/7a/SxGtAdSD_o.png"></p> 
<p> 例子：</p> 
<pre><code class="language-python"># Generator Code
# 生成器 反卷积
class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu  # Number of GPUs available. Use 0 for CPU mode. 可用的 GPUs 数量。
        self.main = nn.Sequential(
            # input is Z, going into a convolution   100维 
            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False), #（1-1）*1+4-2*0=4  
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),
            # state size. (ngf*8) x 4 x 4   4*4*512
            '''
            class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, 
                                                       output_padding=0, groups=1, bias=True, dilation=1)
            '''
            
            
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), # (4-1)*2-2*1+4=8
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # state size. (ngf*4) x 8 x 8   8*8*256
            
            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False), # (8-1)*2-2*1+4=16
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # state size. (ngf*2) x 16 x 16  16*16*128
            
            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False), # (16-1)*2-2*1+4=32
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            # state size. (ngf) x 32 x 32   32*32*64
            
            
            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False), # (32-1)*2-2*1+4=64
            nn.Tanh()
            # state size. (nc) x 64 x 64   64*64*3
        )

    def forward(self, input):
        return self.main(input)</code></pre> 
<h2> <img alt="" height="752" src="https://images2.imgbox.com/17/cf/tMITCB5O_o.png" width="1200">2  卷积计算</h2> 
<pre><code class="language-python">Conv2d(in_channels, out_channels, kernel_size, stride=1,padding=0, dilation=1, groups=1,bias=True, padding_mode=‘zeros’)</code></pre> 
<p>         五个常用参数的含义如下：</p> 
<ul><li>in_channels：输入的通道数目；</li><li>out_channels：输出的通道数目；</li><li>kernel_size：卷积核的大小；</li><li>stride：卷积每次滑动的步长；</li><li>padding：填充，设置在所有边界增加值为 0 的边距的大小。</li></ul> 
<p>        卷积神将网络的计算公式：<strong>N=(W-F+2P)/S+1</strong>，其中N:输出大小，W:输入大小，F:卷积核大小，P:填充值的大小，S:步长大小</p> 
<p style="text-align:center;"><img alt="" height="412" src="https://images2.imgbox.com/3f/6e/NsdmQWsj_o.png" width="676"></p> 
<p> 例子：</p> 
<pre><code class="language-python"># batch norm 和leaky relu函数促进了健康的梯度流
class Discriminator(nn.Module):
    def __init__(self, ngpu):
        super(Discriminator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is (nc) x 64 x 64     128,3*64*64
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False), # 64-4+2/2+1=32  
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf) x 32 x 32  64*32*32
            
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), # 32-4+2/2+1=16
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True), 
            # state size. (ndf*2) x 16 x 16  128*16*16
            
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), # 16-4+2/2+1=8
            nn.BatchNorm2d(ndf * 4), # 256
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*4) x 8 x 8   256*8*8
            
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), # 8-4+2/2+1=4
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*8) x 4 x 4  512*4*4
            
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), # 4-4/2+1=1
            nn.Sigmoid() # 128,1*1024
        )

    def forward(self, input):
        return self.main(input)</code></pre> 
<p> <img alt="" height="711" src="https://images2.imgbox.com/7d/ae/JdwBMPVj_o.png" width="1200"></p> 
<p> 一些点：</p> 
<ul><li>Conv1d：用于文本数据，只对宽度进行卷积，对高度不进行卷积，而Conv2d：用于图像数据，对宽度和高度都进行卷积</li><li>Conv2d(输入通道数, 输出通道数, kernel_size（长和宽）)，当卷积核为方形时，只写一个就可以，但是当卷积核不是方形时，长和宽都要写，如下nn.Conv2d(H,W,....)</li></ul> 
<p></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/124701e4a1259979d9e8a81e912748a2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何完成一个三维引导项目:第一步 手眼标定（眼在手外）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e4524d13a79598538d589629d16783e4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">京训钉自动播放下一章(2021版)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>