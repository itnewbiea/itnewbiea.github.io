<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Linux机器内核参数理解(一) - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Linux机器内核参数理解(一)" />
<meta property="og:description" content="一、fs.aio-nr &amp; fs.aio-max-nr aio-nr is the running total of the number of events specified on the io_setup system call for all currently active aio contexts. If aio-nr reaches aio-max-nr then io_setup will fail with EAGAIN. Note that raising aio-max-nr does not result in the pre-allocation or re-sizing of any kernel data structures. aio-nr shows the current system-wide number of asynchronous io requests. aio-max-nr allows you to change the maximum value aio-nr can grow to." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/8b8e030ac4862af1def01e827ed292ae/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-16T11:27:12+08:00" />
<meta property="article:modified_time" content="2023-03-16T11:27:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Linux机器内核参数理解(一)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<h3>一、fs.aio-nr &amp; fs.aio-max-nr</h3> 
<pre>aio-nr is the running total of the number of events specified on the
io_setup system call for all currently active aio contexts. If aio-nr
reaches aio-max-nr then io_setup will fail with EAGAIN. Note that
raising aio-max-nr does not result in the pre-allocation or re-sizing
of any kernel data structures.
aio-nr shows the current system-wide number of asynchronous io requests.  
aio-max-nr allows you to change the maximum value aio-nr can grow to.
​
aio-nr      当前系统上的异步IO请求个数
aio-max-nr  系统上允许同时进行的最大异步IO请求个数
#查看
$ cat /proc/sys/fs/aio-nr
1536
$ cat /proc/sys/fs/aio-max-nr
65536</pre> 
<h3><a name="t1"></a>二、fs.file-max &amp; fs.file-nr</h3> 
<pre>The value in file-max denotes the maximum number of file-
handles that the Linux kernel will allocate. When you get lots
of error messages about running out of file handles, you might
want to increase this limit.
​
Historically,the kernel was able to allocate file handles
dynamically, but not to free them again. The three values in
file-nr denote the number of allocated file handles, the number
of allocated but unused file handles, and the maximum number of
file handles. Linux 2.6 always reports 0 as the number of free
file handles -- this is not an error, it just means that the
number of allocated file handles exactly matches the number of
used file handles.
​
max-file 表示系统级别的能够打开的文件句柄的数量。是对整个系统的限制，并不是针对用户的。
file-nr 三个值分别是系统级别当前打开的文件句柄数量、已经打开但是未使用的文件句柄数量、系统级别支持的最大文件句柄数量
​
#查看
$ cat /proc/sys/fs/file-max
95296
$ cat /proc/sys/fs/file-nr
992 0   95296</pre> 
<h3><a name="t2"></a>三、fs.nr_open</h3> 
<pre>This denotes the maximum number of file-handles a process can
allocate. Default value is 1024*1024 (1048576) which should be
enough for most machines. Actual limit depends on RLIMIT_NOFILE
resource limit.
​
nr_open 表示每个进程允许打开的最大文件句柄数量。
​
#查看
$ cat /proc/sys/fs/nr_open
1048576</pre> 
<h3><a name="t3"></a>四、kernel.sem</h3> 
<pre>SysV信号量用于对共享内存访问时的锁管理。解决进程间同步、异步的锁问题。
​
[root@centos101 ~]# sysctl -a | grep sem
kernel.sem = 250    32000   32  128
这四个数字分别是：SEMMSL，SEMMNS，SEMOPM，SEMMNI
​
SEMMSL：内核参数，控制每个信号量集的最大信号量数。
SEMMNS：内核参数，控制系统范围内能使用的最大信号量数
SEMOPM：semop()函数（内核函数，用来操作信号量）每次调用锁能操作的一个信号量集中的最大信号量。
SEMMNI：内核中信号量集的最大数量。
内核中信号量集是由很多个信号量组成的，操作系统可以对一个信号量集做一个原子操作，所有系统中信号量的最大数目=每个信号量集的大小*信号量集的个数。即：SEMMNS=SEMMSL*SEMMNI。
而SEMOPM指信号量集进行一个原子操作时，可以操作的信号量数，通常：SEMOPM大于SEMMSL
#查看
$ cat /proc/sys/kernel/sem
250 32000   32  128</pre> 
<h3><a name="t4"></a>五、kernel.shmmax &amp; kernel.shmall &amp; kernel.shmmni</h3> 
<pre>kernel.shmmax:表示单个共享内存段(shared memory segment)的最大值，单位bytes
kernel.shmall:表示整个系统内可以为共享内存配置的页面数。Defines the total amount of shared memory pages that can be used on the system at one time. A page is 4096 bytes on the AMD64 and Intel 64 architecture
kernel.shmmni:表示整个系统内共享内存段最大个数。Defines the system-wide maximum number of shared memory segments. The default value is 4096 on all systems.
#查看
$ cat /proc/sys/kernel/shmmax
18446744073692774399
$ cat /proc/sys/kernel/shmall
18446744073692774399
$ cat /proc/sys/kernel/shmmni
4096</pre> 
<h3><a name="t5"></a>六、kernel.msgmni &amp; kernel.msgmnb &amp; kernel.msgmax</h3> 
<pre>kernel.msgmni:表示系统内最大消息队列个数   Defines the maximum number of message queue identifiers, and therefore the maximum number of queues.
kernel.msgmnb:表示单个消息队列的最大容量   Defines the maximum size in bytes of a single message queue.
kernel.msgmax:表示消息队列中单个消息的最大容量   Defines the maximum allowable size in bytes of any single message in a message queue. 
​
#查看
$ cat /proc/sys/kernel/msgmni
1942
$ cat /proc/sys/kernel/msgmnb
16384
$ cat /proc/sys/kernel/msgmax
8192</pre> 
<h3><a name="t6"></a>七、net.core.netdev_max_backlog</h3> 
<pre>net.core.netdev_max_backlog: 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。.Sets the maximum number of packets, queued on the INPUT side, when the interface receives packets faster than kernel can process them.
#查看
$ cat /proc/sys/net/core/netdev_max_backlog
10000</pre> 
<h3><a name="t7"></a>八、net.core.rmem_default &amp; net.core.rmem_max &amp; net.core.wmem_default &amp; net.core.wmem_max</h3> 
<pre>Set the max OS send buffer size (wmem) and receive buffer size (rmem) to 12 MB for queues on all protocols. In other words set the amount of memory that is allocated for each TCP socket when it is opened or created while transferring files
​
net.core.rmem_default:默认的socket套接字接收数据包内存大小(bytes)   Defines the default receive window size. Default Socket Receive Buffer
net.core.rmem_max:最大的socket套接字接收数据包内存大小(bytes)   Defines the default receive window size. Default Socket Receive Buffer
net.core.wmem_default:默认的socket套接字发送数据包内存大小(bytes) Default Socket Send Buffer
net.core.wmem_max:最大的socket套接字发送数据包内存大小(bytes)   Maximum Socket Send Buffer
​
#查看
$ cat /proc/sys/net/core/rmem_default
262144
$ cat /proc/sys/net/core/rmem_max
4194304
$ cat /proc/sys/net/core/wmem_default
262144
$ cat /proc/sys/net/core/wmem_max
4194304</pre> 
<h3><a name="t8"></a>九、net.core.somaxconn &amp; net.ipv4.tcp_max_syn_backlog</h3> 
<pre>net.core.somaxconn: 默认值为128,Increase number of incoming connections. somaxconn defines the number of request_sock structures allocated per each listen call. The queue is persistent through the life of the listen socket.
net.ipv4.tcp_max_syn_backlog:默认值为128, Maximum number of remembered connection requests, which did not yet receive an acknowledgment from connecting client.
​
socket接收的所有连接都是存放在队列类型的数据结构中，关键问题是这种队列有两个，而且其长度都是可以设置的。
分别是下面两个内核参数：
/proc/sys/net/ipv4/tcp_max_syn_backlog
/proc/sys/net/core/somaxconn
其中：
tcp_max_syn_backlog是指定所能接受SYN同步包的最大客户端数量，即半连接上限；
somaxconn是指服务端所能accept即处理数据的最大客户端数量，即完成连接上限。
对于没有调优的新装的centOS6.5系统，这两个参数的值都是128。
这么描述虽然精确，但是没有一定基础，不熟练网络编程的人理解起来很费劲。
打个简单的比方： 
某某发布公告要邀请四海之内若干客人到场参加酒席。客人参加酒席分为两个步骤：
1、到大厅；
2、找到座位(吃东西，比如糖果、饭菜、酒等)。
tcp_max_syn_backlog用于指定酒席现场面积允许容纳多少人进来；
somaxconn用于指定有多少个座位。
显然tcp_max_syn_backlog&gt;=somaxconn。
如果要前来的客人数量超过tcp_max_syn_backlog，那么多出来的人虽然会跟主任见面握手，但是要在门外等候；
如果到大厅的客人数量大于somaxconn，那么多出来的客人就会没有位置坐(必须坐下才能吃东西)，只能等待有人吃完有空位了才能吃东西。
那么问题来了：
somaxconn是内核里的参数，listen函数有个参数backlog，如果在listen方法里面指定该参数大于somaxconn的值，重新编译并启动程序，服务端所能接收的完整的连接数上限是backlog呢还是somaxconn？
答案很简单，listen方法指定的backlog是在用户态指定的，内核态的参数优先级高于用户态的参数，所以即使在listen方法里面指定backlog是一个大于somaxconn的值，socket在内核态运行时还会检查一次somaxconn，如果连接数超过somaxconn就会等待。
就相当于主人指定了能有多少座位没用，客人到了现场，准备入座时，还要看酒店的客户经理判断能有多少个座位。
结论：
在没有调优的centOS6.5版本的服务器上，由于受到系统级别的限制，在该服务器上运行的服务端程序，在同一时间，最大只能接受128个客户端发起持久连接，并且只能处理128个客户端的数据通信。
​
#查看
cat /proc/sys/net/core/somaxconn
4096
$ cat /proc/sys/net/ipv4/tcp_max_syn_backlog
4096</pre> 
<h3><a name="t9"></a>十、net.ipv4.tcp_keepalive_time &amp; net.ipv4.tcp_keepalive_intvl &amp; net.ipv4.tcp_keepalive_probes</h3> 
<pre>$ cat /proc/sys/net/ipv4/tcp_keepalive_time
7200
$ cat /proc/sys/net/ipv4/tcp_keepalive_intvl
75
$ cat /proc/sys/net/ipv4/tcp_keepalive_probes
9
The first two parameters are expressed in seconds, and the last is the pure number. This means that the keepalive routines wait for two hours (7200 secs) before sending the first keepalive probe, and then resend it every 75 seconds. If no ACK response is received for nine consecutive times, the connection is marked as broken.
​
#查看
cat /proc/sys/net/ipv4/tcp_keepalive_time
60
$ cat /proc/sys/net/ipv4/tcp_keepalive_intvl
20
$ cat /proc/sys/net/ipv4/tcp_keepalive_probes
3</pre> 
<h3><a name="t10"></a>十一、net.ipv4.tcp_mem &amp; net.ipv4.tcp_rmem &amp; net.ipv4.tcp_wmem</h3> 
<pre>sysctl -w net.ipv4.tcp_mem='8388608 8388608 8388608' 
TCP Autotuning setting. "The tcp_mem variable defines how the TCP stack should behave when it comes to memory usage. ... The first value specified in the tcp_mem variable tells the kernel the low threshold. Below this point, the TCP stack do not bother at all about putting any pressure on the memory usage by different TCP sockets. ... The second value tells the kernel at which point to start pressuring memory usage down. ... The final value tells the kernel how many memory pages it may use maximally. If this value is reached, TCP streams and packets start getting dropped until we reach a lower memory usage again. This value includes all TCP sockets currently in use." 
net.ipv4.tcp_mem:确定TCP栈应该如何反映内存使用，每个值的单位都是内存页（通常是4KB）。第一个值是内存使用的下限；第二个值告诉内核到达此值时开始压缩内存使用；第三个值是内存使用的上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。对于较大的BDP可以增大这些值（注意，其单位是内存页而不是字节,内存页通常是4KB）。
/proc/sys/net/ipv4/tcp_rmem: 为自动调优定义socket使用的内存。第一个值是为socket接收缓冲区分配的最少字节数；第二个值是默认值（该值会被rmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；第三个值是接收缓冲区空间的最大字节数（该值会被rmem_max覆盖）。
/proc/sys/net/ipv4/tcp_wmem: 为自动调优定义socket使用的内存。第一个值是为socket发送缓冲区分配的最少字节数；第二个值是默认值（该值会被wmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；第三个值是发送缓冲区空间的最大字节数（该值会被wmem_max覆盖）。
​
#查看
$ cat /proc/sys/net/ipv4/tcp_mem
21726   28969   43452
$ cat /proc/sys/net/ipv4/tcp_rmem
4096    87380   6291456
$ cat /proc/sys/net/ipv4/tcp_wmem
4096    16384   4194304</pre> 
<p>转载于:https://my.oschina.net/207miner/blog/2996138</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/66c76fdec034ebaf09527bda24a40f76/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AppCompatTextView文本设置为大写</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e6158c597c48695caab53870ad99a4e6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C# Winform应用程序重启</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>