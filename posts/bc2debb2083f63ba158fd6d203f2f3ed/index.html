<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文学习】mixup系列（mixup、cutMix、manifold mixup、patchUp、saliencyMix、puzzleMix、co-Mixup、FMix） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【论文学习】mixup系列（mixup、cutMix、manifold mixup、patchUp、saliencyMix、puzzleMix、co-Mixup、FMix）" />
<meta property="og:description" content="mixup是一种简单而又有效的数据增强方法，涨点利器，该方法在图像、文本、语音、推荐、GAN、对抗样本防御等多个领域都能显著提高效果。mixup论文被ICLR2018收录，后续又出现了一系列改进方法。我在本文中详细分析讨论了mixup，并介绍了几种典型的改进方法。
文章目录 一、mixup1, mixup方法2, mixup的讨论2.1 mixup效果如何？2.2 为什么使用Beta分布？2.3 参数 α \alpha α有何影响，如何选择？2.4 是否可以使用多个样本混合？2.5 为什么要使用凸组合？ 二、mixup的后续改进1, 各改进方法简介1.1 cutMix1.2 manifold mixup1.3 patchUp1.4 puzzleMix1.5 saliency Mix1.6 fMix1.7 co-Mix 2, 各改进方法对比 三、读后感 一、mixup 论文(ICLR2018收录，arxiv发表时间：2017-10-25)：https://arxiv.org/abs/1710.09412
源码(698星)：https://github.com/facebookresearch/mixup-cifar10
1, mixup方法 mixup对两个样本-标签数据对按比例相加后生成新的样本-标签数据：
x ~ = λ x i &#43; ( 1 − λ ) x j \tilde{x} = \lambda x_{i} &#43; (1- \lambda) x_{j} x~=λxi​&#43;(1−λ)xj​, 其中 x x x为输入向量
y ~ = λ y i &#43; ( 1 − λ ) y j \tilde{y} = \lambda y_{i} &#43; (1- \lambda) y_{j} y~​=λyi​&#43;(1−λ)yj​, 其中 y y y为标签的one-hot编码" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/bc2debb2083f63ba158fd6d203f2f3ed/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-24T20:25:49+08:00" />
<meta property="article:modified_time" content="2022-06-24T20:25:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文学习】mixup系列（mixup、cutMix、manifold mixup、patchUp、saliencyMix、puzzleMix、co-Mixup、FMix）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atelier-sulphurpool-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>  mixup是一种简单而又有效的数据增强方法，涨点利器，该方法在图像、文本、语音、推荐、GAN、对抗样本防御等多个领域都能显著提高效果。mixup论文被ICLR2018收录，后续又出现了一系列改进方法。我在本文中详细分析讨论了mixup，并介绍了几种典型的改进方法。<br> </p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#mixup_7" rel="nofollow">一、mixup</a></li><li><ul><li><a href="#1_mixup_10" rel="nofollow">1, mixup方法</a></li><li><a href="#2_mixup_33" rel="nofollow">2, mixup的讨论</a></li><li><ul><li><a href="#21_mixup_34" rel="nofollow">2.1 mixup效果如何？</a></li><li><a href="#22_Beta_41" rel="nofollow">2.2 为什么使用Beta分布？</a></li><li><a href="#23_alpha_56" rel="nofollow">2.3 参数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
          
           
            
            
              α 
             
            
           
             \alpha 
            
           
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>有何影响，如何选择？</a></li><li><a href="#24__66" rel="nofollow">2.4 是否可以使用多个样本混合？</a></li><li><a href="#25__76" rel="nofollow">2.5 为什么要使用凸组合？</a></li></ul> 
  </li></ul> 
  </li><li><a href="#mixup_83" rel="nofollow">二、mixup的后续改进</a></li><li><ul><li><a href="#1__84" rel="nofollow">1, 各改进方法简介</a></li><li><ul><li><a href="#11_cutMix_86" rel="nofollow">1.1 cutMix</a></li><li><a href="#12_manifold_mixup_90" rel="nofollow">1.2 manifold mixup</a></li><li><a href="#13_patchUp_94" rel="nofollow">1.3 patchUp</a></li><li><a href="#14_puzzleMix_101" rel="nofollow">1.4 puzzleMix</a></li><li><a href="#15_saliency_Mix_106" rel="nofollow">1.5 saliency Mix</a></li><li><a href="#16_fMix_111" rel="nofollow">1.6 fMix</a></li><li><a href="#17_coMix_117" rel="nofollow">1.7 co-Mix</a></li></ul> 
   </li><li><a href="#2__122" rel="nofollow">2, 各改进方法对比</a></li></ul> 
  </li><li><a href="#_166" rel="nofollow">三、读后感</a></li></ul> 
</div> 
<p></p> 
<hr color="#000000" size='1"'> 
<h2><a id="mixup_7"></a>一、mixup</h2> 
<p>论文(ICLR2018收录，arxiv发表时间：2017-10-25)：<a href="https://arxiv.org/abs/1710.09412" rel="nofollow">https://arxiv.org/abs/1710.09412</a><br> 源码(698星)：<a href="https://github.com/facebookresearch/mixup-cifar10">https://github.com/facebookresearch/mixup-cifar10</a></p> 
<h3><a id="1_mixup_10"></a>1, mixup方法</h3> 
<p>  mixup对两个样本-标签数据对按比例相加后生成新的样本-标签数据：<br>   <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          ~ 
         
        
       
         = 
        
       
         λ 
        
        
        
          x 
         
        
          i 
         
        
       
         + 
        
       
         ( 
        
       
         1 
        
       
         − 
        
       
         λ 
        
       
         ) 
        
        
        
          x 
         
        
          j 
         
        
       
      
        \tilde{x} = \lambda x_{i} + (1- \lambda) x_{j} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.66786em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span class="" style="top: -3.35em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.22222em;">~</span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault">λ</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord mathdefault">λ</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>, 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>为输入向量<br>   <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          y 
         
        
          ~ 
         
        
       
         = 
        
       
         λ 
        
        
        
          y 
         
        
          i 
         
        
       
         + 
        
       
         ( 
        
       
         1 
        
       
         − 
        
       
         λ 
        
       
         ) 
        
        
        
          y 
         
        
          j 
         
        
       
      
        \tilde{y} = \lambda y_{i} + (1- \lambda) y_{j} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8623em; vertical-align: -0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span><span class="" style="top: -3.35em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">~</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">λ</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord mathdefault">λ</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>, 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         y 
        
       
      
        y 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span></span></span>为标签的one-hot编码<br>   <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         λ 
        
       
         ∈ 
        
       
         [ 
        
       
         0 
        
       
         , 
        
       
         1 
        
       
         ] 
        
       
      
        \lambda \in [0, 1] 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.73354em; vertical-align: -0.0391em;"></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>是概率值，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         λ 
        
       
         ∼ 
        
       
         B 
        
       
         e 
        
       
         t 
        
       
         a 
        
       
         ( 
        
       
         α 
        
       
         , 
        
       
         α 
        
       
         ) 
        
       
      
        \lambda \sim Beta(\alpha,\alpha) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mclose">)</span></span></span></span></span> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         即 
        
       
         λ 
        
       
         服 
        
       
         从 
        
       
         参 
        
       
         数 
        
       
         都 
        
       
         为 
        
       
         α 
        
       
         的 
        
       
         B 
        
       
         e 
        
       
         t 
        
       
         a 
        
       
         分 
        
       
         布 
        
       
      
        即\lambda 服从参数都为\alpha的Beta分布 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord cjk_fallback">即</span><span class="mord mathdefault">λ</span><span class="mord cjk_fallback">服</span><span class="mord cjk_fallback">从</span><span class="mord cjk_fallback">参</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">都</span><span class="mord cjk_fallback">为</span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mord cjk_fallback">的</span><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord cjk_fallback">分</span><span class="mord cjk_fallback">布</span></span></span></span></span><br> 核心代码如下，即插即用：</p> 
<pre><code class="prism language-c">criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">CrossEntropyLoss</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> x<span class="token punctuation">,</span> y in train_loader<span class="token operator">:</span>
    x<span class="token punctuation">,</span> y <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token function">cuda</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span><span class="token function">cuda</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">Mixup inputs<span class="token punctuation">.</span></span></span>
    lam <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span><span class="token function">beta</span><span class="token punctuation">(</span>alpha<span class="token punctuation">,</span> alpha<span class="token punctuation">)</span>
    index <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token function">randperm</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">cuda</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    mixed_x <span class="token operator">=</span> lam <span class="token operator">*</span> x <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> lam<span class="token punctuation">)</span> <span class="token operator">*</span> x<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">]</span>
    <span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">Mixup loss<span class="token punctuation">.</span>    </span></span>
    pred <span class="token operator">=</span> <span class="token function">model</span><span class="token punctuation">(</span>mixed_x<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> lam <span class="token operator">*</span> <span class="token function">criterion</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> lam<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">criterion</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span><span class="token function">zero_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span><span class="token function">backward</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span><span class="token function">step</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>  代码中并没有直接按照公式计算新的标签<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          y 
         
        
          ~ 
         
        
       
      
        \tilde{y} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8623em; vertical-align: -0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span><span class="" style="top: -3.35em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">~</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span class=""></span></span></span></span></span></span></span></span></span>，而是把损失函数也修改成了线性组合的形式。可以自行推导一下，对于交叉熵损失CE，这种方法和计算<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          y 
         
        
          ~ 
         
        
       
      
        \tilde{y} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8623em; vertical-align: -0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span><span class="" style="top: -3.35em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">~</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span class=""></span></span></span></span></span></span></span></span></span>之后再计算一个单独的损失函数是等效的。而这种写法可以直接使用torch.nn.CrossEntropyLoss()（因为它仅支持整数型的y），所以非常方便。</p> 
<h3><a id="2_mixup_33"></a>2, mixup的讨论</h3> 
<h4><a id="21_mixup_34"></a>2.1 mixup效果如何？</h4> 
<p>原文试验数据：<br> <img src="https://images2.imgbox.com/dc/46/h6GuR6hF_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/9a/10/iSi9MUIG_o.png" alt="在这里插入图片描述"><br>   从原文试验结果中可以看出，mixup在ImageNet-2012上面经过200epoch后在几个网络上提高了1.2 ~ 1.5个百分点。在CIFAR-10上提高1.0 ~ 1.4个百分点，在CIFAR-100上提高1.9 ~ 4.5个百分点。可以说是非常惊艳的。<br>   作者继续试验了在语音数据、表格数据和GAN上也都有不错的效果。<br>   作者还发现mixup不仅具有好的泛化性能，也具有很好的鲁棒性，无论对于含噪声标签的数据还是对抗样本攻击，都表现出不错的鲁棒性。</p> 
<h4><a id="22_Beta_41"></a>2.2 为什么使用Beta分布？</h4> 
<p>  使用Beta分布在数学上不是必须的，只是它比较零活方便。Beta分布有两个参数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         β 
        
       
      
        \beta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span></span></span></span></span>，我们看图1中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         β 
        
       
      
        \beta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span></span></span></span></span>相同时的Beta分布概率密度曲线：</p> 
<p><img src="https://images2.imgbox.com/b8/09/nweBFru7_o.png" alt="在这里插入图片描述"></p> 
<center> 
 <b><font size="3">图1. Beta分布概率密度曲线</font></b> 
</center> 
<p>  从图1中可以看出，当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
         = 
        
       
         β 
        
       
         = 
        
       
         1 
        
       
      
        \alpha=\beta=1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>时，等于（0，1）均匀分布；当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
         = 
        
       
         β 
        
       
         &lt; 
        
       
         1 
        
       
      
        \alpha=\beta&lt;1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>时，表现为两头的概率大，中间的概率小，当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
         = 
        
       
         β 
        
       
         → 
        
       
         0 
        
       
      
        \alpha=\beta \to 0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span></span></span></span></span>时，相当于{0，1}二项分布，要么取0，要么取1，等于原始数据没有增强，也就是论文中所说的经验风险最小化ERM；当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
         = 
        
       
         β 
        
       
         &gt; 
        
       
         1 
        
       
      
        \alpha=\beta&gt;1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>时，表现为两头概率小，中间概率大，类似正态分布，当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
         = 
        
       
         β 
        
       
         → 
        
       
         ∞ 
        
       
      
        \alpha=\beta\to\infty 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord">∞</span></span></span></span></span>时，概率恒等于0.5，相当于两个样本各取一半。所以使用Beta分布相当灵活，只需要调整参数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
         , 
        
       
         β 
        
       
      
        \alpha,\beta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span></span></span></span></span>的值，就可以得到多样化的[0,1]区间内的概率分布，使用非常方便。</p> 
<p>  我用PreActResNet18在CIFAR-10上进行了几种分布的对比试验，试验共训练200epoch，学习率lr=0.1, 100epoch和150epoch时分别缩减到1/10，momentum=0.9, weight_decay=1e-4。本文以下所有试验参数配置相同。</p> 
<p><img src="https://images2.imgbox.com/c9/33/eSwPyu3s_o.png" alt="在这里插入图片描述"></p> 
<center> 
 <b><font size="3">图2. mixup中分别使用Beta分布和平均分布、正态分布的对比</font></b> 
</center> 
<p>  从图2中也可以看出，使用均匀分布或者正态分布和使用Beta分布相应参数的效果基本差不多。注意其中正态分布经过了truncate到（0，1）区间，这样会有一些样本等于是直接用了原始样本而没有进行mixup，所以效果会比其他几条线稍稍差一些。</p> 
<h4><a id="23_alpha_56"></a>2.3 参数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>有何影响，如何选择？</h4> 
<p><img src="https://images2.imgbox.com/65/68/NkUNr3HL_o.png" alt="在这里插入图片描述"></p> 
<center> 
 <b><font size="3">图3. 在CIFAR-10上不同alpha参数的测试集精度</font></b> 
</center> 
<p><em>其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          α 
         
        
          = 
         
        
          0 
         
        
       
         \alpha=0 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span></span></span></span></span>意味着不使用mixup，就是论文中说的ERM方法。</em></p> 
<p>  可以看出，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>在0.2 ~ 2之间效果都差不多，说明mixup对<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>参数并不是很敏感。但如果<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>过小，等于没有进行mixup的原始数据，如果<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>过大，等于所有输入都是各取一半混合，样本的多样性和没有增强的原始数据是一样的，但由于标签不直接，所以学习会慢一些，但最终精度和原始数据差不多。<br>   限于资源，我没有进行ImageNet上的试验，作者指出ImageNet上<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>在0.2 ~ 0.4之间效果更好，这可能意味着ImageNet上数据已经够多，应更多的保留原始数据成分，适当少用一些混合，效果更好。所以在不同的数据集也有必要对<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>进行一定的调参。<br>   还有一点需要指出，从图中可以看出，使用mixup以后训练抖动会大一些，也就是说训练没有原来稳定。</p> 
<h4><a id="24__66"></a>2.4 是否可以使用多个样本混合？</h4> 
<p>  文中给出的方法使用了两个样本进行混合，我们自然会想到使用更多的样本混合效果会更好吗？也就是说：<br>   <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          ~ 
         
        
       
         = 
        
        
        
          ∑ 
         
         
         
           i 
          
         
           = 
          
         
           1 
          
         
        
          N 
         
        
        
        
          λ 
         
        
          i 
         
        
        
        
          x 
         
        
          i 
         
        
       
      
        \tilde{x} =\sum_{i=1}^N \lambda _{i} x_{i} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.66786em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span class="" style="top: -3.35em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.22222em;">~</span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.28094em; vertical-align: -0.29971em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.981231em;"><span class="" style="top: -2.40029em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.29971em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>,  <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          y 
         
        
          ~ 
         
        
       
         = 
        
        
        
          ∑ 
         
         
         
           i 
          
         
           = 
          
         
           1 
          
         
        
          N 
         
        
        
        
          λ 
         
        
          i 
         
        
        
        
          y 
         
        
          i 
         
        
       
      
        \tilde{y} =\sum_{i=1}^N \lambda _{i} y_{i} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8623em; vertical-align: -0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span><span class="" style="top: -3.35em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">~</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.28094em; vertical-align: -0.29971em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.981231em;"><span class="" style="top: -2.40029em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.29971em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>  其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          ∑ 
         
         
         
           i 
          
         
           = 
          
         
           1 
          
         
        
          N 
         
        
        
        
          λ 
         
        
          i 
         
        
       
         = 
        
       
         1 
        
       
      
        \sum_{i=1}^N \lambda _{i}=1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.28094em; vertical-align: -0.29971em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.981231em;"><span class="" style="top: -2.40029em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.29971em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span></p> 
<p>  使用狄利克雷分布可以方便的实现这个公式，我在CIFAR10中进行了试验，结果如下图，N=2时等同于Beta分布，效果最好，N越大效果越差，N≤4效果仍能比原始数据稍好，N&gt;5时效果还不如原始数据。什么原因还没有想明白。论文中提到作者也考虑了狄利克雷分布，但计算比较耗时且没有更好的效果。我不知道作者所指是否和我相同，我的试验中速度并没有比Beta分布变慢（即使N=10）。<br> <img src="https://images2.imgbox.com/4c/ea/qsidJGGG_o.png" alt="在这里插入图片描述"></p> 
<center> 
 <b><font size="3">图4. 使用狄利克雷分布进行多个样本的mixup试验</font></b> 
</center> 
<center>
  （图中dirichletN表示对N张图混合，N=1时相当于原始数据，N=2时相当于Beta分布） 
</center> 
<h4><a id="25__76"></a>2.5 为什么要使用凸组合？</h4> 
<p>  凸组合（convex combination）是指线性组合中各项系数之和为1。当然我们直觉上也能想象使用凸组合得到的新样本在数值上是和原样本持平的，应该会好。但效果怎样我还是想试一试：<br> <img src="https://images2.imgbox.com/91/85/Bbt82ynC_o.png" alt="在这里插入图片描述"></p> 
<center> 
 <b><font size="3">图5. 不使用凸组合的mixup试验</font></b> 
</center> 
<p>  结果发现，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ∑ 
        
       
         λ 
        
       
      
        \sum\lambda 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00001em; vertical-align: -0.25001em;"></span><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">∑</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">λ</span></span></span></span></span> 小于1时效果变差很多。</p> 
<h2><a id="mixup_83"></a>二、mixup的后续改进</h2> 
<h3><a id="1__84"></a>1, 各改进方法简介</h3> 
<p>  mixup方法成功挖了一个新坑，迅速涌现了大批改进和应用，我收集了部分方法改进性的论文进行介绍。除了这些改进性方法外，还有很多mixup<a href="https://arxiv.org/abs/2010.04819" rel="nofollow">理论分析</a>及交叉应用的论文，比如mixUp<a href="https://arxiv.org/abs/1903.02709v4" rel="nofollow">应用于GAN</a>，应用于对抗攻击防御，<a href="https://arxiv.org/abs/2002.07394" rel="nofollow">应用于标签降噪</a>等，感兴趣的可自行阅读。</p> 
<h4><a id="11_cutMix_86"></a>1.1 cutMix</h4> 
<p>paper(ICCV2019 oral,2019-5-13): <a href="https://arxiv.org/abs/1905.04899v2" rel="nofollow">https://arxiv.org/abs/1905.04899v2</a><br> code(739星): <a href="https://github.com/clovaai/CutMix-PyTorch">https://github.com/clovaai/CutMix-PyTorch</a><br>   cutMix方法另辟蹊径，不从数值角度对两个样本插值，而是从图像的空间角度考虑，把一张图片上的某个随机矩形区域剪裁到另一张图片上生成新图片。标签的处理和mixUp是一样的，都是按照新样本中两个原样本的比例确定新的混合标签的比例。这种新的处理更适合图像中信息连续性这个特点，所以作者试验认为效果比mixup更好。方法示意图见1.4节PuzzleMix插图。</p> 
<h4><a id="12_manifold_mixup_90"></a>1.2 manifold mixup</h4> 
<p>paper(ICML2019,2019-5-11): <a href="https://arxiv.org/abs/1806.05236" rel="nofollow">https://arxiv.org/abs/1806.05236</a><br> code(310星): <a href="https://github.com/vikasverma1077/manifold_mixup">https://github.com/vikasverma1077/manifold_mixup</a><br>   manifold mixup对mixup进行扩展，把输入数据（raw input data）混合扩展到对中间隐层输出混合。至于对中间隐层混合更有效的原因，作者强行解释了一波。首先给出了现象级的解释，即这种混合带来了三个优势：平滑决策边界、拉大低置信空间（拉开各类别高置信空间的间距）、展平隐层输出的数值。至于这三点为什么有效，从作者说法看这应该是一种业界共识。然后作者又从数学上分析了第三点，即为什么manifold mixup可以实现展平中间隐层输出。总之这篇论文的理论解释部分比较深奥，毕竟作者有Yoshua Bengio大神。</p> 
<h4><a id="13_patchUp_94"></a>1.3 patchUp</h4> 
<p>paper(2020-6-14): <a href="https://arxiv.org/abs/2006.07794" rel="nofollow">https://arxiv.org/abs/2006.07794</a><br> code(12星): <a href="https://github.com/chandar-lab/PatchUp">https://github.com/chandar-lab/PatchUp</a><br>   这个也是Bengio组出品。PatchUp方法在manifold mixup基础上，又借鉴了cutMix在空间维度剪裁的思路，对中间隐层输出也进行剪裁，对两个不同样本的中间隐层剪裁块进行互换或插值，文中称互换法为硬patchUp，插值法为软patchUp。试验发现互换法在识别精度上更好，插值法在对抗攻击的鲁棒性上更好。这篇论文作者没有再进行深度解释，仅仅给出了一个现象级对比，就是patchUp方法的隐层激活值比较高。<br>   manifold和patchUp的官方开源代码都是对网络本身代码进行了修改，不能即插即用到其他网络中，我实现了一份即插即用版，链接见文末。<br> <img src="https://images2.imgbox.com/fe/23/QcY9cGkL_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="14_puzzleMix_101"></a>1.4 puzzleMix</h4> 
<p>paper(ICML2020, 2020-9-15) <a href="https://arxiv.org/abs/2009.06962" rel="nofollow">https://arxiv.org/abs/2009.06962</a><br> code(60星): <a href="https://github.com/snu-mllab/PuzzleMix">https://github.com/snu-mllab/PuzzleMix</a><br>   puzzleMix在cutMix基础上加入了显著性分析。因为cutMix合成的图片可能剪裁块正好来自于源图片的非重要区域或者正好把目标图片的重要区域遮挡，这明显和生成的标签不符。因此puzzle Mix首先计算各样本的显著性区域，仅剪裁显著性区域，又进一步加入了一些复杂精细的优化操作，从试验数据看效果很不错。<br> <img src="https://images2.imgbox.com/9a/ed/a0yH612L_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="15_saliency_Mix_106"></a>1.5 saliency Mix</h4> 
<p>paper(ICLR2021,2020-6-2):<a href="https://arxiv.org/abs/2006.01791" rel="nofollow">https://arxiv.org/abs/2006.01791</a><br> code(0星)：<a href="https://github.com/SaliencyMix/SaliencyMix">https://github.com/SaliencyMix/SaliencyMix</a><br>   saliency Mix也是在cutMix基础上加入了显著性分析，但没有更多的优化操作措施，效果似乎不如puzzleMix。<br> <img src="https://images2.imgbox.com/20/38/EttNg64l_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="16_fMix_111"></a>1.6 fMix</h4> 
<p>paper(ICLR2021,2020-6-24):<a href="https://arxiv.org/abs/2002.12047" rel="nofollow">https://arxiv.org/abs/2002.12047</a><br> code(249星):<a href="https://github.com/ecs-vlc/FMix">https://github.com/ecs-vlc/FMix</a><br>   fMix在cutMix基础上改进，把剪裁区域从矩形转换为不规则形状，这样增加了数据样本空间规模。作者还先对图像进行傅里叶变换来提取低频分量（？）。除了在几个常用图像数据集上进行试验外，作者还把方法应用了情感分类这种一维数据上。但作者没有对方法为什么有效进行深入的理论分析。这篇论文以临界得分被ICLR2021录用。<br> <img src="https://images2.imgbox.com/d2/88/NmPqtJb8_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="17_coMix_117"></a>1.7 co-Mix</h4> 
<p>paper(ICLR2021 oral, 2021-2-5): <a href="https://arxiv.org/abs/2102.03065" rel="nofollow">https://arxiv.org/abs/2102.03065</a><br> code(27星): <a href="https://github.com/snu-mllab/Co-Mixup">https://github.com/snu-mllab/Co-Mixup</a><br>   co-Mix方法在剪裁+显著性的基础上继续改进，把从两个样本混合变成从多个样本中提取显著性区域并混合。该文使用显著性测度对显著性进行量化，并引入超模-子模分析方法，设计了一个子模最小化算法来实现在生成图片中尽可能多的累积显著性区域。这样生成的图片能够保证最大的显著性测度，同时还保持标签的多样性。该文是ICLR2021的oral，提出的理论方法确实比较深奥和高档。但综合多个文献的试验数据看，该方法的效果似乎还不如puzzleMix。<br> <img src="https://images2.imgbox.com/a2/44/ElckyBSw_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2__122"></a>2, 各改进方法对比</h3> 
<p><img src="https://images2.imgbox.com/af/70/cVFCr0Sl_o.png" alt="在这里插入图片描述"></p> 
<center> 
 <b><font size="3">图6. mixup系列族谱（日期表示首次在arxiv上发表的时间）</font></b> 
</center> 
<p>  图6中给出了Mixup系列（不完全收录）发展过程。每篇论文都会宣称自己是SOTA，但是由于每篇论文中使用的模型的参数、训练超参等不可能完全相同，所以直接根据某一篇论文中给出的数据对比它们的性能并不准确和公平，下面我把各论文在cifar-100中试验结果整理出来，我们可以通过交叉分析多个文献的数据综合对比它们的效果。</p> 
<center> 
 <b><font size="3">表1. 各文献中给出的各方法精度损失(%)，使用CIFAR-100数据集和ResNet18作为baseline(其中cutMix使用PyramidNet-200)</font></b> 
</center> 
<table><thead><tr><th>方法⬇\文献→</th><th>mixup</th><th>cutMix</th><th>manifold</th><th>patchUp</th><th>saliencyMix</th><th>puzzleMix</th><th>co-Mixup</th><th>FMix</th></tr></thead><tbody><tr><td><strong>baseline</strong></td><td>25.6</td><td>16.45</td><td>24.01</td><td>24.622</td><td>22.46</td><td>23.67</td><td>23.59</td><td>24.78</td></tr><tr><td><strong>mixup</strong></td><td>21.1</td><td>15.63</td><td>22.11</td><td>22.326</td><td>-</td><td>23.16</td><td>22.43</td><td>22.56</td></tr><tr><td><strong>cutMix</strong></td><td>-</td><td>14.47</td><td>-</td><td>22.184</td><td>19.42</td><td>23.20</td><td>21.29</td><td>20.49</td></tr><tr><td><strong>manifold</strong></td><td>-</td><td>-</td><td>20.34</td><td>21.396</td><td>-</td><td>20.98</td><td>21.64</td><td>-</td></tr><tr><td><strong>patchUp</strong></td><td>-</td><td>-</td><td>-</td><td>19.120</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td><strong>saliencyMix</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>19.29</td><td>-</td><td>-</td><td>-</td></tr><tr><td><strong>puzzleMix</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>19.62</td><td>20.62</td><td>-</td></tr><tr><td><strong>co-Mixup</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>19.87</td><td>-</td></tr><tr><td><strong>FMix</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>20.15</td></tr></tbody></table> 
<center> 
 <b><font size="3">表2. 各文献中给出的各方法精度损失(%)，使用CIFAR-100和WideResNet28-10作为baseline(其中cutMix使用PyramidNet-200)</font></b> 
</center> 
<table><thead><tr><th>方法⬇\文献→</th><th>mixup</th><th>cutMix</th><th>manifold</th><th>patchUp</th><th>saliencyMix</th><th>puzzleMix</th><th>co-Mixup</th><th>FMix</th></tr></thead><tbody><tr><td><strong>baseline</strong></td><td>19.4</td><td>16.45</td><td>21.72</td><td>22.442</td><td>18.80</td><td>21.14</td><td>21.70</td><td>21.74</td></tr><tr><td><strong>mixup</strong></td><td>17.5</td><td>15.63</td><td>18.89</td><td>18.726</td><td>-</td><td>18.27</td><td>20.08</td><td>18.81</td></tr><tr><td><strong>cutMix</strong></td><td>-</td><td>14.47</td><td>-</td><td>18.316</td><td>16.66</td><td>17.50</td><td>20.14</td><td>18.04</td></tr><tr><td><strong>manifold</strong></td><td>-</td><td>-</td><td>18.04</td><td>18.352</td><td>-</td><td>17.40</td><td>20.55</td><td>-</td></tr><tr><td><strong>patchUp</strong></td><td>-</td><td>-</td><td>-</td><td>16.134</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td><strong>saliencyMix</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>16.56</td><td>-</td><td>-</td><td>-</td></tr><tr><td><strong>puzzleMix</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>16.23</td><td>15.95</td><td>19.24</td><td>-</td></tr><tr><td><strong>co-Mixup</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>19.15</td><td>-</td></tr><tr><td><strong>FMix</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>17.97</td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/bb/eb/CXDD177o_o.png" alt="在这里插入图片描述"></p> 
<center> 
 <b><font size="3">图7. 表1表2数据可视化（都减去各自的baseline）</font></b> 
</center> 
<p>一些结论：<br> 1，几乎所有人都测出来自己误差最小（与之前的文献对比），而很多情况下别人测出来的结果却未必（哈哈）；<br> 2，综合各种因素看，感觉效果最好的应该是patchUp &gt; puzzleMix &gt; 其他；<br> 3，虽然随着时间发展模型精度越来越高，但程序也越来越复杂。mixup原方法可以很方便的使用在CNN、RNN、GAN等各种场合，但使用剪裁的cutMix及后续方法似乎都只能用在CNN场合。而涉及到隐层修改的manifold和patchUp还需要修改网络本身各层的写法，官方开源代码不能够即插即用。<br>   想到在pytorch中使用钩子操作(hook)可以修改中间层，因此我自己<strong>实现了一份可以即插即用的manifold和patchUp</strong>，请参见我另一篇博客：<a href="https://blog.csdn.net/Brikie/article/details/114222605">https://blog.csdn.net/Brikie/article/details/114222605</a></p> 
<h2><a id="_166"></a>三、读后感</h2> 
<p>  mixup方法开创了一种根据两个或多个数据样本生成混合样本的数据增强方式，相比之下，传统的数据增强方法多是根据单个样本进行变换得到新样本。这种人类无法学习的混合形式数据却能被神经网络学习并得到精度更高，泛化更好，鲁棒性更强的结果，这一点很让人吃惊。这或许揭示了神经网络和人脑学习模式的不同，揭示了神经网络的某种本质属性，但目前为止似乎还没有人彻底解释清这种本质。所以mixup相关研究还刚开始，远未结束。<br>   毕竟初学，时间也有限，文中难免大量错误，还请各位读者帮忙指出，我及时更正，以方便后来的读者阅读。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/10e8e84bcf2ab3d09963ec44dc450f0f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vscode运行卡顿解决方案</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b04b3cf57b5960fd26902a340b27ff85/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ubuntu20.04 cuda opencv</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>