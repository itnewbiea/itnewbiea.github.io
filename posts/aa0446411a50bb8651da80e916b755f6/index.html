<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>2021年CVPR论文Deep Two-View Structure-from-Motion Revisited阅读笔记 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="2021年CVPR论文Deep Two-View Structure-from-Motion Revisited阅读笔记" />
<meta property="og:description" content="这篇博客分享的是2021年发表于CVPR上的一篇论文《Deep Two-View Structure-from-Motion Revisited》。该论文使用深度学习的方法，按照经典的SfM流程来解决两视图SfM问题。
论文地址：https://arxiv.org/abs/2104.00556v1论文代码：https://github.com/jytime/Deep-SfM-Revisited *文中出现的部分英文术语翻译&amp;理解：
ill-posed problem：病态位姿方程问题，即无法解出pose的问题up-to-scale：尺度未定的scaled：尺度确定的
1 概述 论文指出：现有的基于深度学习的方法通过从两个连续帧中恢复绝对姿势，或从单个图像预测深度图来描述该问题，这两种方法都是ill-posed问题（依据Ill-posed problems in early vision论文所述，ill-posed problem包括the recovery of motion and optical flow, shape from shading, surface interpolation, and edge detection等）。相比之下，作者建议要按照经典SfM流程来使用深度学习方法，去解决两视图SfM的问题。
论文所提出的方法包括：1. 用于预测两帧之间密集匹配的光流估计网络；2. 用于从2D光流匹配中计算相机相对位姿的归一化位姿估计模型；3. 利用对极几何约束来缩小搜索空间，精化密集匹配，并估计相对深度图的尺度不变的深度估计网络。
实验表明，论文所提出的方法在KITTI depth、KITTI VO、MVS、Scenes11和SUN3D数据集中计算相对位姿和深度估计等方面的表现都优于目前所有最先进的两视图SfM方法。
论文的贡献主要包括如下三点：
回顾了深度学习在SfM中的应用，并提出了一个新的两视图SfM深度学习框架，以解决ill-posedness。该框架结合了深度学习和经典几何方法的优点。提出了一个尺度不变的深度估计模型来处理深度真值和估计深度之间的尺度误匹配问题。所提出的方法在两视图SfM问题中对相对位姿估计和深度估计方面都优于以往的各种基准方法。 2 现有方法 作者将现有的用深度学习来解决两视图SfM的方法分为两类：
第一类方法（Figure 1 Type I）将单目相机的深度和位姿估计当成一个联合优化的问题。该类方法使用两个网络：一个网络负责从单个图像中估计up-to-scale的深度，另一个网络负责从两张输入图像中预测up-to-scale的相机位姿。这两个网络在计算时是相互独立的。比如SfMLearner、GeoNet等。另一类方法（Figure 1 Type II）从图像对中推断出scaled的相机位姿和scaled的深度，并使用多视图几何理论对其进行迭代优化。这类方法包括DeMoN、BANet、DeepV2D、DeepSFM等。 论文所提出的方法（Figure 1 Our Approach）首先使用深度光流网络估计两帧之间的密集匹配点，从中采样一组高度可靠的匹配，然后使用GPU加速的经典五点RANSAC算法计算相机的相对位姿。由于这些相机的相对位姿具有尺度不确定性，所以估计的深度也会受到尺度不确定性的影响。因此，为了用（已知尺度的）深度真值来监督估计的尺度不确定性深度，论文提出了一种结合尺度特定损失的尺度不变性深度估计网络来估计最终的相对深度图。因为有相机位姿，深度估计网络的搜索空间可以缩小为极线，因此比直接用估计的相机位姿对光流进行三角剖分具有更高的精度。
3 方法流程 3.1 Optical Flow Estimation 深度光流法可以处理大位移以及无纹理、遮挡和非朗伯曲面。论文使用最先进的网络DICL-Flow，在两个连续帧之间生成密集匹配点。该网络使用位移不变匹配代价学习策略和soft-argmin投影层来确保网络学习密集匹配点，而不是图像流的回归。该网络只需要在合成数据集上进行训练，即可应用于真实的测试数据集上。
3.2 Essential Matrix Estimation 与以前所有基于深度学习的从输入图像回归相机位姿的方法不同，论文使用匹配点来计算相机位姿。关键问题是：如何从光流中稳健地过滤掉噪声密集的匹配，以仅保留高质量的匹配？作者发现，只需简单地使用SIFT关键点位置（注意，这里不是使用SIFT进行匹配）生成mask即可在所有数据集中都获得很好的结果。该假设是，光流在纹理丰富的区域可以获得更准确的结果。mask内各位置的光流匹配由RANSAC通过GPU加速进行过滤，以避免动态物体的干扰。在获得基本矩阵E后，使用矩阵分解恢复相机姿态（R,t）。
3.3 Scale-Invariant Depth Estimation 论文提出了一种Scale-Invariant Matching方法来恢复up-to-scale的密集深度图。给定一个像点x，生成L个匹配候选点。在标准平面扫描问题中，匹配候选对象的采样分布根据比例因子α而变，如Figure 2所示。此外，由于不知道问题中的绝对尺度，论文将平移向量t进行了归一化。然后，将估计的深度d固定住，匹配候选点的分布就是尺度不变的了。最后，为了使估计深度和深度真值兼容，需要相应地对估计深度进行缩放来匹配深度真值。（由于这里不方便输入公式，因此仅使用文字进行简单描述，具体公式细节参见原论文）" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/aa0446411a50bb8651da80e916b755f6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-10T14:59:26+08:00" />
<meta property="article:modified_time" content="2022-07-10T14:59:26+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">2021年CVPR论文Deep Two-View Structure-from-Motion Revisited阅读笔记</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>  这篇博客分享的是2021年发表于CVPR上的一篇论文《Deep Two-View Structure-from-Motion Revisited》。该论文使用深度学习的方法，按照经典的SfM流程来解决两视图SfM问题。</p> 
<ul><li>论文地址：<a href="https://arxiv.org/abs/2104.00556v1" rel="nofollow">https://arxiv.org/abs/2104.00556v1</a></li><li>论文代码：<a href="https://github.com/jytime/Deep-SfM-Revisited">https://github.com/jytime/Deep-SfM-Revisited</a></li></ul> 
<p><img src="https://images2.imgbox.com/33/b7/x3y5GB1h_o.png" alt="在这里插入图片描述"><br>   <br> *文中出现的部分英文术语翻译&amp;理解：</p> 
<ul><li>ill-posed problem：病态位姿方程问题，即无法解出pose的问题</li><li>up-to-scale：尺度未定的</li><li>scaled：尺度确定的<br>   </li></ul> 
<h4><a id="1__11"></a>1 概述</h4> 
<p>  论文指出：现有的基于深度学习的方法通过从两个连续帧中恢复绝对姿势，或从单个图像预测深度图来描述该问题，这两种方法都是ill-posed问题（依据<a href="http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=97686E564EA89F07FE5F46EE02D67FCC?doi=10.1.1.73.4774&amp;rep=rep1&amp;type=pdf" rel="nofollow">Ill-posed problems in early vision</a>论文所述，ill-posed problem包括the recovery of motion and optical flow, shape from shading, surface interpolation, and edge detection等）。相比之下，作者建议要按照经典SfM流程来使用深度学习方法，去解决两视图SfM的问题。<br>   论文所提出的方法包括：1. 用于预测两帧之间密集匹配的光流估计网络；2. 用于从2D光流匹配中计算相机相对位姿的归一化位姿估计模型；3. 利用对极几何约束来缩小搜索空间，精化密集匹配，并估计相对深度图的尺度不变的深度估计网络。<br>   实验表明，论文所提出的方法在KITTI depth、KITTI VO、MVS、Scenes11和SUN3D数据集中计算相对位姿和深度估计等方面的表现都优于目前所有最先进的两视图SfM方法。<br>   <br>   论文的贡献主要包括如下三点：</p> 
<ol><li>回顾了深度学习在SfM中的应用，并提出了一个新的两视图SfM深度学习框架，以解决ill-posedness。该框架结合了深度学习和经典几何方法的优点。</li><li>提出了一个尺度不变的深度估计模型来处理深度真值和估计深度之间的尺度误匹配问题。</li><li>所提出的方法在两视图SfM问题中对相对位姿估计和深度估计方面都优于以往的各种基准方法。</li></ol> 
<h4><a id="2__21"></a>2 现有方法</h4> 
<p>  作者将现有的用深度学习来解决两视图SfM的方法分为两类：</p> 
<ul><li>第一类方法（Figure 1 Type I）将单目相机的深度和位姿估计当成一个联合优化的问题。该类方法使用两个网络：一个网络负责从单个图像中估计up-to-scale的深度，另一个网络负责从两张输入图像中预测up-to-scale的相机位姿。这两个网络在计算时是相互独立的。比如<a href="https://arxiv.org/abs/1704.07813v2" rel="nofollow">SfMLearner</a>、<a href="https://arxiv.org/abs/1803.02276" rel="nofollow">GeoNet</a>等。</li><li>另一类方法（Figure 1 Type II）从图像对中推断出scaled的相机位姿和scaled的深度，并使用多视图几何理论对其进行迭代优化。这类方法包括<a href="https://arxiv.org/abs/1612.02401" rel="nofollow">DeMoN</a>、<a href="https://arxiv.org/abs/1806.04807" rel="nofollow">BANet</a>、<a href="https://arxiv.org/abs/1812.04605" rel="nofollow">DeepV2D</a>、<a href="https://arxiv.org/abs/1912.09697" rel="nofollow">DeepSFM</a>等。</li></ul> 
<p><img src="https://images2.imgbox.com/99/26/y1NKLjr7_o.png" alt="在这里插入图片描述"><br>   论文所提出的方法（Figure 1 Our Approach）首先使用深度光流网络估计两帧之间的密集匹配点，从中采样一组高度可靠的匹配，然后使用GPU加速的经典五点RANSAC算法计算相机的相对位姿。由于这些相机的相对位姿具有尺度不确定性，所以估计的深度也会受到尺度不确定性的影响。因此，为了用（已知尺度的）深度真值来监督估计的尺度不确定性深度，论文提出了一种结合尺度特定损失的尺度不变性深度估计网络来估计最终的相对深度图。因为有相机位姿，深度估计网络的搜索空间可以缩小为极线，因此比直接用估计的相机位姿对光流进行三角剖分具有更高的精度。</p> 
<h4><a id="3__29"></a>3 方法流程</h4> 
<h5><a id="31_Optical_Flow_Estimation_30"></a>3.1 Optical Flow Estimation</h5> 
<p>  深度光流法可以处理大位移以及无纹理、遮挡和非朗伯曲面。论文使用最先进的网络DICL-Flow，在两个连续帧之间生成密集匹配点。该网络使用位移不变匹配代价学习策略和soft-argmin投影层来确保网络学习密集匹配点，而不是图像流的回归。该网络只需要在合成数据集上进行训练，即可应用于真实的测试数据集上。</p> 
<h5><a id="32_Essential_Matrix_Estimation_32"></a>3.2 Essential Matrix Estimation</h5> 
<p>  与以前所有基于深度学习的从输入图像回归相机位姿的方法不同，论文使用匹配点来计算相机位姿。关键问题是：如何从光流中稳健地过滤掉噪声密集的匹配，以仅保留高质量的匹配？作者发现，只需简单地使用SIFT关键点位置（注意，这里不是使用SIFT进行匹配）生成mask即可在所有数据集中都获得很好的结果。该假设是，光流在纹理丰富的区域可以获得更准确的结果。mask内各位置的光流匹配由RANSAC通过GPU加速进行过滤，以避免动态物体的干扰。在获得基本矩阵E后，使用矩阵分解恢复相机姿态（R,t）。</p> 
<h5><a id="33_ScaleInvariant_Depth_Estimation_34"></a>3.3 Scale-Invariant Depth Estimation</h5> 
<p>  论文提出了一种Scale-Invariant Matching方法来恢复up-to-scale的密集深度图。给定一个像点x，生成L个匹配候选点。在标准平面扫描问题中，匹配候选对象的采样分布根据比例因子α而变，如Figure 2所示。此外，由于不知道问题中的绝对尺度，论文将平移向量t进行了归一化。然后，将估计的深度d固定住，匹配候选点的分布就是尺度不变的了。最后，为了使估计深度和深度真值兼容，需要相应地对估计深度进行缩放来匹配深度真值。（由于这里不方便输入公式，因此仅使用文字进行简单描述，具体公式细节参见原论文）<br> <img src="https://images2.imgbox.com/73/64/PTWRmSNu_o.png" alt="在这里插入图片描述"><br>   这种尺度不变的匹配策略在论文所提出的框架中起着至关重要的作用，因为它使得所提出的网络不再遭受尺度误匹配（scale misalignment）的问题。其他方法无法从这种尺度不变匹配中获益，因为它们通常通过预测绝对尺度来避免尺度误匹配问题。</p> 
<h5><a id="34_Loss_Function_38"></a>3.4 Loss Function</h5> 
<p>  论文所提出的框架在深度图真值和位姿真值的监督下进行端到端的训练。论文中使用Huber损失来计算预测深度和深度真值之间的差异。如果相机位姿真值和深度真值都给定，还可以通过计算2D点的刚性流（rigid ﬂow）来更新光流网络。</p> 
<h4><a id="4__40"></a>4 实验</h4> 
<h5><a id="41__41"></a>4.1 数据集</h5> 
<ul><li>KITTI Depth：主要用于自动驾驶场景中的单目深度估计，不考虑相机运动和动态目标。</li><li>KITTI VO：主要用于相机位姿估计。它包含10个带有相机位姿真值的序列（超过20,000帧）。</li><li>MVS：通过视频序列和近距离场景构建的来自不同来源的室外场景。</li><li>Scenes11：由随机形状和运动生成的合成数据集。尽管不是真实的图像，但它仍带有精确的深度和位姿数据。</li><li>SUN3D：提供了带有深度和位姿数据的室内图像，这里的深度和位姿数据是带有噪音的。</li></ul> 
<h5><a id="42_Depth_Evaluation_47"></a>4.2 Depth Evaluation</h5> 
<p>  KITTI Depth数据集上的定量结果如Table 1所示，定性结果如Figure 3所示：<br> <img src="https://images2.imgbox.com/7c/43/ggJQ8QBc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/89/11/EXtRQajR_o.png" alt="在这里插入图片描述"><br>   MVS、Scenes11和SUN3D数据集上的定量结果如Table 2所示，定性结果如Figure 4所示：<br> <img src="https://images2.imgbox.com/98/a6/psnsG1bu_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b9/58/5amHLeyG_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="43_Camera_Pose_Estimation_54"></a>4.3 Camera Pose Estimation</h5> 
<p>  KITTI VO数据集上的结果如Table 3和Figure 5所示：<br> <img src="https://images2.imgbox.com/35/f9/g08Rga4h_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8c/86/HheISsOP_o.png" alt="在这里插入图片描述"><br>   MVS、Scenes11和SUN3D数据集上的结果如Table 4所示：<br> <img src="https://images2.imgbox.com/b9/68/9YRhWnL8_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="44_Framework_Analysis_and_Justification_60"></a>4.4 Framework Analysis and Justification</h5> 
<p>  基于光流的相机位姿估计结果如Table 5所示：<br> <img src="https://images2.imgbox.com/1c/96/kBoO1yEc_o.png" alt="在这里插入图片描述"><br>   尺度误匹配问题的测试结果如Table 6所示：<br> <img src="https://images2.imgbox.com/99/76/jZ4IAQnF_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="5__65"></a>5 总结</h4> 
<p>  论文重新讨论了基于深度神经网络的两视图SfM问题。首先，作者认为现有的基于深度学习的SfM方法将深度估计或位姿估计表述为ill-posed问题。然后，作者提出了一个新的深度两视图SfM框架，该框架遵循经典的well-posed SfM流程。大量的实验表明，论文所提出的方法在位姿和深度估计方面都优于所有目前最先进的方法，具有明显的优势。作者认为，未来可以将该框架进行扩展，如三视图SfM和多视图SfM，其中循环一致性和时间一致性可以进一步约束这些已经适定的问题。</p> 
<p>  *部分翻译或理解可能会存在偏差，仅供参考，欢迎讨论。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/824d883aad752d901fe6dd693903386d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue进阶之路：vue3.2-setup语法糖、组合式API、状态库Pinia归纳总结</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1359295d04a9d389ffea24532fa62ed3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Anacoda &#43; pytorch 环境下安装 DGL_GPU</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>