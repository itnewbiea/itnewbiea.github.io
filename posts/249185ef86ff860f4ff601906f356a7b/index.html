<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python爬虫——简单网页抓取（实战案例）小白篇 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python爬虫——简单网页抓取（实战案例）小白篇" />
<meta property="og:description" content="在着手写爬虫抓取网页之前，要先把其需要的知识线路理清楚。
首先：了解相关的Http协议知识；其次：熟悉Urllib、Requests库；再者：开发工具的掌握 PyCharm、Fiddler；最后：网页爬取案例； 下面就按这个路线逐一讲讲各部分的内容；
一、Http协议
HTTP协议是一个应用层面向对象协议，也叫超文本传输协议。
是基于TCP协议的可靠传输，采用客户端/服务器端模式，指定了客户端可能发送给服务器什么样的消息，以及服务端给出什么样的响应。
1）HTTP协议组成
HTTP协议请求由状态行、请求头和请求正文三部分组成；
请求端的HTTP报文叫做请求报文，响应端的叫做响应报文，通常，并不一定要有报文主体。
状态行：包括请求方式Method、资源路径URL、协议版本Version；请求头：包括一些访问的域名、用户代理、Cookie等信息；请求正文：就是HTTP请求的数据，允许为空字符串； 2）HTTP 请求方法
HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD方法。
HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。
以上是HTTP所有的方法，但常见的方法有以下：
3）HTTP的请求响应模型
HTTP协议永远都是客户端发起请求，服务器回送响应。
这样就限制了使用HTTP协议，无法实现在客户端没有发起请求的时候，服务器将消息推送给客户端。
注意：HTTP协议是一个无状态的协议，同一个客户端的这次请求和上次请求是没有对应关系。
4）HTTP的状态码
Http状态码能告诉我们当前请求响应的状态，通过状态码判断和分析服务器的运行状态，以便更好的进行下一步的操作。
5）工作流程
一次HTTP操作称为一个事务，其工作过程可分为四步：
首先客户机与服务器需要建立连接，只要单击某个超级链接，HTTP的工作开始。建立连接后，客户机发送一个请求给服务器；请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容。服务器接到请求后，给予相应的响应信息；其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上，然后客户机与服务器断开连接。 如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。
这些过程由HTTP自己完成，我们等待信息显示就可以了。
二、Urllib、Requests库
这两个库都建议新手能够熟练掌握，在Python爬虫中使用频率非常高。
下方也有分享安装教程资源，有需要的点击进群找群管理获取即可↓↓
&lt;Python学习交流群 安装教程资源获取&gt;
Urllib库
Urllib是Python中请求url连接的官方标准库，在Python2中主要为urllib和urllib2，在Python3中整合成了urllib。
1）主要包含以下4个模块
urllib.request：用于打开和阅读URLurllib.error：包含由引发的异常urllib.requesturllib.parse：用于解析URLurllib.robotparser：用于解析robot.txt文件 2）urllib.request.urlopen()
模块定义了有助于在复杂环境中打开URL（主要是HTTP）的函数和类-基本身份验证和摘要身份验证，重构定向，Cookie等。
语法结构
urllib.request.urlopen（url，data = None，[ timeout，] *，cafile = None，capath = None，cadefault = False，context = None） 语法详解
url：传入的对象可以是url也可以是一个request的对象；data：data必须是一个直接要发送搭服务器的其他数据的对象，如果没有data数据的话可以为None，也可以不写。timeout：以秒为单位制定用于组织链接尝试之类的操作超时； 3）urllib.openurl：返回对象的方法
# 设置一个url对象 url = &#34;百度一下，你就知道&#34; #发送请求，打开url up = urllib." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/249185ef86ff860f4ff601906f356a7b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-05T21:29:37+08:00" />
<meta property="article:modified_time" content="2024-01-05T21:29:37+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python爬虫——简单网页抓取（实战案例）小白篇</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>在着手写爬虫抓取网页之前，要先把其需要的知识线路理清楚。</strong></p> 
<ul><li>首先：了解相关的Http协议知识；</li><li>其次：熟悉Urllib、Requests库；</li><li>再者：开发工具的掌握 PyCharm、Fiddler；</li><li>最后：网页爬取案例；</li></ul> 
<p>下面就按这个路线逐一讲讲各部分的内容；</p> 
<p><strong>一、Http协议</strong></p> 
<p>HTTP协议是一个应用层面向对象协议，也叫超文本传输协议。</p> 
<p>是基于TCP协议的可靠传输，采用客户端/服务器端模式，指定了客户端可能发送给服务器什么样的消息，以及服务端给出什么样的响应。</p> 
<p><strong>1）HTTP协议组成</strong></p> 
<p>HTTP协议请求由状态行、请求头和请求正文三部分组成；</p> 
<p>请求端的HTTP报文叫做请求报文，响应端的叫做响应报文，通常，并不一定要有报文主体。</p> 
<ul><li>状态行：包括请求方式Method、资源路径URL、协议版本Version；</li><li>请求头：包括一些访问的域名、用户代理、Cookie等信息；</li><li>请求正文：就是HTTP请求的数据，允许为空字符串；</li></ul> 
<p><strong>2）HTTP 请求方法</strong></p> 
<p>HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD方法。</p> 
<p>HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。</p> 
<p><img src="https://images2.imgbox.com/61/8f/5cKBfCIO_o.jpg" alt=""></p> 
<p>以上是HTTP所有的方法，但常见的方法有以下：</p> 
<p><img src="https://images2.imgbox.com/b7/87/AuQK57Nz_o.jpg" alt=""></p> 
<p><strong>3）HTTP的请求响应模型</strong></p> 
<p>HTTP协议永远都是客户端发起请求，服务器回送响应。</p> 
<p>这样就限制了使用HTTP协议，无法实现在客户端没有发起请求的时候，服务器将消息推送给客户端。</p> 
<blockquote> 
 <p>注意：HTTP协议是一个无状态的协议，同一个客户端的这次请求和上次请求是没有对应关系。</p> 
</blockquote> 
<p><strong>4）HTTP的状态码</strong></p> 
<p>Http状态码能告诉我们当前请求响应的状态，通过状态码判断和分析服务器的运行状态，以便更好的进行下一步的操作。</p> 
<p><img src="https://images2.imgbox.com/e7/7e/LqS7US2H_o.jpg" alt=""></p> 
<p><strong>5）工作流程</strong></p> 
<p>一次HTTP操作称为一个事务，其工作过程可分为四步：</p> 
<ul><li>首先客户机与服务器需要建立连接，只要单击某个超级链接，HTTP的工作开始。</li><li>建立连接后，客户机发送一个请求给服务器；请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容。</li><li>服务器接到请求后，给予相应的响应信息；其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。</li><li>客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上，然后客户机与服务器断开连接。</li></ul> 
<p><strong>如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。</strong></p> 
<p>这些过程由HTTP自己完成，我们等待信息显示就可以了。</p> 
<p><strong>二、Urllib、Requests库</strong></p> 
<p><strong>这两个库都建议新手能够熟练掌握，在Python爬虫中使用频率非常高。</strong></p> 
<p>下方也有分享安装教程资源，有需要的点击进群找群管理获取即可↓↓</p> 
<p>&lt;Python学习交流群 安装教程资源获取&gt;</p> 
<p><strong>Urllib库</strong></p> 
<p>Urllib是Python中请求url连接的官方标准库，在Python2中主要为urllib和urllib2，在Python3中整合成了urllib。</p> 
<p><strong>1）主要包含以下4个模块</strong></p> 
<ul><li>urllib.request：用于打开和阅读URL</li><li>urllib.error：包含由引发的异常urllib.request</li><li>urllib.parse：用于解析URL</li><li>urllib.robotparser：用于解析robot.txt文件</li></ul> 
<p><strong>2）urllib.request.urlopen()</strong></p> 
<p>模块定义了有助于在复杂环境中打开URL（主要是HTTP）的函数和类-基本身份验证和摘要身份验证，重构定向，Cookie等。</p> 
<p>语法结构</p> 
<pre><code>urllib.request.urlopen（url，data = None，[ timeout，] *，cafile = None，capath = None，cadefault = False，context = None）
</code></pre> 
<p>语法详解</p> 
<ul><li>url：传入的对象可以是url也可以是一个request的对象；</li><li>data：data必须是一个直接要发送搭服务器的其他数据的对象，如果没有data数据的话可以为None，也可以不写。</li><li>timeout：以秒为单位制定用于组织链接尝试之类的操作超时；</li></ul> 
<p><strong>3）urllib.openurl：返回对象的方法</strong></p> 
<pre><code>  # 设置一个url对象
  url = "百度一下，你就知道"
  #发送请求，打开url
  up = urllib.request.urlopen(url)
  print(type(up))
  
  #返回对象
&gt;&gt;&gt; &lt;class 'http.client.HTTPResponse'&gt;
</code></pre> 
<p>上面我们可以看出返回的对象是HTTPResponse类型的，下面一起来看一下HTTPResponse类的方法</p> 
<p>发送GET请求：</p> 
<pre><code>from urllib import request
response = request.urlopen('百度一下，你就知道')
print(response.read().decode())
</code></pre> 
<p>发送post请求：</p> 
<pre><code>from urllib import reuqest
response = 	request.urlopen('Method Not Allowed', data=b'word=hello')
print(response.read().decode())   #decode()解码
</code></pre> 
<p>我们已经知道在讲urlopen中传入一个网址的时候他就会主动的去访问目标的网址，会返回一个HTTPResponse类型的对象，那么我们是怎么知道我们发送的get请求和post请求呢？</p> 
<pre><code>    def get_method(self):
    """Return a string indicating the HTTP request method."""
    default_method = "POST" if self.data is not None else "GET"
    return getattr(self, 'method', default_method)
</code></pre> 
<p>这个是源码中的一个方法，根据代码我们得知如果data是不为空的话，就是post请求，否则就是get请求。</p> 
<p><strong>4）urllib.request.Request</strong></p> 
<p>上面的urllib是可对网页发起请求，在我们实际的爬虫应用中，如果频繁的访问一个网页，网站就会识别我们是不是爬虫，这个时候我们就要利用Request来伪装我们的请求头。</p> 
<pre><code>urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
</code></pre> 
<p><strong>推荐使用request()来进行访问的，因为使用request()来进行访问有两点好处：</strong></p> 
<ul><li>可以直接进行post请求，不需要将 data参数转换成JSON格式</li><li>直接进行GET请求，不需要自己拼接url参数</li></ul> 
<p><img src="https://images2.imgbox.com/3b/2b/khsd01cD_o.jpg" alt=""></p> 
<p>如果只进行基本的爬虫网页抓取，那么urllib足够用了。</p> 
<p><strong>Requests库</strong></p> 
<p>requests库是一个常用于http请求的模块，可以方便的对网页进行爬取，是学习python爬虫比较好的http请求模块，比urllib库更加简洁，并且自带json解析器。</p> 
<p>掌握了它，Cookies、登录验证、代理设置等操作都不是事儿。</p> 
<p><strong>1）request提供的方法</strong></p> 
<p><img src="https://images2.imgbox.com/2a/c4/XRxlAXXO_o.jpg" alt=""></p> 
<pre><code>get(url,params,**kwargs)
</code></pre> 
<ul><li>url: 需要爬取的网站地址。</li><li>params: url中的额外参数，字典或者字节流格式，可选。</li><li>**kwargs : 控制访问的参数</li></ul> 
<pre><code>post(url, data=None, json=None, **kwargs):
</code></pre> 
<ul><li>url: 需要爬取的网站地址。</li><li>data:传递的内容。</li><li>json:json格式传递的内容</li><li>**kwargs : 控制访问的参数</li></ul> 
<pre><code>request(method, url, **kwargs):
</code></pre> 
<ul><li>method:需要使用的方法</li><li>url：爬行的路径</li><li>**kwargs : 控制访问的参数</li></ul> 
<p><strong>2）控制访问的参数</strong></p> 
<p><img src="https://images2.imgbox.com/95/19/x8XJEigf_o.jpg" alt=""></p> 
<p>通过上面方法返回的是一个Response对象，该对象有以下这些属性和方法：</p> 
<p><img src="https://images2.imgbox.com/ab/45/qaHI4ABv_o.jpg" alt=""></p> 
<p><strong>requests的优势在于使用简单，相同一个功能，用requests实现起来代码量要少很多。</strong></p> 
<p>戳这里可获取相关的Python基础学习资料哦↓↓↓</p> 
<p><strong>下面的所有资料我全部打包好了并且上传至CSDN官方，需要的点击👇获取！</strong></p> 
<h3><a id="center__Python2023_httpsmpweixinqqcomsZxS5RO3rb59V5z2G4hLRUA_208"></a> 
 <center> 
  <a href="https://mp.weixin.qq.com/s/ZxS5RO3rb59V5z2G4hLRUA" rel="nofollow">👉 [Python零基础2023入门资料包] 👈</a> 
 </center></h3> 
<p><img src="https://images2.imgbox.com/7e/de/UuO4EvI8_o.jpg" alt=""></p> 
<p><strong>三、开发工具的掌握</strong></p> 
<ul><li>PyCharm – 交互很好的Python一款IDE；</li><li>Fiddler – 网页请求监控工具，我们可以使用它来了解用户触发网页请求后发生的详细步骤；</li></ul> 
<p><strong>（1）PyCharm</strong></p> 
<p>PyCharm是一款著名的Python IDE开发工具，拥有一整套可以帮助用户在使用Python语言开发时提高其效率的工具。</p> 
<p><strong>具备基本的调试、语法高亮、Project管理、代码跳转、智能提示、自动完成、单元测试、版本控制。</strong></p> 
<p><strong>此外，该IDE提供了一些高级功能，以用于支持Django框架下的专业Web开发。</strong></p> 
<p>pycharm官网分为两个版本：</p> 
<p>第一个版本是Professional(专业版本)，这个版本功能更加强大，主要是为Python和web开发者而准备，是需要付费的。</p> 
<p>第二个版本是社区版，一个专业版的阉割版，比较轻量级，主要是为Python和数据专家而准备的。</p> 
<p>一般我们做开发，下载专业版本比较合适。</p> 
<p><img src="https://images2.imgbox.com/e7/13/TA4uk7JU_o.jpg" alt=""></p> 
<p><strong>（2）Fiddler</strong></p> 
<p>Fiddler是一个http调试代理，它能够记录所有的你电脑和互联网之间的http通讯。</p> 
<p>Fiddler 可以也可以让你检查所有的http通讯，设置断点，以及Fiddle 所有的“进出”的数据（指cookie,html,js,css等文件，这些都可以让你胡乱修改的意思）。</p> 
<p>要比其他的网络调试器要更加简单，因为它仅仅暴露http通讯还有提供一个用户友好的格式。</p> 
<p>1）基本工作原理</p> 
<p><img src="https://images2.imgbox.com/18/1b/dZisL2zY_o.jpg" alt=""></p> 
<p>会话的概念：一次请求和一次响应就是一个会话。</p> 
<p>2）fiddler主界面</p> 
<p><img src="https://images2.imgbox.com/ae/d7/J4lya39Z_o.jpg" alt=""></p> 
<p>工具栏</p> 
<p><img src="https://images2.imgbox.com/cb/65/t9u39OTu_o.png" alt=""></p> 
<ul><li>1：给会话添加备注信息</li><li>2：重新加载当前会话</li><li>3：删除会话选项</li><li>4：放行，和断点对应</li><li>5：响应模式。也即是，当Fiddler拿到远程的response后是缓存起来一次响应给客户端还是以stream的方式直接响应。</li><li>6：解码，有些请求是被编码的，点击这个按钮后可以根据响应的编码格式自动解码。</li><li>7：查找会话</li><li>8：保存会话</li><li>9：截屏。截屏后，会以会话的方式返回一个截图</li></ul> 
<p>判断get和post请求</p> 
<p><img src="https://images2.imgbox.com/4a/33/exAuyn6d_o.jpg" alt=""></p> 
<p>会话列表属性说明</p> 
<p><img src="https://images2.imgbox.com/a6/93/W1jeYlYP_o.jpg" alt=""></p> 
<p>查看请求头和请求体</p> 
<p><img src="https://images2.imgbox.com/3c/a2/NXEBm9F1_o.jpg" alt=""></p> 
<p>3）Fiddler设置过滤（抓取指定服务器地址的数据）</p> 
<p>填写服务器地址多个ip之间需要用;隔开，或者每个ip或域名单独写一行；</p> 
<p><img src="https://images2.imgbox.com/65/38/2zbB4rPL_o.jpg" alt=""></p> 
<p>Hosts 第二个下拉选说明：</p> 
<p>No Host Filter 不设置hosts过滤</p> 
<p>Hide The Following Hosts 隐藏过滤到的域名</p> 
<p>Show Only The Following Hosts 只显示过滤到的域名</p> 
<p>Flag The Following Hosts 标记过滤到的域名</p> 
<p>类型过滤：图片、CSS、JS这类的静态素材也不需要看的情况下，直接全部过滤掉</p> 
<pre><code>.*\.(bmp|css|js|gif|ico|jp?g|png|swf|woff)
</code></pre> 
<p><img src="https://images2.imgbox.com/3d/cb/09ERIMpk_o.png" alt="在这里插入图片描述"></p> 
<p>4）开启 Https 抓包监听</p> 
<p>Fiddler 默认下，Fiddler不会捕获HTTPS会话，需要你设置下。</p> 
<p><img src="https://images2.imgbox.com/d8/10/hiwKOvZI_o.jpg" alt=""></p> 
<p><img src="https://images2.imgbox.com/dd/ea/wIvLTnr9_o.jpg" alt=""></p> 
<p>from all processes : 抓取所有的 https 程序, 包括 本机 和 手机</p> 
<p>from browsers only : 只抓取浏览器中的 https 请求</p> 
<p>from non-browsers only : 只抓取除了浏览器之外的所有 https 请求</p> 
<p>from remote clients only ： 抓取远程的客户端的 https ,可以代表手机</p> 
<p>Fiddler是最好用的Web调试工具之一，它能记录所有客户端和服务器的http和https请求，允许你监视，设置断点，甚至修改输入输出数据。</p> 
<p>Fiddler包含了一个强大的基于事件脚本的子系统，并且能使用.net语言进行扩展。</p> 
<p>你对HTTP 协议越了解， 你就能越掌握Fiddler的使用方法，你越使用Fiddler，就越能帮助你了解HTTP协议。</p> 
<p><strong>四、网页爬取案例</strong></p> 
<p>案例1：爬取百度翻译接口</p> 
<pre><code>python
import requests

base_url = 'https://fanyi.baidu.com/sug'
kw = input('请输入要翻译的英文单词：')
data = {
    'kw': kw
}
headers = {
    # 由于百度翻译没有反扒措施，因此可以不写请求头
    'content-length': str(len(data)),
    'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'referer': '百度翻译-200种语言互译、沟通全世界！',
    'x-requested-with': 'XMLHttpRequest'
}
response = requests.post(base_url, headers=headers, data=data)
# print(response.json())
#结果：{'errno': 0, 'data': [{'k': 'python', 'v': 'n. 蟒; 蚺蛇;'}, {'k': 'pythons', 'v': 'n. 蟒; 蚺蛇;  python的复数;'}]}

#-----------------------------把他变成一行一行
result=''
for i in response.json()['data']:
    result+=i['v']+'\n'
print(kw+'的翻译结果为：')
print(result)
</code></pre> 
<p><img src="https://images2.imgbox.com/83/67/ltTcEyxL_o.jpg" alt=""></p> 
<p>案例2：爬取百度产品列表</p> 
<pre><code> # 1.导包
  import requests
  
  # 2.确定url
  base_url = '百度产品大全'
  
  # 3.发送请求，获取响应
  response = requests.get(base_url)
  
  # 4.查看页面内容,可能出现 乱码
  # print(response.text)
  # print(response.encoding)
  
  # 5.解决乱码
  
  # 方法一：转换成utf-8格式
  # response.encoding='utf-8'
  # print(response.text)
  
  #方法二：解码为utf-8
  with open('index.html', 'w', encoding='utf-8') as fp:
      fp.write(response.content.decode('utf-8'))
      
  print(response.status_code)
  print(response.headers)
  print(type(response.text))
  print(type(response.content))
</code></pre> 
<p>案例3：爬取百度贴吧前十页（get请求）</p> 
<pre><code># _--------------------爬取百度贴吧搜索某个贴吧的前十页
import requests, os

base_url = '登录_百度贴吧
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36',
}
dirname = './tieba/woman/'
if not os.path.exists(dirname):
    os.makedirs(dirname)
for i in range(0, 10):
    params = {
        'ie': 'utf-8',
        'kw': '美女',
        'pn': str(i * 50)
    }
    response = requests.get(base_url, headers=headers, params=params)
    with open(dirname + '美女第%s页.html' % (i+1), 'w', encoding='utf-8') as file:
        file.write(response.content.decode('utf-8'))
</code></pre> 
<h3><a id="Python_414"></a>关于Python技术储备</h3> 
<p>学好 Python 不论是就业还是做副业赚钱都不错，但要学会 Python 还是要有一个学习规划。最后大家分享一份全套的 Python 学习资料，给那些想学习 Python 的小伙伴们一点帮助！</p> 
<p><mark>包括：Python激活码+安装包、Python web开发，Python爬虫，Python数据分析，Python自动化测试学习等教程。带你从零基础系统性的学好Python！</mark></p> 
<h4><a id="Python_419"></a>一、Python学习大纲</h4> 
<p>Python所有方向的技术点做的整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照上面的知识点去找对应的学习资源，保证自己学得较为全面。<br> <img src="https://images2.imgbox.com/bf/92/spJATkOV_o.jpg" alt="在这里插入图片描述"></p> 
<h4><a id="Python_424"></a>二、Python必备开发工具</h4> 
<p><img src="https://images2.imgbox.com/73/7c/DrzTqy7S_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_428"></a>三、入门学习视频</h4> 
<p><img src="https://images2.imgbox.com/66/99/sWfKeHvj_o.png" alt=""></p> 
<h4><a id="_432"></a>四、实战案例</h4> 
<p>光学理论是没用的，要学会跟着一起敲，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。<img src="https://images2.imgbox.com/b6/2c/G1DdXSMa_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="python_436"></a>五、python副业兼职与全职路线</h4> 
<p><img src="https://images2.imgbox.com/9f/30/xRLlNQdG_o.png" alt="在这里插入图片描述"></p> 
<p><strong><mark>上述这份完整版的Python全套学习资料已经上传CSDN官方，如果需要可以微信扫描下方CSDN官方认证二维码 即可领取</mark></strong></p> 
<blockquote> 
 <p>👉<a href="https://mp.weixin.qq.com/s/ZxS5RO3rb59V5z2G4hLRUA" rel="nofollow">[[<font color="red">CSDN大礼包：</font>《python兼职资源&amp;全套学习资料》免费分享]]</a><font color="green">（<strong>安全链接，放心点击</strong>）</font></p> 
</blockquote> 
<center> 
 <img src="https://images2.imgbox.com/27/6b/FAUEtxu2_o.png"> 
 <center> 
 </center> 
</center>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3c4c240569beef68db1b5213fcb88b50/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">状态机（有限状态机（Finite State Machine, FSM）、推进自动机（Pushdown Automata）、并发状态机、分层状态机）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/38843a6406ff5f7f668f1e140177aae5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mac 安装Nginx教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>