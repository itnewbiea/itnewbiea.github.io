<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【高性能计算】CUDA编程之线程存储与原子操作（教程与代码-2） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【高性能计算】CUDA编程之线程存储与原子操作（教程与代码-2）" />
<meta property="og:description" content="3 线程、同步、存储器 3.1 线程与存储
tid=blockIdx.x（当前块的ID）*blockDim.x（当前块里面的线程数量）&#43;threadIdx.x（当前线程在块中的ID）gridDim.x*blockDim.x来计算，前者代表了本次启动的块的数量，而后者代表了每个块里面的线程数量，然后每次while循环，tid变量加上这个值，向后偏移以得到下个任务的索引所有线程都有一个寄存器堆，它是最快的。共享内存只能被块中的线程访问，但比全局内存块。全局内存是最慢的，但可以被所有的块访问。常量和纹理内存用于特殊用途所有通过cudaMalloc分配的存储器都是全局内存本地内存和寄存器堆对每个线程都是唯一的。寄存器是每个线程可用的最快存储器。当内核中使用的变量在寄存器堆中装不下的时候，将会使用本地内存存储它们，这叫寄存器溢出“读取旧值-累加-回写新值”操作是不可被其他线程扰乱的原子性的整体完成的。使用atomicAdd进行原子累加的内核函数使用原子操作后程序具有更大的执行代价。可以通过使用共享内存来加速这些原子累加操作
GPU卡从逻辑上对用户提供了64KB的常量内存空间，可以用来存储内核执行期间所需要的恒定数据常量内存有助于节省全局内存的访问带宽warp整体进行一次常量内存的读取，结果广播给warp里的32个线程。同时，常量内存具有cache缓冲。当后续的在邻近位置上访问，将不会发生额外的从显存过来的传输。每个warp里的32个线程，进行一致性的相同常量内存位置读取的时候，这种广播效果和cache命中效果可以节省执行时间当程序进行具有很大程度上的空间邻近性的访存的时候，纹理变得非常高效。空间邻近性的意思是，每个线程的读取位置都和其他线程的读取位置邻近。请一定要确保纹理引用被定义成全局静态变量，同时还要确保它不能作为参数传递给任何其他函数。原子操作-求和 #include &lt;stdio.h&gt; #define NUM_THREADS 10000 #define SIZE 10 #define BLOCK_WIDTH 100 __global__ void gpu_increment_without_atomic(int *d_a) { // Calculate thread id for current thread int tid = blockIdx.x * blockDim.x &#43; threadIdx.x; // each thread increments elements wrapping at SIZE variable tid = tid % SIZE; d_a[tid] &#43;= 1; } __global__ void gpu_increment_atomic(int *d_a) { // Calculate thread id for current thread int tid = blockIdx." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/1688bf9b0ec61a595c6546fb57f6e2f3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-25T13:09:01+08:00" />
<meta property="article:modified_time" content="2022-02-25T13:09:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【高性能计算】CUDA编程之线程存储与原子操作（教程与代码-2）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="3__0"></a>3 线程、同步、存储器</h3> 
<p><strong>3.1 线程与存储</strong></p> 
<ul><li>tid=blockIdx.x（当前块的ID）*blockDim.x（当前块里面的线程数量）+threadIdx.x（当前线程在块中的ID）</li><li>gridDim.x*blockDim.x来计算，前者代表了本次启动的块的数量，而后者代表了每个块里面的线程数量，然后每次while循环，tid变量加上这个值，向后偏移以得到下个任务的索引</li><li>所有线程都有一个寄存器堆，它是最快的。</li><li>共享内存只能被块中的线程访问，但比全局内存块。</li><li>全局内存是最慢的，但可以被所有的块访问。</li><li>常量和纹理内存用于特殊用途</li><li>所有通过cudaMalloc分配的存储器都是全局内存</li><li>本地内存和寄存器堆对每个线程都是唯一的。</li><li>寄存器是每个线程可用的最快存储器。</li><li>当内核中使用的变量在寄存器堆中装不下的时候，将会使用本地内存存储它们，这叫寄存器溢出</li><li>“读取旧值-累加-回写新值”操作是不可被其他线程扰乱的原子性的整体完成的。</li><li>使用atomicAdd进行原子累加的内核函数</li><li>使用原子操作后程序具有更大的执行代价。可以通过使用共享内存来加速这些原子累加操作<br> GPU卡从逻辑上对用户提供了64KB的常量内存空间，可以用来存储内核执行期间所需要的恒定数据</li><li>常量内存有助于节省全局内存的访问带宽</li><li>warp整体进行一次常量内存的读取，结果广播给warp里的32个线程。同时，常量内存具有cache缓冲。当后续的在邻近位置上访问，将不会发生额外的从显存过来的传输。每个warp里的32个线程，进行一致性的相同常量内存位置读取的时候，这种广播效果和cache命中效果可以节省执行时间</li><li>当程序进行具有很大程度上的空间邻近性的访存的时候，纹理变得非常高效。空间邻近性的意思是，每个线程的读取位置都和其他线程的读取位置邻近。</li><li>请一定要确保纹理引用被定义成全局静态变量，同时还要确保它不能作为参数传递给任何其他函数。</li><li>原子操作-求和</li></ul> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">NUM_THREADS</span> <span class="token expression"><span class="token number">10000</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">SIZE</span>  <span class="token expression"><span class="token number">10</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">BLOCK_WIDTH</span> <span class="token expression"><span class="token number">100</span></span></span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_increment_without_atomic</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>d_a<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token comment">// Calculate thread id for current thread</span>
    <span class="token keyword">int</span> tid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token comment">// each thread increments elements wrapping at SIZE variable</span>
    tid <span class="token operator">=</span> tid <span class="token operator">%</span> SIZE<span class="token punctuation">;</span>
    d_a<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_increment_atomic</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>d_a<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token comment">// Calculate thread id for current thread</span>
    <span class="token keyword">int</span> tid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token comment">// each thread increments elements wrapping at SIZE variable</span>
    tid <span class="token operator">=</span> tid <span class="token operator">%</span> SIZE<span class="token punctuation">;</span>
    <span class="token function">atomicAdd</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">[</span>tid<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span><span class="token operator">*</span>argv<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d total threads in %d blocks writing into %d array elements\n"</span><span class="token punctuation">,</span>
        NUM_THREADS<span class="token punctuation">,</span> NUM_THREADS <span class="token operator">/</span> BLOCK_WIDTH<span class="token punctuation">,</span> SIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// declare and allocate host memory</span>
    <span class="token keyword">int</span> h_a<span class="token punctuation">[</span>SIZE<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> ARRAY_BYTES <span class="token operator">=</span> SIZE <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// declare and allocate GPU memory</span>
    <span class="token keyword">int</span> <span class="token operator">*</span>d_a<span class="token punctuation">,</span> <span class="token operator">*</span>d_aA<span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">,</span> ARRAY_BYTES<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Initialize GPU memory to zero</span>
    <span class="token function">cudaMemset</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>d_a<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> ARRAY_BYTES<span class="token punctuation">)</span><span class="token punctuation">;</span>
    gpu_increment_without_atomic <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span>NUM_THREADS <span class="token operator">/</span> BLOCK_WIDTH<span class="token punctuation">,</span> BLOCK_WIDTH <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span>d_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// copy back the array to host memory</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_a<span class="token punctuation">,</span> d_a<span class="token punctuation">,</span> ARRAY_BYTES<span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Number of times a particular Array index has been incremented without atomic add is: \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> SIZE<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"index: %d --&gt; %d times\n "</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_aA<span class="token punctuation">,</span> ARRAY_BYTES<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Initialize GPU memory to zero</span>
    <span class="token function">cudaMemset</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>d_aA<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> ARRAY_BYTES<span class="token punctuation">)</span><span class="token punctuation">;</span>
    gpu_increment_atomic <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span>NUM_THREADS <span class="token operator">/</span> BLOCK_WIDTH<span class="token punctuation">,</span> BLOCK_WIDTH <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span>d_aA<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// copy back the array to host memory</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_a<span class="token punctuation">,</span> d_aA<span class="token punctuation">,</span> ARRAY_BYTES<span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Number of times a particular Array index has been incremented is: \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> SIZE<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> 
    <span class="token punctuation">{<!-- --></span> 
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"index: %d --&gt; %d times\n "</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
    <span class="token punctuation">}</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_aA<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<ul><li>常量内存与纹理内存</li></ul> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"stdio.h"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span><span class="token string">&lt;iostream&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h&gt;</span></span>
<span class="token comment">//Defining two constants</span>
__constant__ <span class="token keyword">int</span> constant_f<span class="token punctuation">;</span>
__constant__ <span class="token keyword">int</span> constant_g<span class="token punctuation">;</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">N</span>   <span class="token expression"><span class="token number">5</span></span></span>
<span class="token comment">//Kernel function for using constant memory</span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_constant_memory</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>d_in<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_out<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">//Thread index for current kernel</span>
    <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>  
    d_out<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> constant_f<span class="token operator">*</span>d_in<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+</span> constant_g<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">NUM_THREADS</span> <span class="token expression"><span class="token number">5</span></span></span>
texture <span class="token operator">&lt;</span><span class="token keyword">float</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> cudaReadModeElementType<span class="token operator">&gt;</span> textureRef<span class="token punctuation">;</span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_texture_memory</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_out<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>idx <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">float</span> temp <span class="token operator">=</span> <span class="token function">tex1D</span><span class="token punctuation">(</span>textureRef<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        d_out<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> temp<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">//Defining Arrays for host</span>
    <span class="token keyword">float</span> h_in<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">,</span> h_out<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token comment">//Defining Pointers for device</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>d_in<span class="token punctuation">,</span> <span class="token operator">*</span>d_out<span class="token punctuation">;</span>
    <span class="token comment">// 常量内存</span>
    <span class="token keyword">int</span> h_f <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token keyword">int</span> h_g <span class="token operator">=</span> <span class="token number">20</span><span class="token punctuation">;</span>
    <span class="token comment">// allocate the memory on the cpu</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_in<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_out<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Initializing Array</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        h_in<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">//Copy Array from host to device</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_in<span class="token punctuation">,</span> h_in<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Copy constants to constant memory</span>
    <span class="token function">cudaMemcpyToSymbol</span><span class="token punctuation">(</span>constant_f<span class="token punctuation">,</span> <span class="token operator">&amp;</span>h_f<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span>cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMemcpyToSymbol</span><span class="token punctuation">(</span>constant_g<span class="token punctuation">,</span> <span class="token operator">&amp;</span>h_g<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Calling kernel with one block and N threads per block</span>
    gpu_constant_memory <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> N <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span>d_in<span class="token punctuation">,</span> d_out<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Coping result back to host from device memory</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_out<span class="token punctuation">,</span> d_out<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Printing result on console</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Use of Constant memory on GPU \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"The expression for input %f is %f\n"</span><span class="token punctuation">,</span> h_in<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> h_out<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">//Free up memory</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_in<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_out<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// 纹理内存</span>
    <span class="token comment">//Calculate number of blocks to launch</span>
    <span class="token keyword">int</span> num_blocks <span class="token operator">=</span> N <span class="token operator">/</span> NUM_THREADS <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>N <span class="token operator">%</span> NUM_THREADS<span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token number">1</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Declare device pointer</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>d_outM<span class="token punctuation">;</span>
    <span class="token comment">// allocate space on the device for the result</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_outM<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// allocate space on the host for the results</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>h_outM <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>N<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Declare and initialize host array</span>
    <span class="token keyword">float</span> h_inM<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        h_inM<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token keyword">float</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">//Define CUDA Array</span>
    cudaArray <span class="token operator">*</span>cu_Array<span class="token punctuation">;</span>
    <span class="token function">cudaMallocArray</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>cu_Array<span class="token punctuation">,</span> <span class="token operator">&amp;</span>textureRef<span class="token punctuation">.</span>channelDesc<span class="token punctuation">,</span> N<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Copy data to CUDA Array</span>
    <span class="token function">cudaMemcpyToArray</span><span class="token punctuation">(</span>cu_Array<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> h_inM<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>N<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// bind a texture to the CUDA array</span>
    <span class="token function">cudaBindTextureToArray</span><span class="token punctuation">(</span>textureRef<span class="token punctuation">,</span> cu_Array<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Call Kernel   </span>
    gpu_texture_memory <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span>num_blocks<span class="token punctuation">,</span> NUM_THREADS <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> d_outM<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">// copy result back to host</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_outM<span class="token punctuation">,</span> d_outM<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>N<span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Use of Texture memory on GPU: \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Texture element at %d is : %f\n"</span><span class="token punctuation">,</span>i<span class="token punctuation">,</span> h_outM<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token function">free</span><span class="token punctuation">(</span>h_outM<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_outM<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFreeArray</span><span class="token punctuation">(</span>cu_Array<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaUnbindTexture</span><span class="token punctuation">(</span>textureRef<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<ul><li>全局、局部、共享内存</li></ul> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">N</span> <span class="token expression"><span class="token number">5</span></span></span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_global_memory</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>d_a<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token comment">// "array" is a pointer into global memory on the device</span>
    d_a<span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_local_memory</span><span class="token punctuation">(</span><span class="token keyword">float</span> d_in<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> t_local<span class="token punctuation">;</span>    
    t_local <span class="token operator">=</span> d_in <span class="token operator">*</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>     
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Value of Local variable in current thread is: %d \n"</span><span class="token punctuation">,</span> t_local<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_shared_memory</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>d_a<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token comment">// Defining local variables which are private to each thread</span>
    <span class="token keyword">int</span> i<span class="token punctuation">,</span> index <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">float</span> average<span class="token punctuation">,</span> sum <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
    <span class="token comment">//Define shared memory</span>
    __shared__ <span class="token keyword">float</span> sh_arr<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
    sh_arr<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> d_a<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">// This ensures all the writes to shared memory have completed</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;=</span> index<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> 
    <span class="token punctuation">{<!-- --></span> 
        sum <span class="token operator">+=</span> sh_arr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span> 
    <span class="token punctuation">}</span>
    average <span class="token operator">=</span> sum <span class="token operator">/</span> <span class="token punctuation">(</span>index <span class="token operator">+</span> <span class="token number">1.0f</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    d_a<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> average<span class="token punctuation">;</span> 
    sh_arr<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> average<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span><span class="token operator">*</span>argv<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token comment">// Define Host Array</span>
    <span class="token keyword">float</span> h_a<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token comment">//Define device pointer </span>
    <span class="token keyword">float</span> <span class="token operator">*</span>d_a<span class="token punctuation">;</span>       
    <span class="token comment">// 全局内存</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span>N<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// now copy data from host memory to device memory </span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>d_a<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>h_a<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span>N<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// launch the kernel </span>
    gpu_global_memory <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> N <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span>d_a<span class="token punctuation">)</span><span class="token punctuation">;</span>  
    <span class="token comment">// copy the modified array back to the host memory</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>h_a<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>d_a<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span>N<span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Array in Global Memory is: \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Printing result on console</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"At Index: %d --&gt; %f \n"</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// 本地内存</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Use of Local Memory on GPU:\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    gpu_local_memory <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> N <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  
    <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
      
    <span class="token comment">// 共享内存 </span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// allocate global memory on the device</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// now copy data from host memory  to device memory </span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>d_a<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>h_a<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    gpu_shared_memory <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span> <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span>d_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// copy the modified array back to the host memory</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>h_a<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span>d_a<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Use of Shared Memory on GPU:  \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Printing result on console</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"The running average after %d element is %f \n"</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<ul><li>多线程</li></ul> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"stdio.h"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span><span class="token string">&lt;iostream&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h&gt;</span></span>
<span class="token comment">//Defining number of elements in Array</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">N</span>   <span class="token expression"><span class="token number">50000</span></span></span>
<span class="token comment">//Defining Kernel function for vector addition</span>
__global__ <span class="token keyword">void</span> <span class="token function">gpuAdd</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>d_a<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>d_b<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>d_c<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">//Getting block index of current kernel</span>
    <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>    
    <span class="token keyword">while</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> N<span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        d_c<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> d_a<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+</span> d_b<span class="token punctuation">[</span>tid<span class="token punctuation">]</span><span class="token punctuation">;</span>
        tid <span class="token operator">+=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
        
<span class="token punctuation">}</span>
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">//Defining host arrays</span>
    <span class="token keyword">int</span> h_a<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">,</span> h_b<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">,</span> h_c<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token comment">//Defining device pointers</span>
    <span class="token keyword">int</span> <span class="token operator">*</span>d_a<span class="token punctuation">,</span> <span class="token operator">*</span>d_b<span class="token punctuation">,</span> <span class="token operator">*</span>d_c<span class="token punctuation">;</span>
    <span class="token comment">// allocate the memory</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_b<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_c<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Initializing Arrays</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> i<span class="token operator">*</span>i<span class="token punctuation">;</span>
        h_b<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// Copy input arrays from host to device memory</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_a<span class="token punctuation">,</span> h_a<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_b<span class="token punctuation">,</span> h_b<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Calling kernels with N blocks and one thread per block, passing device pointers as parameters</span>
    gpuAdd <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span>d_a<span class="token punctuation">,</span> d_b<span class="token punctuation">,</span> d_c<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Copy result back to host memory from device memory</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_c<span class="token punctuation">,</span> d_c<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">int</span> Correct <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Vector addition on GPU \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Printing result on console</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> h_b<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">!=</span> h_c<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">{<!-- --></span>
            Correct <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        
    <span class="token punctuation">}</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>Correct <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"GPU has computed Sum Correctly\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">else</span>
    <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"There is an Error in GPU Computation\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">//Free up memory</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_b<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_c<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>3.2 向量点乘与矩阵乘法例子</strong></p> 
<ul><li>向量点乘</li><li>矩阵乘法</li></ul> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"stdio.h"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;math.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">TILE_SIZE</span> <span class="token expression"><span class="token number">2</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">N</span> <span class="token expression"><span class="token number">1024</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">threadsPerBlock</span> <span class="token expression"><span class="token number">512</span></span></span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_dot</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>d_a<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_b<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_c<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">//Declare shared memory</span>
    __shared__ <span class="token keyword">float</span> partial_sum<span class="token punctuation">[</span>threadsPerBlock<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token comment">//Calculate index for shared memory </span>
    <span class="token keyword">int</span> index <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token comment">//Calculate Partial Sum</span>
    <span class="token keyword">float</span> sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">while</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> N<span class="token punctuation">)</span> 
    <span class="token punctuation">{<!-- --></span>
        sum <span class="token operator">+=</span> d_a<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">*</span> d_b<span class="token punctuation">[</span>tid<span class="token punctuation">]</span><span class="token punctuation">;</span>
        tid <span class="token operator">+=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span> <span class="token comment">// 前者代表了本次启动的块的数量，而后者代表了每个块里面的线程数量</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// Store partial sum in shared memory</span>
    partial_sum<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">=</span> sum<span class="token punctuation">;</span>
    <span class="token comment">// synchronize threads </span>
    <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// Calculating partial sum for whole block in reduce operation  </span>
    <span class="token keyword">int</span> i <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>index <span class="token operator">&lt;</span> i<span class="token punctuation">)</span>
            partial_sum<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">+=</span> partial_sum<span class="token punctuation">[</span>index <span class="token operator">+</span> i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        i <span class="token operator">/=</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">//Store block partial sum in global memory</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>index <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
        d_c<span class="token punctuation">[</span>blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> partial_sum<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token comment">//Matrix multiplication using non shared kernel</span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_Matrix_Mul_nonshared</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>d_a<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_b<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_c<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> size<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> row<span class="token punctuation">,</span> col<span class="token punctuation">;</span>
    col <span class="token operator">=</span> TILE_SIZE <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    row <span class="token operator">=</span> TILE_SIZE <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k<span class="token operator">&lt;</span> size<span class="token punctuation">;</span> k<span class="token operator">++</span><span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        d_c<span class="token punctuation">[</span>row<span class="token operator">*</span>size <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">+=</span> d_a<span class="token punctuation">[</span>row <span class="token operator">*</span> size <span class="token operator">+</span> k<span class="token punctuation">]</span> <span class="token operator">*</span> d_b<span class="token punctuation">[</span>k <span class="token operator">*</span> size <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
<span class="token comment">// Matrix multiplication using shared kernel</span>
__global__ <span class="token keyword">void</span> <span class="token function">gpu_Matrix_Mul_shared</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>d_a<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_b<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_c<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> size<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> row<span class="token punctuation">,</span> col<span class="token punctuation">;</span>
    <span class="token comment">//Defining Shared Memory</span>
    __shared__ <span class="token keyword">float</span> shared_a<span class="token punctuation">[</span>TILE_SIZE<span class="token punctuation">]</span><span class="token punctuation">[</span>TILE_SIZE<span class="token punctuation">]</span><span class="token punctuation">;</span>
    __shared__ <span class="token keyword">float</span> shared_b<span class="token punctuation">[</span>TILE_SIZE<span class="token punctuation">]</span><span class="token punctuation">[</span>TILE_SIZE<span class="token punctuation">]</span><span class="token punctuation">;</span>
    col <span class="token operator">=</span> TILE_SIZE <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    row <span class="token operator">=</span> TILE_SIZE <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span> size <span class="token operator">/</span> TILE_SIZE<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> 
    <span class="token punctuation">{<!-- --></span>
        shared_a<span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">]</span><span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> d_a<span class="token punctuation">[</span>row<span class="token operator">*</span> size <span class="token operator">+</span> <span class="token punctuation">(</span>i<span class="token operator">*</span>TILE_SIZE <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
        shared_b<span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">]</span><span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> d_b<span class="token punctuation">[</span><span class="token punctuation">(</span>i<span class="token operator">*</span>TILE_SIZE <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">)</span> <span class="token operator">*</span> size <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>TILE_SIZE<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span>
            d_c<span class="token punctuation">[</span>row<span class="token operator">*</span>size <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">+=</span> shared_a<span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">*</span> shared_b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
<span class="token keyword">int</span> <span class="token function">main_dot</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">//Declare Host Array</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>h_a<span class="token punctuation">,</span> <span class="token operator">*</span>h_b<span class="token punctuation">,</span> h_c<span class="token punctuation">,</span> <span class="token operator">*</span>partial_sum<span class="token punctuation">;</span>
    <span class="token comment">//Declare device Array</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>d_a<span class="token punctuation">,</span> <span class="token operator">*</span>d_b<span class="token punctuation">,</span> <span class="token operator">*</span>d_partial_sum<span class="token punctuation">;</span>
    <span class="token comment">//Calculate total number of blocks per grid</span>
    <span class="token keyword">int</span> block_calc <span class="token operator">=</span> <span class="token punctuation">(</span>N <span class="token operator">+</span> threadsPerBlock <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> threadsPerBlock<span class="token punctuation">;</span>
    <span class="token keyword">int</span> blocksPerGrid <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">32</span> <span class="token operator">&lt;</span> block_calc <span class="token operator">?</span> <span class="token number">32</span> <span class="token operator">:</span> block_calc<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// allocate memory on the host side</span>
    h_a <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    h_b <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    partial_sum <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>blocksPerGrid <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// allocate the memory on the device</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_b<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_partial_sum<span class="token punctuation">,</span> blocksPerGrid <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// fill the host array with data</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>
        h_b<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_a<span class="token punctuation">,</span> h_a<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_b<span class="token punctuation">,</span> h_b<span class="token punctuation">,</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//Call kernel </span>
    gpu_dot <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span>blocksPerGrid<span class="token punctuation">,</span> threadsPerBlock <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span><span class="token punctuation">(</span>d_a<span class="token punctuation">,</span> d_b<span class="token punctuation">,</span> d_partial_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// copy the array back to host memory</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>partial_sum<span class="token punctuation">,</span> d_partial_sum<span class="token punctuation">,</span> blocksPerGrid <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// Calculate final dot product on host</span>
    h_c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>blocksPerGrid<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        h_c <span class="token operator">+=</span> partial_sum<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"The computed dot product is: %f\n"</span><span class="token punctuation">,</span> h_c<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">cpu_sum</span><span class="token expression"><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token punctuation">(</span>x<span class="token operator">*</span><span class="token punctuation">(</span>x<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span></span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>h_c <span class="token operator">==</span> <span class="token function">cpu_sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">(</span>N <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"The dot product computed by GPU is correct\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">else</span>
    <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Error in dot product computation"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    
    <span class="token comment">// free memory on host and device</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_b<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_partial_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>h_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>h_b<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>partial_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">int</span> <span class="token function">main_matrix</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> size <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">;</span>
    <span class="token comment">//Define Host Array</span>
    <span class="token keyword">float</span> h_a<span class="token punctuation">[</span>size<span class="token punctuation">]</span><span class="token punctuation">[</span>size<span class="token punctuation">]</span><span class="token punctuation">,</span> h_b<span class="token punctuation">[</span>size<span class="token punctuation">]</span><span class="token punctuation">[</span>size<span class="token punctuation">]</span><span class="token punctuation">,</span>h_result<span class="token punctuation">[</span>size<span class="token punctuation">]</span><span class="token punctuation">[</span>size<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token comment">//Defining device Array</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>d_a<span class="token punctuation">,</span> <span class="token operator">*</span>d_b<span class="token punctuation">,</span> <span class="token operator">*</span>d_result<span class="token punctuation">;</span> 
    <span class="token comment">//Initialize host Array</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j<span class="token operator">&lt;</span>size<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span>
        <span class="token punctuation">{<!-- --></span>
            h_a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>
            h_b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> j<span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">,</span> size<span class="token operator">*</span>size<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_b<span class="token punctuation">,</span> size<span class="token operator">*</span>size <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_result<span class="token punctuation">,</span> size<span class="token operator">*</span>size<span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//copy host array to device array</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_a<span class="token punctuation">,</span> h_a<span class="token punctuation">,</span> size<span class="token operator">*</span>size<span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_b<span class="token punctuation">,</span> h_b<span class="token punctuation">,</span> size<span class="token operator">*</span>size<span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">//Define grid and block dimensions</span>
    dim3 <span class="token function">dimGrid</span><span class="token punctuation">(</span>size <span class="token operator">/</span> TILE_SIZE<span class="token punctuation">,</span> size <span class="token operator">/</span> TILE_SIZE<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    dim3 <span class="token function">dimBlock</span><span class="token punctuation">(</span>TILE_SIZE<span class="token punctuation">,</span> TILE_SIZE<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">//gpu_Matrix_Mul_nonshared &lt;&lt; &lt;dimGrid, dimBlock &gt;&gt; &gt; (d_a, d_b, d_result, size);</span>
    gpu_Matrix_Mul_shared <span class="token operator">&lt;&lt;</span> <span class="token operator">&lt;</span>dimGrid<span class="token punctuation">,</span> dimBlock <span class="token operator">&gt;&gt;</span> <span class="token operator">&gt;</span> <span class="token punctuation">(</span>d_a<span class="token punctuation">,</span> d_b<span class="token punctuation">,</span> d_result<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_result<span class="token punctuation">,</span> d_result<span class="token punctuation">,</span> size<span class="token operator">*</span>size <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"The result of Matrix multiplication is: \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span> size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
    <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span>
        <span class="token punctuation">{<!-- --></span>
            <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%f   "</span><span class="token punctuation">,</span> h_result<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_b<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>d_result<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
    <span class="token function">main_matrix</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4a0dce8a06c0336a04f2992ecbaf5b2c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">不能使用鼠标在vi中实现粘贴功能</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e19e054e23921a36a6db7d3beb58373b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Maven 项目中，“Dependency ‘xxxx‘ not found“ 解决过程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>