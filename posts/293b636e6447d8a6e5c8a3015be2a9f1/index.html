<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PolicyGradient算法玩CartPole和MountainCar代码Pytorch版本 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PolicyGradient算法玩CartPole和MountainCar代码Pytorch版本" />
<meta property="og:description" content="参考链接 代码参考了莫烦python的policy gradient算法，特别感谢！！！
库版本号 gym0.26.2
gym-notices 0.0.8
pytorch 1.11.0 &#43; cu115
python 3.8.12
gym0.26.2版本用法略有不同，涉及到step函数、render函数渲染图像方法，我会额外开一篇帖子介绍
PolicyGradient决策代码 玩CartPole倒立摆和玩MountainCar的PolicyGradient算法决策代码是一样的，也就是下面的RL_brain.py文件
import torch import torch.nn as nn import torch.nn.functional as F import numpy as np np.random.seed(1) torch.manual_seed(1) class NetWork(nn.Module): &#34;&#34;&#34; 神经网络结构 # 全连接1 # 全连接2 # ReLU &#34;&#34;&#34; def __init__(self, n_actions, n_features, n_neuron=10): super(NetWork, self).__init__() self.net = nn.Sequential( nn.Linear(in_features=n_features, out_features=n_neuron, bias=True), nn.Linear(in_features=n_neuron, out_features=n_actions, bias=True), nn.ReLU() ) def forward(self, x): return self.net(x) class PolicyGradient: &#34;&#34;&#34; PolicyGradient算法 &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/293b636e6447d8a6e5c8a3015be2a9f1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-29T15:24:51+08:00" />
<meta property="article:modified_time" content="2023-06-29T15:24:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PolicyGradient算法玩CartPole和MountainCar代码Pytorch版本</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>参考链接</h2> 
<p>代码参考了莫烦python的policy gradient算法，特别感谢！！！</p> 
<h2><a id="_3"></a>库版本号</h2> 
<p>gym0.26.2<br> gym-notices 0.0.8<br> pytorch 1.11.0 + cu115<br> python 3.8.12</p> 
<p>gym0.26.2版本用法略有不同，涉及到step函数、render函数渲染图像方法，我会额外开一篇帖子介绍</p> 
<h2><a id="PolicyGradient_11"></a>PolicyGradient决策代码</h2> 
<p>玩CartPole倒立摆和玩MountainCar的PolicyGradient算法决策代码是一样的，也就是下面的RL_brain.py文件</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">NetWork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    神经网络结构
    # 全连接1
    # 全连接2
    # ReLU
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 n_actions<span class="token punctuation">,</span>
                 n_features<span class="token punctuation">,</span>
                 n_neuron<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>NetWork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>n_features<span class="token punctuation">,</span>
                      out_features<span class="token operator">=</span>n_neuron<span class="token punctuation">,</span>
                      bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>n_neuron<span class="token punctuation">,</span>
                      out_features<span class="token operator">=</span>n_actions<span class="token punctuation">,</span>
                      bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">PolicyGradient</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    PolicyGradient算法
    """</span>
    <span class="token comment"># 初始化</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 n_actions<span class="token punctuation">,</span>
                 n_features<span class="token punctuation">,</span>
                 n_neuron<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
                 learning_rate<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
                 reward_decay<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>n_actions <span class="token operator">=</span> n_actions
        self<span class="token punctuation">.</span>n_features <span class="token operator">=</span> n_features
        self<span class="token punctuation">.</span>n_neuron <span class="token operator">=</span> n_neuron
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> learning_rate
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> reward_decay

        <span class="token comment"># 之前Q-learning算法定义一个memory共同存储observation action reward</span>
        <span class="token comment"># 这里定义三个memory分别存储observation action reward</span>
        <span class="token comment"># self.ep_obs存储observation</span>
        <span class="token comment"># self.ep_as存储action</span>
        <span class="token comment"># self.ep_rs存储reward</span>
        <span class="token comment"># learn网络的时候用memory里边全部内容学习，没有batch_size学习，相对简单些</span>
        self<span class="token punctuation">.</span>ep_obs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>ep_as<span class="token punctuation">,</span> self<span class="token punctuation">.</span>ep_rs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>net <span class="token operator">=</span> NetWork<span class="token punctuation">(</span>n_actions<span class="token operator">=</span>self<span class="token punctuation">.</span>n_actions<span class="token punctuation">,</span>
                           n_features<span class="token operator">=</span>self<span class="token punctuation">.</span>n_features<span class="token punctuation">,</span>
                           n_neuron<span class="token operator">=</span>self<span class="token punctuation">.</span>n_neuron<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>params<span class="token operator">=</span>self<span class="token punctuation">.</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                      lr<span class="token operator">=</span>self<span class="token punctuation">.</span>lr<span class="token punctuation">)</span>

    <span class="token comment"># 选行为（和Q-learning算法相比有改变）</span>
    <span class="token keyword">def</span> <span class="token function">choose_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> observation<span class="token punctuation">)</span><span class="token punctuation">:</span>
        s <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>observation<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>net<span class="token punctuation">(</span>s<span class="token punctuation">)</span>  <span class="token comment"># 给net一个输入</span>
        prob_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>out<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        prob_weights <span class="token operator">=</span> prob_weights<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 根据概率抽样得到一个action</span>
        action <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>prob_weights<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> p<span class="token operator">=</span>prob_weights<span class="token punctuation">)</span>
        <span class="token keyword">return</span> action

    <span class="token comment"># 存储回合 transition（和Q-learning算法相比有改变）</span>
    <span class="token keyword">def</span> <span class="token function">store_transition</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> s<span class="token punctuation">,</span> a<span class="token punctuation">,</span> r<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>ep_obs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ep_as<span class="token punctuation">.</span>append<span class="token punctuation">(</span>a<span class="token punctuation">)</span>  <span class="token comment"># 这句是什么意思</span>
        self<span class="token punctuation">.</span>ep_rs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>r<span class="token punctuation">)</span>

    <span class="token comment"># 学习更新参数（和Q-learning算法相比有改变）</span>
    <span class="token keyword">def</span> <span class="token function">learn</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># discount and normalize episode reward</span>
        discounted_ep_rs_norm <span class="token operator">=</span> self<span class="token punctuation">.</span>_discount_and_norm_rewards<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 转换成torch.tensor数据类型</span>
        s <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ep_obs<span class="token punctuation">)</span><span class="token punctuation">)</span>
        action <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ep_as<span class="token punctuation">)</span><span class="token punctuation">)</span>

        discounted_ep_rs_norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>discounted_ep_rs_norm<span class="token punctuation">)</span>

        <span class="token comment"># net输出</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>net<span class="token punctuation">(</span>s<span class="token punctuation">)</span>

        <span class="token comment"># train on episode</span>
        <span class="token comment"># loss = nn.CrossEntropyLoss(reduction='none')(out, action) 没有带weight</span>
        neg_log_prob <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>out<span class="token punctuation">,</span> action<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>neg_log_prob <span class="token operator">*</span> discounted_ep_rs_norm<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># empty episode data</span>
        self<span class="token punctuation">.</span>ep_obs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>ep_as<span class="token punctuation">,</span> self<span class="token punctuation">.</span>ep_rs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> discounted_ep_rs_norm<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 衰减回合的reward（和Q-learning算法相比有新内容）</span>
    <span class="token keyword">def</span> <span class="token function">_discount_and_norm_rewards</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># discount episode rewards</span>
        discounted_ep_rs <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ep_rs<span class="token punctuation">)</span>
        running_add <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">reversed</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>ep_rs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            running_add <span class="token operator">=</span> running_add <span class="token operator">*</span> self<span class="token punctuation">.</span>gamma <span class="token operator">+</span> self<span class="token punctuation">.</span>ep_rs<span class="token punctuation">[</span>t<span class="token punctuation">]</span>
            discounted_ep_rs<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">=</span> running_add

        <span class="token comment"># normalize episode rewards</span>
        discounted_ep_rs <span class="token operator">-=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>discounted_ep_rs<span class="token punctuation">)</span>
        discounted_ep_rs <span class="token operator">/=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>discounted_ep_rs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> discounted_ep_rs
</code></pre> 
<h2><a id="CartPole_137"></a>玩CartPole倒立摆控制代码</h2> 
<p>run_CartPole.py</p> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">"""
gym: 0.26.2
gym-notices 0.0.8
"""</span>

<span class="token keyword">import</span> gym
<span class="token keyword">from</span> RL_brain <span class="token keyword">import</span> PolicyGradient
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

RENDER <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># 在屏幕上显示模拟窗口会拖慢运行速度 我们等计算机学得差不多了再显示模拟</span>
DISPLAY_REWARD_THRESHOLD <span class="token operator">=</span> <span class="token number">10000</span>  <span class="token comment"># 当回合总reward大于400时显示模拟窗口</span>

env <span class="token operator">=</span> gym<span class="token punctuation">.</span>make<span class="token punctuation">(</span><span class="token string">'CartPole-v1'</span><span class="token punctuation">)</span>
<span class="token comment"># env = gym.make('CartPole-v1', render_mode='human')  # CartPole模拟</span>
env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 普通的Policy Gradient方法，使得回合的variance比较大，所以我们选了</span>
env <span class="token operator">=</span> env<span class="token punctuation">.</span>unwrapped  <span class="token comment"># 取消限制</span>


<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">)</span>  <span class="token comment"># 显示可用action</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">)</span>  <span class="token comment"># 显示可用state的observation</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>high<span class="token punctuation">)</span>  <span class="token comment"># 显示observation最高值</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>low<span class="token punctuation">)</span>  <span class="token comment"># 显示observation最低值</span>


<span class="token comment"># 定义</span>
RL <span class="token operator">=</span> PolicyGradient<span class="token punctuation">(</span>
    n_actions<span class="token operator">=</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>n<span class="token punctuation">,</span>
    n_features<span class="token operator">=</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    n_neuron<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span>
    reward_decay<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span>  <span class="token comment"># gamma</span>
<span class="token punctuation">)</span>

<span class="token keyword">for</span> i_episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">424</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    observation<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> RENDER<span class="token punctuation">:</span>
            env_test <span class="token operator">=</span> gym<span class="token punctuation">.</span>make<span class="token punctuation">(</span><span class="token string">'CartPole-v1'</span><span class="token punctuation">,</span> render_mode<span class="token operator">=</span><span class="token string">'human'</span><span class="token punctuation">)</span>
            env_test<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># env.render()</span>

        action <span class="token operator">=</span> RL<span class="token punctuation">.</span>choose_action<span class="token punctuation">(</span>observation<span class="token punctuation">)</span>

        observation_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> truncated<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>

        RL<span class="token punctuation">.</span>store_transition<span class="token punctuation">(</span>observation<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">)</span>  <span class="token comment"># 存储这一回合的transition</span>

        <span class="token keyword">if</span> done<span class="token punctuation">:</span>
            ep_rs_sum <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>RL<span class="token punctuation">.</span>ep_rs<span class="token punctuation">)</span>

            <span class="token keyword">if</span> <span class="token string">'running_reward'</span> <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                running_reward <span class="token operator">=</span> ep_rs_sum
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                running_reward <span class="token operator">=</span> running_reward <span class="token operator">*</span> <span class="token number">0.99</span> <span class="token operator">+</span> ep_rs_sum <span class="token operator">*</span> <span class="token number">0.01</span>

            <span class="token keyword">if</span> running_reward <span class="token operator">&gt;</span> DISPLAY_REWARD_THRESHOLD<span class="token punctuation">:</span>
                RENDER <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># 判断是否显示</span>

            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'episode:'</span><span class="token punctuation">,</span> i_episode<span class="token punctuation">,</span> <span class="token string">' reward:'</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>running_reward<span class="token punctuation">)</span><span class="token punctuation">)</span>

            vt <span class="token operator">=</span> RL<span class="token punctuation">.</span>learn<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 学习，输出vt，我们下节课讲这个vt的作用</span>

            <span class="token keyword">if</span> i_episode <span class="token operator">==</span> <span class="token number">423</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>vt<span class="token punctuation">)</span>  <span class="token comment"># plot这个回合的vt</span>
                plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'episode steps'</span><span class="token punctuation">)</span>
                plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'normalized state-action value'</span><span class="token punctuation">)</span>
                plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">break</span>

        observation <span class="token operator">=</span> observation_
</code></pre> 
<h2><a id="MountainCar_215"></a>玩MountainCar小车控制代码</h2> 
<p>run_MountainCar.py</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> gym
<span class="token keyword">from</span> RL_brain <span class="token keyword">import</span> PolicyGradient
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt


DISPLAY_REWARD_THRESHOLD <span class="token operator">=</span> <span class="token number">200</span>  <span class="token comment"># renders environment if total episode reward is greater than this threshold</span>

<span class="token comment"># episode: 154  reward: -10667</span>
<span class="token comment"># episode: 387  reward: -2009</span>
<span class="token comment"># episode: 489  reward: -1006</span>
<span class="token comment"># episdoe: 628  reward: -502</span>

RENDER <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># rendering wastes time</span>

env <span class="token operator">=</span> gym<span class="token punctuation">.</span>make<span class="token punctuation">(</span><span class="token string">'MountainCar-v0'</span><span class="token punctuation">)</span>
env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># reproducible, general Policy gradient has high variance</span>
env <span class="token operator">=</span> env<span class="token punctuation">.</span>unwrapped

<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>high<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>low<span class="token punctuation">)</span>

RL <span class="token operator">=</span> PolicyGradient<span class="token punctuation">(</span>
    n_actions<span class="token operator">=</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>n<span class="token punctuation">,</span>
    n_features<span class="token operator">=</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    n_neuron<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span>
    reward_decay<span class="token operator">=</span><span class="token number">0.995</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token keyword">for</span> i_episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">31</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    observation<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> RENDER<span class="token punctuation">:</span>
            <span class="token comment"># env.render()</span>
            <span class="token keyword">pass</span>

        action <span class="token operator">=</span> RL<span class="token punctuation">.</span>choose_action<span class="token punctuation">(</span>observation<span class="token punctuation">)</span>

        observation_<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> truncated<span class="token punctuation">,</span> info <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>  <span class="token comment"># reward = -1 in all cases</span>

        RL<span class="token punctuation">.</span>store_transition<span class="token punctuation">(</span>observation<span class="token punctuation">,</span> action<span class="token punctuation">,</span> reward<span class="token punctuation">)</span>

        <span class="token keyword">if</span> done<span class="token punctuation">:</span>
            <span class="token comment"># 每个回合结束之后更新网络参数</span>
            ep_rs_sum <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>RL<span class="token punctuation">.</span>ep_rs<span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token string">'running_reward'</span> <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment"># 如果当前模块不包含running_reward</span>
                running_reward <span class="token operator">=</span> ep_rs_sum
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                running_reward <span class="token operator">=</span> running_reward <span class="token operator">*</span> <span class="token number">0.99</span> <span class="token operator">+</span> ep_rs_sum <span class="token operator">*</span> <span class="token number">0.01</span>

            <span class="token keyword">if</span> running_reward <span class="token operator">&gt;</span> DISPLAY_REWARD_THRESHOLD<span class="token punctuation">:</span>
                RENDER <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># rendering</span>

            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'episode:'</span><span class="token punctuation">,</span> i_episode<span class="token punctuation">,</span> <span class="token string">' reward:'</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>running_reward<span class="token punctuation">)</span><span class="token punctuation">)</span>

            vt <span class="token operator">=</span> RL<span class="token punctuation">.</span>learn<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># train</span>

            <span class="token keyword">if</span> i_episode <span class="token operator">==</span> <span class="token number">30</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>vt<span class="token punctuation">)</span>  <span class="token comment"># plot the episode vt</span>
                plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'episode steps'</span><span class="token punctuation">)</span>
                plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'normalized state-action value'</span><span class="token punctuation">)</span>
                plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token keyword">break</span>

        observation <span class="token operator">=</span> observation_
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ab5543fbb97eb798de1e33394c5eac6d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SQL多列合并成一列</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/925124181d504c2f6ffbd51fc27d7da2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">gin框架：微信公众号-服务器配置（token验证）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>