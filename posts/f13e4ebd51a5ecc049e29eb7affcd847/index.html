<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CUDA中动态Global Memory分配和操作 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CUDA中动态Global Memory分配和操作" />
<meta property="og:description" content="CUDA中动态Global Memory分配和操作 CUDA中动态Global Memory分配和操作1. Heap Memory Allocation2. Interoperability with Host Memory API3. Examples3.1. Per Thread Allocation3.2. Per Thread Block Allocation3.3. Allocation Persisting Between Kernel Launches 动态全局内存分配和操作仅受计算能力 2.x 及更高版本的设备支持。
__host__ __device__ void* malloc(size_t size); __device__ void *__nv_aligned_device_malloc(size_t size, size_t align); __host__ __device__ void free(void* ptr); 从全局内存中的固定大小的堆中动态分配和释放内存。
__host__ __device__ void* memcpy(void* dest, const void* src, size_t size); 从 src 指向的内存位置复制 size 个字节到 dest 指向的内存位置。
__host__ __device__ void* memset(void* ptr, int value, size_t size); 将 ptr 指向的内存块的 size 字节设置为 value（解释为无符号字符）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/f13e4ebd51a5ecc049e29eb7affcd847/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-04T14:25:04+08:00" />
<meta property="article:modified_time" content="2022-06-04T14:25:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CUDA中动态Global Memory分配和操作</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="CUDAGlobal_Memory_0"></a>CUDA中动态Global Memory分配和操作</h2> 
<p></p> 
<div class="toc"> 
 <h4> </h4> 
 <ul><li><a href="#CUDAGlobal_Memory_0" rel="nofollow">CUDA中动态Global Memory分配和操作</a></li><li><ul><li><a href="#1_Heap_Memory_Allocation_29" rel="nofollow">1. Heap Memory Allocation</a></li><li><a href="#2_Interoperability_with_Host_Memory_API_44" rel="nofollow">2. Interoperability with Host Memory API</a></li><li><a href="#3_Examples_51" rel="nofollow">3. Examples</a></li><li><ul><li><a href="#31_Per_Thread_Allocation_52" rel="nofollow">3.1. Per Thread Allocation</a></li><li><a href="#32_Per_Thread_Block_Allocation_87" rel="nofollow">3.2. Per Thread Block Allocation</a></li><li><a href="#33_Allocation_Persisting_Between_Kernel_Launches_131" rel="nofollow">3.3. Allocation Persisting Between Kernel Launches</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<br> 
<img src="https://images2.imgbox.com/39/42/QtjrFRxZ_o.gif" alt="在这里插入图片描述"> 
<p></p> 
<p>动态全局内存分配和操作仅受计算能力 2.x 及更高版本的设备支持。</p> 
<pre><code class="prism language-C++">__host__ __device__ void* malloc(size_t size);
__device__ void *__nv_aligned_device_malloc(size_t size, size_t align);
__host__ __device__  void free(void* ptr);
</code></pre> 
<p>从全局内存中的固定大小的堆中动态分配和释放内存。</p> 
<pre><code class="prism language-C++">__host__ __device__ void* memcpy(void* dest, const void* src, size_t size);
</code></pre> 
<p>从 <code>src</code> 指向的内存位置复制 <code>size</code> 个字节到 <code>dest</code> 指向的内存位置。</p> 
<pre><code class="prism language-C++">__host__ __device__ void* memset(void* ptr, int value, size_t size);
</code></pre> 
<p>将 <code>ptr</code> 指向的内存块的 <code>size</code> 字节设置为 <code>value</code>（解释为无符号字符）。</p> 
<p>CUDA 内核中的 <code>malloc()</code> 函数从设备堆中分配至少 <code>size</code> 个字节，并返回一个指向已分配内存的指针，如果没有足够的内存来满足请求，则返回 NULL。返回的指针保证与 16 字节边界对齐。</p> 
<p>内核中的 CUDA <code>__nv_aligned_device_malloc()</code> 函数从设备堆中分配至少 <code>size</code> 个字节，并返回一个指向已分配内存的指针，如果内存不足以满足请求的大小或对齐，则返回 NULL。分配内存的地址将是 <code>align</code> 的倍数。 <code>align</code> 必须是 2 的非零幂。</p> 
<p>CUDA 内核中的 <code>free()</code> 函数释放 <code>ptr</code> 指向的内存，该内存必须由先前对 <code>malloc()</code> 或 <code>__nv_aligned_device_malloc()</code> 的调用返回。如果 <code>ptr</code> 为 NULL，则忽略对 <code>free()</code> 的调用。使用相同的 <code>ptr</code> 重复调用 <code>free()</code> 具有未定义的行为。</p> 
<p>给定 CUDA 线程通过 <code>malloc()</code> 或 <code>__nv_aligned_device_malloc()</code> 分配的内存在 CUDA 上下文的生命周期内保持分配状态，或者直到通过调用 <code>free()</code> 显式释放。它可以被任何其他 CUDA 线程使用，即使在随后的内核启动时也是如此。任何 CUDA 线程都可以释放由另一个线程分配的内存，但应注意确保不会多次释放同一指针。</p> 
<h3><a id="1_Heap_Memory_Allocation_29"></a>1. Heap Memory Allocation</h3> 
<p>设备内存堆具有固定大小，必须在任何使用 <code>malloc()、__nv_aligned_device_malloc() 或 free()</code> 的程序加载到上下文之前指定该大小。 如果任何程序在没有明确指定堆大小的情况下使用 <code>malloc() 或 __nv_aligned_device_malloc()</code> ，则会分配 8 MB 的默认堆。</p> 
<p>以下 API 函数获取和设置堆大小：</p> 
<ul><li><code>cudaDeviceGetLimit(size_t* size, cudaLimitMallocHeapSize)</code></li><li><code>cudaDeviceSetLimit(cudaLimitMallocHeapSize, size_t size)</code></li></ul> 
<p>授予的堆大小至少为 <code>size</code> 个字节。 <code>cuCtxGetLimit() 和 cudaDeviceGetLimit()</code> 返回当前请求的堆大小。</p> 
<p>当模块被加载到上下文中时，堆的实际内存分配发生，或者显式地通过 CUDA 驱动程序 API（参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#module" rel="nofollow">模块</a>），或者隐式地通过 CUDA 运行时 API（参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-c-runtime" rel="nofollow">CUDA 运行时</a>）。 如果内存分配失败，模块加载会产生 <code>CUDA_ERROR_SHARED_OBJECT_INIT_FAILED</code> 错误。</p> 
<p>一旦发生模块加载，堆大小就无法更改，并且不会根据需要动态调整大小。</p> 
<p>除了通过主机端 CUDA API 调用（例如 <code>cudaMalloc()</code>）分配为设备堆保留的内存之外。</p> 
<h3><a id="2_Interoperability_with_Host_Memory_API_44"></a>2. Interoperability with Host Memory API</h3> 
<p>通过设备 <code>malloc()</code> 或 <code>__nv_aligned_device_malloc()</code> 分配的内存不能使用运行时释放（即，通过从设备内存调用任何空闲内存函数）。</p> 
<p>同样，通过运行时分配的内存（即，通过从设备内存调用任何内存分配函数）不能通过 <code>free()</code> 释放。</p> 
<p>此外，在设备代码中调用 <code>malloc()</code> 或 <code>__nv_aligned_device_malloc()</code> 分配的内存不能用于任何运行时或驱动程序 API 调用（即 <code>cudaMemcpy</code>、<code>cudaMemset</code> 等）。</p> 
<h3><a id="3_Examples_51"></a>3. Examples</h3> 
<h4><a id="31_Per_Thread_Allocation_52"></a>3.1. Per Thread Allocation</h4> 
<pre><code class="prism language-C++">#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;

__global__ void mallocTest()
{
    size_t size = 123;
    char* ptr = (char*)malloc(size);
    memset(ptr, 0, size);
    printf("Thread %d got pointer: %p\n", threadIdx.x, ptr);
    free(ptr);
}

int main()
{
    // Set a heap size of 128 megabytes. Note that this must
    // be done before any kernel is launched.
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);
    mallocTest&lt;&lt;&lt;1, 5&gt;&gt;&gt;();
    cudaDeviceSynchronize();
    return 0;
}
</code></pre> 
<p>上面的代码将会输出:</p> 
<pre><code>Thread 0 got pointer: 00057020
Thread 1 got pointer: 0005708c
Thread 2 got pointer: 000570f8
Thread 3 got pointer: 00057164
Thread 4 got pointer: 000571d0
</code></pre> 
<p>注意每个线程如何遇到 <code>malloc()</code> 和 <code>memset()</code> 命令，从而接收和初始化自己的分配。 （确切的指针值会有所不同：这些是说明性的。）</p> 
<h4><a id="32_Per_Thread_Block_Allocation_87"></a>3.2. Per Thread Block Allocation</h4> 
<pre><code class="prism language-C++">#include &lt;stdlib.h&gt;

__global__ void mallocTest()
{
    __shared__ int* data;

    // The first thread in the block does the allocation and then
    // shares the pointer with all other threads through shared memory,
    // so that access can easily be coalesced.
    // 64 bytes per thread are allocated.
    if (threadIdx.x == 0) {
        size_t size = blockDim.x * 64;
        data = (int*)malloc(size);
    }
    __syncthreads();

    // Check for failure
    if (data == NULL)
        return;

    // Threads index into the memory, ensuring coalescence
    int* ptr = data;
    for (int i = 0; i &lt; 64; ++i)
        ptr[i * blockDim.x + threadIdx.x] = threadIdx.x;

    // Ensure all threads complete before freeing 
    __syncthreads();

    // Only one thread may free the memory!
    if (threadIdx.x == 0)
        free(data);
}

int main()
{
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);
    mallocTest&lt;&lt;&lt;10, 128&gt;&gt;&gt;();
    cudaDeviceSynchronize();
    return 0;
}
</code></pre> 
<h4><a id="33_Allocation_Persisting_Between_Kernel_Launches_131"></a>3.3. Allocation Persisting Between Kernel Launches</h4> 
<pre><code class="prism language-C++">#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;

#define NUM_BLOCKS 20

__device__ int* dataptr[NUM_BLOCKS]; // Per-block pointer

__global__ void allocmem()
{
    // Only the first thread in the block does the allocation
    // since we want only one allocation per block.
    if (threadIdx.x == 0)
        dataptr[blockIdx.x] = (int*)malloc(blockDim.x * 4);
    __syncthreads();

    // Check for failure
    if (dataptr[blockIdx.x] == NULL)
        return;

    // Zero the data with all threads in parallel
    dataptr[blockIdx.x][threadIdx.x] = 0;
}

// Simple example: store thread ID into each element
__global__ void usemem()
{
    int* ptr = dataptr[blockIdx.x];
    if (ptr != NULL)
        ptr[threadIdx.x] += threadIdx.x;
}

// Print the content of the buffer before freeing it
__global__ void freemem()
{
    int* ptr = dataptr[blockIdx.x];
    if (ptr != NULL)
        printf("Block %d, Thread %d: final value = %d\n",
                      blockIdx.x, threadIdx.x, ptr[threadIdx.x]);

    // Only free from one thread!
    if (threadIdx.x == 0)
        free(ptr);
}

int main()
{
    cudaDeviceSetLimit(cudaLimitMallocHeapSize, 128*1024*1024);

    // Allocate memory
    allocmem&lt;&lt;&lt; NUM_BLOCKS, 10 &gt;&gt;&gt;();

    // Use memory
    usemem&lt;&lt;&lt; NUM_BLOCKS, 10 &gt;&gt;&gt;();
    usemem&lt;&lt;&lt; NUM_BLOCKS, 10 &gt;&gt;&gt;();
    usemem&lt;&lt;&lt; NUM_BLOCKS, 10 &gt;&gt;&gt;();

    // Free memory
    freemem&lt;&lt;&lt; NUM_BLOCKS, 10 &gt;&gt;&gt;();

    cudaDeviceSynchronize();

    return 0;
}
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fcabe19e8951f2a6ab4a452505627e58/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">某微信小程序连锁超市响应参数解密</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/20a3ce88fd5c1e0e756f22b194ffb834/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">春节期间重装anaconda&#43;opencv血泪史分享</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>