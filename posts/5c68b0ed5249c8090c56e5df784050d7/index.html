<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>学术派 |用深度学习实现2D到3D的转换 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="学术派 |用深度学习实现2D到3D的转换" />
<meta property="og:description" content="随着 5G 时代的到来，VR 端应用呈爆发式增长，3D 内容作为构建 VR 生态的主力输出一直深受广大用户的追捧和喜爱。针对目前 3D 内容过少，质量不高、生产昂贵等现状，爱奇艺作为国内领先的互联网视频媒体，自然首当其冲，以真实世界的 3D 内容为基础，研究2D 转 3D 技术，实现更优质的 VR 端的 3D 内容生态的构建，满足更多用户的日常需求。
相对于 2D 内容，优质的 3D 内容有输出符合真实景深关系的能力，让用户在观看时具有更好的观影体验。下面我们从技术的角度，介绍爱奇艺如何赋予2D内容真实的景深关系，实现 2D 内容到 3D 内容的转换。
面临的挑战 目前 2D 转 3D 技术主要问题是转制成本太高，不能大面积使用，如使用一般的策略很难适用多种场景的真实 3D 视差关系，这很容易让用户感到不适。
综合以上原因，我们考虑采用深度学习方法，通过对大量 3D 电影（side-by-side 的双目介质）真实视差的学习与建模，完成单目视图到双目视图的转换。
以下是2D转3D技术面临的几个挑战：
数据集质量
3D介质中包含大量不符合真实视差关系的双目视图 受相机参数的影响，同类场景的视差在不同的 3D 介质中不统一 帧间抖动
场景多样化，需要保证视差预测的连续性与准确性 重构视图的遮挡区域空洞的填补 3D 效果的评价指标难以量化
同类场景具有不同的并且满足真实世界的视差关系 3D 效果依靠人工评价，过于主观 模型原型思路 通过对大量用户的调研发现，除去特效场景刺激眼球外，3D 介质的 3D 感知越符合真实世界越受用户喜爱，因此在模型构建上必须符合真实世界的 3D 观感——双目视觉
图1双目相机成像与视差原理
如图 1 左所示，两个相机拍摄同一场景生成的图像会存在差异，这种差异叫视差，其产于与真实的三维空间。视差不能通过平移消除，同时离相机近的物体视差偏移较大，反之越小。
人的左右眼就如同图中的左右相机一样，分别获取对应图像后，通过大脑合成处理这种差异，从而获取真实世界的 3D 感知，通过图 1 右可得出视差与相机焦距和轴间距间的关系：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/5c68b0ed5249c8090c56e5df784050d7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-08T19:15:00+08:00" />
<meta property="article:modified_time" content="2020-05-08T19:15:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">学术派 |用深度学习实现2D到3D的转换</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align: left">随着 5G 时代的到来，VR 端应用呈爆发式增长，3D 内容作为构建 VR 生态的主力输出一直深受广大用户的追捧和喜爱。针对目前 3D 内容过少，质量不高、生产昂贵等现状，爱奇艺作为国内领先的互联网视频媒体，自然首当其冲，以真实世界的 3D 内容为基础，研究2D 转 3D 技术，实现更优质的 VR 端的 3D 内容生态的构建，满足更多用户的日常需求。</p> 
 <p style="text-align: left">相对于 2D 内容，优质的 3D 内容有输出符合真实景深关系的能力，让用户在观看时具有更好的观影体验。下面我们从技术的角度，介绍爱奇艺如何赋予2D内容真实的景深关系，实现 2D 内容到 3D 内容的转换。</p> 
 <p><strong> 面临的挑战 </strong></p> 
 <p style="text-align: left">目前 2D 转 3D 技术主要问题是转制成本太高，不能大面积使用，如使用一般的策略很难适用多种场景的真实 3D 视差关系，这很容易让用户感到不适。<br></p> 
 <p style="text-align: left">综合以上原因，我们考虑采用深度学习方法，通过对大量 3D 电影（side-by-side 的双目介质）真实视差的学习与建模，完成单目视图到双目视图的转换。</p> 
 <p style="text-align: left">以下是2D转3D技术面临的几个挑战：</p> 
 <ul><li><p><strong>数据集质量</strong></p></li></ul> 
 <pre class="has"><code class="language-">3D介质中包含大量不符合真实视差关系的双目视图
受相机参数的影响，同类场景的视差在不同的 3D 介质中不统一
</code></pre> 
 <ul><li><p><strong>帧间抖动</strong></p></li></ul> 
 <pre class="has"><code class="language-">场景多样化，需要保证视差预测的连续性与准确性
重构视图的遮挡区域空洞的填补
</code></pre> 
 <ul><li><p><strong>3D 效果的评价指标难以量化</strong></p></li></ul> 
 <pre class="has"><code class="language-">同类场景具有不同的并且满足真实世界的视差关系
3D 效果依靠人工评价，过于主观
</code></pre> 
 <p><strong> 模型原型思路 </strong></p> 
 <p style="text-align: left">通过对大量用户的调研发现，除去特效场景刺激眼球外，3D 介质的 3D 感知越符合真实世界越受用户喜爱，因此在模型构建上必须符合真实世界的 3D 观感——双目视觉</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/ef/b3/Xtkgcq90_o.png"></p> 
 <p style="text-align: center"><em>图1双目相机成像与视差原理</em></p> 
 <p style="text-align: left">如<em>图 1 </em><em>左</em>所示，两个相机拍摄同一场景生成的图像会存在差异，这种差异叫视差，其产于与真实的三维空间。视差不能通过平移消除，同时离相机近的物体视差偏移较大，反之越小。<br></p> 
 <p style="text-align: left">人的左右眼就如同图中的左右相机一样，分别获取对应图像后，通过大脑合成处理这种差异，从而获取真实世界的 3D 感知，通过<em>图 1</em> <em>右</em>可得出视差与相机焦距和轴间距间的关系：</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/b1/8c/fhlPqP0a_o.png"></p> 
 <p style="text-align: center"><em>公式（1）</em></p> 
 <p>其中 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 1.05ex;height: 1.02ex;vertical-align: -0.02ex;" viewbox="0 -442 465 453"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <path d="M 347 338 Q 337 338 294 349 T 231 360 Q 211 360 197 356 T 174 346 T 162 335 T 155 324 L 153 320 Q 150 317 138 317 Q 117 317 117 325 Q 117 330 120 339 Q 133 378 163 406 T 229 440 Q 241 442 246 442 Q 271 442 291 425 T 329 392 T 367 375 Q 389 375 411 408 T 434 441 Q 435 442 449 442 H 462 Q 468 436 468 434 Q 468 430 463 420 T 449 399 T 432 377 T 418 358 L 411 349 Q 368 298 275 214 T 160 106 L 148 94 L 163 93 Q 185 93 227 82 T 290 71 Q 328 71 360 90 T 402 140 Q 406 149 409 151 T 424 153 Q 443 153 443 143 Q 443 138 442 134 Q 425 72 376 31 T 278 -11 Q 252 -11 232 6 T 193 40 T 155 57 Q 111 57 76 -3 Q 70 -11 59 -11 H 54 H 41 Q 35 -5 35 -2 Q 35 13 93 84 Q 132 129 225 214 T 340 322 Q 352 338 347 338 Z"></path> 
      </g> 
     </g> 
    </g> 
   </svg>为物体距离相机的深度， 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 1.29ex;height: 1.02ex;vertical-align: -0.02ex;" viewbox="0 -442 572 453"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <path d="M 52 289 Q 59 331 106 386 T 222 442 Q 257 442 286 424 T 329 379 Q 371 442 430 442 Q 467 442 494 420 T 522 361 Q 522 332 508 314 T 481 292 T 458 288 Q 439 288 427 299 T 415 328 Q 415 374 465 391 Q 454 404 425 404 Q 412 404 406 402 Q 368 386 350 336 Q 290 115 290 78 Q 290 50 306 38 T 341 26 Q 378 26 414 59 T 463 140 Q 466 150 469 151 T 485 153 H 489 Q 504 153 504 145 Q 504 144 502 134 Q 486 77 440 33 T 333 -11 Q 263 -11 227 52 Q 186 -10 133 -10 H 127 Q 78 -10 57 16 T 35 71 Q 35 103 54 123 T 99 143 Q 142 143 142 101 Q 142 81 130 66 T 107 46 T 94 41 L 91 40 Q 91 39 97 36 T 113 29 T 132 26 Q 168 26 194 71 Q 203 87 217 139 T 245 247 T 261 313 Q 266 340 266 352 Q 266 380 251 392 T 217 404 Q 177 404 142 372 T 93 290 Q 91 281 88 280 T 72 278 H 58 Q 52 284 52 289 Z"></path> 
      </g> 
     </g> 
    </g> 
   </svg>为三维映射到二维的图像平面， 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 1.24ex;height: 2.05ex;vertical-align: -0.46ex;" viewbox="0 -705 550 910"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <path d="M 118 -162 Q 120 -162 124 -164 T 135 -167 T 147 -168 Q 160 -168 171 -155 T 187 -126 Q 197 -99 221 27 T 267 267 T 289 382 V 385 H 242 Q 195 385 192 387 Q 188 390 188 397 L 195 425 Q 197 430 203 430 T 250 431 Q 298 431 298 432 Q 298 434 307 482 T 319 540 Q 356 705 465 705 Q 502 703 526 683 T 550 630 Q 550 594 529 578 T 487 561 Q 443 561 443 603 Q 443 622 454 636 T 478 657 L 487 662 Q 471 668 457 668 Q 445 668 434 658 T 419 630 Q 412 601 403 552 T 387 469 T 380 433 Q 380 431 435 431 Q 480 431 487 430 T 498 424 Q 499 420 496 407 T 491 391 Q 489 386 482 386 T 428 385 H 372 L 349 263 Q 301 15 282 -47 Q 255 -132 212 -173 Q 175 -205 139 -205 Q 107 -205 81 -186 T 55 -132 Q 55 -95 76 -78 T 118 -61 Q 162 -61 162 -103 Q 162 -122 151 -136 T 127 -157 L 118 -162 Z"></path> 
      </g> 
     </g> 
    </g> 
   </svg>为相机焦距， 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 0.97ex;height: 1.59ex;vertical-align: -0.02ex;" viewbox="0 -694 429 705"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <path d="M 73 647 Q 73 657 77 670 T 89 683 Q 90 683 161 688 T 234 694 Q 246 694 246 685 T 212 542 Q 204 508 195 472 T 180 418 L 176 399 Q 176 396 182 402 Q 231 442 283 442 Q 345 442 383 396 T 422 280 Q 422 169 343 79 T 173 -11 Q 123 -11 82 27 T 40 150 V 159 Q 40 180 48 217 T 97 414 Q 147 611 147 623 T 109 637 Q 104 637 101 637 H 96 Q 86 637 83 637 T 76 640 T 73 647 Z M 336 325 V 331 Q 336 405 275 405 Q 258 405 240 397 T 207 376 T 181 352 T 163 330 L 157 322 L 136 236 Q 114 150 114 114 Q 114 66 138 42 Q 154 26 178 26 Q 211 26 245 58 Q 270 81 285 114 T 318 219 Q 336 291 336 325 Z"></path> 
      </g> 
     </g> 
    </g> 
   </svg>为两个相机间的距离轴间距， 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 1.88ex;height: 1.35ex;vertical-align: -0.35ex;" viewbox="0 -442 832.7 599.8"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <g> 
        <path d="M 52 289 Q 59 331 106 386 T 222 442 Q 257 442 286 424 T 329 379 Q 371 442 430 442 Q 467 442 494 420 T 522 361 Q 522 332 508 314 T 481 292 T 458 288 Q 439 288 427 299 T 415 328 Q 415 374 465 391 Q 454 404 425 404 Q 412 404 406 402 Q 368 386 350 336 Q 290 115 290 78 Q 290 50 306 38 T 341 26 Q 378 26 414 59 T 463 140 Q 466 150 469 151 T 485 153 H 489 Q 504 153 504 145 Q 504 144 502 134 Q 486 77 440 33 T 333 -11 Q 263 -11 227 52 Q 186 -10 133 -10 H 127 Q 78 -10 57 16 T 35 71 Q 35 103 54 123 T 99 143 Q 142 143 142 101 Q 142 81 130 66 T 107 46 T 94 41 L 91 40 Q 91 39 97 36 T 113 29 T 132 26 Q 168 26 194 71 Q 203 87 217 139 T 245 247 T 261 313 Q 266 340 266 352 Q 266 380 251 392 T 217 404 Q 177 404 142 372 T 93 290 Q 91 281 88 280 T 72 278 H 58 Q 52 284 52 289 Z"></path> 
       </g> 
       <g transform="translate(572 -150) scale(0.707)"> 
        <path d="M 117 59 Q 117 26 142 26 Q 179 26 205 131 Q 211 151 215 152 Q 217 153 225 153 H 229 Q 238 153 241 153 T 246 151 T 248 144 Q 247 138 245 128 T 234 90 T 214 43 T 183 6 T 137 -11 Q 101 -11 70 11 T 38 85 Q 38 97 39 102 L 104 360 Q 167 615 167 623 Q 167 626 166 628 T 162 632 T 157 634 T 149 635 T 141 636 T 132 637 T 122 637 Q 112 637 109 637 T 101 638 T 95 641 T 94 647 Q 94 649 96 661 Q 101 680 107 682 T 179 688 Q 194 689 213 690 T 243 693 T 254 694 Q 266 694 266 686 Q 266 675 193 386 T 118 83 Q 118 81 118 75 T 117 65 V 59 Z"></path> 
       </g> 
      </g> 
     </g> 
    </g> 
   </svg>和 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 2.12ex;height: 1.35ex;vertical-align: -0.35ex;" viewbox="0 -442 940.9 599.8"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <g> 
        <path d="M 52 289 Q 59 331 106 386 T 222 442 Q 257 442 286 424 T 329 379 Q 371 442 430 442 Q 467 442 494 420 T 522 361 Q 522 332 508 314 T 481 292 T 458 288 Q 439 288 427 299 T 415 328 Q 415 374 465 391 Q 454 404 425 404 Q 412 404 406 402 Q 368 386 350 336 Q 290 115 290 78 Q 290 50 306 38 T 341 26 Q 378 26 414 59 T 463 140 Q 466 150 469 151 T 485 153 H 489 Q 504 153 504 145 Q 504 144 502 134 Q 486 77 440 33 T 333 -11 Q 263 -11 227 52 Q 186 -10 133 -10 H 127 Q 78 -10 57 16 T 35 71 Q 35 103 54 123 T 99 143 Q 142 143 142 101 Q 142 81 130 66 T 107 46 T 94 41 L 91 40 Q 91 39 97 36 T 113 29 T 132 26 Q 168 26 194 71 Q 203 87 217 139 T 245 247 T 261 313 Q 266 340 266 352 Q 266 380 251 392 T 217 404 Q 177 404 142 372 T 93 290 Q 91 281 88 280 T 72 278 H 58 Q 52 284 52 289 Z"></path> 
       </g> 
       <g transform="translate(572 -150) scale(0.707)"> 
        <path d="M 21 287 Q 22 290 23 295 T 28 317 T 38 348 T 53 381 T 73 411 T 99 433 T 132 442 Q 161 442 183 430 T 214 408 T 225 388 Q 227 382 228 382 T 236 389 Q 284 441 347 441 H 350 Q 398 441 422 400 Q 430 381 430 363 Q 430 333 417 315 T 391 292 T 366 288 Q 346 288 334 299 T 322 328 Q 322 376 378 392 Q 356 405 342 405 Q 286 405 239 331 Q 229 315 224 298 T 190 165 Q 156 25 151 16 Q 138 -11 108 -11 Q 95 -11 87 -5 T 76 7 T 74 17 Q 74 30 114 189 T 154 366 Q 154 405 128 405 Q 107 405 92 377 T 68 316 T 57 280 Q 55 278 41 278 H 27 Q 21 284 21 287 Z"></path> 
       </g> 
      </g> 
     </g> 
    </g> 
   </svg>分别为物体在左右不同相机中成像的坐标，因此可知左右图对应像素 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 1.88ex;height: 1.35ex;vertical-align: -0.35ex;" viewbox="0 -442 832.7 599.8"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <g> 
        <path d="M 52 289 Q 59 331 106 386 T 222 442 Q 257 442 286 424 T 329 379 Q 371 442 430 442 Q 467 442 494 420 T 522 361 Q 522 332 508 314 T 481 292 T 458 288 Q 439 288 427 299 T 415 328 Q 415 374 465 391 Q 454 404 425 404 Q 412 404 406 402 Q 368 386 350 336 Q 290 115 290 78 Q 290 50 306 38 T 341 26 Q 378 26 414 59 T 463 140 Q 466 150 469 151 T 485 153 H 489 Q 504 153 504 145 Q 504 144 502 134 Q 486 77 440 33 T 333 -11 Q 263 -11 227 52 Q 186 -10 133 -10 H 127 Q 78 -10 57 16 T 35 71 Q 35 103 54 123 T 99 143 Q 142 143 142 101 Q 142 81 130 66 T 107 46 T 94 41 L 91 40 Q 91 39 97 36 T 113 29 T 132 26 Q 168 26 194 71 Q 203 87 217 139 T 245 247 T 261 313 Q 266 340 266 352 Q 266 380 251 392 T 217 404 Q 177 404 142 372 T 93 290 Q 91 281 88 280 T 72 278 H 58 Q 52 284 52 289 Z"></path> 
       </g> 
       <g transform="translate(572 -150) scale(0.707)"> 
        <path d="M 117 59 Q 117 26 142 26 Q 179 26 205 131 Q 211 151 215 152 Q 217 153 225 153 H 229 Q 238 153 241 153 T 246 151 T 248 144 Q 247 138 245 128 T 234 90 T 214 43 T 183 6 T 137 -11 Q 101 -11 70 11 T 38 85 Q 38 97 39 102 L 104 360 Q 167 615 167 623 Q 167 626 166 628 T 162 632 T 157 634 T 149 635 T 141 636 T 132 637 T 122 637 Q 112 637 109 637 T 101 638 T 95 641 T 94 647 Q 94 649 96 661 Q 101 680 107 682 T 179 688 Q 194 689 213 690 T 243 693 T 254 694 Q 266 694 266 686 Q 266 675 193 386 T 118 83 Q 118 81 118 75 T 117 65 V 59 Z"></path> 
       </g> 
      </g> 
     </g> 
    </g> 
   </svg>和 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 2.12ex;height: 1.35ex;vertical-align: -0.35ex;" viewbox="0 -442 940.9 599.8"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <g> 
        <path d="M 52 289 Q 59 331 106 386 T 222 442 Q 257 442 286 424 T 329 379 Q 371 442 430 442 Q 467 442 494 420 T 522 361 Q 522 332 508 314 T 481 292 T 458 288 Q 439 288 427 299 T 415 328 Q 415 374 465 391 Q 454 404 425 404 Q 412 404 406 402 Q 368 386 350 336 Q 290 115 290 78 Q 290 50 306 38 T 341 26 Q 378 26 414 59 T 463 140 Q 466 150 469 151 T 485 153 H 489 Q 504 153 504 145 Q 504 144 502 134 Q 486 77 440 33 T 333 -11 Q 263 -11 227 52 Q 186 -10 133 -10 H 127 Q 78 -10 57 16 T 35 71 Q 35 103 54 123 T 99 143 Q 142 143 142 101 Q 142 81 130 66 T 107 46 T 94 41 L 91 40 Q 91 39 97 36 T 113 29 T 132 26 Q 168 26 194 71 Q 203 87 217 139 T 245 247 T 261 313 Q 266 340 266 352 Q 266 380 251 392 T 217 404 Q 177 404 142 372 T 93 290 Q 91 281 88 280 T 72 278 H 58 Q 52 284 52 289 Z"></path> 
       </g> 
       <g transform="translate(572 -150) scale(0.707)"> 
        <path d="M 21 287 Q 22 290 23 295 T 28 317 T 38 348 T 53 381 T 73 411 T 99 433 T 132 442 Q 161 442 183 430 T 214 408 T 225 388 Q 227 382 228 382 T 236 389 Q 284 441 347 441 H 350 Q 398 441 422 400 Q 430 381 430 363 Q 430 333 417 315 T 391 292 T 366 288 Q 346 288 334 299 T 322 328 Q 322 376 378 392 Q 356 405 342 405 Q 286 405 239 331 Q 229 315 224 298 T 190 165 Q 156 25 151 16 Q 138 -11 108 -11 Q 95 -11 87 -5 T 76 7 T 74 17 Q 74 30 114 189 T 154 366 Q 154 405 128 405 Q 107 405 92 377 T 68 316 T 57 280 Q 55 278 41 278 H 27 Q 21 284 21 287 Z"></path> 
       </g> 
      </g> 
     </g> 
    </g> 
   </svg>的视差。</p> 
 <p>同时，考虑到转制的对象为2D介质，因此，通过单目深度估计合成新视点的算法原型诞生：通过<em>公式(1)</em>可知，假设有一个函数 
   <svg xmlns="http://www.w3.org/2000/svg" style="width: 9.47ex;height: 2.26ex;vertical-align: -0.56ex;" viewbox="0 -750 4189.5 1000"> 
    <g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1, 0, 0, -1, 0, 0)"> 
     <g> 
      <g> 
       <path d="M 48 1 Q 31 1 31 11 Q 31 13 34 25 Q 38 41 42 43 T 65 46 Q 92 46 125 49 Q 139 52 144 61 Q 146 66 215 342 T 285 622 Q 285 629 281 629 Q 273 632 228 634 H 197 Q 191 640 191 642 T 193 659 Q 197 676 203 680 H 742 Q 749 676 749 669 Q 749 664 736 557 T 722 447 Q 720 440 702 440 H 690 Q 683 445 683 453 Q 683 454 686 477 T 689 530 Q 689 560 682 579 T 663 610 T 626 626 T 575 633 T 503 634 H 480 Q 398 633 393 631 Q 388 629 386 623 Q 385 622 352 492 L 320 363 H 375 Q 378 363 398 363 T 426 364 T 448 367 T 472 374 T 489 386 Q 502 398 511 419 T 524 457 T 529 475 Q 532 480 548 480 H 560 Q 567 475 567 470 Q 567 467 536 339 T 502 207 Q 500 200 482 200 H 470 Q 463 206 463 212 Q 463 215 468 234 T 473 274 Q 473 303 453 310 T 364 317 H 309 L 277 190 Q 245 66 245 60 Q 245 46 334 46 H 359 Q 365 40 365 39 T 363 19 Q 359 6 353 0 H 336 Q 295 2 185 2 Q 120 2 86 2 T 48 1 Z"></path> 
      </g> 
      <g transform="translate(749)"> 
       <path d="M 94 250 Q 94 319 104 381 T 127 488 T 164 576 T 202 643 T 244 695 T 277 729 T 302 750 H 315 H 319 Q 333 750 333 741 Q 333 738 316 720 T 275 667 T 226 581 T 184 443 T 167 250 T 184 58 T 225 -81 T 274 -167 T 316 -220 T 333 -241 Q 333 -250 318 -250 H 315 H 302 L 274 -226 Q 180 -141 137 -14 T 94 250 Z"></path> 
      </g> 
      <g transform="translate(1138)"> 
       <g> 
        <path d="M 43 1 Q 26 1 26 10 Q 26 12 29 24 Q 34 43 39 45 Q 42 46 54 46 H 60 Q 120 46 136 53 Q 137 53 138 54 Q 143 56 149 77 T 198 273 Q 210 318 216 344 Q 286 624 286 626 Q 284 630 284 631 Q 274 637 213 637 H 193 Q 184 643 189 662 Q 193 677 195 680 T 209 683 H 213 Q 285 681 359 681 Q 481 681 487 683 H 497 Q 504 676 504 672 T 501 655 T 494 639 Q 491 637 471 637 Q 440 637 407 634 Q 393 631 388 623 Q 381 609 337 432 Q 326 385 315 341 Q 245 65 245 59 Q 245 52 255 50 T 307 46 H 339 Q 345 38 345 37 T 342 19 Q 338 6 332 0 H 316 Q 279 2 179 2 Q 143 2 113 2 T 65 2 T 43 1 Z"></path> 
       </g> 
       <g transform="translate(440 -150) scale(0.707)"> 
        <path d="M 21 287 Q 22 290 23 295 T 28 317 T 38 348 T 53 381 T 73 411 T 99 433 T 132 442 Q 161 442 183 430 T 214 408 T 225 388 Q 227 382 228 382 T 236 389 Q 284 441 347 441 H 350 Q 398 441 422 400 Q 430 381 430 363 Q 430 333 417 315 T 391 292 T 366 288 Q 346 288 334 299 T 322 328 Q 322 376 378 392 Q 356 405 342 405 Q 286 405 239 331 Q 229 315 224 298 T 190 165 Q 156 25 151 16 Q 138 -11 108 -11 Q 95 -11 87 -5 T 76 7 T 74 17 Q 74 30 114 189 T 154 366 Q 154 405 128 405 Q 107 405 92 377 T 68 316 T 57 280 Q 55 278 41 278 H 27 Q 21 284 21 287 Z"></path> 
       </g> 
      </g> 
      <g transform="translate(1946.9)"> 
       <path d="M 60 749 L 64 750 Q 69 750 74 750 H 86 L 114 726 Q 208 641 251 514 T 294 250 Q 294 182 284 119 T 261 12 T 224 -76 T 186 -143 T 145 -194 T 113 -227 T 90 -246 Q 87 -249 86 -250 H 74 Q 66 -250 63 -250 T 58 -247 T 55 -238 Q 56 -237 66 -225 Q 221 -64 221 250 T 66 725 Q 56 737 55 738 Q 55 746 60 749 Z"></path> 
      </g> 
      <g transform="translate(2613.7)"> 
       <path d="M 56 347 Q 56 360 70 367 H 707 Q 722 359 722 347 Q 722 336 708 328 L 390 327 H 72 Q 56 332 56 347 Z M 56 153 Q 56 168 72 173 H 708 Q 722 163 722 153 Q 722 140 707 133 H 70 Q 56 140 56 153 Z"></path> 
      </g> 
      <g transform="translate(3669.5)"> 
       <path d="M 366 683 Q 367 683 438 688 T 511 694 Q 523 694 523 686 Q 523 679 450 384 T 375 83 T 374 68 Q 374 26 402 26 Q 411 27 422 35 Q 443 55 463 131 Q 469 151 473 152 Q 475 153 483 153 H 487 H 491 Q 506 153 506 145 Q 506 140 503 129 Q 490 79 473 48 T 445 8 T 417 -8 Q 409 -10 393 -10 Q 359 -10 336 5 T 306 36 L 300 51 Q 299 52 296 50 Q 294 48 292 46 Q 233 -10 172 -10 Q 117 -10 75 30 T 33 157 Q 33 205 53 255 T 101 341 Q 148 398 195 420 T 280 442 Q 336 442 364 400 Q 369 394 369 396 Q 370 400 396 505 T 424 616 Q 424 629 417 632 T 378 637 H 357 Q 351 643 351 645 T 353 664 Q 358 683 366 683 Z M 352 326 Q 329 405 277 405 Q 242 405 210 374 T 160 293 Q 131 214 119 129 Q 119 126 119 118 T 118 106 Q 118 61 136 44 T 179 26 Q 233 26 290 98 L 298 109 L 352 326 Z"></path> 
      </g> 
     </g> 
    </g> 
   </svg>那么就有：</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/5e/14/rnvOkUNk_o.png"></p> 
 <p style="text-align: center"><em>公式（2）</em></p> 
 <p style="text-align: left">通过<em>公式（2）</em>可知，只需要将 <em>图1左</em> 作为训练输入，<em>图1右</em> 作为参考，即可建立深度学习模型，通过大量双目图片对训练估计出函数????。这样就可在已知相机参数(????,????)的前提下获取对应的深度值????，完成单目深度估计的任务。</p> 
 <p style="text-align: left">通过<em>公式（1）</em>与<em>公式（2）</em>可以发现，深度与视差成反比，因此深度估计和视差估计的方法可以互用。Deep3D[1]虽然通过视差概率估计实现2D到3D介质的转换，但固定视差的设定，难以适应不同分辨率2D介质输入；<br></p> 
 <p style="text-align: left">方法[2]没有充分利用双目信息作指导，景深不够细；monodepth[3]在方法[2]的基础上，充分利用了双目信息进行对抗指导，学习到更多深度细节；<br></p> 
 <p style="text-align: left">SfmLearner[4]这类方法引入帧间时序信息，结构较复杂，运行速度慢。因此通过实现及适用性考虑最终我们选择以monodepth为baseline，其框架结构如<em>图2</em>所示：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/9d/46/jB9nJEkr_o.png"></p> 
 <p style="text-align: center"><em>图2 monodepth框架图</em></p> 
 <p style="text-align: left">通过 <em>图2 </em>框架可以看出，该框架在训练过程充分利用双目的有效信息作指导，同时测试过程也只需要单目图片进行输入，所以非常适合用于2D转3D技术的框架。</p> 
 <p><strong> 模型演变 </strong></p> 
 <p style="text-align: left"><strong>解决相机问题</strong></p> 
 <p style="text-align: left">在Baseline模型的基础上，如果直接使用混合的3D电影数据集进行训练，模型将无法收敛或预测不稳定，一个最主要的问题是不同电影使用不同相机参数的摄像机进行拍摄，即使两个非常相似的电影场景，在不同的两部电影中也会有不同的景深分布，表现在模型训练中即为不同的视差值。</p> 
 <p style="text-align: left">与此同时，不同电影的后处理方式，以及会聚相机的引入，会进一步增加建模的难度。在分析相似案例的处理方法中，我们发现可以通过引入条件变分自编码器（CVAE），在训练过程中，把每一组训练集（左右视图）通过网络提取其相机参数等信息，并作为后验信息通过AdaIN[5]的形式引入到单目（左视图）视差图预测中，同时参考[6]中的“双轮训练”，保证了测试时随机采样相机参数分布的正确性。</p> 
 <p style="text-align: left"><strong>解决抖动问题</strong></p> 
 <p style="text-align: left">在解决数据集问题后，进行连续帧预测时，发现存在预测不稳定及抖动的问题。在解决视频生成过程（尤其是连续帧深度图预测）的抖动问题中，目前最为常见的方案包含基于帧间ConvLSTM的[7]和[8]和基于光流的[9]和[10]。其中，[8]在不同尺度的编码和解码的过程中均加入ConvLSTM，隐式的利用时间域上特征的相关性来稳定的预测深度图，而[7]则仅在网络输出的最后一层引入ConvLSTM。</p> 
 <p style="text-align: left">引入ConvLSTM的方法思路简单，但在2D转3D模型中却不适用，[8]使用了较多的ConvLSTM，使得训练较为复杂，不易收敛，[7]由于电影分镜镜头种类多变，单一ConvLSTM预测时易累计误差，使得预测变差。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/2b/44/ChofA8pb_o.png"></p> 
 <p style="text-align: center"><em>图3 vid2vid结构图</em></p> 
 <p style="text-align: left">我们的2D转3D模型采用了类似于[10]的模型结构，如<em>图3</em>所示，将左侧上支路改为输入三帧左视图（t，t-1，t-2），左侧下支路改为输入前两帧预测视差图（t-1，t-2），右上支路为输出当前帧所预测的视差图，右下支路改为输出前一帧预测视差图到当前帧预测视差图的光流图（t-1-&gt;t）及其valid mask图，最终结合右侧上下两支路结果合成当前帧视差图。</p> 
 <p style="text-align: left">其中，在中间高维特征合并处引入上文提及的CVAE模块，用以引入后验相机参数信息。最终，在解决相机参数导致数据集问题的同时，模型能够得到稳定且连续的视差图输出。</p> 
 <p style="text-align: left"><strong>解决“空洞”填补问题</strong></p> 
 <p style="text-align: left">由于新视角的生成，会使部分原本被遮挡的区域在新视角中显露出来，这些信息仅从左视图中是无法获取的，即使通过前后帧的光流信息也很难还原。在生成新视角的后处理过程中，我们参考[11]的模型框架设计，通过视差图来指导获取产生的“空洞”区域，通过图像修补技术解决新视角的“空洞”问题。</p> 
 <p style="text-align: left">3D效果测评 由于拍摄条件不同会导致3D效果不同，所以在2D转3D效果测评中，我们用大量人力对预测的视差图和成片在VR中的3D效果进行综合性的评测。视差图估计如图4：</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/81/66/hDIVSVMz_o.png"></p> 
 <p style="text-align: center"><em>图4 各种场景下的单目视差估计</em></p> 
 <p style="text-align: left"><strong>应用扩展</strong></p> 
 <p style="text-align: left">不仅如此，视差图的预测也能转化为相对深度值，被应用到其他方面，例如3D海报。3D海报是一张2D图片加上其深度关系图，通过一系列的新视点渲染，得到一组动态的，人能感知的立体影像。如图5与图6所示：</p> 
 <p><img src="https://images2.imgbox.com/4b/36/fKrxCkuw_o.png"></p> 
 <p style="text-align: center"><em>图5 复仇者联盟3D海报</em></p> 
 <p><img src="https://images2.imgbox.com/38/24/jNrEkOdh_o.png"></p> 
 <p style="text-align: center"><em>图6 剑干将莫邪3D海报</em></p> 
 <p><strong> References </strong></p> 
 <p>[1]Xie J, Girshick R, Farhadi A. Deep3d: Fully automatic 2d-to-3d video conversionwith deep convolutional neural networks[C]//European Conference on ComputerVision. Springer, Cham, 2016: 842-857.</p> 
 <p>[2]Garg R, BG V K, Carneiro G, et al. Unsupervised cnn for single view depthestimation: Geometry to the rescue[C]//European Conference on Computer Vision.Springer, Cham, 2016: 740-756.</p> 
 <p>[3] Godard C, Mac Aodha O, Brostow G J. Unsupervisedmonocular depth estimation with left-right consistency[C]//Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition. 2017: 270-279.</p> 
 <p>[4] Zhou T, Brown M, Snavely N, et al. Unsupervised learningof depth and ego-motion from video[C]//Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition. 2017: 1851-1858.</p> 
 <p>[5] Huang X, Belongie S. Arbitrary style transfer inreal-time with adaptive instance normalization[C]//Proceedings of the IEEEInternational Conference on Computer Vision. 2017: 1501-1510.</p> 
 <p>[6] Zhu J Y, Zhang R, Pathak D, et al. Toward multimodal image-to-imagetranslation[C]//Advances in neural information processing systems. 2017:465-476.</p> 
 <p>[7] Zhang H, Shen C, Li Y, et al. Exploitingtemporal consistency for real-time video depth estimation[C]//Proceedings ofthe IEEE International Conference on Computer Vision. 2019: 1725-1734.</p> 
 <p>[8] Tananaev D, Zhou H, Ummenhofer B, et al. TemporallyConsistent Depth Estimation in Videos with RecurrentArchitectures[C]//Proceedings of the European Conference on Computer Vision(ECCV). 2018: 0-0.</p> 
 <p>[9] Lin J, Gan C, Han S. Tsm: Temporal shift module forefficient video understanding[C]//Proceedings of the IEEE InternationalConference on Computer Vision. 2019: 7083-7093.</p> 
 <p>[10] Wang T C, Liu M Y, Zhu J Y, et al. Video-to-videosynthesis[J]. arXiv preprint arXiv:1808.06601, 2018.</p> 
 <p>[11]Yu J, Lin Z, Yang J, et al. Free-form imageinpainting with gated convolution[C]//Proceedings of the IEEE InternationalConference on Computer Vision. 2019: 4471-4480.</p> 
 <hr> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/74/71/Q3EwRuEB_o.png"></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/b0/8d/838XE9rm_o.png"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/387b1420598b20c73bccb572953de144/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Liunx服务管理之NFS</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d1a824ae2ec8fc02761cf49fabe2d6e4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【STM32】STM32F407互补PWM进阶-带相移的互补PWM</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>