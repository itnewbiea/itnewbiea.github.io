<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据编程期末大作业2023 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大数据编程期末大作业2023" />
<meta property="og:description" content="目录
一、Hadoop基础操作
二、RDD编程
三、Spark SQL编程
四、Spark Streaming编程
五、Flume的安装配置
一、Hadoop基础操作 按要求完成以下操作：
1、在HDFS中创建目录 /user/root/你的名字。 例如：李四同学 /user/root/lisi，后同。
首先需要启动hdfs，在终端输入如下命令：
start-dfs.sh 在终端输入如下命令创建目录：
hdfs dfs -mkdir -p /user/root/***（你自己的名字全拼，下同） 2、创建本地文件lisi.txt，文件内容为包括Lisi love Hadoop等其他任意输入的6行英文句子，并将该文件上传到HDFS中第1题所创建的目录中。
在终端的root目录下面创建本地文件并输入题目要求的内容：
vim ***.txt 然后我们再在终端输入上传命令：
hdfs dfs -put ***.txt /user/root/*** 3、查看上传到HDFS中的lisi.txt文件的内容。
直接在终端输入查看命令：
hdfs dfs -cat /user/root/***/***.txt 4、在Hadoop官方的示例程序包hadoop-mapreduce-examples-3.1.3.jar中，包括计算Pi值的测试模块，使用hadoop jar命令提交计算Pi的MapReduce任务。
首先，启动yarn，命令如下：
start-yarn.sh 然后进入到hadoop下的mapreduce目录中： cd /usr/local/servers/hadoop/share/hadoop/mapreduce/ 最后执行如下命令即可计算Pi： hadoop jar hadoop-mapreduce-examples-3.1.3.jar pi 4 4 可以看出精度不是很高，上面命令后面的两个数字含义是，第一个4是运行4次map任务，第二个4是每个map任务投掷次数，总投掷次数就是两者相乘，想要提高精度就可以让数字变大，但是很容易出现作业计算失败的异常，这是因为计算内存不够，所以不能调的太大。 5、直接将第4题的计算结果保存到/user/root/lisi目录中lisiPi文件里。
先将计算结果保存到本地系统home目录下：
hadoop jar hadoop-mapreduce-examples-3.1.3.jar pi 4 4 &gt; /home/zhanghc/***PI.txt 然后将***PI.txt文件上传到HDFS的“/user/root/***”目录下并查看结果：
hdfs dfs -put /home/zhanghc/***PI." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/f453a9c15ea9eb6e0bcd86fb85a44b66/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-04T11:29:27+08:00" />
<meta property="article:modified_time" content="2024-01-04T11:29:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据编程期末大作业2023</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81Hadoop%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81Hadoop%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C" rel="nofollow">一、Hadoop基础操作</a></p> 
<p id="%E4%BA%8C%E3%80%81RDD%E7%BC%96%E7%A8%8B-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81RDD%E7%BC%96%E7%A8%8B" rel="nofollow">二、RDD编程</a></p> 
<p id="%E4%B8%89%E3%80%81Spark%20SQL%E7%BC%96%E7%A8%8B-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81Spark%20SQL%E7%BC%96%E7%A8%8B" rel="nofollow">三、Spark SQL编程</a></p> 
<p id="%E5%9B%9B%E3%80%81Spark%20Streaming%E7%BC%96%E7%A8%8B-toc" style="margin-left:40px;"><a href="#%E5%9B%9B%E3%80%81Spark%20Streaming%E7%BC%96%E7%A8%8B" rel="nofollow">四、Spark Streaming编程</a></p> 
<p id="%E4%BA%94%E3%80%81Flume%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE-toc" style="margin-left:40px;"><a href="#%E4%BA%94%E3%80%81Flume%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE" rel="nofollow">五、Flume的安装配置</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3 style="margin-left:.0001pt;text-align:justify;">一、Hadoop基础操作</h3> 
<p style="margin-left:.0001pt;text-align:justify;">按要求完成以下操作：</p> 
<p style="text-align:justify;"><strong>1、在HDFS中创建目录 /user/root/你的名字。  例如：李四同学  /user/root/lisi，后同。</strong></p> 
<p style="text-align:justify;">首先需要启动hdfs，在终端输入如下命令：</p> 
<pre><code class="language-cpp">start-dfs.sh</code></pre> 
<p style="text-align:justify;">在终端输入如下命令创建目录：</p> 
<pre><code class="language-bash">hdfs dfs -mkdir -p /user/root/***（你自己的名字全拼，下同）</code></pre> 
<p style="text-align:justify;"><strong>2、创建本地文件lisi.txt，文件内容为包括Lisi love Hadoop等其他任意输入的6行英文句子，并将该文件上传到HDFS中第1题所创建的目录中。</strong></p> 
<p style="text-align:justify;">在终端的root目录下面创建本地文件并输入题目要求的内容：</p> 
<pre><code class="language-bash">vim ***.txt</code></pre> 
<p style="text-align:justify;">然后我们再在终端输入上传命令：</p> 
<pre><code class="language-bash">hdfs dfs -put ***.txt /user/root/***</code></pre> 
<p style="text-align:justify;"><strong>3、查看上传到HDFS中的lisi.txt文件的内容。</strong></p> 
<p style="text-align:justify;">直接在终端输入查看命令：</p> 
<pre><code class="language-bash">hdfs dfs -cat /user/root/***/***.txt</code></pre> 
<p style="text-align:justify;"><strong>4、在Hadoop官方的示例程序包hadoop-mapreduce-examples-3.1.3.jar中，包括计算Pi值的测试模块，使用hadoop jar命令提交计算Pi的MapReduce任务。</strong></p> 
<p style="text-align:justify;">首先，启动yarn，命令如下：</p> 
<pre><code class="language-bash">start-yarn.sh
</code></pre> 
<p>然后进入到hadoop下的mapreduce目录中： </p> 
<pre><code class="language-bash">cd /usr/local/servers/hadoop/share/hadoop/mapreduce/
</code></pre> 
<p>最后执行如下命令即可计算Pi： </p> 
<pre><code class="language-bash">hadoop jar hadoop-mapreduce-examples-3.1.3.jar pi 4 4</code></pre> 
<p><img alt="" height="711" src="https://images2.imgbox.com/8d/00/dxIoqu4t_o.png" width="1200"></p> 
<p><img alt="" height="766" src="https://images2.imgbox.com/41/00/0OOrFrbm_o.png" width="1200"></p> 
<p>可以看出精度不是很高，上面命令后面的两个数字含义是，第一个4是运行4次map任务，第二个4是每个map任务投掷次数，总投掷次数就是两者相乘，想要提高精度就可以让数字变大，但是很容易出现作业计算失败的异常，这是因为计算内存不够，所以不能调的太大。 </p> 
<p style="text-align:justify;"><strong>5、直接将第4题的计算结果保存到/user/root/lisi目录中lisiPi文件里。</strong></p> 
<p style="text-align:justify;">先将计算结果保存到本地系统home目录下：</p> 
<pre><code class="language-bash">hadoop jar hadoop-mapreduce-examples-3.1.3.jar pi 4 4 &gt; /home/zhanghc/***PI.txt</code></pre> 
<p>然后将***PI.txt文件上传到HDFS的“/user/root/***”目录下并查看结果：</p> 
<pre><code class="language-bash">hdfs dfs -put /home/zhanghc/***PI.txt /user/root/***
hdfs dfs -cat /user/root/***/***PI.txt
</code></pre> 
<p><img alt="" height="222" src="https://images2.imgbox.com/f8/73/WwjpwDdm_o.png" width="1200"></p> 
<h3 id="%E4%BA%8C%E3%80%81RDD%E7%BC%96%E7%A8%8B">二、RDD编程</h3> 
<p style="margin-left:.0001pt;text-align:justify;">现有一份2019年我国部分省份高考分数线数据文件exam2019.csv,共有四个数据字段，字段说明如表1所示：</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><caption>
   表1 高考分数线数据字段说明 
 </caption><thead><tr><th> <p style="margin-left:.0001pt;text-align:center;">字段名称</p> </th><th> <p style="margin-left:.0001pt;text-align:center;">说明</p> </th></tr></thead><tbody><tr><td> <p style="margin-left:.0001pt;text-align:center;">地区</p> </td><td> <p style="margin-left:.0001pt;text-align:center;">省、直辖市或自治区</p> </td></tr><tr><td> <p style="margin-left:.0001pt;text-align:center;">考生类别</p> </td><td> <p style="margin-left:.0001pt;text-align:center;">考生报考类别，如理科</p> </td></tr><tr><td> <p style="margin-left:.0001pt;text-align:center;">批次</p> </td><td> <p style="margin-left:.0001pt;text-align:center;">划定的学校级别，如本科批次</p> </td></tr><tr><td> <p style="margin-left:.0001pt;text-align:center;">分数线</p> </td><td> <p style="margin-left:.0001pt;text-align:center;">达到所属批次的最低分</p> </td></tr></tbody></table> 
<p class="img-center"><img alt="" height="581" src="https://images2.imgbox.com/13/d3/QuMd2icH_o.png" width="400"></p> 
<p style="margin-left:.0001pt;text-align:justify;">为了解2019年全国各地的高考分数线情况，请使用Spark编程，完成以下需求：</p> 
<p><strong>1、读取exam2019.csv并创建RDD。</strong></p> 
<p>首先将该文件上传到我们的终端，我是放在主目录“/home/zhanghc”下的。</p> 
<p>然后启动pyspark：</p> 
<pre><code class="language-bash">pyspark
</code></pre> 
<p>再读取我们的文件并创建RDD： </p> 
<pre><code class="language-bash">&gt;&gt;&gt; data = sc.textFile("file:///home/zhanghc/exam2019.csv")</code></pre> 
<p><strong>2、查找出各地区本科批次的分数线。</strong></p> 
<pre><code class="language-bash"># 对RDD数据进行map操作，拆分每一行数据
&gt;&gt;&gt; data_map = data.map(lambda x: x.split(","))

# 对拆分后的RDD进行filter操作，过滤出本科的数据
&gt;&gt;&gt; data_filter = data_map.filter(lambda x: x[2] == '本科批')

# 对过滤后的RDD进行map操作，抽取出地区和分数线
&gt;&gt;&gt; data_result = data_filter.map(lambda x:(x[0],x[3]))

# 对抽取后的RDD进行reduceByKey操作，按地区进行分组
&gt;&gt;&gt; data_reduce = data_result.reduceByKey(lambda x,y:x+','+y)

# 打印结果
&gt;&gt;&gt; data_reduce.collect()
</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>3、将结果以文本格式存储到HDFS上，命名为/user/root/你的名字exam2019。</strong></p> 
<pre><code class="language-bash">&gt;&gt;&gt; data_reduce.saveAsTextFile("hdfs://localhost:9000/user/root/***exam2019")
</code></pre> 
<p><img alt="" height="574" src="https://images2.imgbox.com/12/73/8nxbeC6i_o.png" width="1200"></p> 
<p>查看上传到HDFS的“***exam2019”中的文件内容：</p> 
<p><img alt="" height="384" src="https://images2.imgbox.com/10/c5/slM86JHb_o.png" width="1200"></p> 
<h3 id="%E4%B8%89%E3%80%81Spark%20SQL%E7%BC%96%E7%A8%8B">三、Spark SQL编程</h3> 
<p style="margin-left:.0001pt;text-align:justify;">某餐饮企业预备使用大数据技术对过往餐饮点评大数据进行分析以提高服务与菜品质量，实现服务升级，具体情况如下：现有一份顾客对某城市餐饮店的点评数据restaurant.csv，记录了不同类别餐饮店在口味、环境、服务等方面的评分，数据共有12列，前10列数据字段的说明如表2所示，最后两列的数据为空则不描述。</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><caption>
   表2 顾客对某城市餐饮店的点评数据字段说明 
 </caption><thead><tr><th> <p style="margin-left:.0001pt;text-align:center;">字段名称</p> </th><th> <p style="margin-left:.0001pt;text-align:center;">字段名称</p> </th></tr></thead><tbody><tr><td style="text-align:center;">类别</td><td> <p style="margin-left:.0001pt;text-align:center;">餐饮店类别</p> </td></tr><tr><td style="text-align:center;">行政区</td><td> <p style="margin-left:.0001pt;text-align:center;">餐饮店所在位置区域</p> </td></tr><tr><td style="text-align:center;">点评数</td><td> <p style="margin-left:.0001pt;text-align:center;">有多少人进行了点评</p> </td></tr><tr><td style="text-align:center;">口味</td><td> <p style="margin-left:.0001pt;text-align:center;">口味评分</p> </td></tr><tr><td style="text-align:center;">环境</td><td> <p style="margin-left:.0001pt;text-align:center;">环境评分</p> </td></tr><tr><td style="text-align:center;">服务</td><td> <p style="margin-left:.0001pt;text-align:center;">服务评分</p> </td></tr><tr><td style="text-align:center;">人均消费</td><td> <p style="margin-left:.0001pt;text-align:center;">人均消费（单位：元）</p> </td></tr><tr><td style="text-align:center;">城市</td><td> <p style="margin-left:.0001pt;text-align:center;">餐饮店所在城市</p> </td></tr><tr><td> <p style="margin-left:.0001pt;text-align:center;">Lng</p> </td><td style="text-align:center;">经度</td></tr><tr><td> <p style="margin-left:.0001pt;text-align:center;">Lat</p> </td><td style="text-align:center;">纬度</td></tr></tbody></table> 
<p><img alt="" height="775" src="https://images2.imgbox.com/2c/c6/UGUEjZ3l_o.png" width="1007"></p> 
<p style="margin-left:.0001pt;text-align:justify;">为探究人们对该城市餐饮店的点评分布情况，分析客户在餐饮方面的消费喜好，请使用Spark SQL进行编程，完成如下需求：</p> 
<p><strong>1、读取restaurant.csv数据，删除最后为空值的两列，再删除含有空值的行。</strong></p> 
<pre><code class="language-bash"># 读取文件
&gt;&gt;&gt; df = spark.read.csv("file:///home/zhanghc/restaurant.csv", header=True)

# 删除最后两列
&gt;&gt;&gt; df = df.drop(df._c10).drop(df._c11)

# 删除含有空值的行
&gt;&gt;&gt; df = df.na.drop()

# 查看结果
&gt;&gt;&gt; df.show()
</code></pre> 
<p><img alt="" height="436" src="https://images2.imgbox.com/42/90/Z0qAXcz4_o.png" width="500"></p> 
<p><strong>2、筛选出口味评分大于7分的数据。</strong></p> 
<pre><code class="language-bash">&gt;&gt;&gt; result1 = df.filter(df.口味 &gt; 7)
&gt;&gt;&gt; result1.show()
</code></pre> 
<p><img alt="" height="317" src="https://images2.imgbox.com/a9/a0/2dpOkrWL_o.png" width="500"></p> 
<p><strong>3、统计各类别餐饮店点评数，并按降序排列。</strong></p> 
<pre><code class="language-bash"># 选出需要的列，转换成rdd
&gt;&gt;&gt; dps = df.select('类别', '点评数').rdd

# 计算每种类别餐饮点评数的总和
&gt;&gt;&gt; dps = dps.map(lambda x:(x[0], int(x[1]))).reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1], False)

# 将计算得出的表格标签进行修改
&gt;&gt;&gt; dps = dps.toDF().withColumnRenamed('_1', '类别').withColumnRenamed('_2', '点评数')

显示结果
&gt;&gt;&gt; dps.show()
</code></pre> 
<p> <img alt="" height="331" src="https://images2.imgbox.com/3d/c6/pRHJZbhz_o.png" width="600"></p> 
<p><strong>4、将步骤2和步骤3的结果保存到HDFS上，命名为/user/root/你的名字restaurant。</strong></p> 
<pre><code class="language-bash">&gt;&gt;&gt; result1.rdd.saveAsTextFile("hdfs://localhost:9000/user/root/***restaurant1")
&gt;&gt;&gt; dps.rdd.saveAsTextFile("hdfs://localhost:9000/user/root/***restaurant2")
</code></pre> 
<p>查看上传到HDFS的“***restaurant”中的文件内容：</p> 
<p><img alt="" height="199" src="https://images2.imgbox.com/0d/2d/qsxzk8Ri_o.png" width="970"></p> 
<h3 id="%E5%9B%9B%E3%80%81Spark%20Streaming%E7%BC%96%E7%A8%8B">四、Spark Streaming编程</h3> 
<p style="margin-left:.0001pt;text-align:justify;">现有一份某饭店的菜单数据文件menu.txt,部分数据如表3所示，每一行有3个字段，分别表示菜品ID、菜名和单价（单位：元）。</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><caption>
   表3 某饭店的菜单数据 
 </caption><tbody><tr><td> <p style="margin-left:.0001pt;text-align:center;">1 香菇肥牛 58</p> <p style="margin-left:.0001pt;text-align:center;">2 麻婆豆腐 32</p> <p style="margin-left:.0001pt;text-align:center;">3 红烧茄子 15</p> <p style="margin-left:.0001pt;text-align:center;">4 小炒凉粉 16</p> <p style="margin-left:.0001pt;text-align:center;">5 京酱肉丝 22</p> <p style="margin-left:.0001pt;text-align:center;">6 剁椒鱼头 48</p> <p style="margin-left:.0001pt;text-align:center;">7 土豆炖鸡 38</p> <p style="margin-left:.0001pt;text-align:center;">8 锅巴香虾 66</p> </td></tr></tbody></table> 
<p style="margin-left:.0001pt;text-align:justify;">一位顾客依次点了麻婆豆腐、土豆炖鸡、红烧茄子和香菇肥牛共4个菜，为实时计算顾客点餐的费用，请使用Spark streaming 编程完成以下操作：</p> 
<p><strong>1、在虚拟机上启动8888端口。</strong></p> 
<p>直接在终端输入如下命令：</p> 
<pre><code class="language-bash">nc -lk 8888
</code></pre> 
<p><strong>2、使用Spark streaming连接虚拟机的8888端口，并实时统计顾客点餐的总费用。</strong></p> 
<p>创建一个py程序名为prizeSum.py，并填入如下代码：</p> 
<pre><code class="language-bash">vi prizeSum.py</code></pre> 
<pre><code class="language-python"># /home/zhanghc/prizeSum.py
from __future__ import print_function
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
import sys

# 从sys.argv中获取主机名和端口号
if len(sys.argv) != 3:
    print("Usage:prizeSum.py &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
    exit(-1)

# 创建SparkContext
sc = SparkContext(appName="pythonSparkStreamingPrizeSum")

# 创建StreamingContext
ssc = StreamingContext(sc, 5)

# 创建函数，实现累加
def accumulate(values, sums):
    return sum(values) + (sums or 0)

# 设置检查点目录
ssc.checkpoint("file:///home/zhanghc/")

initialStateRDD = sc.parallelize([])

# 从指定的主机和端口接收数据流
lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))

# 将数据流中的每一行转换为一个元组
costs= lines.map(lambda x: x.split(" "))

# 将每一行的价格累加
totalCost = costs.map(lambda x: ("总价", int(x[2]))).updateStateByKey(accumulate, initialRDD=initialStateRDD)
totalCost.map(lambda x: x.values())

# 打印结果
totalCost.pprint()

# 启动Streaming处理流
ssc.start()

# 等待程序终止
ssc.awaitTermination()
</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>3、启动Spark streaming程序，在8888端口输入顾客所点的菜单数据，如“3 红烧茄子15”，查看顾客本次点餐的总费用。</strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">启动prizeSum.py程序：</p> 
<pre><code class="language-bash">spark-submit prizeSum.py localhost 8888</code></pre> 
<p><img alt="" height="73" src="https://images2.imgbox.com/9a/7a/Z8m9dfxE_o.png" width="300"></p> 
<p><img alt="" height="216" src="https://images2.imgbox.com/38/2f/EUWQczf1_o.png" width="1200"></p> 
<p><img alt="" height="420" src="https://images2.imgbox.com/ef/08/Eo7ftWJn_o.png" width="300"></p> 
<h3 id="%E4%BA%94%E3%80%81Flume%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE" style="margin-left:.0001pt;text-align:justify;">五、Flume的安装配置</h3> 
<p style="margin-left:.0001pt;text-align:justify;">Flume是非常流行的日志采集系统，可以作为Spark Streaming的高级数据源。请到Flume官网下载Flume安装文件（版本不限），并将其安装到你的系统中。要求把Flume Source设置为netcat类型，从终端上不断给Flume Source发送各种消息，Flume把消息汇集到Sink(这里把Sink类型设置为avro)，由Sink把消息推送给Spark Streaming，由自己编写的Spark Streaming应用程序对消息进行处理。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>1、安装Flume</strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">（1）下载Flume：</p> 
<p style="margin-left:.0001pt;text-align:justify;">到Flume官网下载Flume1.7.0安装文件，下载地址如下：</p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#00000a;"><a class="link-info" href="http://www.apache.org/dyn/closer.lua/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz" rel="nofollow" title="http://www.apache.org/dyn/closer.lua/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz">http://www.apache.org/dyn/closer.lua/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz</a></span></p> 
<p style="margin-left:.0001pt;text-align:justify;">下载完成后上传到虚拟机的“/usr/local/uploads”目录下。</p> 
<p>（2）解压安装包：</p> 
<p>首先进入到“uploads”目录下。</p> 
<pre><code class="language-bash">tar -zxvf apache-flume-1.7.0-bin.tar.gz -C /usr/local     #解压到“/usr/local”目录下
cd /usr/local
mv apache-flume-1.7.0-bin flume     #将解压的文件修改名字为flume，简化操作
chown -R hadoop:hadoop ./flume      #把/usr/local/flume目录的权限赋予当前登录Linux系统的用户，这里假设是hadoop用户</code></pre> 
<p><img alt="" height="212" src="https://images2.imgbox.com/3f/6a/o9S2wMzq_o.png" width="500"></p> 
<p>（3）配置环境变量：</p> 
<p>首先，修改/etc/profile配置文件：</p> 
<pre><code class="language-bash">vi /etc/profile</code></pre> 
<p><img alt="" height="223" src="https://images2.imgbox.com/c3/84/xjMsnMzp_o.png" width="500"></p> 
<pre><code class="language-bash">export FLUME_HOME=/usr/local/flume
export PATH=$PATH:$FLUME_HOME/bin
export FLUME_CONF_DIR=$FLUME_HOME/conf</code></pre> 
<p>使文件生效：</p> 
<pre><code class="language-bash">source /etc/profile</code></pre> 
<p>下面修改 flume-env.sh 配置文件：</p> 
<pre><code class="language-bash">cd /usr/local/flume/conf
cp flume-env.sh.template flume-env.sh
vi flume-env.sh</code></pre> 
<p><img alt="" height="115" src="https://images2.imgbox.com/e6/88/1LAukAb2_o.png" width="600"></p> 
<p>在文件中增加一行内容，用于设置JAVA_HOME变量：</p> 
<pre><code class="language-bash">export JAVA_HOME=/usr/local/servers/jdk</code></pre> 
<p><img alt="" height="213" src="https://images2.imgbox.com/02/47/z0ALICA6_o.png" width="500"></p> 
<p>然后，保存flume-env.sh文件，并退出vim编辑器。</p> 
<p>（4）查看Flume版本信息：</p> 
<pre><code class="language-bash">cd /usr/local/flume
./bin/flume-ng version</code></pre> 
<p><img alt="" height="100" src="https://images2.imgbox.com/87/84/pmafIVkX_o.png" width="500"></p> 
<p><strong>2、使用Flume作为Spark Streaming数据源</strong></p> 
<p>（1）在“/usr/local/flume/conf”目录下创建两个conf文件：</p> 
<p><img alt="" height="150" src="https://images2.imgbox.com/36/a1/df0XYyht_o.png" width="500"></p> 
<pre><code class="language-bash">#/usr/local/flume/conf/avro.conf
  a1.sources = r1
  a1.sinks = k1
  a1.channels = c1

# Describe/configure the source
  a1.sources.r1.type = avro
  a1.sources.r1.channels = c1
  a1.sources.r1.bind = 0.0.0.0
  a1.sources.r1.port = 4141
    #注意这个端口名，在后面的教程中会用得到

# Describe the sink
  a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
  a1.channels.c1.type = memory
  a1.channels.c1.capacity = 1000
  a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
  a1.sources.r1.channels = c1
  a1.sinks.k1.channel = c1</code></pre> 
<pre><code class="language-bash">#/usr/local/flume/conf/netcat.conf
    # Name the components on this agent
    a1.sources = r1
    a1.sinks = k1
    a1.channels = c1

    # Describe/configure the source
    a1.sources.r1.type = netcat
    a1.sources.r1.bind = localhost
    a1.sources.r1.port = 44444
        #同上，记住该端口名

    # Describe the sink
    a1.sinks.k1.type = logger

    # Use a channel which buffers events in memory
    a1.channels.c1.type = memory
    a1.channels.c1.capacity = 1000
    a1.channels.c1.transactionCapacity = 100

    # Bind the source and sink to the channel
    a1.sources.r1.channels = c1
    a1.sinks.k1.channel = c1
</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">（2）Spark准备工作：</p> 
<p style="margin-left:.0001pt;text-align:justify;">首先，到官网下载spark-streaming-flume_2.11-2.3.4.jar：</p> 
<p style="margin-left:.0001pt;text-align:justify;"><a class="link-info" href="https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-flume" rel="nofollow" title="https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-flume">https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-flume</a></p> 
<p style="margin-left:.0001pt;text-align:justify;">上面的网址要是打不开，可以用下面的这个网址：</p> 
<p style="margin-left:.0001pt;text-align:justify;"><a class="link-info" href="https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-flume_2.11" rel="nofollow" title="https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-flume_2.11">https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-flume_2.11</a></p> 
<p class="img-center"><img alt="" height="365" src="https://images2.imgbox.com/1b/58/DOuSzlEf_o.png" width="600"></p> 
<p style="margin-left:.0001pt;text-align:justify;">把这个jar文件放到“/usr/local/spark/jars/flume”目录下。</p> 
<pre><code class="language-bash">cd /usr/local/spark/jars
mkdir flume
cd flume
cp /usr/local/uploads/spark-streaming-flume_2.11-2.3.4.jar .
</code></pre> 
<p><img alt="" height="77" src="https://images2.imgbox.com/98/6e/pGZEf7K2_o.png" width="796"></p> 
<p>然后，修改spark目录下conf/spark-env.sh文件中的SPARK_DIST_CLASSPATH变量。把flume的相关jar包添加到此文件中。</p> 
<pre><code class="language-bash">cd /usr/local/spark/conf
vi spark-env.sh
</code></pre> 
<pre><code class="language-bash">:/usr/local/spark/jars/flume/*:/usr/local/flume/lib/*
</code></pre> 
<p><img alt="" height="420" src="https://images2.imgbox.com/b3/ea/MI7aunST_o.png" width="1200"></p> 
<p>这样，Spark环境就准备好了。</p> 
<p style="margin-left:.0001pt;text-align:justify;">（3）创建flume-to-spark.conf</p> 
<pre><code class="language-bash">cd /usr/local/flume/conf
vi flume-to-spark.conf
</code></pre> 
<pre><code class="language-bash">#flume-to-spark.conf: A single-node Flume configuration
        # Name the components on this agent
        a1.sources = r1
        a1.sinks = k1
        a1.channels = c1

        # Describe/configure the source
        a1.sources.r1.type = netcat
        a1.sources.r1.bind = localhost
        a1.sources.r1.port = 33333

        # Describe the sink
        a1.sinks.k1.type = avro
        a1.sinks.k1.hostname = localhost
        a1.sinks.k1.port =44444

        # Use a channel which buffers events in memory
        a1.channels.c1.type = memory
        a1.channels.c1.capacity = 1000000
        a1.channels.c1.transactionCapacity = 1000000

        # Bind the source and sink to the channel
        a1.sources.r1.channels = c1
        a1.sinks.k1.channel = c1</code></pre> 
<p>#说明：<br> 1、Flume suorce类为netcat，绑定到localhost的33333端口，消息可以通过telnet localhost 33333 发送到flume suorce<br> 2、Flume Sink类为avro，绑定44444端口，flume sink通过localhost 44444端口把消息发送出来。而spark streaming程序一直监听44444端口。</p> 
<p style="margin-left:.0001pt;text-align:justify;">#注意！！先不要启动Flume agent，因为44444端口还没打开，sink的消息无处可去，44444端口由spark streaming程序打开。</p> 
<p style="margin-left:.0001pt;text-align:justify;">（4）编写Spark程序使用Flume数据源</p> 
<p style="margin-left:.0001pt;text-align:justify;">A、创建python文件</p> 
<pre><code class="language-bash">cd /home/zhanghc/sparkcode
mkdir flume
cd flume
vi FlumeEventCount.py</code></pre> 
<p>在FlumeEventCount.py中输入以下代码： </p> 
<pre><code class="language-python">#/home/zhanghc/sparkcode/flume/FlumeEventCount.py
from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.flume import FlumeUtils
import pyspark
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: flume_wordcount.py &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
        exit(-1)

    sc = SparkContext(appName="FlumeEventCount")
    ssc = StreamingContext(sc, 2)

    hostname= sys.argv[1]
    port = int(sys.argv[2])
    stream = FlumeUtils.createStream(ssc, hostname, port,pyspark.StorageLevel.MEMORY_AND_DISK_SER_2)
    stream.count().map(lambda cnt : "Recieve " + str(cnt) +" Flume events!!!!").pprint()

    ssc.start()
    ssc.awaitTermination()</code></pre> 
<p>B、测试运行效果</p> 
<p>注意：可能需要安装pyspark，命令为：</p> 
<pre><code class="language-bash">pip3 install pyspark</code></pre> 
<p><img alt="" height="218" src="https://images2.imgbox.com/02/26/Cuje51Oa_o.png" width="1200"></p> 
<p> 首先，启动Spark streaming程序：</p> 
<pre><code class="language-bash">./bin/spark-submit --driver-class-path /usr/local/spark/jars/*:/usr/local/spark/jars/flume/* /home/zhanghc/sparkcode/flume/FlumeEventCount.py localhost 44444</code></pre> 
<p><img alt="" height="831" src="https://images2.imgbox.com/6b/8c/NMJUvrFI_o.png" width="1200"></p> 
<p>然后，启动一个新的终端，启动Flume Agent：</p> 
<pre><code class="language-bash">cd /usr/local/flume
bin/flume-ng agent --conf ./conf --conf-file ./conf/flume-to-spark.conf --name a1 -Dflume.root.logger=INFO,console</code></pre> 
<p><img alt="" height="743" src="https://images2.imgbox.com/19/e6/RNxmQthG_o.png" width="1200"></p> 
<p>最后，再启动一个新的终端连接44444端口：</p> 
<p>先要安装telnet：</p> 
<pre><code class="language-bash">yum install telnet
</code></pre> 
<p><img alt="" height="819" src="https://images2.imgbox.com/f3/57/ouyUcDb8_o.png" width="1200"></p> 
<p>最后输入命令连接33333端口：</p> 
<pre><code class="language-bash">telnet localhost 33333</code></pre> 
<p>现在你可以在最后这个终端里输入一些字符了。在你输入字符后可以看到第一个终端会显示如下的信息：</p> 
<pre><code class="language-bash">-------------------------------------------
Time: 1488029430000 ms
-------------------------------------------
Received 1 flume events！！！</code></pre> 
<p><img alt="" height="827" src="https://images2.imgbox.com/d5/c8/JbMbYUoS_o.png" width="1200"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/69a2a120d88af8fe9726a43cfbd95a45/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">React Hooks中useState的介绍，并封装为useSetState函数的使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/996f757a8b0fafc614132882561bb245/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【力扣100】51.N皇后 || 斜线判断逻辑</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>