<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>NLP中的对抗训练 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="NLP中的对抗训练" />
<meta property="og:description" content="一. 对抗训练定义 对抗训练是一种引入噪声的训练方式，可以对参数进行正则化，提升模型鲁棒性和泛化能力
1.1 对抗训练特点 相对于原始输入，所添加的扰动是微小的添加的噪声可以使得模型预测错误 1.2 对抗训练的基本概念 就是在原始输入样本 x 上加上一个扰动Δx得到对抗样本，再用其进行训练，这个问题可以抽象成这样一个模型：
m a x θ P ( y ∣ x &#43; Δ x ; θ ) max_{\theta}P(y|x&#43;\Delta x; \theta) maxθ​P(y∣x&#43;Δx;θ)
其中，y 是ground truth, θ \theta θ 是模型参数。意思就是即使在扰动的情况下求使得预测出y的概率最大的参数，扰动可以被定义为：
Δ x = ϵ ∗ s i g n ( ∇ x L ( x , y ; θ ) ) \Delta x= \epsilon *sign(\nabla x L(x,y; \theta)) Δx=ϵ∗sign(∇xL(x,y;θ))
其中，sign为符号函数，L 为损失函数
最后，GoodFellow还总结了对抗训练的两个作用：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/7a678cf626658e533683fbadc2aa8836/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-28T11:32:26+08:00" />
<meta property="article:modified_time" content="2023-04-28T11:32:26+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NLP中的对抗训练</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="__0"></a>一. 对抗训练定义</h4> 
<p>对抗训练是一种引入噪声的训练方式，可以对参数进行正则化，提升模型鲁棒性和泛化能力</p> 
<h5><a id="11__4"></a>1.1 对抗训练特点</h5> 
<ul><li>相对于原始输入，所添加的扰动是微小的</li><li>添加的噪声可以使得模型预测错误</li></ul> 
<h5><a id="12__9"></a>1.2 对抗训练的基本概念</h5> 
<p>就是在原始输入样本 x 上加上一个扰动Δx得到对抗样本，再用其进行训练，这个问题可以抽象成这样一个模型：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          m 
         
        
          a 
         
         
         
           x 
          
         
           θ 
          
         
        
          P 
         
        
          ( 
         
        
          y 
         
        
          ∣ 
         
        
          x 
         
        
          + 
         
        
          Δ 
         
        
          x 
         
        
          ; 
         
        
          θ 
         
        
          ) 
         
        
       
         max_{\theta}P(y|x+\Delta x; \theta) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.1389em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span></span><br> 其中，y 是<code>ground truth</code>, <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
      
        \theta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> 是模型参数。意思就是即使在扰动的情况下求使得预测出y的概率最大的参数，扰动可以被定义为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          Δ 
         
        
          x 
         
        
          = 
         
        
          ϵ 
         
        
          ∗ 
         
        
          s 
         
        
          i 
         
        
          g 
         
        
          n 
         
        
          ( 
         
        
          ∇ 
         
        
          x 
         
        
          L 
         
        
          ( 
         
        
          x 
         
        
          , 
         
        
          y 
         
        
          ; 
         
        
          θ 
         
        
          ) 
         
        
          ) 
         
        
       
         \Delta x= \epsilon *sign(\nabla x L(x,y; \theta)) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.4653em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord">∇</span><span class="mord mathnormal">xL</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">))</span></span></span></span></span></span></p> 
<p>其中，sign为符号函数，L 为损失函数</p> 
<p><strong>最后，GoodFellow还总结了对抗训练的两个作用：</strong></p> 
<ol><li>提高模型应对恶意对抗样本时的鲁棒性</li><li>作为一种regularization，减少overfitting，提高泛化能力</li></ol> 
<h5><a id="13_MinMax_24"></a>1.3 Min-Max公式</h5> 
<p><strong>Madry在2018年的ICLR论文</strong><a href="https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fabs%2F1706.06083" rel="nofollow">Towards Deep Learning Models Resistant to Adversarial Attacks</a>中总结了之前的工作，对抗训练可以统一写成如下格式：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          m 
         
        
          i 
         
         
         
           n 
          
         
           θ 
          
         
         
         
           E 
          
          
          
            ( 
           
          
            x 
           
          
            , 
           
          
            y 
           
          
            ) 
           
          
            ∼ 
           
          
            D 
           
          
         
        
          [ 
         
        
          m 
         
        
          a 
         
         
         
           x 
          
          
          
            Δ 
           
          
            x 
           
          
            ∈ 
           
          
            Ω 
           
          
         
        
          L 
         
        
          ( 
         
        
          x 
         
        
          + 
         
        
          Δ 
         
        
          x 
         
        
          , 
         
        
          y 
         
        
          ; 
         
        
          θ 
         
        
          ) 
         
        
          ] 
         
        
       
         min_{\theta} E_{(x,y) \sim D}[max_{\Delta x \in \Omega }L(x+\Delta x,y; \theta)] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1052em; vertical-align: -0.3552em;"></span><span class="mord mathnormal">mi</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.5198em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3552em;"><span class=""></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal">ma</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">Δ</span><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mtight">Ω</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1774em;"><span class=""></span></span></span></span></span></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)]</span></span></span></span></span></span></p> 
<p>其中D代表训练集，x代表输入，y代表标签，θ是模型参数，L(x,y;θ)是单个样本的loss，Δx是对抗扰动，Ω是扰动空间。这个统一的格式首先由论文<a href="https://arxiv.org/abs/1706.06083" rel="nofollow">《Towards Deep Learning Models Resistant to Adversarial Attacks》</a>提出。</p> 
<p>这个式子可以分步理解如下：</p> 
<blockquote> 
 <p>1、往属于x里边注入扰动Δx，Δx的目标是让L(x+Δx,y;θ)越大越好，也就是说尽可能让现有模型的预测出错；</p> 
 <p>2、当然Δx也不是无约束的，它不能太大，否则达不到“看起来几乎一样”的效果，所以Δx要满足一定的约束，常规的约束是∥Δx∥≤ϵ，其中ϵ是一个常数；</p> 
 <p>3、每个样本都构造出对抗样本x+Δx之后，用(x+Δx,y)作为数据对去最小化loss来更新参数θ（梯度下降）；</p> 
 <p>4、反复交替执行1、2、3步。</p> 
</blockquote> 
<p>由此观之，整个优化过程是max和min交替执行，这确实跟GAN很相似，不同的是，GAN所max的自变量也是模型的参数，而这里max的自变量则是输入（的扰动量），也就是说要对每一个输入都定制一步max。</p> 
<h5><a id="14_FGM_43"></a>1.4 FGM</h5> 
<p>现在的问题是如何计算Δx，它的目标是增大L(x+Δ,y;θ)，而我们知道让loss减少的方法是梯度下降，那反过来，让loss增大的方法自然就是梯度上升，因此可以简单地取</p> 
<p>Δx=ϵ∇xL(x,y;θ)</p> 
<p>当然，为了防止Δx过大，通常要对∇xL(x,y;θ)做些标准化，比较常见的方式是</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          Δ 
         
        
          x 
         
        
          = 
         
        
          ϵ 
         
         
          
          
            ∇ 
           
          
            x 
           
          
            L 
           
          
            ( 
           
          
            x 
           
          
            , 
           
          
            y 
           
          
            ; 
           
          
            θ 
           
          
            ) 
           
          
          
          
            ∥ 
           
          
            ∇ 
           
          
            x 
           
          
            L 
           
          
            ( 
           
          
            x 
           
          
            , 
           
          
            y 
           
          
            ; 
           
          
            θ 
           
          
            ) 
           
          
            ∥ 
           
          
         
        
          或 
         
        
          Δ 
         
        
          x 
         
        
          = 
         
        
          ϵ 
         
        
          s 
         
        
          i 
         
        
          g 
         
        
          n 
         
        
          ( 
         
        
          ∇ 
         
        
          x 
         
        
          L 
         
        
          ( 
         
        
          x 
         
        
          , 
         
        
          y 
         
        
          ; 
         
        
          θ 
         
        
          ) 
         
        
          ) 
         
        
       
         Δx=ϵ \frac{∇xL(x,y;θ)}{∥∇xL(x,y;θ)∥}或Δx=ϵsign(∇xL(x,y;θ)) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.363em; vertical-align: -0.936em;"></span><span class="mord mathnormal">ϵ</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.427em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">∥∇</span><span class="mord mathnormal">xL</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span><span class="mord">∥</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">∇</span><span class="mord mathnormal">xL</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord cjk_fallback">或</span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">ϵs</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord">∇</span><span class="mord mathnormal">xL</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">))</span></span></span></span></span></span></p> 
<p>有了Δx之后，就可以代回式(1)进行优化</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          m 
         
        
          i 
         
         
         
           n 
          
         
           θ 
          
         
         
         
           E 
          
          
          
            ( 
           
          
            x 
           
          
            , 
           
          
            y 
           
          
            ) 
           
          
            ∼ 
           
          
            D 
           
          
         
        
          [ 
         
        
          L 
         
        
          ( 
         
        
          x 
         
        
          + 
         
        
          Δ 
         
        
          x 
         
        
          , 
         
        
          y 
         
        
          ; 
         
        
          θ 
         
        
          ) 
         
        
          ] 
         
        
       
         min_θE_{(x,y)∼D}[L(x+Δx,y;θ)] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1052em; vertical-align: -0.3552em;"></span><span class="mord mathnormal">mi</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.5198em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3552em;"><span class=""></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)]</span></span></span></span></span></span></p> 
<p>这就构成了一种对抗训练方法，被称为<strong>Fast Gradient Method（FGM）</strong>，它由GAN之父Goodfellow在论文<a href="https://arxiv.org/abs/1412.6572" rel="nofollow">《Explaining and Harnessing Adversarial Examples》</a>首先提出。</p> 
<h3><a id="NLP__59"></a>NLP 中的对抗训练</h3> 
<p>对于CV领域的任务，上述对抗训练的流程可以顺利执行下来，因为图像可以视为普通的连续实数向量，ΔxΔx也是一个实数向量，因此x+Δx依然可以是有意义的图像。但NLP不一样，NLP的输入是文本，它本质上是one hot向量（如果还没认识到这一点，欢迎阅读<a href="https://spaces.ac.cn/archives/4122" rel="nofollow">《词向量与Embedding究竟是怎么回事？》</a>），而两个不同的one hot向量，其欧氏距离恒为√2，因此对于理论上不存在什么“小扰动”。</p> 
<p>一个自然的想法是像论文<a href="https://arxiv.org/abs/1605.07725" rel="nofollow">《Adversarial Training Methods for Semi-Supervised Text Classification》</a>一样，将扰动加到Embedding层。这个思路在操作上没有问题，但问题是，扰动后的Embedding向量不一定能匹配上原来的Embedding向量表，这样一来对Embedding层的扰动就无法对应上真实的文本输入，这就不是真正意义上的对抗样本了，因为对抗样本依然能对应一个合理的原始输入。</p> 
<p>那么，在Embedding层做对抗扰动还有没有意义呢？有！实验结果显示，在很多任务中，在Embedding层进行对抗扰动能有效提高模型的性能。</p> 
<h4><a id="_67"></a>思路分析</h4> 
<p>对于CV任务来说，一般输入张量的shape是(b,h,w,c)，这时候我们需要固定模型的batch size（即b），然后给原始输入加上一个shape同样为(b,h,w,c)、全零初始化的<code>Variable</code>，比如就叫做Δx，那么我们可以直接求loss对x的梯度，然后根据梯度给Δx赋值，来实现对输入的干扰，完成干扰之后再执行常规的梯度下降。</p> 
<p>对于NLP任务来说，原则上也要对Embedding层的输出进行同样的操作，Embedding层的输出shape为(b,n,d)，所以也要在Embedding层的输出加上一个shape为(b,n,d)的<code>Variable</code>，然后进行上述步骤。但这样一来，我们需要拆解、重构模型，对使用者不够友好。</p> 
<p>不过，我们可以退而求其次。Embedding层的输出是直接取自于Embedding参数矩阵的，因此我们可以直接对Embedding参数矩阵进行扰动。这样得到的对抗样本的多样性会少一些（因为不同样本的同一个token共用了相同的扰动），但仍然能起到正则化的作用，而且这样实现起来容易得多。</p> 
<h4><a id="_75"></a>代码参考</h4> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">adversarial_training</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> embedding_name<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""给模型添加对抗训练
    其中model是需要添加对抗训练的keras模型，embedding_name
    则是model里边Embedding层的名字。要在模型compile之后使用。
    """</span>
    <span class="token keyword">if</span> model<span class="token punctuation">.</span>train_function <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>  <span class="token comment"># 如果还没有训练函数</span>
        model<span class="token punctuation">.</span>_make_train_function<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 手动make</span>
    old_train_function <span class="token operator">=</span> model<span class="token punctuation">.</span>train_function  <span class="token comment"># 备份旧的训练函数</span>

    <span class="token comment"># 查找Embedding层</span>
    <span class="token keyword">for</span> output <span class="token keyword">in</span> model<span class="token punctuation">.</span>outputs<span class="token punctuation">:</span>
        embedding_layer <span class="token operator">=</span> search_layer<span class="token punctuation">(</span>output<span class="token punctuation">,</span> embedding_name<span class="token punctuation">)</span>
        <span class="token keyword">if</span> embedding_layer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">break</span>
    <span class="token keyword">if</span> embedding_layer <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">'Embedding layer not found'</span><span class="token punctuation">)</span>

    <span class="token comment"># 求Embedding梯度</span>
    embeddings <span class="token operator">=</span> embedding_layer<span class="token punctuation">.</span>embeddings  <span class="token comment"># Embedding矩阵</span>
    gradients <span class="token operator">=</span> K<span class="token punctuation">.</span>gradients<span class="token punctuation">(</span>model<span class="token punctuation">.</span>total_loss<span class="token punctuation">,</span> <span class="token punctuation">[</span>embeddings<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># Embedding梯度</span>
    gradients <span class="token operator">=</span> K<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span> <span class="token operator">+</span> gradients<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># 转为dense tensor</span>

    <span class="token comment"># 封装为函数</span>
    inputs <span class="token operator">=</span> <span class="token punctuation">(</span>model<span class="token punctuation">.</span>_feed_inputs <span class="token operator">+</span>
              model<span class="token punctuation">.</span>_feed_targets <span class="token operator">+</span>
              model<span class="token punctuation">.</span>_feed_sample_weights<span class="token punctuation">)</span>  <span class="token comment"># 所有输入层</span>
    embedding_gradients <span class="token operator">=</span> K<span class="token punctuation">.</span>function<span class="token punctuation">(</span>
        inputs<span class="token operator">=</span>inputs<span class="token punctuation">,</span>
        outputs<span class="token operator">=</span><span class="token punctuation">[</span>gradients<span class="token punctuation">]</span><span class="token punctuation">,</span>
        name<span class="token operator">=</span><span class="token string">'embedding_gradients'</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>  <span class="token comment"># 封装为函数</span>

    <span class="token keyword">def</span> <span class="token function">train_function</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 重新定义训练函数</span>
        grads <span class="token operator">=</span> embedding_gradients<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># Embedding梯度</span>
        delta <span class="token operator">=</span> epsilon <span class="token operator">*</span> grads <span class="token operator">/</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">(</span>grads<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-8</span><span class="token punctuation">)</span>  <span class="token comment"># 计算扰动</span>
        K<span class="token punctuation">.</span>set_value<span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> K<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span> <span class="token operator">+</span> delta<span class="token punctuation">)</span>  <span class="token comment"># 注入扰动</span>
        outputs <span class="token operator">=</span> old_train_function<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>  <span class="token comment"># 梯度下降</span>
        K<span class="token punctuation">.</span>set_value<span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> K<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span> <span class="token operator">-</span> delta<span class="token punctuation">)</span>  <span class="token comment"># 删除扰动</span>
        <span class="token keyword">return</span> outputs

    model<span class="token punctuation">.</span>train_function <span class="token operator">=</span> train_function  <span class="token comment"># 覆盖原训练函数</span>
</code></pre> 
<p>定义好上述函数后，给Keras模型增加对抗训练就只需要一行代码了：</p> 
<pre><code class="prism language-python"><span class="token comment"># 写好函数后，启用对抗训练只需要一行代码</span>
adversarial_training<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'Embedding-Token'</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span>
</code></pre> 
<p>需要指出的是，由于每一步算对抗扰动也需要计算梯度，因此每一步训练一共算了两次梯度，因此每步的训练时间会翻倍。</p> 
<h3><a id="_132"></a>参考</h3> 
<p>https://www.jianshu.com/p/3e354f735fd4</p> 
<p><a href="https://spaces.ac.cn/archives/7234" rel="nofollow">对抗训练浅谈：意义、方法和思考（附Keras实现）</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5f07b9b1b298847ba15e272d93001fa5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">linux通配符和正则表达式深层解析...</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9bb03157daf763207f530e975f1c3b5f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">PCL-1.13.0&#43;VS2019配置（亲测可用）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>