<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习-大模型LLM-微调经验分享&amp;总结 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习-大模型LLM-微调经验分享&amp;总结" />
<meta property="og:description" content="大型语言模型横行，之前非常焦虑，现在全面拥抱。目前也有很多开源项目进行大模型微调等，笔者也做了一阵子大模型了，特此来介绍一下ChatGLM-6B模型微调经验，并汇总了一下目前开源项目&amp;数据。笔者与很多人微调结论不同，本人在采用单指令上进行模型微调，发现模型微调之后，「并没有出现灾难性遗忘现象」。
项目地址：https://github.com/liucongg/ChatGLM-Finetuning
ChatGLM-6B模型微调 模型越大对显卡的要求越高，目前主流对大模型进行微调方法有三种：Freeze方法、P-Tuning方法和Lora方法。笔者也通过这三种方法，在信息抽取任务上，对ChatGLM-6B大模型进行模型微调。为了防止大模型的数据泄露，采用一个领域比赛数据集-汽车工业故障模式关系抽取(https://www.datafountain.cn/competitions/584)，随机抽取50条作为测试集。
详细代码见上面的GitHub链接，并且也被ChatGLM官方收录。
Freeze方法 Freeze方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行TP或PP操作，就可以对大模型进行训练。
微调代码，见finetuning_freeze.py，核心部分如下：
for name, param in model.named_parameters(): if not any(nd in name for nd in [&#34;layers.27&#34;, &#34;layers.26&#34;, &#34;layers.25&#34;, &#34;layers.24&#34;, &#34;layers.23&#34;]): param.requires_grad = False 针对模型不同层进行修改，可以自行修改。训练代码均采用DeepSpeed进行训练，可设置参数包含train_path、model_dir、num_train_epochs、train_batch_size、gradient_accumulation_steps、output_dir、prompt_text等，可根据自己的任务配置。
CUDA_VISIBLE_DEVICES=0 deepspeed finetuning_freeze.py --num_train_epochs 5 --train_batch_size 2 三元组抽取的推理代码，见predict_freeze.py，其他任务可以根据自己的评价标准进行推理预测。
PT方法 PT方法，即P-Tuning方法，参考ChatGLM官方代码(https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md) ，是一种针对于大模型的soft-prompt方法。
P-Tuning(https://arxiv.org/abs/2103.10385)，仅对大模型的Embedding加入新的参数。
P-Tuning-V2(https://arxiv.org/abs/2110.07602)，将大模型的Embedding和每一层前都加上新的参数。
微调代码，见finetuning_pt.py，核心部分如下：
config = ChatGLMConfig.from_pretrained(args.model_dir) config.pre_seq_len = args.pre_seq_len config.prefix_projection = args.prefix_projection model = ChatGLMForConditionalGeneration.from_pretrained(args.model_dir, config=config) for name, param in model.named_parameters(): if not any(nd in name for nd in [&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/6705c2de31f6a06a3f51769cf19bc031/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-25T15:13:12+08:00" />
<meta property="article:modified_time" content="2023-06-25T15:13:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习-大模型LLM-微调经验分享&amp;总结</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>大型语言模型横行，之前非常焦虑，现在全面拥抱。目前也有很多开源项目进行大模型微调等，笔者也做了一阵子大模型了，特此来介绍一下ChatGLM-6B模型微调经验，并汇总了一下目前开源项目&amp;数据。笔者与很多人微调结论不同，本人在采用单指令上进行模型微调，发现模型微调之后，<strong>「并没有出现灾难性遗忘现象」</strong>。</p> 
</blockquote> 
<p>项目地址：<a class="link-info" href="https://github.com/liucongg/ChatGLM-Finetuning" title="https://github.com/liucongg/ChatGLM-Finetuning">https://github.com/liucongg/ChatGLM-Finetuning</a></p> 
<h2><strong>ChatGLM-6B模型微调</strong></h2> 
<p>        模型越大对显卡的要求越高，目前主流对大模型进行微调方法有三种：Freeze方法、P-Tuning方法和Lora方法。笔者也通过这三种方法，在信息抽取任务上，对ChatGLM-6B大模型进行模型微调。为了防止大模型的数据泄露，采用一个领域比赛数据集-汽车工业故障模式关系抽取(https://www.datafountain.cn/competitions/584)，随机抽取50条作为测试集。</p> 
<p>详细代码见上面的GitHub链接，并且也被ChatGLM官方收录。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/4f/42/AO4xRvr6_o.jpg"></p> 
<h3>Freeze方法</h3> 
<p>        Freeze方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行TP或PP操作，就可以对大模型进行训练。</p> 
<p>微调代码，见finetuning_freeze.py，核心部分如下：</p> 
<pre><code>for name, param in model.named_parameters():
    if not any(nd in name for nd in ["layers.27", "layers.26", "layers.25", "layers.24", "layers.23"]):
        param.requires_grad = False</code></pre> 
<p>针对模型不同层进行修改，可以自行修改。训练代码均采用DeepSpeed进行训练，可设置参数包含train_path、model_dir、num_train_epochs、train_batch_size、gradient_accumulation_steps、output_dir、prompt_text等，可根据自己的任务配置。</p> 
<pre><code>CUDA_VISIBLE_DEVICES=0 deepspeed finetuning_freeze.py --num_train_epochs 5 --train_batch_size 2</code></pre> 
<p>三元组抽取的推理代码，见predict_freeze.py，其他任务可以根据自己的评价标准进行推理预测。</p> 
<h3>PT方法</h3> 
<p>        PT方法，即P-Tuning方法，参考ChatGLM官方代码(https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md) ，是一种针对于大模型的soft-prompt方法。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2b/85/z0PHBIQq_o.jpg"></p> 
<ul><li> <p>P-Tuning(https://arxiv.org/abs/2103.10385)，仅对大模型的Embedding加入新的参数。</p> </li><li> <p>P-Tuning-V2(https://arxiv.org/abs/2110.07602)，将大模型的Embedding和每一层前都加上新的参数。</p> </li></ul> 
<p>微调代码，见finetuning_pt.py，核心部分如下：</p> 
<pre><code>config = ChatGLMConfig.from_pretrained(args.model_dir)
config.pre_seq_len = args.pre_seq_len
config.prefix_projection = args.prefix_projection

model = ChatGLMForConditionalGeneration.from_pretrained(args.model_dir, config=config)

for name, param in model.named_parameters():
    if not any(nd in name for nd in ["prefix_encoder"]):
        param.requires_grad = False</code></pre> 
<p>当prefix_projection为True时，为P-Tuning-V2方法，在大模型的Embedding和每一层前都加上新的参数；为False时，为P-Tuning方法，仅在大模型的Embedding上新的参数。</p> 
<p>可设置参数包含train_path、model_dir、num_train_epochs、train_batch_size、gradient_accumulation_steps、output_dir、prompt_text、pre_seq_len、prompt_text等， 可根据自己的任务配置。</p> 
<pre><code>CUDA_VISIBLE_DEVICES=0 deepspeed finetuning_pt.py --num_train_epochs 5 --train_batch_size 2 --pre_seq_len 16</code></pre> 
<p>三元组抽取的推理代码，见predict_pt.py，其他任务可以根据自己的评价标准进行推理预测。</p> 
<h3>Lora方法</h3> 
<p>        Lora方法，即在大型语言模型上对指定参数增加额外的低秩矩阵，并在模型训练过程中，仅训练而外增加的参数。当“秩值”远小于原始参数维度时，新增的低秩矩阵参数量很小，达到仅训练很小的参数，就能获取较好的结果。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/81/e6/BoYkRDWh_o.jpg"></p> 
<ul><li> <p>Lora论文：https://arxiv.org/abs/2106.09685</p> </li><li> <p>官方代码：https://github.com/microsoft/LoRA</p> </li><li> <p>HuggingFace封装的peft库：https://github.com/huggingface/peft</p> </li></ul> 
<p>微调代码，见finetuning_lora.py，核心部分如下：</p> 
<pre><code>model = ChatGLMForConditionalGeneration.from_pretrained(args.model_dir)
config = LoraConfig(r=args.lora_r,
lora_alpha=32,
target_modules=["query_key_value"],
lora_dropout=0.1,
bias="none",
task_type="CAUSAL_LM",
inference_mode=False,
)

model = get_peft_model(model, config)</code></pre> 
<p>可设置参数包含train_path、model_dir、num_train_epochs、train_batch_size、gradient_accumulation_steps、output_dir、prompt_text、lora_r等，可根据自己的任务配置。</p> 
<pre><code>CUDA_VISIBLE_DEVICES=0 deepspeed finetuning_lora.py --num_train_epochs 5 --train_batch_size 2 --lora_r 8</code></pre> 
<p>三元组抽取的推理代码，见predict_lora.py，其他任务可以根据自己的评价标准进行推理预测。</p> 
<p><strong>注意：</strong>对于结果需要保持一致的任务(即关掉dropout，解码关掉do_sample)，需要保存模型的adapter_config.json文件中，inference_mode参数修改成false，并将模型执行model.eval()操作。主要原因是chatglm模型代码中，没有采用Conv1D函数。</p> 
<h2><strong>三元组抽取实验结果</strong></h2> 
<ul><li> <p>模型训练时，最大长度为768，Batch大小为2，训练轮数为5，fp16训练，采用DeepSpeed的Zero-1训练；</p> </li><li> <p>PT为官方的P-Tuning V2训练方法，PT-Only-Embedding表示仅对Embedding进行soft-prompt，Freeze仅训练模型后五层参数，Lora采用低秩矩阵方法训练，秩为8；</p> </li><li> <p>由于之前训练PT在48G-A40显卡上会出现OOM，因此之前进行PT实验时对模型开启了gradient_checkpointing_enable，使得模型显存占用变小，但训练时长增加。</p> </li><li> <p>训练示例：</p> </li></ul> 
<pre><code>prompt_text：你现在是一个信息抽取模型，请你帮我抽取出关系内容为\"性能故障\", \"部件故障\", \"组成\"和 \"检测工具\"的相关三元组，三元组内部用\"_\"连接，三元组之间用\\n分割。文本：
输入：故障现象：发动机水温高，风扇始终是低速转动，高速档不工作，开空调尤其如此。
输出：发动机_部件故障_水温高\n风扇_部件故障_低速转动</code></pre> 
<p>时间换空间，可用很好的解决显卡的资源问题，简单玩玩还可以，如果想要模型达到最优效果或可用快速看到效果，还不如租张A100卡，快速实验，推理阶段再用自己的小破卡。</p> 
<p>笔者找到一家新的算力平台-揽睿星舟，单张A100仅要6.4元/小时，我翻了一圈，算是便宜的了(反正比AutoDL便宜一点，便宜一点是一点吧)。</p> 
<p>        下面实验结果均是在租的80G-A100上进行的实验，与Github里用的A40的实验结果会有些差异，主要在训练时长(纯训练速度，剔除模型保存的时间)。说实话，真的要训练一个大模型，多个A100是必不可少的，可以减少很多模型并行的操作，效果上也更好把控一些。</p> 
<p><strong>微调方法</strong></p> 
<p><strong>PT-Only-Embedding</strong></p> 
<p><strong>PT</strong></p> 
<p><strong>Freeze</strong></p> 
<p><strong>Lora</strong></p> 
<table><thead><tr><td></td><td></td><td></td><td></td><td></td></tr></thead><tbody><tr><td> <p>显卡占用</p> </td><td> <p>37G</p> </td><td> <p>56G</p> </td><td> <p>24G</p> </td><td> <p>39G</p> </td></tr><tr><td> <p>总参数</p> </td><td> <p>6.259B</p> </td><td> <p>7.211B</p> </td><td> <p>6.255B</p> </td><td> <p>6.259B</p> </td></tr><tr><td> <p>可训练参数占比</p> </td><td> <p>0.0586%</p> </td><td> <p>13.26%</p> </td><td> <p>16.10%</p> </td><td> <p>0.0586%</p> </td></tr><tr><td> <p>训练耗时</p> </td><td> <p>20min</p> </td><td> <p>52min</p> </td><td> <p>46min</p> </td><td> <p>25min</p> </td></tr><tr><td> <p>测试结果F1</p> </td><td> <p>0.0</p> </td><td> <p>0.6283</p> </td><td> <p>0.5675</p> </td><td> <p>0.5359</p> </td></tr></tbody></table> 
<p>结果分析：</p> 
<ul><li> <p>效果为PT&gt;Freeze&gt;Lora&gt;PT-Only-Embedding;</p> </li><li> <p>速度为PT-Only-Embedding&gt;Lora&gt;Freeze&gt;PT;</p> </li><li> <p><strong>PT-Only-Embedding效果很不理想，发现在训练时，最后的loss仅能收敛到2.几，而其他机制可以收敛到0.几。分析原因为，输出内容形式与原有语言模型任务相差很大，仅增加额外Embedding参数，不足以改变复杂的下游任务</strong>;</p> </li><li> <p>PT方法占用显存更大，因为也增加了很多而外参数;</p> </li><li> <p>测试耗时，采用float16进行模型推理，由于其他方法均增加了额外参数，因此其他方法的推理耗时会比Freeze方法要高。当然由于是生成模型，所以生成的长度也会影响耗时;</p> </li><li> <p>模型在指定任务上微调之后，并没有丧失原有能力，例如生成“帮我写个快排算法”，依然可以生成-快排代码;</p> </li><li> <p><strong>由于大模型微调都采用大量instruction进行模型训练，仅采用单一的指令进行微调时，对原来其他的指令影响不大，因此并没导致原来模型的能力丧失</strong>;</p> </li><li> <p>上面测试仅代表个人测试结果。</p> </li></ul> 
<p>很多同学在微调后出现了灾难性遗忘现象，但我这边并没有出现，对“翻译任务”、“代码任务”、“问答任务”进行测试，采用freeze模型，可以用test_forgetting.py进行测试，具体测试效果如下：</p> 
<ul><li> <p>翻译任务</p> </li></ul> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ac/9a/qPF6TTyR_o.png"></p> 
<ul><li> <p>代码任务</p> </li></ul> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/bb/9e/iXKHF7jT_o.jpg"></p> 
<ul><li> <p>问答任务</p> </li></ul> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ba/f8/Z5gFVx5X_o.png"></p> 
<p>后面会把生成任务、分类任务做完，请持续关注Github，会定期更新。（太忙了，会抓紧时间更新，并且官方代码也在持续更新，如遇到代码代码调不通的情况，请及时联系我，我在github也给出了我的代码版本和模型版本）</p> 
<h2><strong>中文开源大模型&amp;项目</strong></h2> 
<p>虽然出来很多大模型，但Open的&amp;中文可直接使用的并不多，下面对中文开源大模型、数据集和项目进行一下汇总。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/42/5c/M2RsZn0T_o.jpg"></p> 
<h3>中文开源大模型</h3> 
<p>直接可微调，无需指令增量训练：</p> 
<ul><li> <p>ChatGLM-6B：https://huggingface.co/THUDM/chatglm-6b</p> </li><li> <p>ChatYuan-large-v2：https://huggingface.co/ClueAI/ChatYuan-large-v2</p> </li></ul> 
<p>原始模型多语言or英文，需要中文指令数据集增量训练：</p> 
<ul><li> <p>BloomZ：https://huggingface.co/bigscience/bloomz</p> </li><li> <p>LLama：https://github.com/facebookresearch/llama</p> </li><li> <p>Flan-T5：https://huggingface.co/google/flan-t5-xxl</p> </li><li> <p>OPT：https://huggingface.co/facebook/opt-66b</p> </li></ul> 
<h3>中文开源指令数据</h3> 
<p>下面中文指令集，大多数从Alpaca翻译而来，请看下面项目中data目录。目前通过ChatGPT或者GPT4作为廉价标注工为自己的数据进行数据标注一个不错的思路。</p> 
<ul><li> <p>[1]：https://github.com/LC1332/Chinese-alpaca-lora</p> </li><li> <p>[2]：https://github.com/hikariming/alpaca_chinese_dataset</p> </li><li> <p>[3]：https://github.com/carbonz0/alpaca-chinese-dataset</p> </li><li> <p>[4]：https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM</p> </li><li> <p>[5]：https://github.com/LianjiaTech/BELLE</p> </li><li> <p>[6]：https://huggingface.co/datasets/JosephusCheung/GuanacoDataset</p> </li></ul> 
<h3>开源项目</h3> 
<p>总结下面较火的开源项目：</p> 
<ul><li> <p>BELLE：https://github.com/LianjiaTech/BELLE</p> </li><li> <p>ChatGLM：https://github.com/THUDM/ChatGLM-6B</p> </li><li> <p>Luotuo-Chinese-LLM：https://github.com/LC1332/Luotuo-Chinese-LLM</p> </li><li> <p>stanford_alpaca：https://github.com/tatsu-lab/stanford_alpaca</p> </li></ul> 
<h2><strong>总结</strong></h2> 
<p>目前各大厂的大模型陆陆续续放出，堪称百家争鸣！个人玩家也是全面拥抱，想尽一切办法来训练微调大模型。只愿大家以后可以实现“大模型”自由。愿再无“model-as-a-service”。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/babcf2a1c12a3c69f56d41a553323955/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">docker内使用nvidia-smi命令提示No devices were found</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/000a08c6e26f524c803d7bb13b70d053/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">服务器内部服务器错误的原因和解决办法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>