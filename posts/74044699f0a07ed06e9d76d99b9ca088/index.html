<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>李沐机器学习系列4---全连接层到卷积 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="李沐机器学习系列4---全连接层到卷积" />
<meta property="og:description" content="1 从全连接到卷积 1.1 平移不变性 从概率分布的角度来看卷积的定义, f ( τ ) f(\tau) f(τ)是概率密度， g ( t − τ ) g(t-\tau) g(t−τ)是在这个分布下的均值
( f ∗ g ) ( t ) = ∫ − ∞ ∞ f ( τ ) g ( t − τ ) d τ (f*g)(t)=\int_{-\infin}^{\infin}f(\tau)g(t-\tau)d\tau (f∗g)(t)=∫−∞∞​f(τ)g(t−τ)dτ
2 图像卷积 2.1 互相关运算 import torch from torch import nn from d2l import torch as d2l def corr2d(X, K): #@save &#34;&#34;&#34;计算二维互相关运算&#34;&#34;&#34; h, w = K." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/74044699f0a07ed06e9d76d99b9ca088/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-04T14:48:04+08:00" />
<meta property="article:modified_time" content="2024-01-04T14:48:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">李沐机器学习系列4---全连接层到卷积</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1__0"></a>1 从全连接到卷积</h2> 
<p><img src="https://images2.imgbox.com/07/21/lQfqzDD0_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="11__2"></a>1.1 平移不变性</h3> 
<p><img src="https://images2.imgbox.com/c1/15/S412l6ut_o.png" alt="在这里插入图片描述"><br> 从概率分布的角度来看卷积的定义,<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
       
         τ 
        
       
         ) 
        
       
      
        f(\tau) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.1132em;">τ</span><span class="mclose">)</span></span></span></span></span>是概率密度，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         g 
        
       
         ( 
        
       
         t 
        
       
         − 
        
       
         τ 
        
       
         ) 
        
       
      
        g(t-\tau) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1132em;">τ</span><span class="mclose">)</span></span></span></span></span>是在这个分布下的均值<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
        
          f 
         
        
          ∗ 
         
        
          g 
         
        
          ) 
         
        
          ( 
         
        
          t 
         
        
          ) 
         
        
          = 
         
         
         
           ∫ 
          
          
          
            − 
           
          
            ∞ 
           
          
         
           ∞ 
          
         
        
          f 
         
        
          ( 
         
        
          τ 
         
        
          ) 
         
        
          g 
         
        
          ( 
         
        
          t 
         
        
          − 
         
        
          τ 
         
        
          ) 
         
        
          d 
         
        
          τ 
         
        
       
         (f*g)(t)=\int_{-\infin}^{\infin}f(\tau)g(t-\tau)d\tau 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.3846em; vertical-align: -0.9703em;"></span><span class="mop"><span class="mop op-symbol large-op" style="margin-right: 0.4445em; position: relative; top: -0.0011em;">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.4143em;"><span class="" style="top: -1.7881em; margin-left: -0.4445em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">∞</span></span></span></span><span class="" style="top: -3.8129em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∞</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.9703em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.1132em;">τ</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1132em;">τ</span><span class="mclose">)</span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right: 0.1132em;">τ</span></span></span></span></span></span></p> 
<p><img src="https://images2.imgbox.com/d5/40/kvCVA5Qf_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2__11"></a>2 图像卷积</h2> 
<h3><a id="21__12"></a>2.1 互相关运算</h3> 
<p><img src="https://images2.imgbox.com/fd/f5/B7Knbhq8_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

<span class="token keyword">def</span> <span class="token function">corr2d</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> K<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""计算二维互相关运算"""</span>
    h<span class="token punctuation">,</span> w <span class="token operator">=</span> K<span class="token punctuation">.</span>shape
    Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> h <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> w <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            Y<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>X<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i <span class="token operator">+</span> h<span class="token punctuation">,</span> j<span class="token punctuation">:</span>j <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">*</span> K<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> Y
</code></pre> 
<h3><a id="22__28"></a>2.2 特征影射和感受野</h3> 
<blockquote> 
 <p>特征映射（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。 在卷积神经网络中，对于某一层的任意元素，其感受野（receptive field）是指在前向传播期间可能影响计算的所有元素（来自所有先前层）。</p> 
</blockquote> 
<h2><a id="3__31"></a>3 填充</h2> 
<p>边界进行0填充,主要是autograd如何解决，前向比较容易实现</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn


<span class="token comment"># 为了方便起见，我们定义了一个计算卷积层的函数。</span>
<span class="token comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span>
<span class="token keyword">def</span> <span class="token function">comp_conv2d</span><span class="token punctuation">(</span>conv2d<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 这里的（1，1）表示批量大小和通道数都是1</span>
    X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    Y <span class="token operator">=</span> conv2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token comment"># 省略前两个维度：批量大小和通道</span>
    <span class="token keyword">return</span> Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列</span>
conv2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
comp_conv2d<span class="token punctuation">(</span>conv2d<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">.</span>shape
</code></pre> 
<p><img src="https://images2.imgbox.com/6f/2c/Zgxi4xDI_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4c/a7/T7gS5Y20_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="4__54"></a>4 多输入多输出通道</h2> 
<h3><a id="41__55"></a>4.1 多输入通道</h3> 
<p>输入通道数量c_i对应核的维度<br> <img src="https://images2.imgbox.com/5c/fb/IM8Ocaly_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="42__58"></a>4.2 多输出通道</h3> 
<p><img src="https://images2.imgbox.com/df/4e/5bnq41wC_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="43_11_60"></a>4.3 1*1卷积层</h3> 
<p>改变输入的通道数量，但是保持相同的高度和宽度<br> <img src="https://images2.imgbox.com/5f/6a/7bHQbRQ3_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="5__63"></a>5 汇聚层</h2> 
<p>最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层<br> 汇聚层的两个作用：</p> 
<ul><li>降低卷积层对<strong>位置的敏感性</strong></li><li>降低对空间<strong>降采样</strong>表示的敏感性</li></ul> 
<h3><a id="51__68"></a>5.1 最大汇聚层和平均汇聚层</h3> 
<p><img src="https://images2.imgbox.com/ce/7b/p9znyEK9_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="52__70"></a>5.2 多个通道</h3> 
<p>多个通道对每个通道进行单独计算，不会进行汇总。<br> <img src="https://images2.imgbox.com/90/e5/EVZBSHiZ_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="6_AlexNet_85"></a>6 AlexNet</h2> 
<h3><a id="61__86"></a>6.1 早期的图像发展历程</h3> 
<p><img src="https://images2.imgbox.com/ac/16/7uke1AfE_o.png" alt="在这里插入图片描述"><br> 通过神经网络学习特征<br> <img src="https://images2.imgbox.com/be/64/PwRBF8Tr_o.png" alt="在这里插入图片描述"><br> 突破的关键要素：<br> 1）数据<br> 2）硬件</p> 
<h3><a id="AlexNet_94"></a>AlexNet</h3> 
<p><img src="https://images2.imgbox.com/84/02/Dg0NNdbi_o.png" alt="在这里插入图片描述"><br> 改进方法：<br> 1）网络维度更大，参数接近1G<br> 2）激活函数，更换成Relu<br> 3) 容量控制和预处理<br> 使用图像增强，翻转，裁切，和变色</p> 
<h2><a id="8_VGG_101"></a>8 VGG</h2> 
<p>出现了代码块，将一些可以复用的网络结构进行封装</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">def</span> <span class="token function">vgg_block</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span>
                                kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        in_channels <span class="token operator">=</span> out_channels
    layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>
</code></pre> 
<p>也可以使用pytorch的自定义块功能</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

<span class="token keyword">class</span> <span class="token class-name">VGGBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>VGGBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            in_channels <span class="token operator">=</span> out_channels
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>vgg_block <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>vgg_block<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

</code></pre> 
<p><img src="https://images2.imgbox.com/d3/4f/TtUjsd7Q_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="9_NiN_140"></a>9 NiN块</h2> 
<p>核心的思想：<br> 1）取消了全连接层，最后用一个全局平均汇聚层，生成对数几率<br> 2）使用串联网络</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">def</span> <span class="token function">nin_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nin_block<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nin_block<span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nin_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 标签类别数是10</span>
    nin_block<span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span>
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/42/3e/LINF2AKI_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="10_GoogleNet_173"></a>10 GoogleNet</h2> 
<p>核心思想：<br> 1）使用不同大小的卷积核组合是有利的</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">class</span> <span class="token class-name">Inception</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># c1--c4是每条路径的输出通道数</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Inception<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token comment"># 线路1，单1x1卷积层</span>
        self<span class="token punctuation">.</span>p1_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c1<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 线路2，1x1卷积层后接3x3卷积层</span>
        self<span class="token punctuation">.</span>p2_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p2_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c2<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 线路3，1x1卷积层后接5x5卷积层</span>
        self<span class="token punctuation">.</span>p3_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c3<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p3_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c3<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c3<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span>
        self<span class="token punctuation">.</span>p4_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>p4_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> c4<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        p1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p1_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        p2 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p2_2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p2_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        p3 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p3_2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p3_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        p4 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p4_2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>p4_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 在通道维度上连结输出</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>p1<span class="token punctuation">,</span> p2<span class="token punctuation">,</span> p3<span class="token punctuation">,</span> p4<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/05/27/Nv1KXdCB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/00/37/yWQa3DRr_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="11__209"></a>11 批量规范化</h2> 
<p>训练网络的一些关键挑战：<br> 1）标准化输入特征<br> 2）中间变量，这些变量分布中的这种偏移可能会阻碍网络的收敛，<br> 3)更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要<br> <img src="https://images2.imgbox.com/bf/bd/K9Edsgic_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="111__215"></a>11.1 批量规范化应用在全连接层</h3> 
<p><img src="https://images2.imgbox.com/e8/64/DfTLIUXV_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">def</span> <span class="token function">batch_norm</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> moving_mean<span class="token punctuation">,</span> moving_var<span class="token punctuation">,</span> eps<span class="token punctuation">,</span> momentum<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>is_grad_enabled<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span>
        X_hat <span class="token operator">=</span> <span class="token punctuation">(</span>X <span class="token operator">-</span> moving_mean<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>moving_var <span class="token operator">+</span> eps<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
            <span class="token comment"># 使用全连接层的情况，计算特征维上的均值和方差</span>
            mean <span class="token operator">=</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            var <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span>
            <span class="token comment"># 这里我们需要保持X的形状以便后面可以做广播运算</span>
            mean <span class="token operator">=</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            var <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># 训练模式下，用当前的均值和方差做标准化</span>
        X_hat <span class="token operator">=</span> <span class="token punctuation">(</span>X <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>var <span class="token operator">+</span> eps<span class="token punctuation">)</span>
        <span class="token comment"># 更新移动平均的均值和方差</span>
        moving_mean <span class="token operator">=</span> momentum <span class="token operator">*</span> moving_mean <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> mean
        moving_var <span class="token operator">=</span> momentum <span class="token operator">*</span> moving_var <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> momentum<span class="token punctuation">)</span> <span class="token operator">*</span> var
    Y <span class="token operator">=</span> gamma <span class="token operator">*</span> X_hat <span class="token operator">+</span> beta  <span class="token comment"># 缩放和移位</span>
    <span class="token keyword">return</span> Y<span class="token punctuation">,</span> moving_mean<span class="token punctuation">.</span>data<span class="token punctuation">,</span> moving_var<span class="token punctuation">.</span>data
</code></pre> 
<p>定义一个BatchNOorm, 因为batchNorm中的gamma和beta这两个参数是需要再训练中进行更新的，</p> 
<pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> BatchNorm<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> num_dims<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>标准的batchNorm的实现</p> 
<pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="12_Resnet_272"></a>12 Resnet</h2> 
<p>使用嵌套函数来理解Resnet的作用，每个附加层都应该更容易地包含原始函数作为其元素之一<br> <img src="https://images2.imgbox.com/24/35/oAOhcn3x_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/18/50/2oEFsDHF_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">class</span> <span class="token class-name">Residual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                 use_1x1conv<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> use_1x1conv<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                                   kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>strides<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        Y <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>Y<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">:</span>
            X <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        Y <span class="token operator">+=</span> X
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/a0/b2/NtSlKHw4_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python">b1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                   
<span class="token keyword">def</span> <span class="token function">resnet_block</span><span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> num_residuals<span class="token punctuation">,</span>
                 first_block<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    blk <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_residuals<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token keyword">not</span> first_block<span class="token punctuation">:</span>
            blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Residual<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span>
                                use_1x1conv<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Residual<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> blk

b2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> first_block<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
b3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
b4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>resnet_block<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
b5 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>resnet_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> b4<span class="token punctuation">,</span> b5<span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape:\t'</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

lr<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span>
train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> resize<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">)</span>
d2l<span class="token punctuation">.</span>train_ch6<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/9c/fd/ZSOKtTXl_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="12_DenseNet_343"></a>12 DenseNet</h2> 
<p>思路来源于函数的泰勒展开，<br> <img src="https://images2.imgbox.com/d4/6c/aEgkBaKm_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/24/85/VubYjt2I_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l


<span class="token keyword">def</span> <span class="token function">conv_block</span><span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
<span class="token keyword">class</span> <span class="token class-name">DenseBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_convs<span class="token punctuation">,</span> input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DenseBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layer<span class="token punctuation">.</span>append<span class="token punctuation">(</span>conv_block<span class="token punctuation">(</span>
                num_channels <span class="token operator">*</span> i <span class="token operator">+</span> input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layer<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>net<span class="token punctuation">:</span>
            Y <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            <span class="token comment"># 连接通道维度上每个块的输入和输出</span>
            X <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> X
blk <span class="token operator">=</span> DenseBlock<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
Y<span class="token punctuation">.</span>shape

<span class="token keyword">def</span> <span class="token function">transition_block</span><span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/23b6fd4a95f30439ae8a1dd06ba052d4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【开源】基于JAVA语言的智能教学资源库系统</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6aff51899d1d8ce5a0316cb41a9a95d0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">嵌入式Linux之Ubuntu学习笔记（shell脚本条件判断，函数，循环）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>