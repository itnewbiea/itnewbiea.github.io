<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention论文阅读笔记 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention论文阅读笔记" />
<meta property="og:description" content="-cvpr2023
-当前attention机制存在的问题：
①利用im2col方式计算local attention 需要消耗很大的计算资源
② window attention存在固定的设计模式，如窗口应该如何移动，引入人工干涉。
-Method
-.Shift as Depthwise Convolution
作者首先从新的角度上剖析了im2col的原理，并用深度卷积重新实现local attention 机制。
①im2col实现的local attention:以2*2的特征图为例，先进行padding，而后通过3*3的滑动窗口得到H*W个窗口值，再进行展平，得到键值对。
②feature shift实现的local attention：以2*2的特征图为例，按照左上，上，右上，左，中，右，左下，下，右下的方式移动特征图窗口。得到九个不同的特征，再生成键值对。
③作者提出的利用Depthwise实现的local attention：以2*2的特征图为例，先进行padding，而后通过不同的固定权重的3*3的窗口得到九个不同的特征，再生成键值对。
-Deformed Shifting Module
通过将原来的 Im2Col 函数切换为 depthwise convolutions，局部注意力的效率得到了极大的提升。尽管如此，精心设计的内核权重仍然将键和值限制在固定的相邻位置，这可能不是捕获不同特征的最佳解决方案。因此，本文提出了一种新颖的可变形移位模块，以进一步增强局部注意力的灵活性。具体来说，我们在 shiftwise 卷积中利用设计范例，并引入并行卷积路径，其中内核参数在训练过程中随机初始化和学习。与将特征向不同方向移动的固定核相比，可学习内核可以解释为所有局部特征的线性组合。这类似于可变形卷积网络 中的不规则感受野。
①局部注意力中的键值对是利用一个更灵活的模块来提取的，该模块可以提高模型的容量和捕获更多样性的特征。
②可学习的卷积核与DCN中的可变形技术很相似。类似于DCN中四个相邻像素的双线性插值，我们的变形移位模块可以看作是局部窗口内特征的线性组合。这最终有助于增强空间采样位置和模型输入的几何变换。
③使用重新参数化技术来将这两条并行路径转换为一个单一的卷积。这样，我们就可以在保持推理效率的同时提高模型的计算能力" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/bdf61d38ab5d1ff6197c2e8cdc106f3f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-17T16:25:31+08:00" />
<meta property="article:modified_time" content="2023-04-17T16:25:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention论文阅读笔记</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><img alt="" height="185" src="https://images2.imgbox.com/66/78/NQz789AJ_o.png" width="1065"></p> 
<p>-cvpr2023</p> 
<p>-当前attention机制存在的问题：</p> 
<p>        ①利用im2col方式计算local attention 需要消耗很大的计算资源</p> 
<p>        ② window attention存在固定的设计模式，如窗口应该如何移动，引入人工干涉。</p> 
<p>-Method</p> 
<p>-<span style="color:#000000;"><strong>.Shift as Depthwise Convolution</strong></span></p> 
<p><img alt="" height="682" src="https://images2.imgbox.com/3e/43/GlrXAzvg_o.png" width="931"></p> 
<p>         作者首先从新的角度上剖析了im2col的原理，并用深度卷积重新实现local attention 机制。</p> 
<p>        ①im2col实现的local attention:以2*2的特征图为例，先进行padding，而后通过3*3的滑动窗口得到H*W个窗口值，再进行展平，得到键值对。</p> 
<p>        ②feature shift实现的local attention：以2*2的特征图为例，按照左上，上，右上，左，中，右，左下，下，右下的方式移动特征图窗口。得到九个不同的特征，再生成键值对。</p> 
<p>        ③作者提出的利用Depthwise实现的local attention：以2*2的特征图为例，先进行padding，而后通过不同的固定权重的3*3的窗口得到九个不同的特征，再生成键值对。</p> 
<p></p> 
<p> -<span style="color:#000000;"><strong>Deformed Shifting Module</strong></span></p> 
<p>        通过将原来的 Im2Col 函数切换为 depthwise convolutions，局部注意力的效率得到了极大的提升。尽管如此，精心设计的内核权重仍然将键和值限制在固定的相邻位置，这可能不是捕获不同特征的最佳解决方案。因此，本文提出了一种新颖的可变形移位模块，以进一步增强局部注意力的灵活性。具体来说，我们在 shiftwise 卷积中利用设计范例，并引入并行卷积路径，其中内核参数在训练过程中随机初始化和学习。与将特征向不同方向移动的固定核相比，可学习内核可以解释为所有局部特征的线性组合。这类似于可变形卷积网络 中的不规则感受野。</p> 
<p><img alt="" height="306" src="https://images2.imgbox.com/8f/27/YoyxpQLt_o.png" width="503"></p> 
<p>         ①局部注意力中的键值对是利用一个更灵活的模块来提取的，该模块可以提高模型的容量和捕获更多样性的特征。</p> 
<p>        ②可学习的卷积核与DCN中的可变形技术很相似。类似于DCN中四个相邻像素的双线性插值，我们的变形移位模块可以看作是局部窗口内特征的线性组合。这最终有助于增强空间采样位置和模型输入的几何变换。</p> 
<p>        ③使用重新参数化技术来将这两条并行路径转换为一个单一的卷积。这样，我们就可以在保持推理效率的同时提高模型的计算能力</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/34e21d7df99a84ea4d5029f3fa3688f5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">从零开始手把手教利用electorn&#43;vue搭建一套客户端开发环境</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9dea7e37ae0e32e9e1b53902439b1319/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">rpm包下载</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>