<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>HDFS学习笔记 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="HDFS学习笔记" />
<meta property="og:description" content="目录 一、为什么需要分布式存储二、分布式的基础架构分析1.分布式系统常见的组织形式？2.什么是主从模式？3.Hadoop是哪种模式？ 三、HDFS的基础架构四、HDFS的Shell操作1.进程启停管理1.1一键启停脚本1.2单进程启停 2.文件系统操作命令2.1HDFS文件系统基本信息2.2命令体系2.2.1创建文件夹2.2.2查看指定目录下内容2.2.3上传文件到HDFS指定目录下2.2.4查看HDFS文件内容2.2.5下载HDFS文件2.2.6拷贝HDFS文件2.2.7追加数据到HDFS文件中2.2.8HDFS移动、重命名操作2.2.9HDFS数据删除操作 2.3HDFS WEB浏览 3.HDFS客户端 -Jetbrains产品插件3.1 Big Data Tools插件3.2 配置Windows3.3配置Big Data Tools插件3.4使用Big Data Tools插件 五、HDFS存储原理1.存储原理2.fsck命令2.1HDFS副本块数量的配置2.2fsck命令检查文件的副本数2.3block配置 3.NameNode元数据3.1edits文件3.2.fsimage文件3.3NameNode元数据管理维护3.4元数据合并控制参数3.5SecondaryNameNode的作用 4.HDFS的读写流程4.1数据写入流程4.2数据读取流程 一、为什么需要分布式存储 •数据量太大，单机存储能力有上限，需要靠数量来解决问题
•数量的提升带来的是网络传输、磁盘读写、CPU、内存等各方面的综合提升。 分布式组合在一起可以达到1&#43;1&gt;2的效果
二、分布式的基础架构分析 1.分布式系统常见的组织形式？ •去中心化模式：没有明确中心，大家协调工作（区块链、P2P）
•中心化模式：有明确的中心，基于中心节点分配工作
2.什么是主从模式？ 主从模式（Master-Slaves）就是中心化模式，表示有一个主节点来作为管理者，管理协调下属一批从节点工作。也称一主多从模式。
3.Hadoop是哪种模式？ 主从模式（中心化模式）的架构
三、HDFS的基础架构 主角色：NameNode
•HDFS系统的主角色，是一个独立的进程
•负责管理HDFS整个文件系统
•负责管理DataNode
从角色：DataNode
•HDFS系统的从角色，是一个独立进程
•主要负责数据的存储，即存入数据和取出数据
主角色辅助角色：SecondaryNameNode
•NameNode的辅助，是一个独立进程
•主要帮助NameNode完成元数据整理工作（打杂）
四、HDFS的Shell操作 1.进程启停管理 1.1一键启停脚本 Hadoop HDFS组件内置了HDFS集群的一键启停脚本。
•$HADOOP_HOME/sbin/start-dfs.sh，一键启动HDFS集群
执行原理：
•在执行此脚本的机器上，启动SecondaryNameNode
•读取core-site.xml内容（fs.defaultFS项），确认NameNode所在机器，启动NameNode
•读取workers内容，确认DataNode所在机器，启动全部DataNode
•$HADOOP_HOME/sbin/stop-dfs.sh，一键关闭HDFS集群
执行原理：
•在执行此脚本的机器上，关闭SecondaryNameNode
•读取core-site.xml内容（fs.defaultFS项），确认NameNode所在机器，关闭NameNode
•读取workers内容，确认DataNode所在机器，关闭全部NameNode
1.2单进程启停 除了一键启停外，也可以单独控制进程的启停。
$HADOOP_HOME/sbin/hadoop-daemon.sh，此脚本可以单独控制所在机器的进程的启停 用法：
hadoop-daemon.sh (start|status|stop) (namenode|secondarynamenode|datanode) $HADOOP_HOME/bin/hdfs，此程序也可以用以单独控制所在机器的进程的启停 用法：
hdfs --daemon (start|status|stop) (namenode|secondarynamenode|datanode) 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/f8d2eee46ab419acb39e428b0e8c0c09/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-18T14:12:27+08:00" />
<meta property="article:modified_time" content="2023-04-18T14:12:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">HDFS学习笔记</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_3" rel="nofollow">一、为什么需要分布式存储</a></li><li><a href="#_8" rel="nofollow">二、分布式的基础架构分析</a></li><li><ul><li><a href="#1_10" rel="nofollow">1.分布式系统常见的组织形式？</a></li><li><a href="#2_16" rel="nofollow">2.什么是主从模式？</a></li><li><a href="#3Hadoop_20" rel="nofollow">3.Hadoop是哪种模式？</a></li></ul> 
  </li><li><a href="#HDFS_23" rel="nofollow">三、HDFS的基础架构</a></li><li><a href="#HDFSShell_47" rel="nofollow">四、HDFS的Shell操作</a></li><li><ul><li><a href="#1_49" rel="nofollow">1.进程启停管理</a></li><li><ul><li><a href="#11_51" rel="nofollow">1.1一键启停脚本</a></li><li><a href="#12_75" rel="nofollow">1.2单进程启停</a></li></ul> 
   </li><li><a href="#2_95" rel="nofollow">2.文件系统操作命令</a></li><li><ul><li><a href="#21HDFS_97" rel="nofollow">2.1HDFS文件系统基本信息</a></li><li><a href="#22_129" rel="nofollow">2.2命令体系</a></li><li><ul><li><a href="#221_147" rel="nofollow">2.2.1创建文件夹</a></li><li><a href="#222_165" rel="nofollow">2.2.2查看指定目录下内容</a></li><li><a href="#223HDFS_179" rel="nofollow">2.2.3上传文件到HDFS指定目录下</a></li><li><a href="#224HDFS_201" rel="nofollow">2.2.4查看HDFS文件内容</a></li><li><a href="#225HDFS_224" rel="nofollow">2.2.5下载HDFS文件</a></li><li><a href="#226HDFS_237" rel="nofollow">2.2.6拷贝HDFS文件</a></li><li><a href="#227HDFS_244" rel="nofollow">2.2.7追加数据到HDFS文件中</a></li><li><a href="#228HDFS_255" rel="nofollow">2.2.8HDFS移动、重命名操作</a></li><li><a href="#229HDFS_265" rel="nofollow">2.2.9HDFS数据删除操作</a></li></ul> 
    </li><li><a href="#23HDFS_WEB_293" rel="nofollow">2.3HDFS WEB浏览</a></li></ul> 
   </li><li><a href="#3HDFS_Jetbrains_297" rel="nofollow">3.HDFS客户端 -Jetbrains产品插件</a></li><li><ul><li><a href="#31_Big_Data_Tools_299" rel="nofollow">3.1 Big Data Tools插件</a></li><li><a href="#32_Windows_303" rel="nofollow">3.2 配置Windows</a></li><li><a href="#33Big_Data_Tools_321" rel="nofollow">3.3配置Big Data Tools插件</a></li><li><a href="#34Big_Data_Tools_331" rel="nofollow">3.4使用Big Data Tools插件</a></li></ul> 
  </li></ul> 
  </li><li><a href="#HDFS_334" rel="nofollow">五、HDFS存储原理</a></li><li><ul><li><a href="#1_336" rel="nofollow">1.存储原理</a></li><li><a href="#2fsck_348" rel="nofollow">2.fsck命令</a></li><li><ul><li><a href="#21HDFS_350" rel="nofollow">2.1HDFS副本块数量的配置</a></li><li><a href="#22fsck_383" rel="nofollow">2.2fsck命令检查文件的副本数</a></li><li><a href="#23block_399" rel="nofollow">2.3block配置</a></li></ul> 
   </li><li><a href="#3NameNode_415" rel="nofollow">3.NameNode元数据</a></li><li><ul><li><a href="#31edits_417" rel="nofollow">3.1edits文件</a></li><li><a href="#32fsimage_423" rel="nofollow">3.2.fsimage文件</a></li><li><a href="#33NameNode_427" rel="nofollow">3.3NameNode元数据管理维护</a></li><li><a href="#34_443" rel="nofollow">3.4元数据合并控制参数</a></li><li><a href="#35SecondaryNameNode_457" rel="nofollow">3.5SecondaryNameNode的作用</a></li></ul> 
   </li><li><a href="#4HDFS_463" rel="nofollow">4.HDFS的读写流程</a></li><li><ul><li><a href="#41_465" rel="nofollow">4.1数据写入流程</a></li><li><a href="#42_488" rel="nofollow">4.2数据读取流程</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_3"></a>一、为什么需要分布式存储</h2> 
<p>•数据量太大，单机存储能力有上限，需要靠数量来解决问题</p> 
<p>•数量的提升带来的是网络传输、磁盘读写、CPU、内存等各方面的综合提升。 分布式组合在一起可以达到1+1&gt;2的效果</p> 
<h2><a id="_8"></a>二、分布式的基础架构分析</h2> 
<h3><a id="1_10"></a>1.分布式系统常见的组织形式？</h3> 
<p>•去中心化模式：没有明确中心，大家协调工作（区块链、P2P）</p> 
<p>•中心化模式：有明确的中心，基于中心节点分配工作</p> 
<h3><a id="2_16"></a>2.什么是主从模式？</h3> 
<p>主从模式（Master-Slaves）就是中心化模式，表示有一个主节点来作为管理者，管理协调下属一批从节点工作。也称一主多从模式。</p> 
<h3><a id="3Hadoop_20"></a>3.Hadoop是哪种模式？</h3> 
<p>主从模式（中心化模式）的架构</p> 
<h2><a id="HDFS_23"></a>三、HDFS的基础架构</h2> 
<p><img src="https://images2.imgbox.com/aa/cd/oZoWTC7l_o.png" alt="在这里插入图片描述"></p> 
<p><strong>主角色：NameNode</strong></p> 
<p>•HDFS系统的主角色，是一个独立的进程</p> 
<p>•负责管理HDFS整个文件系统</p> 
<p>•负责管理DataNode</p> 
<p><strong>从角色：DataNode</strong></p> 
<p>•HDFS系统的从角色，是一个独立进程</p> 
<p>•主要负责数据的存储，即存入数据和取出数据</p> 
<p><strong>主角色辅助角色：SecondaryNameNode</strong></p> 
<p>•NameNode的辅助，是一个独立进程</p> 
<p>•主要帮助NameNode完成元数据整理工作（打杂）</p> 
<h2><a id="HDFSShell_47"></a>四、HDFS的Shell操作</h2> 
<h3><a id="1_49"></a>1.进程启停管理</h3> 
<h4><a id="11_51"></a>1.1一键启停脚本</h4> 
<p><strong>Hadoop HDFS组件内置了HDFS集群的一键启停脚本。</strong></p> 
<p>•<code>$HADOOP_HOME/sbin/start-dfs.sh</code>，一键启动HDFS集群</p> 
<p><strong>执行原理：</strong></p> 
<p>•<em>在执行此脚本的机器上，启动SecondaryNameNode</em></p> 
<p><em>•读取core-site.xml内容（fs.defaultFS项），确认NameNode所在机器，启动NameNode</em></p> 
<p><em>•读取workers内容，确认DataNode所在机器，启动全部DataNode</em></p> 
<p><em>•$<code>HADOOP_HOME/sbin/stop-dfs.sh</code>，一键关闭HDFS集群</em></p> 
<p><strong>执行原理：</strong></p> 
<p>•<em>在执行此脚本的机器上，关闭SecondaryNameNode</em></p> 
<p><em>•读取core-site.xml内容（fs.defaultFS项），确认NameNode所在机器，关闭NameNode</em></p> 
<p><em>•读取workers内容，确认DataNode所在机器，关闭全部NameNode</em></p> 
<h4><a id="12_75"></a>1.2单进程启停</h4> 
<p>除了一键启停外，也可以单独控制进程的启停。</p> 
<ol><li><code>$HADOOP_HOME/sbin/hadoop-daemon.sh</code>，此脚本可以单独控制所在机器的进程的启停</li></ol> 
<p>用法：</p> 
<pre><code class="prism language-shell">hadoop-daemon.sh <span class="token punctuation">(</span>start<span class="token operator">|</span>status<span class="token operator">|</span>stop<span class="token punctuation">)</span> <span class="token punctuation">(</span>namenode<span class="token operator">|</span>secondarynamenode<span class="token operator">|</span>datanode<span class="token punctuation">)</span>
</code></pre> 
<ol start="2"><li><code>$HADOOP_HOME/bin/hdfs</code>，此程序也可以用以单独控制所在机器的进程的启停</li></ol> 
<p>用法：</p> 
<pre><code class="prism language-shell">hdfs --daemon <span class="token punctuation">(</span>start<span class="token operator">|</span>status<span class="token operator">|</span>stop<span class="token punctuation">)</span> <span class="token punctuation">(</span>namenode<span class="token operator">|</span>secondarynamenode<span class="token operator">|</span>datanode<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="2_95"></a>2.文件系统操作命令</h3> 
<h4><a id="21HDFS_97"></a>2.1HDFS文件系统基本信息</h4> 
<p>HDFS作为分布式存储的文件系统，有其对数据的路径表达方式。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          • 
         
        
          H 
         
        
          D 
         
        
          F 
         
        
          S 
         
        
          同 
         
        
          L 
         
        
          i 
         
        
          n 
         
        
          u 
         
        
          x 
         
        
          系统一样，均是以 
         
        
          / 
         
        
          作为根目录的组织形式 
         
        
       
      
        \textcolor{red}{•HDFS同Linux系统一样，均是以/作为根目录的组织形式} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord" style="color: red;">•</span><span class="mord mathnormal" style="margin-right: 0.0278em; color: red;">HD</span><span class="mord mathnormal" style="margin-right: 0.0576em; color: red;">FS</span><span class="mord cjk_fallback" style="color: red;">同</span><span class="mord mathnormal" style="color: red;">L</span><span class="mord mathnormal" style="color: red;">in</span><span class="mord mathnormal" style="color: red;">ux</span><span class="mord cjk_fallback" style="color: red;">系统一样，均是以</span><span class="mord" style="color: red;">/</span><span class="mord cjk_fallback" style="color: red;">作为根目录的组织形式</span></span></span></span></span></p> 
<p>•Linux： <code>/usr/local/hello.txt</code></p> 
<p>•HDFS： <code>/usr/local/hello.txt</code><br> <img src="https://images2.imgbox.com/50/d2/YvkW2IpE_o.png" alt="在这里插入图片描述"><br> **如何区分呢？**通过协议头来区分</p> 
<p>•Linux：<code>file:///</code></p> 
<p>•HDFS：<code>hdfs://namenode:port/</code></p> 
<p><strong>如上路径：</strong></p> 
<p>•Linux：<code>file:///usr/local/hello.txt</code></p> 
<p>•HDFS：<code>hdfs://node1:8020/usr/local/hello.txt</code></p> 
<p><strong>协议头file:/// 或 hdfs://node1:8020/<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           可以省略 
          
         
        
       
         \textcolor{red}{可以省略} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord cjk_fallback" style="color: red;">可以省略</span></span></span></span></span></strong></p> 
<p>•需要提供Linux路径的参数，会自动识别为file://</p> 
<p>•需要提供HDFS路径的参数，会自动识别为hdfs://</p> 
<p>除非你明确需要写或不写会有BUG，否则一般不用写协议头</p> 
<h4><a id="22_129"></a>2.2命令体系</h4> 
<p>关于HDFS文件系统的操作命令，Hadoop提供了2套命令体系</p> 
<p>•hadoop命令（老版本用法），用法：</p> 
<pre><code class="prism language-shell">hadoop fs <span class="token punctuation">[</span>generic options<span class="token punctuation">]</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/0e/3e/7huCAOQF_o.png" alt="在这里插入图片描述"><br> •hdfs命令（新版本用法），用法：</p> 
<pre><code class="prism language-shell">hdfs dfs <span class="token punctuation">[</span>generic options<span class="token punctuation">]</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/3b/f5/YdCUTKiv_o.png" alt="在这里插入图片描述"><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          两者在文件系统操作上，用法完全一致，用哪个都可以，某些特殊操作需要选择 
         
        
          h 
         
        
          a 
         
        
          d 
         
        
          o 
         
        
          o 
         
        
          p 
         
        
          命令或 
         
        
          h 
         
        
          d 
         
        
          f 
         
        
          s 
         
        
          命令 
         
        
       
      
        \textcolor{red}{两者在文件系统操作上，用法完全一致，用哪个都可以，某些特殊操作需要选择hadoop命令或hdfs命令} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord cjk_fallback" style="color: red;">两者在文件系统操作上，用法完全一致，用哪个都可以，某些特殊操作需要选择</span><span class="mord mathnormal" style="color: red;">ha</span><span class="mord mathnormal" style="color: red;">d</span><span class="mord mathnormal" style="color: red;">oo</span><span class="mord mathnormal" style="color: red;">p</span><span class="mord cjk_fallback" style="color: red;">命令或</span><span class="mord mathnormal" style="color: red;">h</span><span class="mord mathnormal" style="margin-right: 0.1076em; color: red;">df</span><span class="mord mathnormal" style="color: red;">s</span><span class="mord cjk_fallback" style="color: red;">命令</span></span></span></span></span></p> 
<h5><a id="221_147"></a>2.2.1创建文件夹</h5> 
<pre><code class="prism language-shell">hadoop fs -mkdir <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>path<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.
hdfs dfs -mkdir <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>path<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.
</code></pre> 
<p>path 为待创建的目录</p> 
<p>-p选项的行为与Linux <code>mkdir -p</code>一致，它会沿着路径创建父目录</p> 
<p>示例</p> 
<pre><code class="prism language-shell">hadoop fs -mkdir -p /itcast/bigdata
hdfs fs -mkdir -p /itheima/hadoop
</code></pre> 
<h5><a id="222_165"></a>2.2.2查看指定目录下内容</h5> 
<pre><code class="prism language-shell">hadoop fs -ls <span class="token punctuation">[</span>-h<span class="token punctuation">]</span> <span class="token punctuation">[</span>-R<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">&lt;</span>path<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.<span class="token punctuation">]</span> 
hdfs dfs -ls <span class="token punctuation">[</span>-h<span class="token punctuation">]</span> <span class="token punctuation">[</span>-R<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">&lt;</span>path<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.<span class="token punctuation">]</span>
</code></pre> 
<p>path 指定目录路径</p> 
<p>-h 人性化显示文件size</p> 
<p>-R 递归查看指定目录及其子目录<br> <strong>示例查看根目录：</strong><br> <img src="https://images2.imgbox.com/8a/5b/wytIjkyL_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="223HDFS_179"></a>2.2.3上传文件到HDFS指定目录下</h5> 
<pre><code class="prism language-shell">hadoop fs -put <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>localsrc<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>
hdfs dfs -put <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>localsrc<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>
</code></pre> 
<p>-f 覆盖目标文件（同名文件已存在的情况下）</p> 
<p>-p 保留访问和修改时间，所有权和权限。</p> 
<p>localsrc 本地文件系统（客户端所在机器）</p> 
<p>dst 目标文件系统（HDFS）</p> 
<p><strong>示例：</strong></p> 
<pre><code class="prism language-shell">hadoop fs -put words.txt /itcast
hdfs dfs -put file:///etc/profile hdfs://node1:8020/itcast<span class="token comment">#前面是本地，后面的是hdfs文件系统</span>
</code></pre> 
<h5><a id="224HDFS_201"></a>2.2.4查看HDFS文件内容</h5> 
<pre><code class="prism language-shell">hadoop fs -cat <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. 
hdfs dfs -cat <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.
</code></pre> 
<p>读取指定文件全部内容，显示在标准输出控制台</p> 
<p><strong>示例：</strong></p> 
<pre><code class="prism language-shell">hadoop fs -cat /itcast/words.txt
hdfs dfs -cat /itcast/profile
</code></pre> 
<p>读取大文件可以使用管道符配合more，这样在读取大文件时可以通过按空格翻页不至于干崩终端</p> 
<pre><code class="prism language-shell">hadoop fs -cat <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token operator">|</span> <span class="token function">more</span>
hdfs dfs -cat <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token operator">|</span> <span class="token function">more</span>
</code></pre> 
<h5><a id="225HDFS_224"></a>2.2.5下载HDFS文件</h5> 
<pre><code class="prism language-shell">hadoop fs -get <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>localdst<span class="token operator">&gt;</span>
hdfs dfs -get <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>localdst<span class="token operator">&gt;</span><span class="token comment">#前面写HDFS路径，后面写Linux路径</span>
</code></pre> 
<p>下载文件到本地文件系统指定目录，localdst必须是目录</p> 
<p>​ -f 覆盖目标文件（同名文件已存在的情况下）</p> 
<p>​ -p 保留访问和修改时间，所有权和权限。</p> 
<h5><a id="226HDFS_237"></a>2.2.6拷贝HDFS文件</h5> 
<pre><code class="prism language-shell">hadoop fs -cp <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span> 
hdfs dfs -cp <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span><span class="token comment">#从HDFS复制到HDFS</span>
</code></pre> 
<h5><a id="227HDFS_244"></a>2.2.7追加数据到HDFS文件中</h5> 
<pre><code class="prism language-shell">hadoop fs -appendToFile <span class="token operator">&lt;</span>localsrc<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>
hdfs dfs -appendToFile <span class="token operator">&lt;</span>localsrc<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span><span class="token comment">#前面是本地的文件，后面是hdfs文件</span>
</code></pre> 
<p>将所有给定本地文件的内容追加到给定dst文件。<br> dst如果文件不存在，将创建该文件。<br> 如果为-，则输入为从标准输入中读取。</p> 
<h5><a id="228HDFS_255"></a>2.2.8HDFS移动、重命名操作</h5> 
<pre><code class="prism language-shell">hadoop fs -mv <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>
hdfs dfs -mv <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>
</code></pre> 
<p>移动文件到指定文件夹下<br> 可以使用该命令移动数据，重命名文件的名称</p> 
<h5><a id="229HDFS_265"></a>2.2.9HDFS数据删除操作</h5> 
<pre><code class="prism language-shell">hadoop fs -rm -r <span class="token punctuation">[</span>-skipTrash<span class="token punctuation">]</span> URI <span class="token punctuation">[</span>URI <span class="token punctuation">..</span>.<span class="token punctuation">]</span>
hdfs dfs -rm -r <span class="token punctuation">[</span>-skipTrash<span class="token punctuation">]</span> URI <span class="token punctuation">[</span>URI <span class="token punctuation">..</span>.<span class="token punctuation">]</span>
</code></pre> 
<p>删除指定路径的文件或文件夹</p> 
<p>​ -r 删除文件夹</p> 
<p>​ -skipTrash 跳过回收站，直接删除</p> 
<pre><code class="prism language-html"><span class="token comment">&lt;!--回收站功能默认关闭，如果要开启需要在core-site.xml内配置：--&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>fs.trash.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>1440<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span><span class="token comment">&lt;!--回收站的保留文件时间区间，1440代表一个星期的时间--&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
 
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>fs.trash.checkpoint.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>120<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span><span class="token comment">&lt;!--回收站检查的间隔，120代表每隔两小时检查一次，如果超过期限的就清理掉--&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token comment">&lt;!--无需重启集群，在哪个机器配置的，在哪个机器执行命令就生效。
回收站默认位置在：/user/用户名(hadoop)/.Trash--&gt;</span>
</code></pre> 
<h4><a id="23HDFS_WEB_293"></a>2.3HDFS WEB浏览</h4> 
<p>除了使用命令操作HDFS文件系统外，在HDFS的WEB UI上也可以查看HDFS文件系统的内容<br> <img src="https://images2.imgbox.com/88/cd/Z3271NjJ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3HDFS_Jetbrains_297"></a>3.HDFS客户端 -Jetbrains产品插件</h3> 
<h4><a id="31_Big_Data_Tools_299"></a>3.1 Big Data Tools插件</h4> 
<p>如图，在设置-&gt;Plugins（插件）-&gt; Marketplace（市场），搜索Big Data Tools，点击Install安装即可<br> <img src="https://images2.imgbox.com/85/c2/KuZFaFab_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="32_Windows_303"></a>3.2 配置Windows</h4> 
<p>需要对Windows系统做一些基础设置，配合插件使用</p> 
<p>•解压Hadoop安装包到Windows系统，如解压到：E:\hadoop-3.3.4</p> 
<p>•设置$HADOOP_HOME环境变量指向：E:\hadoop-3.3.4</p> 
<p>•下载</p> 
<p>•hadoop.dll<a href="https://github.com/steveloughran/winutils/blob/master/hadoop-3.0.0/bin/hadoop.dll">下载链接</a></p> 
<p>•winutils.exe<a href="https://github.com/steveloughran/winutils/blob/master/hadoop-3.0.0/bin/winutils.exe">下载链接</a></p> 
<p>•可以自行下载，或从课程资料中获取</p> 
<p>•将hadoop.dll和winutils.exe放入$HADOOP_HOME/bin中</p> 
<h4><a id="33Big_Data_Tools_321"></a>3.3配置Big Data Tools插件</h4> 
<p>打开插件<br> <img src="https://images2.imgbox.com/45/7f/ZvRtTh7F_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6a/79/zVkVoq0r_o.png" alt="在这里插入图片描述">或者如图指定Windows上解压的Hadoop安装文件夹的etc/hadoop目录也可以</p> 
<p>会自动读取配置文件连接上HDFS</p> 
<p>因为选取configuration files directory方式连接的话，ide会读取本机Hadoop配置文件core-site.xml,但我们在Windows上解压的Hadoop文件并没有修改过配置文件，所以如果想用这种方式进行连接，需要先在Linux上将etc\hadoop文件夹打包下载替换掉Windows的文件夹。<br> <img src="https://images2.imgbox.com/ca/6a/baVmwNj3_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="34Big_Data_Tools_331"></a>3.4使用Big Data Tools插件</h4> 
<p><img src="https://images2.imgbox.com/0c/f3/D6YVrvYI_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/9a/b8/CLnIRvBD_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="HDFS_334"></a>五、HDFS存储原理</h2> 
<h3><a id="1_336"></a>1.存储原理</h3> 
<p>问题：文件大小不一，不利于统一管理</p> 
<p>解决：设定统一的管理单位，block块</p> 
<p><em>Block块：HDFS的最小存储单位，每个block块256MB（可以修改）</em></p> 
<p>问题：如果丢失或损坏了某个block块，导致文件不完整了怎么办（block块越多，损坏的几率越大）</p> 
<p>解决：通过多个副本（备份）解决，每个block块都有2个（可修改）备份，每个副本都复制到其他服务器一份</p> 
<h3><a id="2fsck_348"></a>2.fsck命令</h3> 
<h4><a id="21HDFS_350"></a>2.1HDFS副本块数量的配置</h4> 
<p>设置默认文件上传到HDFS中拥有的副本数量，可以在<code>hdfs-site.xml</code>中配置如下属性：</p> 
<pre><code class="prism language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<p>这个属性默认是3，一般情况下，我们无需主动配置（除非需要设置非3的数值）</p> 
<p>如果需要自定义这个属性，请修改每一台服务器的<code>hdfs-site.xml</code>文件，并设置此属性。</p> 
<p>•除了配置文件外，我们还可以在上传文件的时候，临时决定被上传文件以多少个副本存储。</p> 
<pre><code class="prism language-shell">hadoop fs -D dfs.replication<span class="token operator">=</span><span class="token number">2</span> -put test.txt /tmp/
</code></pre> 
<p>如上命令，就可以在上传test.txt的时候，临时设置其副本数为2</p> 
<p>•对于已经存在HDFS的文件，修改dfs.replication属性不会生效，如果要修改已存在文件可以通过命令</p> 
<pre><code class="prism language-shell">hadoop fs -setrep <span class="token punctuation">[</span>-R<span class="token punctuation">]</span> <span class="token number">2</span> path
</code></pre> 
<p>如上命令，指定path的内容将会被修改为2个副本存储。</p> 
<p>-R选项可选，使用-R表示对子目录也生效。</p> 
<h4><a id="22fsck_383"></a>2.2fsck命令检查文件的副本数</h4> 
<p>我们可以使用hdfs提供的fsck命令来检查文件的副本数</p> 
<pre><code class="prism language-shell">hdfs <span class="token function">fsck</span> path <span class="token punctuation">[</span>-files <span class="token punctuation">[</span>-blocks <span class="token punctuation">[</span>-locations<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> 
<p>fsck可以检查指定路径是否正常</p> 
<p>•-files可以列出路径内的文件状态</p> 
<p>•-files -blocks 输出文件块报告（有几个块，多少副本）</p> 
<p>•-files -blocks -locations 输出每一个block的详情<br> <img src="https://images2.imgbox.com/2e/d5/ipvNyMr5_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="23block_399"></a>2.3block配置</h4> 
<p>对于块（block），hdfs默认设置为256MB一个，也就是1GB文件会被划分为4个block存储。</p> 
<p>块大小可以通过参数：</p> 
<pre><code class="prism language-html"> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.blocksize<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>268435456<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>设置HDFS块大小，单位是b<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
 <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<p>如上，设置为256MB</p> 
<h3><a id="3NameNode_415"></a>3.NameNode元数据</h3> 
<p><img src="https://images2.imgbox.com/cc/18/tGc0VVuq_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="31edits_417"></a>3.1edits文件</h4> 
<p>edits文件，是一个流水账文件，记录了hdfs中的每一次操作，以及本次操作影响的文件其对应的block<br> <img src="https://images2.imgbox.com/40/0c/Th5wOJvy_o.png" alt="在这里插入图片描述"><br> edits记录每一次HDFS的操作逐渐变得越来越大，所以，会存在多个edits文件确保不会有超大edits的存在，保证检索性能</p> 
<h4><a id="32fsimage_423"></a>3.2.fsimage文件</h4> 
<p>当用户想要查看某文件内容时，需要在全部的edits中搜索（还需要按顺序从头到尾，避免后期改名或删除文件），效率非常低，于是需要将全部的edits文件合并为最终结果，即可得到一个fsimage文件。<br> <img src="https://images2.imgbox.com/13/df/8FvXA8XH_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="33NameNode_427"></a>3.3NameNode元数据管理维护</h4> 
<p>NameNode基于edits和FSImage的配合，完成整个文件系统文件的管理。</p> 
<p>① 每次对HDFS的操作，均被edits文件记录</p> 
<p>②edits达到大小上线后，开启新的edits记录</p> 
<p>③ 定期进行edits的合并操作</p> 
<p><em>•如当前没有fsimage文件， 将全部edits合并为第一个fsimage</em></p> 
<p><em>•如当前已存在fsimage文件，将全部edits和已存在的fsimage进行合并，形成新的fsimage</em></p> 
<p>④重复123流程</p> 
<h4><a id="34_443"></a>3.4元数据合并控制参数</h4> 
<p>对于元数据的合并，是一个定时过程，基于：</p> 
<p>•<code>dfs.namenode.checkpoint.period</code>，默认3600（秒）即1小时</p> 
<p>•<code>dfs.namenode.checkpoint.txns</code>，默认1000000，即100W次事务</p> 
<p>只要有一个达到条件就执行。</p> 
<p>检查是否达到条件，默认60秒检查一次，基于：</p> 
<p>•<code>dfs.namenode.checkpoint.check.period</code>，默认60（秒），来决定</p> 
<h4><a id="35SecondaryNameNode_457"></a>3.5SecondaryNameNode的作用</h4> 
<p>SecondaryNameNode是HDFS中的一个辅助角色</p> 
<p>SecondaryNameNode会通过http从NameNode拉取数据（edits和fsimage）然后合并完成后提供给NameNode使用。<br> <img src="https://images2.imgbox.com/52/37/1U5Laojw_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4HDFS_463"></a>4.HDFS的读写流程</h3> 
<h4><a id="41_465"></a>4.1数据写入流程</h4> 
<p>①客户端向NameNode发起请求</p> 
<p>②NameNode审核权限、剩余空间后，满足条件允许写入，并告知客户端写入的DataNode地址</p> 
<p>③客户端向指定的DataNode发送数据包</p> 
<p>④ 被写入数据的DataNode同时完成数据副本的复制工作，将其接收的数据分发给其它DataNode</p> 
<p>⑤如上图，DataNode1复制给DataNode2，然后基于DataNode2复制给Datanode3和DataNode4</p> 
<p>⑥写入完成客户端通知NameNode，NameNode做元数据记录工作</p> 
<p><img src="https://images2.imgbox.com/8f/3b/Wef8yhh9_o.png" alt="在这里插入图片描述"><br> 关键信息点：</p> 
<p>•<font color="FF0000">NameNode不负责数据写入</font>，只负责元数据记录和权限审批</p> 
<p>•客户端直接<font color="FF0000">向1台DataNode</font>写数据，这个DataNode一般是<font color="FF0000">离客户端最近（网络距离）</font>的那一个</p> 
<p>•数据块副本的复制工作，<font color="FF0000">由DataNode之间自行完成</font>（构建一个PipLine，按顺序复制分发，如图1给2, 2给3和4）</p> 
<h4><a id="42_488"></a>4.2数据读取流程</h4> 
<p>①客户端向NameNode申请读取某文件</p> 
<p>② NameNode判断客户端权限等细节后，允许读取，并返回此文件的block列表</p> 
<p>③客户端拿到block列表后自行寻找DataNode读取即可<br> <img src="https://images2.imgbox.com/24/a0/d3ZcT9rn_o.png" alt="在这里插入图片描述"><br> 关键点：</p> 
<p>①数据同样不通过NameNode提供</p> 
<p>②NameNode提供的block列表，会基于网络距离计算尽量提供离客户端最近的</p> 
<p>这是因为1个block有3份，会尽量找离客户端最近的那一份让其读取</p> 
<p><strong>总结：</strong></p> 
<p><strong>1、对于客户端读取HDFS数据的流程中，一定要知道</strong></p> 
<p>不论读、还是写，NameNode都不经手数据，均是客户端和DataNode直接通讯</p> 
<p>不然对NameNode压力太大</p> 
<p><strong>2、写入和读取的流程，简单来说就是：</strong></p> 
<p>•NameNode做授权判断（是否能写、是否能读）</p> 
<p>•客户端直连DataNode进行写入（由DataNode自己完成副本复制）、客户端直连DataNode进行block读取</p> 
<p>•写入，客户端会被分配找离自己最近的DataNode写数据</p> 
<p>•读取，客户端拿到的block列表，会是网络距离最近的一份<br> <strong>3、网络距离</strong></p> 
<p>•最近的距离就是在同一台机器</p> 
<p>•其次就是同一个局域网（交换机）</p> 
<p>•再其次就是跨越交换机</p> 
<p>•再其次就是跨越数据中心</p> 
<p>HDFS内置网络距离计算算法，可以通过IP地址、路由表来推断网络距离</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/03a8767e05d4400e4fbe708c3a0c1ec6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python批量重命名文件吗，00001，不是补零,在前面插入0</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a1cc606b3218d3fceae1ecc285ad7f0c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">CRFill：Generative Image Inpainting with Auxiliary Contextual Reconstruction论文阅读笔记</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>