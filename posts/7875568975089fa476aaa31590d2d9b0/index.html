<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【pytorch】实现简单的CNN卷积神经网络 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【pytorch】实现简单的CNN卷积神经网络" />
<meta property="og:description" content="目录
一. 卷积层 nn.Conv2d()
参数设置
代码示例
二. 池化层 nn.MaxPool1d() 和 nn.MaxPool2d()
参数设置
代码示例
三. 激活函数层 nn.ReLU()
参数设置
四. CNN的简单实现
一. 卷积层 nn.Conv2d() 参数设置 在Pytorch的nn模块中，封装了nn.Conv2d()类作为二维卷积的实现。参数如下图所示
in_channels：输入张量的channels数out_channels：输出张量的channels数kernel_size：卷积核的大小。一般我们会使用3x3这种两个数相同的卷积核，这种情况只需要写kernel_size = 3就行了。如果左右两个数不同，比如3x5，那么写作kernel_size = (3, 5)stride = 1：步长padding：填充图像的上下左右，后面的常数代表填充的多少（行数、列数），默认为0。padding = 1时，若原始图像大小为32x32，则padding后的图像大小为34x34dilation = 1：是否采用空洞卷积，默认为1（即不采用）groups = 1：决定了是否采用分组卷积，默认为1可以参考一下：groups参数的理解bias = True：是否要添加偏置参数作为可学习参数的一个，默认为Truepadding_mode = ‘zeros’：padding的模式，默认采用零填充 前三个参数需要手动提供，后面的都有默认值
代码示例 class Net(nn.Module): def __init__(self): nn.Module.__init__(self) self.conv2d = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=4,stride=2,padding=1) def forward(self, x): print(x.requires_grad) x = self.conv2d(x) return x # 查看卷积层的权重和偏置 print(net.conv2d.weight) print(net.conv2d.bias) 二. 池化层 nn.MaxPool1d() 和 nn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/7875568975089fa476aaa31590d2d9b0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-18T11:41:55+08:00" />
<meta property="article:modified_time" content="2023-04-18T11:41:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【pytorch】实现简单的CNN卷积神经网络</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80.%20%E5%8D%B7%E7%A7%AF%E5%B1%82%20nn.Conv2d()-toc" style="margin-left:0px;"><a href="#%E4%B8%80.%20%E5%8D%B7%E7%A7%AF%E5%B1%82%20nn.Conv2d%28%29" rel="nofollow">一. 卷积层 nn.Conv2d()</a></p> 
<p id="%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE-toc" style="margin-left:40px;"><a href="#%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE" rel="nofollow">参数设置</a></p> 
<p id="%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-toc" style="margin-left:40px;"><a href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B" rel="nofollow">代码示例</a></p> 
<p id="%E4%BA%8C.%20%E6%B1%A0%E5%8C%96%E5%B1%82nn.MaxPool1d%20%E5%92%8C%20nn.MaxPool2d-toc" style="margin-left:0px;"><a href="#%E4%BA%8C.%20%E6%B1%A0%E5%8C%96%E5%B1%82nn.MaxPool1d%20%E5%92%8C%20nn.MaxPool2d" rel="nofollow">二. 池化层 nn.MaxPool1d() 和 nn.MaxPool2d()</a></p> 
<p id="%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE-toc" style="margin-left:40px;"><a href="#%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE" rel="nofollow">参数设置</a></p> 
<p id="%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-toc" style="margin-left:40px;"><a href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B" rel="nofollow">代码示例</a></p> 
<p id="%E4%B8%89.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%B1%82nn.ReLU()-toc" style="margin-left:0px;"><a href="#%E4%B8%89.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%B1%82nn.ReLU%28%29" rel="nofollow">三. 激活函数层 nn.ReLU()</a></p> 
<p id="%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE-toc" style="margin-left:40px;"><a href="#%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE" rel="nofollow">参数设置</a></p> 
<p id="%E5%9B%9B.%20CNN%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#%E5%9B%9B.%20CNN%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0" rel="nofollow">四. CNN的简单实现</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80.%20%E5%8D%B7%E7%A7%AF%E5%B1%82%20nn.Conv2d()">一. 卷积层 nn.Conv2d()</h2> 
<h3 id="%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE">参数设置</h3> 
<p>在Pytorch的nn模块中，封装了nn.Conv2d()类作为二维卷积的实现。参数如下图所示</p> 
<p><img alt="卷积形参" src="https://images2.imgbox.com/ea/4f/ALGLdqzv_o.png"></p> 
<ul><li><span style="background-color:#fef2f0;">in_channels</span>：输入张量的channels数</li><li><span style="background-color:#fef2f0;">out_channels</span>：输出张量的channels数</li><li><span style="background-color:#fef2f0;">kernel_size</span>：卷积核的大小。一般我们会使用3x3这种两个数相同的卷积核，这种情况只需要写<code>kernel_size = 3</code>就行了。如果左右两个数不同，比如3x5，那么写作<code>kernel_size = (3, 5)</code></li><li><span style="background-color:#fef2f0;">stride = 1</span>：步长</li><li><span style="background-color:#fef2f0;">padding</span>：填充图像的上下左右，后面的常数代表填充的多少（行数、列数），默认为0。padding = 1时，若原始图像大小为<code>32x32</code>，则padding后的图像大小为<code>34x34</code></li><li><span style="background-color:#fef2f0;">dilation = 1</span>：是否采用空洞卷积，默认为1（即不采用）</li><li><span style="background-color:#fef2f0;">groups = 1</span>：决定了是否采用分组卷积，默认为1<a href="https://blog.csdn.net/monsterhoho/article/details/80173400" title="可以参考一下：groups参数的理解">可以参考一下：groups参数的理解</a></li><li><span style="background-color:#fef2f0;">bias = True</span>：是否要添加偏置参数作为可学习参数的一个，默认为True</li><li><span style="background-color:#fef2f0;">padding_mode = ‘zeros’</span>：<code>padding</code>的模式，默认采用零填充</li></ul> 
<p>前三个参数需要手动提供，后面的都有默认值</p> 
<h3 id="%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B">代码示例</h3> 
<pre><code class="language-python">class Net(nn.Module):
    def __init__(self):
        nn.Module.__init__(self)
        self.conv2d = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=4,stride=2,padding=1)

    def forward(self, x):
        print(x.requires_grad)
        x = self.conv2d(x)
        return x
    
# 查看卷积层的权重和偏置
print(net.conv2d.weight)
print(net.conv2d.bias)
</code></pre> 
<h2 id="%E4%BA%8C.%20%E6%B1%A0%E5%8C%96%E5%B1%82nn.MaxPool1d%20%E5%92%8C%20nn.MaxPool2d">二. 池化层 nn.MaxPool1d() 和 nn.MaxPool2d()</h2> 
<h3>参数设置</h3> 
<pre><code class="language-python">class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code></pre> 
<pre><code class="language-python">class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code></pre> 
<ul><li><span style="background-color:#fff5e6;">kernel_size</span>：(int or tuple)max pooling的窗口大小</li><li><span style="background-color:#fff5e6;">stride</span>：(int or tuple, optional)max pooling的窗口移动的步长。默认值是kernel_size</li><li><span style="background-color:#fff5e6;">padding</span>：(int or tuple, optional)输入的每一条边补充0的层数</li><li><span style="background-color:#fff5e6;">dilation</span>：(int or tuple, optional)控制窗口中元素步幅的参数</li><li><span style="background-color:#fff5e6;">return_indices</span>：如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助</li><li><span style="background-color:#fff5e6;">ceil_mode</span>：如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li></ul> 
<h3>代码示例</h3> 
<pre><code class="language-python">import torch
import torch.nn as nn
from torch.autograd import Variable

input = Variable(torch.randn(2, 5, 5))
m1 = nn.MaxPool1d(3, stride=2)
m2 = nn.MaxPool2d(3, stride=2)
output1 = m1(input)
output2 = m2(input)

print(input)
print(output1)
print(output2)
</code></pre> 
<pre><code class="language-python"># 输出结果，可对比二者区别
tensor([[[ 2.4444e-01,  1.0226e+00,  2.4089e-01,  4.3374e-01,  8.6254e-01],
         [-1.1597e-01, -5.1438e-01,  4.9354e-01,  1.3846e+00, -1.4846e+00],
         [-5.2985e-01, -9.7652e-01, -1.1763e+00, -1.0564e+00,  1.8538e+00],
         [-1.5157e+00,  2.4466e-03, -1.3180e+00, -6.4395e-01,  1.6216e-01],
         [ 5.0826e-01, -4.2336e-01, -1.1817e+00, -3.9826e-01,  1.1857e-01]],

        [[-7.9605e-01,  2.2759e-01,  2.1400e+00, -2.2706e-01,  9.8575e-01],
         [-3.0485e+00, -6.6409e-01,  2.9864e-01,  1.3190e+00, -1.5249e+00],
         [ 3.1127e-01,  4.2901e-01,  1.0026e+00,  6.4803e-01,  9.4203e-01],
         [-5.6758e-01,  3.2101e-01, -4.5395e-01,  1.8376e+00, -8.6135e-01],
         [ 7.8916e-01, -1.3624e+00, -1.3352e+00, -2.5927e+00, -3.1461e-01]]])
tensor([[[ 1.0226,  0.8625],
         [ 0.4935,  1.3846],
         [-0.5298,  1.8538],
         [ 0.0024,  0.1622],
         [ 0.5083,  0.1186]],

        [[ 2.1400,  2.1400],
         [ 0.2986,  1.3190],
         [ 1.0026,  1.0026],
         [ 0.3210,  1.8376],
         [ 0.7892, -0.3146]]])

tensor([[[1.0226, 1.8538],
         [0.5083, 1.8538]],

        [[2.1400, 2.1400],
         [1.0026, 1.8376]]])

Process finished with exit code 0
</code></pre> 
<h2 id="%E4%B8%89.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%B1%82nn.ReLU()">三. 激活函数层 nn.ReLU()</h2> 
<h3>参数设置</h3> 
<pre><code class="language-python">nn.ReLU(inplace=True)</code></pre> 
<p>nn.ReLU()用来实现Relu函数，实现非线性。ReLU函数有个<span style="background-color:#fefcd8;">inplace参数</span>，如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存。之所以可以覆盖是因为在计算ReLU的反向传播时，只需根据输出就能够推算出反向传播的梯度。一般不使用inplace操作。</p> 
<h2 id="%E5%9B%9B.%20CNN%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0">四. CNN的简单实现</h2> 
<pre><code class="language-python">import os
import numpy as np
import torch
import torch.nn as nn
import torch.utils.data as Data
import torchvision
import matplotlib.pyplot as plt

# 循环次数
EPOCH = 2
BATCH_SIZE = 50

# 下载mnist数据集
train_data = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=torchvision.transforms.ToTensor(),
                                        download=True, )
# (60000, 28, 28)
print(train_data.data.size())
# (60000)
print(train_data.targets.size())

train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)

# 测试集
test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)

# (2000, 1, 28, 28)
# 标准化
test_x = torch.unsqueeze(test_data.data, dim=1).type(torch.FloatTensor)[:2000] / 255.
test_y = test_data. targets[:2000]


# 建立pytorch神经网络
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        #   第一部分卷积
        self.conv1 = nn.Sequential(
            nn.Conv2d(
                in_channels=1,
                out_channels=32,
                kernel_size=5,
                stride=1,
                padding=2,
                dilation=1
            ),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
        )
        #   第二部分卷积
        self.conv2 = nn.Sequential(
            nn.Conv2d(
                in_channels=32,
                out_channels=64,
                kernel_size=3,
                stride=1,
                padding=1,
                dilation=1
            ),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
        )
        #   全连接+池化+全连接
        self.ful1 = nn.Linear(64 * 7 * 7, 512)
        self.drop = nn.Dropout(0.5)
        self.ful2 = nn.Sequential(nn.Linear(512, 10), nn.Softmax(dim=1))

    #   前向传播
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)
        x = self.ful1(x)
        x = self.drop(x)
        output = self.ful2(x)

        return output


cnn = CNN()
# 指定优化器
optimizer = torch.optim.Adam(cnn.parameters(), lr=1e-3)
# 指定loss函数
loss_func = nn.CrossEntropyLoss()

for epoch in range(EPOCH):
    for step, (b_x, b_y) in enumerate(train_loader):
        #   计算loss并修正权值
        output = cnn(b_x)
        loss = loss_func(output, b_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        #   打印
        if step % 50 == 0:
            test_output = cnn(test_x)

            pred_y = torch.max(test_output, 1)[1].data.numpy()
            accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))
            print('Epoch: %2d' % epoch, ', loss: %.4f' % loss.data.numpy(), ', accuracy: %.4f' % accuracy)

</code></pre> 
<pre><code class="language-python"># 输出结果
torch.Size([60000, 28, 28])
torch.Size([60000])
Epoch:  0 , loss: 2.3028 , accuracy: 0.0885
Epoch:  0 , loss: 1.7379 , accuracy: 0.7115
Epoch:  0 , loss: 1.6186 , accuracy: 0.8775
Epoch:  0 , loss: 1.5239 , accuracy: 0.9120
Epoch:  0 , loss: 1.6127 , accuracy: 0.9170
Epoch:  0 , loss: 1.5039 , accuracy: 0.9250
Epoch:  0 , loss: 1.4878 , accuracy: 0.9415
Epoch:  0 , loss: 1.5210 , accuracy: 0.9430
Epoch:  0 , loss: 1.4822 , accuracy: 0.9425
Epoch:  0 , loss: 1.5443 , accuracy: 0.9505
Epoch:  0 , loss: 1.4634 , accuracy: 0.9510
Epoch:  0 , loss: 1.5371 , accuracy: 0.9310
Epoch:  0 , loss: 1.4888 , accuracy: 0.9585
Epoch:  0 , loss: 1.4767 , accuracy: 0.9575
Epoch:  0 , loss: 1.5294 , accuracy: 0.9610
Epoch:  0 , loss: 1.4813 , accuracy: 0.9650
Epoch:  0 , loss: 1.4972 , accuracy: 0.9635
Epoch:  0 , loss: 1.5218 , accuracy: 0.9585
Epoch:  0 , loss: 1.4837 , accuracy: 0.9605
Epoch:  0 , loss: 1.4762 , accuracy: 0.9595
Epoch:  0 , loss: 1.5419 , accuracy: 0.9565
Epoch:  0 , loss: 1.4810 , accuracy: 0.9590
Epoch:  0 , loss: 1.4621 , accuracy: 0.9575
Epoch:  0 , loss: 1.5410 , accuracy: 0.9595
Epoch:  1 , loss: 1.4650 , accuracy: 0.9670
Epoch:  1 , loss: 1.4890 , accuracy: 0.9610
Epoch:  1 , loss: 1.4875 , accuracy: 0.9630
Epoch:  1 , loss: 1.4800 , accuracy: 0.9680
Epoch:  1 , loss: 1.5326 , accuracy: 0.9655
Epoch:  1 , loss: 1.4763 , accuracy: 0.9670
Epoch:  1 , loss: 1.5177 , accuracy: 0.9685
Epoch:  1 , loss: 1.4612 , accuracy: 0.9520
Epoch:  1 , loss: 1.4632 , accuracy: 0.9605
Epoch:  1 , loss: 1.5207 , accuracy: 0.9615
Epoch:  1 , loss: 1.5021 , accuracy: 0.9645
Epoch:  1 , loss: 1.5303 , accuracy: 0.9645
Epoch:  1 , loss: 1.4821 , accuracy: 0.9565
Epoch:  1 , loss: 1.4812 , accuracy: 0.9660
Epoch:  1 , loss: 1.4762 , accuracy: 0.9685
Epoch:  1 , loss: 1.4812 , accuracy: 0.9690
Epoch:  1 , loss: 1.4614 , accuracy: 0.9490
Epoch:  1 , loss: 1.4740 , accuracy: 0.9580
Epoch:  1 , loss: 1.4625 , accuracy: 0.9695
Epoch:  1 , loss: 1.5190 , accuracy: 0.9685
Epoch:  1 , loss: 1.5242 , accuracy: 0.9645
Epoch:  1 , loss: 1.4612 , accuracy: 0.9755
Epoch:  1 , loss: 1.4812 , accuracy: 0.9580
Epoch:  1 , loss: 1.4812 , accuracy: 0.9625

Process finished with exit code 0</code></pre> 
<hr> 
<p>最后这个完整代码是刚学的时候网上找的，不太记得出处了</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0d1a3fd7ecaf7e92822c2ebf81f1dca7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Vue服务端渲染</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/03a8767e05d4400e4fbe708c3a0c1ec6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python批量重命名文件吗，00001，不是补零,在前面插入0</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>