<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>训练神经网络(上)激活函数 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="训练神经网络(上)激活函数" />
<meta property="og:description" content="本文介绍几种激活函数,只作为个人笔记.观看视频为cs231n
文章目录 前言
一、Sigmoid函数
二、tanh函数
三、ReLU函数
四、Leaky ReLU函数
五、ELU函数
六.在实际应用中寻找激活函数的做法
总结
前言 激活函数是用来加入非线性因素的，提高神经网络对模型的表达能力，解决线性模型所不能解决的问题。
一、Sigmoid函数 这个函数大家应该熟悉在逻辑回归中曾用到这个sigmoid函数
这个函数可以将负无穷和正无穷映射到(0,1)上即
如果你有一个非常大的输入值那么就会接近于1
如果你有一个非常小的输入值那么就会接近于0
但这个函数会出现几个问题.
1.饱和神经元使得梯度消失
梯度下降法（以及相关的L-BFGS算法等）在使用随机初始化权重的深度网络上效果不好的技术原因是：梯度会变得非常小。具体而言，当使用反向传播方法计算导数的时候，随着网络的深度的增加，反向传播的梯度（从输出层到网络的最初几层）的幅度值会急剧地减小。结果就造成了整体的损失函数相对于最初几层的权重的导数非常小。这样，当使用梯度下降法的时候，最初几层的权重变化非常缓慢，以至于它们不能够从样本中进行有效的学习。这种问题通常被称为“梯度的弥散”.
从图中我们可以看到在图的两端的梯度越来越接近为0,经过链式法则后会让梯度流消失,这样在使用梯度下降法时参数会更新的非常缓慢,也就是上面所说的梯度弥散问题.
2.sigmoid函数是一个非零中心的函数
神经网络：激活函数非0中心导致的问题 - 知乎 (zhihu.com)
这里有篇知乎文章可以帮助大家更好理解,其实大家把公式推一下就可以理解了,最后导致所有的w参数只能每次沿正或者负方向变换就导致,以z字形逼近最优参数,导致梯度下降的收敛较慢.
3.对e指数的计算量有一点大
二、tanh函数 双曲正切函数是双曲函数的一种。双曲正切函数在数学语言上一般写作 tanh。它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在.
三、ReLU函数 ReLU，全称为：Rectified Linear Unit，是一种人工神经网络中常用的激活函数，通常意义下，其指代数学中的斜坡函数，即
ReLU函数的优点
SGD算法的收敛速度比sigmoid和 tanh 快;(梯度不会饱和，解决了梯度消失问题)计算复杂度低，不需要进行指数运算;适合用于后向传播。 ReLU函数的缺点
ReLU的输出不是zero-centered;ReLU在训练的时候很&#34;脆弱”，一不小心有可能导致神经元&#34;坏死”。举个例子:由于RelU在x&lt;0时梯度为0，这样就导致负的梯度在这个ReLU被置零，而且这个神经元有可能再也不会被任何数据激活。如果这个情况发生了，那么这个神经元之后的梯度就永远是0了，也就是ReLU神经元坏死了，不再对任何数据有所响应。实际操作中，如果你的learning rate很大，那么很有可能你网络中的40%的神经元都坏死了。当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。，DeadReLU Problem （神经元坏死现象)︰某些神经元可能永远不会被激活，导致相应参数永远不会被更新(在负数部分，梯度为0)。产生这种现象的两个原因:参数初始化问题; learning rate太高导致在训练过程中参数更新太大。解决方法:采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。ReLU不会对数据做幅度压缩，所以数据的幅度会随着模型层数的增加不断扩张。
四、Leaky ReLU函数 Leaky ReLU函数通过把x xx的非常小的线性分量给予负输入0.01 x 来调整负值的零梯度问题。
Leaky有助于扩大ReLU函数的范围，通常α 的值为0.01左右。
Leaky ReLU的函数范围是负无穷到正无穷。
五、ELU函数 没有Dead ReLU问题，输出的平均值接近0，以0为中心。ELU 通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习。ELU函数在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。ELU函数的计算强度更高。与Leaky ReLU类似，尽管理论上比ReLU要好，但目前在实践中没有充分的证据表明ELU总是比ReLU好。 六.在实际应用中寻找激活函数的做法 总结 本文介绍了神经网络的几种激活函数,后面还会更新剩余几个激活函数,大致了解一下每个函数的图像的样子,以及在实践中我们一般寻找激活函数的做法." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/18b0299dc7bbacafd227fae3f8690f7d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-03T19:02:03+08:00" />
<meta property="article:modified_time" content="2024-01-03T19:02:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">训练神经网络(上)激活函数</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>本文介绍几种激活函数,只作为个人笔记.观看视频为cs231n</p> 
</blockquote> 
<p></p> 
<div> 
 <h4 id="%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95">文章目录</h4> 
</div> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="%E4%B8%80%E3%80%81Sigmoid%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81Sigmoid%E5%87%BD%E6%95%B0" rel="nofollow">一、Sigmoid函数</a></p> 
<p id="%E4%BA%8C%E3%80%81tanh%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81tanh%E5%87%BD%E6%95%B0" rel="nofollow">二、tanh函数</a></p> 
<p id="%E4%B8%89%E3%80%81ReLU%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81ReLU%E5%87%BD%E6%95%B0" rel="nofollow">三、ReLU函数</a></p> 
<p id="%E5%9B%9B%E3%80%81Leaky%20ReLU%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81Leaky%20ReLU%E5%87%BD%E6%95%B0" rel="nofollow">四、Leaky ReLU函数</a></p> 
<p id="%E4%BA%94%E3%80%81ELU%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81ELU%E5%87%BD%E6%95%B0" rel="nofollow">五、ELU函数</a></p> 
<p id="%E5%85%AD.%E5%9C%A8%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E5%AF%BB%E6%89%BE%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%81%9A%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E5%85%AD.%E5%9C%A8%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E5%AF%BB%E6%89%BE%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%81%9A%E6%B3%95" rel="nofollow">六.在实际应用中寻找激活函数的做法</a></p> 
<p id="%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E5%89%8D%E8%A8%80"><a id="_7"></a>前言</h2> 
<p>激活函数是用来加入非线性因素的，提高神经网络对模型的表达能力，解决线性模型所不能解决的问题。</p> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81Sigmoid%E5%87%BD%E6%95%B0"><a id="pandas_16"></a>一、Sigmoid函数</h2> 
<p>这个函数大家应该熟悉在逻辑回归中曾用到这个sigmoid函数</p> 
<p><img alt="" height="75" src="https://images2.imgbox.com/7e/8c/7IY57CbZ_o.png" width="471"></p> 
<p><img alt="" height="560" src="https://images2.imgbox.com/ec/c3/dM7OWK5i_o.png" width="593"></p> 
<p>这个函数可以将负无穷和正无穷映射到(0,1)上即</p> 
<p>如果你有一个非常大的输入值那么就会接近于1</p> 
<p>如果你有一个非常小的输入值那么就会接近于0</p> 
<p>但这个函数会出现几个问题.</p> 
<p><strong>1.饱和神经元使得梯度消失</strong></p> 
<p>梯度下降法（以及相关的L-BFGS算法等）在使用随机初始化权重的深度网络上效果不好的技术原因是：梯度会变得非常小。具体而言，当使用反向传播方法计算导数的时候，随着网络的深度的增加，反向传播的梯度（从输出层到网络的最初几层）的幅度值会急剧地减小。结果就造成了整体的损失函数相对于最初几层的权重的导数非常小。这样，当使用梯度下降法的时候，最初几层的权重变化非常缓慢，以至于它们不能够从样本中进行有效的学习。这种问题通常被称为“梯度的弥散”.</p> 
<p><img alt="" height="298" src="https://images2.imgbox.com/4d/c7/sdaW3Axl_o.png" width="670"></p> 
<p>从图中我们可以看到在图的两端的梯度越来越接近为0,经过链式法则后会让梯度流消失,这样在使用梯度下降法时参数会更新的非常缓慢,也就是上面所说的梯度弥散问题.</p> 
<p><strong>2.sigmoid函数是一个非零中心的函数</strong></p> 
<p><img alt="" height="838" src="https://images2.imgbox.com/3f/0b/ef0DAUUI_o.png" width="1200"></p> 
<p><a href="https://zhuanlan.zhihu.com/p/114173868" rel="nofollow" title="神经网络：激活函数非0中心导致的问题 - 知乎 (zhihu.com)">神经网络：激活函数非0中心导致的问题 - 知乎 (zhihu.com)</a></p> 
<p>这里有篇知乎文章可以帮助大家更好理解,其实大家把公式推一下就可以理解了,最后导致所有的w参数只能每次沿正或者负方向变换就导致,以z字形逼近最优参数,导致梯度下降的收敛较慢.</p> 
<p><strong>3.对e指数的计算量有一点大</strong></p> 
<h2 id="%E4%BA%8C%E3%80%81tanh%E5%87%BD%E6%95%B0"><a id="_19"></a>二、tanh函数</h2> 
<p><img alt="" height="700" src="https://images2.imgbox.com/d3/0f/BDZfczpY_o.png" width="1200"></p> 
<p>双曲正切函数是双曲函数的一种。双曲正切函数在数学语言上一般写作 tanh。它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在.</p> 
<p class="img-center"><img alt="" height="95" src="https://images2.imgbox.com/06/43/jbFuB887_o.png" width="282"></p> 
<h2 id="%E4%B8%89%E3%80%81ReLU%E5%87%BD%E6%95%B0"><a id="1_20"></a>三、ReLU函数</h2> 
<p><img alt="" height="637" src="https://images2.imgbox.com/1e/91/yS8YFSE6_o.png" width="557"></p> 
<p>ReLU，全称为：Rectified Linear Unit，是一种人工神经网络中常用的激活函数，通常意义下，其指代数学中的斜坡函数，即<br><img alt="f(x)=max(0,x)" class="mathcode" src="https://images2.imgbox.com/4d/9a/bjn2IPwe_o.png"></p> 
<p>ReLU函数的优点</p> 
<ol><li>SGD算法的收敛速度比sigmoid和 tanh 快;(梯度不会饱和，解决了梯度消失问题)</li><li>计算复杂度低，不需要进行指数运算;</li><li>适合用于后向传播。</li></ol> 
<p>ReLU函数的缺点</p> 
<ol><li>ReLU的输出不是zero-centered;</li><li>ReLU在训练的时候很"脆弱”，一不小心有可能导致神经元"坏死”。举个例子:由于RelU在x&lt;0时梯度为0，这样就导致负的梯度在这个ReLU被置零，而且这个神经元有可能再也不会被任何数据激活。如果这个情况发生了，那么这个神经元之后的梯度就永远是0了，也就是ReLU神经元坏死了，不再对任何数据有所响应。实际操作中，如果你的learning rate很大，那么很有可能你网络中的40%的神经元都坏死了。当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。，DeadReLU Problem （神经元坏死现象)︰某些神经元可能永远不会被激活，导致相应参数永远不会被更新(在负数部分，梯度为0)。产生这种现象的两个原因:参数初始化问题; learning rate太高导致在训练过程中参数更新太大。解决方法:采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</li><li>ReLU不会对数据做幅度压缩，所以数据的幅度会随着模型层数的增加不断扩张。<br>  </li></ol> 
<h2 id="%E5%9B%9B%E3%80%81Leaky%20ReLU%E5%87%BD%E6%95%B0">四、Leaky ReLU函数</h2> 
<p><img alt="" height="593" src="https://images2.imgbox.com/bc/95/O3McI7gV_o.png" width="580"></p> 
<p><img alt="" height="179" src="https://images2.imgbox.com/25/d6/O4YDyzSK_o.png" width="844"></p> 
<p>Leaky ReLU函数通过把x xx的非常小的线性分量给予负输入0.01 x 来调整负值的零梯度问题。<br> Leaky有助于扩大ReLU函数的范围，通常α 的值为0.01左右。<br> Leaky ReLU的函数范围是负无穷到正无穷。</p> 
<h2 id="%E4%BA%94%E3%80%81ELU%E5%87%BD%E6%95%B0">五、ELU函数</h2> 
<p><img alt="" height="709" src="https://images2.imgbox.com/d3/c4/wLFIycKf_o.png" width="1200"></p> 
<p></p> 
<ol><li>没有Dead ReLU问题，输出的平均值接近0，以0为中心。</li><li>ELU 通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习。</li><li>ELU函数在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。</li><li>ELU函数的计算强度更高。与Leaky ReLU类似，尽管理论上比ReLU要好，但目前在实践中没有充分的证据表明ELU总是比ReLU好。</li></ol> 
<h2 id="%E5%85%AD.%E5%9C%A8%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E5%AF%BB%E6%89%BE%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%81%9A%E6%B3%95">六.在实际应用中寻找激活函数的做法</h2> 
<p><img alt="" height="513" src="https://images2.imgbox.com/a4/03/fH9zJyO5_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<hr> 
<h2 id="%E6%80%BB%E7%BB%93"><a id="_45"></a>总结</h2> 
<p>本文介绍了神经网络的几种激活函数,后面还会更新剩余几个激活函数,大致了解一下每个函数的图像的样子,以及在实践中我们一般寻找激活函数的做法.</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/14945e6489ed6e764cb88bdd240a01a5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JavaSE学习笔记 2023-12-27 --Java8.0新增特性</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/800243c56ebfdf3cf3d96cf1592c778b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">JavaSE学习笔记 2023-12-28 --MySQL</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>