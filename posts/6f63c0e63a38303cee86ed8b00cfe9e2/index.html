<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>零拷贝技术( DMA、PageCache) - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="零拷贝技术( DMA、PageCache)" />
<meta property="og:description" content="文章目录 DMA传统I/O利用DMA的IO 零拷贝mmap &#43; writemmap详解 sendfile真正的零拷贝 为什么需要内核缓存区？大文件传输：异步IO&#43;直接IO总结 DMA 直接内存访问（Direct Memory Access）
什么是DMA？
在进行数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与，可以去干别的事情。
传统I/O 在没有 DMA 技术前，全程数据拷贝都需要CPU来做，严重消耗CPU。
利用DMA的IO 利用DMA之后：
4次数据拷贝，其中DMA和CPU分别拷贝2次(CPU的时间多宝贵啊)2次系统调用导致的4 次用户态与内核态的上下文切换 DMA 控制器进行数据传输的过程：
用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的用户缓冲区中，进程进入阻塞状态，用户态切换至内核态；
操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 可以执行其他任务；
DMA 进一步将 I/O 请求发送给磁盘；
磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；
DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务；
当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；
CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回，内核态切换至用户态；
利用DMA的IO完整流程图：
DMA 的工作方式如下：
1、CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存；
2、接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/6f63c0e63a38303cee86ed8b00cfe9e2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-19T16:24:22+08:00" />
<meta property="article:modified_time" content="2022-09-19T16:24:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">零拷贝技术( DMA、PageCache)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#DMA_1" rel="nofollow">DMA</a></li><li><ul><li><a href="#IO_5" rel="nofollow">传统I/O</a></li><li><a href="#DMAIO_9" rel="nofollow">利用DMA的IO</a></li></ul> 
  </li><li><a href="#_68" rel="nofollow">零拷贝</a></li><li><ul><li><a href="#mmap__write_72" rel="nofollow">mmap + write</a></li><li><ul><li><a href="#mmap_92" rel="nofollow">mmap详解</a></li></ul> 
   </li><li><a href="#sendfile_125" rel="nofollow">sendfile</a></li><li><a href="#_138" rel="nofollow">真正的零拷贝</a></li></ul> 
  </li><li><a href="#_162" rel="nofollow">为什么需要内核缓存区？</a></li><li><a href="#IOIO_188" rel="nofollow">大文件传输：异步IO+直接IO</a></li><li><a href="#_201" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<h2><a id="DMA_1"></a>DMA</h2> 
<p>直接内存访问（Direct Memory Access）<br> 什么是DMA？<br> 在进行数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与，可以去干别的事情。</p> 
<h3><a id="IO_5"></a>传统I/O</h3> 
<p>在没有 DMA 技术前，全程数据拷贝都需要CPU来做，严重消耗CPU。<br> <img src="https://images2.imgbox.com/9c/b5/yrQCaTIP_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="DMAIO_9"></a>利用DMA的IO</h3> 
<p>利用DMA之后：</p> 
<ul><li>4次数据拷贝，其中DMA和CPU分别拷贝2次(CPU的时间多宝贵啊)</li><li>2次系统调用导致的<code>4 次</code>用户态与内核态的<code>上下文切换</code></li></ul> 
<p>DMA 控制器进行数据传输的过程：<br> <img src="https://images2.imgbox.com/66/04/6143Nrg1_o.png" alt="在这里插入图片描述"></p> 
<ul><li> <p>用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的用户缓冲区中，进程进入<code>阻塞状态</code>，<strong>用户态切换至内核态</strong>；</p> </li><li> <p>操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 <code>CPU 可以执行其他任务</code>；</p> </li><li> <p>DMA 进一步将 I/O 请求发送给磁盘；</p> </li><li> <p>磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起<code>中断</code>信号，告知自己缓冲区已满；</p> </li><li> <p>DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务；</p> </li><li> <p>当 DMA 读取了足够多的数据，就会发送<code>中断</code>信号给 CPU；</p> </li><li> <p>CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回，<strong>内核态切换至用户态</strong>；</p> </li></ul> 
<p><strong>利用DMA的IO完整流程图：</strong><br> <img src="https://images2.imgbox.com/cd/39/pdMxoDXu_o.png" alt="在这里插入图片描述"><br> <strong>DMA 的工作方式如下</strong>：<br> <img src="https://images2.imgbox.com/ff/b8/am3tjTIJ_o.png" alt="在这里插入图片描述"></p> 
<ul><li> <p>1、CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存；</p> </li><li> <p>2、接下来，<code>DMA</code> 控制器会向<code>磁盘控制器</code>发出指令，通知它从磁盘读数据到其内部的缓冲区中，</p> </li><li> <p>3、接着磁盘控制器将缓冲区的数据传输到内存；</p> </li><li> <p>4、数据拷贝成功之后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；</p> </li><li> <p>5、DMA 控制器收到信号后，DMA 控制器通过<code>中断</code>通知 CPU 指令完成，CPU 就可以直接取内存里面现成的数据了；</p> </li></ul> 
<p>可以看到，仅仅在传送开始和结束时需要 CPU 干预，其他任务交由DMA处理。</p> 
<p>因为发生了<code>read+write两次系统调用</code>，所以一共发生了 <code>4 次用户态与内核态的上下文切换</code></p> 
<p>上下文切换的成本并不小，一次切换需要耗时几十纳秒到几微秒</p> 
<p>还发生了 <code>4 次数据拷贝</code>，其中<code>两次是 CPU参与的拷贝</code>。</p> 
<p><strong>如何优化？</strong><br> 减少<code>「用户态与内核态的上下文切换」和「数据拷贝」</code>的次数。</p> 
<blockquote> 
 <p>1、如何减少「用户态与内核态的<code>上下文切换</code>」的次数呢？</p> 
</blockquote> 
<p>读取磁盘数据的时候，之所以要发生上下文切换，是因为<strong>用户空间没有权限操作磁盘或网卡</strong>，这些操作设备的过程只能<code>交由OS内核</code>来完成。所以需要系统调用进行上下文切换，切换到内核态。<br> 所以，减少上下文切换到次数的办法就是：<br> <strong>减少系统调用的次数</strong></p> 
<blockquote> 
 <p>2、如何减少<code>「数据拷贝」</code>的次数？</p> 
</blockquote> 
<p><code>从内核的读缓冲区-----用户的缓冲区里----- socket 的缓冲区里</code>，这个过程是没有必要的。</p> 
<p>因为文件传输的应用场景中，在<code>用户空间</code>我们并不会对数据「再加工」，所以数据实际上<code>可以不用搬运到用户空间</code>，因此用户的缓冲区是没有必要存在的。</p> 
<h2><a id="_68"></a>零拷贝</h2> 
<p>零拷贝技术实现的方式通常有 2 种：</p> 
<ul><li>mmap(内存映射) + write</li><li>sendfile</li></ul> 
<h3><a id="mmap__write_72"></a>mmap + write</h3> 
<p>在前面我们知道，read() 系统调用的过程中会把<code>内核缓冲区</code>的数据拷贝到<code>用户</code>的缓冲区里，于是为了减少这一步开销，我们可以<code>用 mmap() 替换 read() 系统调用函数</code>。</p> 
<p><code>mmap系统调用函数</code>会直接把内核缓冲区里的数据<code>共享</code>到用户空间，这样，操作系统内核与用户空间就<code>不需要再进行任何的数据拷贝操作。</code><br> <img src="https://images2.imgbox.com/0b/74/OXPLhzUb_o.png" alt="在这里插入图片描述"><br> 具体过程如下：</p> 
<ul><li> <p>应用进程调用了 <code>mmap</code>后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，<code>应用进程跟操作系统</code>内核<code>「共享」</code>这个缓冲区；</p> </li><li> <p>应用进程再调用 <code>write</code>，操作系统直接将<code>内核缓冲区</code>的数据拷贝到 <code>socket</code> 缓冲区中，这一切都发生在内核态，由 <code>CPU</code> 来搬运数据；</p> </li><li> <p>最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。</p> </li></ul> 
<p>性能如何？</p> 
<ul><li> <p><code>可以减少一次数据拷贝的过程。</code></p> </li><li> <p>但这还不是最理想的零拷贝，因为 <code>把内核缓冲区的数据拷贝到 socket 缓冲区里</code>的工作仍然需要通过<code>CPU</code>完成，</p> </li><li> <p>而且<code>仍然需要 4 次上下文切换，因为系统调用还是 2 次。</code></p> </li></ul> 
<h4><a id="mmap_92"></a>mmap详解</h4> 
<p><strong>是什么？</strong><br> mmap是一种实现<code>内存映射文件</code>的方法。<br> 即：将一个<code>文件</code>映射到<code>用户进程的地址空间</code>，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的<code>一一对映关系</code>。</p> 
<p>实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动<code>回写脏页面</code>到对应的文件磁盘上，即完成了对文件的操作，又不必再调用read,write等系统调用函数。</p> 
<p>相应地，内核空间对这段区域的修改也直接反映到用户空间，从而可以实现不同用户进程间的<code>文件共享</code>。</p> 
<p><strong>mmap内存映射的实现过程，总的来说可以分为三个阶段：</strong><br> 1、进程启动映射过程，并在<code>虚拟地址空间</code>中为映射<code>创建虚拟映射区域</code></p> 
<p>2、调用<code>mmap</code>实现<code>文件的物理地址和进程虚拟地址</code>的一一映射关系</p> 
<p>注：前两个阶段仅在于创建虚拟区间并完成地址映射，还没有将任何文件数据拷贝至主存。真正的文件读取是当进程发起读或写操作时开始。</p> 
<p>3、进程发起对这片映射空间的访问，引发<code>缺页</code>中断，实现文件到内核缓冲区的拷贝</p> 
<ul><li>进程的读或写操作访问虚拟地址空间这一段映射地址，通过查询<code>页表</code>，发现这一段地址并不在物理页面上。因为目前只建立了<code>地址映射</code>，真正的硬盘数据还没有拷贝到内存中，因此引发<code>缺页异常</code>。</li><li>缺页异常进行一系列判断，确定无非法操作后，内核发起<code>请求调页</code>过程。</li><li>调页完成后。进程即可对这片内核缓冲区进行读写操作，如果写操作改变了其内容，一定时间后系统会自动回写脏页面到对应磁盘地址，也即完成了<code>写入到文件的过程</code>。</li></ul> 
<p>mmap的功能：<br> 1、上面已经分析了，mmap最大的功能就是<code>减少了数据的拷贝次数</code></p> 
<p>2、提供了<code>进程间共享内存及相互通信</code>的方式。<br> 不管是父子进程还是无亲缘关系的进程，都可以将自身用户空间映射到同一个文件。从而通过各自对映射区域的改动，达到进程间通信和进程间共享的目的。</p> 
<p>同时，如果进程A和进程B都映射了区域C，当A第一次读取C时通过缺页从磁盘复制文件页到内存中；但当B再读C的相同页面时，虽然也会产生缺页异常，但是不再需要从磁盘中复制文件过来，而可直接使用已经保存在内存中的文件数据。</p> 
<p>3、可用于实现高效的<code>大规模数据传输</code>。<br> 内存空间不足，是制约大数据操作的一个方面，解决方案往往是借助硬盘空间协助操作，补充内存的不足。但是进一步会造成大量的文件I/O操作，极大影响效率。这个问题可以通过<code>mmap</code>映射很好的解决。换句话说，但凡是需要用磁盘空间代替内存的时候，mmap都可以发挥其功效。</p> 
<h3><a id="sendfile_125"></a>sendfile</h3> 
<p>3次数据拷贝，其中CPU拷贝一次<br> 1次系统调用<br> 2 次用户态与内核态的上下文切换</p> 
<p>在 Linux 内核版本 2.1 中，提供了一个<code>专门发送文件</code>的系统调用函数 <code>sendfile</code>。</p> 
<p>首先，它可以替代前面的 <code>read() 和 write()</code> 这两个系统调用，这样就<code>可以减少一次系统调用</code>，也就<code>减少了 2 次上下文切换的开销</code>。</p> 
<p>其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就<code>减少了一次数据拷贝</code>，<br> 现在一共只有 2 次上下文切换，和 3 次数据拷贝。如下图：<br> <img src="https://images2.imgbox.com/79/33/ydAPmagO_o.png" alt="在这里插入图片描述"><br> 但是这<code>还不是真正的零拷贝技术</code>。</p> 
<h3><a id="_138"></a>真正的零拷贝</h3> 
<pre><code class="prism language-java"><span class="token number">2</span>次数据拷贝，无CPU参与拷贝
<span class="token number">1</span>次系统调用
<span class="token number">2</span> 次用户态与内核态的上下文切换
</code></pre> 
<p>从 Linux 内核 2.4 版本开始起，<code>sendfile() 系统调用</code>的过程发生了点变化，具体过程如下：</p> 
<ul><li> <p>通过 <code>DMA</code> 将<code>磁盘</code>上的数据拷贝到<code>内核缓冲区</code>里；</p> </li><li> <p>只将<code>缓冲区描述符和数据长度</code>传到 socket 缓冲区，而<code>内核缓存中的数据</code>则通过<code>网卡的 SG-DMA 控制器</code>直接拷贝到<code>网卡的缓冲区</code>里，这样就减少了一次数据拷贝；</p> </li></ul> 
<p>所以，这个过程之中，只进行了 2 次数据拷贝，如下图：<br> <img src="https://images2.imgbox.com/20/13/ssj5ZEig_o.png" alt="在这里插入图片描述"><br> <strong>性能如何？</strong></p> 
<ul><li>全程没有通过 <code> CPU</code> 来搬运数据，所有的数据都是通过 DMA 来进行传输的。</li><li>只需要 2 次上下文切换和2次数据拷贝，就可以完成文件的传输，</li></ul> 
<p>所以，总体来看，零拷贝技术可以把文件传输的性能<code>提高至少一倍</code>以上。</p> 
<p><strong>kafka和Nginx都使用了零拷贝技术</strong></p> 
<h2><a id="_162"></a>为什么需要内核缓存区？</h2> 
<p><img src="https://images2.imgbox.com/10/08/wLQnwIjo_o.png" alt="在这里插入图片描述"><br> 现在回过头再来看，为什么不直接将磁盘数据拷贝到网卡，而要在中间加一个<code>内核缓存区</code>呢？<br> ——核心原因是<strong>磁盘读写太慢了</strong></p> 
<blockquote> 
 <p>内核缓存区做了什么？</p> 
</blockquote> 
<ul><li>缓存最近被访问的数据；</li><li>预读功能；</li><li>内核的 <code>I/O 调度算法</code>会缓存尽可能多的 I/O 请求在 内核缓存区中，最后<code>「合并」</code>成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作；</li></ul> 
<p><strong>1、缓存最近被访问的数据</strong><br> 最近访问过的数据接下来很可能还会被访问，所以利用<code>PageCache</code> 缓存最近被访问的数据，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存在 PageCache 中。当PageCache的空间不足时，淘汰<code>最久未被访问</code>的缓存。</p> 
<p><strong>2、预读功能</strong><br> 利用<code>空间局部性</code>原理，假设 read 方法每次只会读 <code>32 KB</code> 的字节，虽然 read 刚开始只会读 <code>0 ～ 32 KB</code> 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低，如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。</p> 
<p>这两个做法都在于解决<code>读写磁盘相比读写内存的速度慢太多了</code>这一痛点，<strong>大大提高了读写磁盘的性能。</strong></p> 
<p><strong>所以零拷贝使用 内核缓存区技术进一步提升性能。</strong></p> 
<p>但是由于<strong>内核缓存区不适合传输大文件</strong>，所以零拷贝不适合传输大文件<br> 因为每当用户访问这些大文件的时候，内核就会把它们载入 内核缓存区中，于是 内核缓存区空间很快被这些大文件占满。<strong>其他「热点」的小文件可能就无法充分使用到 内核缓存区</strong>，于是这样磁盘读写的性能就会下降了；</p> 
<p>所以，内核缓存区中的大文件数据，不但没有享受到缓存带来的好处，却还耗费 DMA 多拷贝到 内核缓存区一次；</p> 
<p>那针对大文件的传输，我们应该使用什么方式呢？</p> 
<h2><a id="IOIO_188"></a>大文件传输：异步IO+直接IO</h2> 
<p>回顾最初的例子，当调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回，如下图：<br> <img src="https://images2.imgbox.com/33/c6/HkctfQ45_o.png" alt="在这里插入图片描述"><br> 对于阻塞的问题，可以用<code>异步 I/O</code> 来解决，它的工作方式如下图：<br> <img src="https://images2.imgbox.com/b3/07/sjBRyOae_o.png" alt="在这里插入图片描述"><br> 可以发现，<strong>异步 I/O 并没有涉及到 内核缓存区</strong>。</p> 
<p>绕开 内核缓存区的 I/O 叫<code>直接 I/O</code>，使用 内核缓存区的 I/O 则叫<code>缓存 I/O</code>。<br> 通常，对于磁盘，<code>异步 I/O 只支持直接 I/O</code>。</p> 
<p>所以，针对大文件的传输的方式，应该使用<code>异步 I/O + 直接 I/O</code>来替代零拷贝技术。</p> 
<h2><a id="_201"></a>总结</h2> 
<p><img src="https://images2.imgbox.com/e7/c6/AuKv8MWn_o.png" alt="在这里插入图片描述"><br> <strong>DMA和传统IO</strong><br> 早期 I/O 操作，内存与磁盘的数据传输的工作都是由 CPU 完成的，而此时 CPU 不能执行其他任务，会特别浪费 CPU 资源。</p> 
<p>于是，为了解决这一问题，DMA 技术就出现了，实际数据传输工作由 <code>DMA</code> 控制器来完成，CPU 不需要参与数据传输的工作。</p> 
<p><strong>零拷贝</strong><br> <code>传统 IO</code> 的工作方式，从<code>硬盘</code>读取数据，然后再通过<code>网卡</code>向外发送，需要进行 4次上下文切换，和 4 次数据拷贝，更糟糕的是其中两次都是CPU完成的。</p> 
<p>为了提高<code>文件传输</code>的性能，于是就出现了<code>零拷贝技术</code>，只有<code>一个sendfile系统调用</code>导致的<code>2 次用户态与内核态的上下文切换</code>，只进行了 <code>2 次数据拷贝(磁盘——pageCache——网卡)</code>，全程没有通过 CPU 来搬运数据，所有的数据都是通过 <code>DMA</code> 来进行传输的。</p> 
<p><strong>需要注意的是</strong>，零拷贝技术中，数据没有进入用户缓冲区，所以<code>用户进程</code>无法对文件内容作进一步的加工的，比如<code>压缩数据再发送</code>。</p> 
<p><strong>内核缓存区</strong><br> 零拷贝技术是基于 内核缓存区的，内核缓存区具有</p> 
<ul><li>缓存最近访问的数据</li><li>预读数据</li><li>协助 <code>I/O 调度算法</code>实现了 <code>IO 合并</code></li></ul> 
<p>提升了访问缓存数据的性能，解决了磁盘IO慢的问题，进一步提升了零拷贝的性能。</p> 
<p><strong>大文件传输</strong><br> 当传输大文件时，不能使用零拷贝，因为可能由于 内核缓存区 被大文件占据，而导致其他的<code>「热点」小文件</code>无法利用到 内核缓存区，并且大文件的缓存命中率不高，这时就需要使用<code>「异步 IO + 直接 IO 」</code>的方式。</p> 
<p><strong>异步对应同步</strong><br> <strong>直接对应缓存</strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e7563cb6c80a32f5e8ea0c2147592924/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">linux常用命令语句（全）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/44884393f55b9407279d0c70328a42d8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Laravel框架教程 入门篇（一）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>