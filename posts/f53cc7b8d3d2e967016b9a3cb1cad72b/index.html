<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习经典论文翻译1】AlexNet-ImageNet Classification with Deep Convolutional Neural Networks全文翻译 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【深度学习经典论文翻译1】AlexNet-ImageNet Classification with Deep Convolutional Neural Networks全文翻译" />
<meta property="og:description" content="翻译：莫天池
版本号：V1.0.0
2016年3月
转载请与作者/译者联系，邮箱：motianchi@163.com
因为各种各样的原因，大二结束之后就再也没在CSDN更新过博客了。现在已经读到研二，并且准备出国了，遂决定还是要维护一下自己学术专业方面的博客，一方面作为笔记，另一方面也是督促自己学习。
我读研的课题方向和出国读博想研究的方向都是深度学习，之前很长一段时间都没有找到正确的学习这门学问方向，很迷茫，于是月初决定用最笨的办法——翻译经典论文，来逼着自己吃透CNN。这是我完成的第一篇翻译，是大神Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton三人基于ImageNet做的识别图像的深度卷积神经网AlexNet《ImageNet Classification with Deep Convolutional Neural Networks》。
因为这是我第一次全文翻译英文学术文章，加上英语水平和专业水平都有限，所以肯定有很多不准确的地方，所有标有【？】的位置是我自己还有疑问之处，欢迎各位批评指正！！！可以在博客上留言，也可以发邮件到motianchi@163.com与我交流。
如上图所示的中英文对照版pdf，以及AlexNet的原版论文，欢迎到http://pan.baidu.com/s/1NpEG2下载。本博客只提供译文。
ImageNet Classification with Deep Convolutional Neural Networks Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton
摘要
我们训练了一个大型深度卷积神经网络来将ImageNet LSVRC-2010数据集中的120万张高清图片分到1000个不同的类别中。在测试数据中，我们将Top-1错误（分配的第一个类错误）和Top-5错误（分配的前五个类全错）分别降到了37.5%和17.0%，这比之前的技术水平要好得多。这个神经网络拥有6千万的参数和65万个神经元，共有五个卷积层，其中一些卷积层后面跟着最大池化层，还有利用softmax函数进行1000类分类的最后三个全连接层。为了让训练速度更快，我们使用不饱和【？non-saturating】神经元，并利用高效的GPU实现卷积操作。为了减少全连接层的过拟合，我们采用了一种最近研发出来的正则化方法——“DROPOUT”，它被证明十分有效。我们也在比赛中加入了这一模型的一个变体，第二名的26.2%相比，我们通过将TOP-5错误降到了15.3%而获胜。
1 引言
最近的物体识别方法都应用了很重要的机器学习方法，为了提高他们的表现，我们可以收集更大的数据集，学习训练更强大的模型，并用更好的技术来避免过拟合。直到最近，有标签的数据集都是相对较小的，一般只有万张的数量级（比如【16，8，9，12】）。单一的认知任务可以在这个数量级的数据集上得到很好地解决，特别是当其通过标签保存变形技术被放大的时候。比如，现在在MNIST数据集上最低的数字识别错误率已经接近了人类的认知水平（&lt;0.3%）【4】。但是模型识别现实背景中物体的能力就表现得不太稳定了，所以为了训练识别这些物体提供大量的数据集是很有必要的。实际上，使用小数据集的缺陷已经被普遍认同了，但直到最近收集百万有标签图片的数据集才成为可能。这些新的大型数据集包括LabelMe【23】（包含大量被完全分割的图片），还有ImageNet【6】（由1500万张被标记的高清图片组成，覆盖了2.2万个类别）。
为了从百万张图片中学习到数千个物体，我们需要一个有强大学习能力的模型。然而，物体识别任务极高的复杂度意味着即使拥有ImageNet这么大的数据集，这个问题也很难被具体化。所以我们的模型也需要大量先验知识去补全所有缺失数据。卷积神经网络（CNNs）就是一种这样的模型[16, 11, 13, 18, 15, 22, 26]。它们的学习能力可以通过控制网络的深度和宽度来调整，它们也可以对图片的本质（高层属性）做出强大而且基本准确的假设（统计上的稳定性，以及像素依赖的局部性特征）。因此，与同样大小的标准的前馈神经网相比，CNNs有更少的连接、参数，所以更易于训练，而且CNNs的理论最佳表现仅比前馈神经网络稍差。
虽然CNNs质量很好，而且对于局部结构非常高效，但其应用代价对于大量的高清图片而言还是昂贵到可怕。幸运的是，最近，GPU可以被应用于高度优化的2D卷积的实现，它们足够强大，能够加速大型CNNs的训练过程。而且最近的数据集比如ImageNet包含了足量的有标签样本，可以用来训练这些模型，而没有太严重的过拟合。
本文的主要贡献包括：我们在ImageNet的2010和2012数据集集上训练了最大的CNNs之一，并且达到了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现，以及其他所有训练CNNs的固有操作，并将其公之于众。我们的网络包含一系列新的不同凡响的特征，这提高了它的表现性能，减少了它的训练时间，具体情况在第三章介绍。
即使我们拥有120万的标签样例，我们的网络的巨大体积也使得过拟合成了一个严重的问题，所以我们需要一系列技术去克服过拟合，这将在第四章中描述。
我们的网络最终包含5个卷积层和3个全连接层，这个深度也许是很重要的：我们发现去掉任意一个卷积层都会导致更差的表现，即使每个卷积层仅包含不到1%的模型参数。
最后，网络的大小主要被GPU中可获得的存储数量，以及可忍受训练时间所限制。
我们的网络需要在两台GTX 580 3GB GPUs训练五至六天。我们所有的实验都表明，只要等到更快的GPU和更大的数据集出现，其结果能够被进一步提高。
2 数据集
ImageNet是一个拥有超过1500万张带标签的高清图片的数据集，这些图片大约属于2.2万个类别。这些图片收集自网络并由亚马逊的Turk群智工具进行人工标记。从2010年开始，作为帕斯卡物体可视化挑战的一部分，一项被称为ILSVRC的比赛每年都会进行。
ILSVRC使用ImageNet的一个子集，这个子集包含大约1000个类别，每个类别大概包涵1000张图。总共大概有120万张训练图片，5万张验证图片和15万张测试图片。
2010年的ILSVRC数据集是唯一一个测试集标签可得的版本，所以我们用它进行我们的大部分实验。
因为我们也把我们的模型加入了2012年的ILSVRC比赛，所以我们在第六章也讨论了这个数据集上的实验结果，但其测试集标签不可得，在ImageNet上，通常检验两类错误率：TOP-1和TOP-5，TOP-5错误表示测试图片的标签不在模型所认为的 可能性最大的五个标签中。
ImageNet包含各种清晰度的图片，而我们的系统要求输入维度恒定，因此，我们对图片进行采样，获得固定大小的256X256的分辨率，对于每张长方形的图，我们将短边按比例调整为256，然后取中心区域的256X256像素。我们并未使用其他方法对图片进行预处理，除了把每个像素减去整个训练集的平均值【？？？except for subtracting the mean activity over the training set from each pixel." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/f53cc7b8d3d2e967016b9a3cb1cad72b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-03-10T23:34:32+08:00" />
<meta property="article:modified_time" content="2016-03-10T23:34:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习经典论文翻译1】AlexNet-ImageNet Classification with Deep Convolutional Neural Networks全文翻译</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="font-size:18px"><span style="font-size:18px">翻译：莫天池</span><br> <span style="font-size:18px">版本号：V1.0.0</span><br> <span style="font-size:18px">2016年3月</span><br> 转载请与作者/译者联系，邮箱：motianchi@163.com</span></p> 
<p><span style="font-size:18px"><br> </span></p> 
<p><span style="font-size:18px">因为各种各样的原因，大二结束之后就再也没在CSDN更新过博客了。现在已经读到研二，并且准备出国了，遂决定还是要维护一下自己学术专业方面的博客，一方面作为笔记，另一方面也是督促自己学习。</span></p> 
<p><span style="font-size:18px"><br> </span></p> 
<p><span style="font-size:18px">我读研的课题方向和出国读博想研究的方向都是深度学习，之前很长一段时间都没有找到正确的学习这门学问方向，很迷茫，于是月初决定用最笨的办法——翻译经典论文，来逼着自己吃透CNN。这是我完成的第一篇翻译，是大神<span style="font-size:18px">Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton三人基于ImageNet做的识别图像的深度卷积神经网AlexNet《ImageNet Classification with Deep Convolutional Neural Networks》。</span></span></p> 
<p><span style="font-size:18px"><span style="font-size:18px"><span style="color:#ff0000"><strong><u><br> </u></strong></span></span></span></p> 
<p><span style="font-size:18px"><span style="font-size:18px"><span style="color:#ff0000"><strong><u>因为这是我第一次全文翻译英文学术文章，加上英语水平和专业水平都有限，所以肯定有很多不准确的地方，所有标有【？】的位置是我自己还有疑问之处，欢迎各位批评指正！！！可以在博客上留言，也可以发邮件到motianchi@163.com与我交流。</u></strong></span></span></span></p> 
<p><span style="font-size:18px"><span style="font-size:18px"><img src="https://images2.imgbox.com/77/8d/R1Prn6LM_o.png" alt=""><br> </span></span></p> 
<p><span style="font-size:18px"><span style="font-size:18px">如上图所示的中英文对照版pdf，以及AlexNet的原版论文，欢迎到<a target="_blank" href="http://pan.baidu.com/s/1NpEG2" rel="nofollow noopener noreferrer">http://pan.baidu.com/s/1NpEG2</a>下载。本博客只提供译文。</span></span></p> 
<p><br> </p> 
<h2>ImageNet Classification with Deep Convolutional Neural Networks</h2> 
<p><span style="font-size:18px">Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</span></p> 
<p><span style="font-size:18px"></span> </p> 
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td> <p><strong><span style="font-size:18px">摘要</span></strong></p> <p><span style="font-size:18px">我们训练了一个大型深度卷积神经网络来将ImageNet LSVRC-2010数据集中的120万张高清图片分到1000个不同的类别中。在测试数据中，我们将Top-1错误（分配的第一个类错误）和Top-5错误（分配的前五个类全错）分别降到了37.5%和17.0%，这比之前的技术水平要好得多。这个神经网络拥有6千万的参数和65万个神经元，共有五个卷积层，其中一些卷积层后面跟着最大池化层，还有利用softmax函数进行1000类分类的最后三个全连接层。为了让训练速度更快，我们使用不饱和【？non-<span style="font-size:18px">saturating</span>】神经元，并利用高效的GPU实现卷积操作。为了减少全连接层的过拟合，我们采用了一种最近研发出来的正则化方法——“DROPOUT”，它被证明十分有效。我们也在比赛中加入了这一模型的一个变体，第二名的26.2%相比，我们通过将TOP-5错误降到了15.3%而获胜。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">1 引言</span></strong></p> <p><span style="font-size:18px">最近的物体识别方法都应用了很重要的机器学习方法，为了提高他们的表现，我们可以收集更大的数据集，学习训练更强大的模型，并用更好的技术来避免过拟合。直到最近，有标签的数据集都是相对较小的，一般只有万张的数量级（比如【16，8，9，12】）。单一的认知任务可以在这个数量级的数据集上得到很好地解决，特别是当其通过标签保存变形技术被放大的时候。比如，现在在MNIST数据集上最低的数字识别错误率已经接近了人类的认知水平（&lt;0.3%）【4】。但是模型识别现实背景中物体的能力就表现得不太稳定了，所以为了训练识别这些物体提供大量的数据集是很有必要的。实际上，使用小数据集的缺陷已经被普遍认同了，但直到最近收集百万有标签图片的数据集才成为可能。这些新的大型数据集包括LabelMe【23】（包含大量被完全分割的图片），还有ImageNet【6】（由1500万张被标记的高清图片组成，覆盖了2.2万个类别）。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">为了从百万张图片中学习到数千个物体，我们需要一个有强大学习能力的模型。然而，物体识别任务极高的复杂度意味着即使拥有ImageNet这么大的数据集，这个问题也很难被具体化。所以我们的模型也需要大量先验知识去补全所有缺失数据。卷积神经网络（CNNs）就是一种这样的模型[16, 11, 13, 18, 15, 22, 26]。它们的学习能力可以通过控制网络的深度和宽度来调整，它们也可以对图片的本质（高层属性）做出强大而且基本准确的假设（统计上的稳定性，以及像素依赖的局部性特征）。因此，与同样大小的标准的前馈神经网相比，CNNs有更少的连接、参数，所以更易于训练，而且CNNs的理论最佳表现仅比前馈神经网络稍差。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">虽然CNNs质量很好，而且对于局部结构非常高效，但其应用代价对于大量的高清图片而言还是昂贵到可怕。幸运的是，最近，GPU可以被应用于高度优化的2D卷积的实现，它们足够强大，能够加速大型CNNs的训练过程。而且最近的数据集比如ImageNet包含了足量的有标签样本，可以用来训练这些模型，而没有太严重的过拟合。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">本文的主要贡献包括：我们在ImageNet的2010和2012数据集集上训练了最大的CNNs之一，并且达到了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现，以及其他所有训练CNNs的固有操作，并将其公之于众。我们的网络包含一系列新的不同凡响的特征，这提高了它的表现性能，减少了它的训练时间，具体情况在第三章介绍。</span></p> <p><span style="font-size:18px">即使我们拥有120万的标签样例，我们的网络的巨大体积也使得过拟合成了一个严重的问题，所以我们需要一系列技术去克服过拟合，这将在第四章中描述。</span></p> <p><span style="font-size:18px">我们的网络最终包含5个卷积层和3个全连接层，这个深度也许是很重要的：我们发现去掉任意一个卷积层都会导致更差的表现，即使每个卷积层仅包含不到1%的模型参数。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">最后，网络的大小主要被GPU中可获得的存储数量，以及可忍受训练时间所限制。</span></p> <p><span style="font-size:18px">我们的网络需要在两台GTX 580 3GB GPUs训练五至六天。我们所有的实验都表明，只要等到更快的GPU和更大的数据集出现，其结果能够被进一步提高。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">2 数据集</span></strong></p> <p><span style="font-size:18px">ImageNet是一个拥有超过1500万张带标签的高清图片的数据集，这些图片大约属于2.2万个类别。这些图片收集自网络并由亚马逊的Turk群智工具进行人工标记。从2010年开始，作为帕斯卡物体可视化挑战的一部分，一项被称为ILSVRC的比赛每年都会进行。</span></p> <p><span style="font-size:18px">ILSVRC使用ImageNet的一个子集，这个子集包含大约1000个类别，每个类别大概包涵1000张图。总共大概有120万张训练图片，5万张验证图片和15万张测试图片。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">2010年的ILSVRC数据集是唯一一个测试集标签可得的版本，所以我们用它进行我们的大部分实验。</span></p> <p><span style="font-size:18px">因为我们也把我们的模型加入了2012年的ILSVRC比赛，所以我们在第六章也讨论了这个数据集上的实验结果，但其测试集标签不可得，在ImageNet上，通常检验两类错误率：TOP-1和TOP-5，TOP-5错误表示测试图片的标签不在模型所认为的 可能性最大的五个标签中。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">ImageNet包含各种清晰度的图片，而我们的系统要求输入维度恒定，因此，我们对图片进行采样，获得固定大小的256X256的分辨率，对于每张长方形的图，我们将短边按比例调整为256，然后取中心区域的256X256像素。我们并未使用其他方法对图片进行预处理，除了把每个像素减去整个训练集的平均值【？？？except for subtracting the mean activity over the training set from each pixel.】所以我们的模型是在原始的RGB像素值上训练出来的。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">3 模型体系结构</span></strong></p> <p><span style="font-size:18px">网络的体系结构如图2.它包含8个学习层——五个卷积层3个全连接层。接下来，我们讨论一些我们的网络中创新的，或者不常见的结构。3.1~3.4节按照我们心目中对它们重要性的评估进行排序，越重要越靠前。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">3.1 ReLU非线性</span></p> <p><span style="font-size:18px">对神经元输出f的标准建模方法是将输入x函数变换为f(x) = tanh(x)或f(x) = (1 + e<sup>-x</sup>)<sup>-1</sup>。<img src="https://images2.imgbox.com/67/b5/P8nDezZ7_o.png" alt=""></span></p> <p><span style="font-size:18px">从梯度下降的训练时间而言，这种饱和【？saturating】的非线性比使用非饱和的非线性f(x) = max(0,x)要慢得多。根据Nair和Hinton说法【20】，我们让神经元使用这种非线性——修正线性单元（ReLUs）。使用ReLU的深度卷积神经网络比使用tanh的网络训练速度快几倍。</span></p> <p><span style="font-size:18px">图一展示了一个特定的四层CNN在CIFAR-10数据集上达到25%训练错误所需要的迭代次数。</span></p> <p><span style="font-size:18px">这张图显示，如果我们采用传统的饱和神经元，我们将不可能为这项工作训练如此庞大的神经网络。</span></p> </td></tr></tbody></table> 
<img src="https://images2.imgbox.com/48/cb/Au6SkCJl_o.png" alt=""> 
<br> 
<p></p> 
<p><span style="font-size:18px"></span> </p> 
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td> <p><span style="font-size:18px">我们并不是最早考虑替换传统CNN神经元模型的人。比如，J【11】等人宣称利用f (x) = |tanh(x)|非线性在Caltech-101数据集上做对比度归一化（Contrast Normalization，CN）和局部平均值池化表现得很好。</span></p> <p><span style="font-size:18px">然而，关于这个数据集的主要问题是要防止过拟合，所以他们观察到的效果，与我们报告的使用ReLUs时对训练集的适应（fit）累积能力不同。更快的学习对于在大型数据集上训练大型模型的表现有重大影响。</span></p> </td></tr><tr><td> <p></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">3.2 多GPU并行训练</span></strong></p> <p><span style="font-size:18px">单个的GTX580GPU只有3G的存储空间，这会限制能够在其上训练的网络大小。充分训练网络需要120万张训练样本图，这对于一个GPU而言量太大了，所以我们将网络分布在两个GPU上。</span></p> <p><span style="font-size:18px">现在的GPU非常适合做跨GPU并行运算，因为它们可以直接向彼此的存储中做读写操作，而无需通过宿主机存储。</span></p> <p><span style="font-size:18px">我们采用的这种并行模式主要是将各一半的网络内核（或神经元）放在每个GPU上，然后再采用一个小技巧：将GPU通信限制在某些特定的层上。</span></p> <p><span style="font-size:18px">这意味着，比如，第三层的内核从所有的第二层内核映射（kernel map）中获得输入，但是，第四层的内核只从和自己在同一个GPU上的第三层内核中获得输入。</span></p> <p><span style="font-size:18px">选择一种连接模式对于交互验证是个问题，但这允许我们精确调整连接的数量，直到计算量落入一个可接受的范围内。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">由此产生的结构会和所谓的“柱状（columnar）”CNN有些类似（由Ciresan等人【5】开发），只是我们的“柱子”不是独立的（见图2）。与用一个GPU训练每个卷积层只有一半的内核的网络相比，这种结构将我们的TOP-1错误和TOP-5错误分别降低了1.7%和1.2%。双GPU结构网络比单GPU网络所需的训练时间要稍微少一些。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">3.3 局部反应归一化（Local Response Normalization）</span></strong></p> <p><span style="font-size:18px">ReLUs有一个很赞的属性，它们无需对输入数据进行归一化来避免其饱和【？】。如果至少有一些训练样例为ReLU产生了正输入，那么这个神经元就会进行学习。</span></p> <p><span style="font-size:18px">然而，我们还是发现下面这种归一化的模式能够更好地泛化。</span></p> <p><span style="font-size:18px">设由第i个内核计算(x, y)位置的ReLU非线性的活动为a<sup>i</sup><sub>x,y</sub>，反应归一化（response-normalized）活动b<sup>i</sup><sub>x,y</sub>如下公式所示：</span></p> <p><span style="font-size:18px"><img src="https://images2.imgbox.com/d6/0d/1Qz0N154_o.png" alt=""><br> </span></p> <p></p> </td></tr><tr><td> <p><span style="font-size:18px">其中，累加公式中的n表示同一空间【GPU？】上邻接于该位置的所有内核映射的数量，N表示这一层的所有内核数。内核映射的顺序当然是任意的，并且是在训练之前就定好了的。</span></p> <p><span style="font-size:18px">这种反应归一化（response normalization）实现了一种模仿生物神经元的横向抑制【层间不通讯？？】，让神经元在利用不同内核进行计算的大规模活动中产生竞争。常数k，n，α和β是超系数，它们的值由验证集决定。我们取k = 2, n = 5, α= 10<sup>-4</sup>, and β = 0.75.我们在特定层使用ReLU非线性之后应用这种归一化（见3.5）</span></p> </td></tr><tr><td> <p><span style="font-size:18px">这种模式与J【11】提出的局部对比度归一化有点类似，但我们的方法更准确的描述应该是亮度归一化，因为我们并不减去均值。</span></p> <p><span style="font-size:18px">反应归一化将我们的TOP-1和TOP-5错误分别降低了1.4%和1.2%。我们还在CIFAR-10数据集上验证了该模式的效果：四层CNN不用归一化错误率为13%，用了之后降到了11%。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">3.4 重叠池化</span></strong></p> <p><span style="font-size:18px">CNN中的池化层负责对同一内核映射中相邻的神经元组的输出求和【？summarize the outputs of neighboring groups of neurons in the same kernel map.】。一般地，被邻接的池化单元求和的邻居节点是没有重复的【17，11，4】。</span></p> <p><span style="font-size:18px">为了更加精确，一个池化层可以看做由相隔s个像素占据的池化单元组成的网格所构成，每个单元负责对相邻的z*z范围的中心区域求和。若设s=z，我们就能够获得用于大多数CNN的传统的局部池化方法。若设s&lt;z，我们就得到了有重叠的池化。</span></p> <p><span style="font-size:18px">这就是我们在自己的网络中使用的方法，s=2，z=3.与无重叠的s=z=2相比，这一模式在产生相同维度的输出时分别将TOP1和TOP5降低了0.4%和0.3%。</span></p> <p><span style="font-size:18px">我们还观察到，采用有重叠的池化能稍稍让模型更难过拟合。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">3.5 整体结构</span></strong></p> <p><span style="font-size:18px">现在我们可以描述我们的CNN的整体结构了。如图2，这个网络包含8个加权的层：前五个是卷积层，后三个是全连接层。</span></p> <p><span style="font-size:18px">最后一个全连接层输出一个1000维的softmax来表达对于1000个类别的预测。</span></p> <p><span style="font-size:18px">我们的网络采取取最大值的多标量罗吉斯回归，它与maximizing the average across training cases of the log-probability of the correct label under the prediction distribution【？】等价</span></p> </td></tr><tr><td> <p><span style="font-size:18px">第2、4、5个卷积层的内核只与前一层与自己同在一个GPU上的内核映射相连接。</span></p> <p><span style="font-size:18px">第三层的内核与全部的第二层内核映射相连接。全连接层的神经元与上层神经元全都有连接。</span></p> <p><span style="font-size:18px">反应归一化层跟在第二个卷积层后面。最大值池化层（如3.4所讨论的）跟在反应归一化层后面和第五个卷积层后面。ReLU非线性被应用在每个卷积层和全连接层</span></p> </td></tr><tr><td> <p><span style="font-size:18px">第一个卷积层的输入是224 × 224 × 3【注：3是RGB】的图像，然后用96个11 × 11 × 3的步长为4像素的内核去过滤（步长是相邻神经元感知区域中心之间的距离）。</span></p> <p><span style="font-size:18px">第二个卷积层将第一个卷积层的输出作为输入（反应归一化并池化），然后用256个5 × 5 × 48的内核进行过滤。</span></p> <p><span style="font-size:18px">第三、四、五层卷积层前后相连，之间没有池化层和归一化层。第三个卷积层有384个3 × 3 × 256的内核，连接着第二个卷积层的输出（归一化+池化）。第四个卷积层有384个3 × 3 × 192的内核，第五个卷积层有256个3 × 3 × 192的内核。每个全连接层各有4096个神经元。</span></p> </td></tr></tbody></table> 
<img src="https://images2.imgbox.com/46/84/7wc7ldBj_o.png" alt=""> 
<br> 
<p></p> 
<p><span style="font-size:18px"></span></p> 
<p>图2：一个关于我们的CNN的结构的描述，明确地勾勒出两个GPU之间的对应关系。一个GPU运行某一个层画在这幅图上部的那部分的同时，另一个GPU会运行同一层画在这幅图下部的那部分。两个GPU只在特定的层通讯。</p> 网络的输入是150,528维的，而除输入层外，余下五个卷积层和三个全连接层分别有253440，186624，64896，64896，43264，4096，4096，1000个神经元。 
<br> 
<p><span style="font-size:18px"></span> </p> 
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td> <p><strong><span style="font-size:18px">4. 减少过拟合</span></strong></p> <p><span style="font-size:18px">我们的神经网络拥有6000万的参数，虽然ILSVRC的1000个类别将从图片到标签的映射限制在10个bits，这依然不足以训练这么多的参数而不造成过拟合。下面，我们将介绍两种对付过拟合的基本方法。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">4.1 数据集放大<strong>DataAugmentation</strong></span></strong></p> <p><span style="font-size:18px">最简单最常用的减少过拟合的方法就是利用标签保存变形技术人工放大数据集【如文献25，4，5】。我们采取了两种不同形式的数据放大，它们都允许在仅对原图做少量计算的情况下产生变形的新图，所以变形后的新图无需存储在硬盘中。</span></p> <p><span style="font-size:18px">在我们的实现中，变形的新图由Python在CPU上计算产生，与此同时，GPU仍在计算其他的之前批次的图片。所以这种放大数据集的方式是很高效很节省计算资源的。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">第一种放大数据集（产生新图）的方式由图片翻译【？translations】和水平镜像组成，我们通过从256X256的图片中随机抽取224X224的区块（及其水平镜像）来实现这种方法，并在这些抽取后得到的区块上训练我们的神经网络。</span></p> <p><span style="font-size:18px"> </span></p> <p><span style="font-size:18px">这种方法为我们的训练集增加了2048个因子【？ This increases the size of our training set by a factor of 2048】，虽然这些生成的训练图片明显是相互关联的。如果不采用这种方法，我们的网络会出现严重的过拟合，进而迫使我们采用更小的网络。在测试过程中，网络会抽取五个（四角和中间）224 x 224的区块及其水平镜像进行预测，然后将softmax层对这十个区块做出的预测取平均。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">第二种放大数据集的方法是对训练图片的RGB频谱密度进行改变。特别地，我们在整个ImageNet训练集上对RGB像素进行主成分分析（PCA），对于每张训练图像，我们通过均值为0，方差为0.1的高斯分布产生一个随机值（译者：设为a），然后通过向图像中加入更大比例的相应的本征值的a倍，把其主成分翻倍【we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1】。因此，对于每个RGB像素 I<sub>xy</sub> =[I<sub>xy</sub><sup>R</sup> , I<sub>xy</sub> <sup>G</sup> , I<sub>xy</sub> <sup>B </sup>]<sup>T</sup>我们加入的值如下：<img src="https://images2.imgbox.com/26/71/qRYy3NN7_o.png" alt=""></span></p> </td></tr><tr><td> <p></p> <p><span style="font-size:18px">其中，p<sub>i</sub> 和λ<sub>i</sub>分别是第i个特征向量和第i个3x3RGB协方差矩阵的本征值。而αi是前面所述的随机变量。对于一张特定的训练图片的所有像素，每个αi仅被抽取一次，直到这张图再次被用于训练才会再次提取随机变量。</span></p> <p><span style="font-size:18px">这一方案能够近似地捕捉原始图像的一些重要特征，即那些不随光线强度与颜色变化的物体特质。这一方法把top-1错误降低了1%。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">4.2 DROPOUT</span></strong></p> <p><span style="font-size:18px">降低测试错误的一种有效方法是联立多种不同模型的预测结果【1，3】，但这种方法对于大型神经网络来说似乎太昂贵了，需要好几天去训练。然而，有一种非常高效的模型联立方法，只需要在训练过程中消耗一到两个因子。这种新近研究出来的技术叫做“DROPOUT”【10】，它会以50%的概率将每个隐藏层神经元置零。</span></p> <p><span style="font-size:18px">以这种方法被置零的神经元不再参与前馈和BP过程。</span></p> <p><span style="font-size:18px">所以每次一个输入进来之后，这个神经网都会被置于不同的结构，但所有这些结构共享同一套参数。这种技术降低了神经元间相互适应的复杂性，因为每个神经元都不可能依赖其他特定某个神经元的表现。</span></p> <p><span style="font-size:18px">因此，模型被迫学习更加健壮的特征，使之能够被许多不同的随机神经元子集使用。在测试中，我们使用所有的神经元，但是把它们的输出乘以0.5，这是一种对大量dropout网络产生的预测分布的几何均值的合理近似。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">我们在图2中的前两个全连接层使用dropout。否则，我们的网络会表现出严重的过拟合。dropout大概会让达到收敛所需要的迭代次数翻倍。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">5. 学习过程的细节</span></strong></p> <p><span style="font-size:18px">我们每个训练批次有128个样本，在其上采用随机梯度下降进行训练。设置增量【？momentum】为0.9，权值衰退因子为0.0005.我们发现小的权重衰退因子对于模型学习很重要，换句话说，权重衰退因子在这里不光是个正则化因子，它还可以减少模型错误。权值w的更新规则是：</span></p> <p><img src="https://images2.imgbox.com/b0/41/e50TKYIg_o.png" alt=""></p> <p><span style="font-size:18px">其中，i是迭代次数，v是增量【？momentum】，ε是学习速率，<img src="https://images2.imgbox.com/37/e5/0iuYvIrE_o.png" alt="">是第i批次的目标函数关于w的导数（wi的偏导数）Di的平均值【？is the average over the ith batch D<sub>i </sub>of the derivative of the objective with respect to w, evaluated at w<sub>i</sub>.】</span></p> </td></tr><tr><td> <p><span style="font-size:18px">我们将每一层的权值利用均值为0方差为0.01的高斯分布随机初始化，我们用常数1初始化第2、4、5卷积层和全连接隐藏层的偏置神经元（常数单元）。这种初始化通过向ReLUs提供正输入，加速了学习的早期过程。我们将其它层的偏置神经元初始化为0.</span></p> </td></tr><tr><td> <p><span style="font-size:18px">在整个学习过程中，我们在所有层都使用人工调整的相等的学习速率。我们采用的启发式方法是当验证误差不在降低时，就把当前的学习速率除以10。学习速率初始化为0.01，并在结束前减小3次。（做三次除以10）</span></p> <p><span style="font-size:18px">我们大概用120万张图片把我们的网络训练了约90轮，在两个NVIDIA GTX 580 3GB GPU上这大概要5到6天。</span></p> </td></tr></tbody></table> 
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td> <p><strong><span style="font-size:18px">6 实验结果</span></strong></p> <p><span style="font-size:18px">我们在ILSVRC-2010数据集上的实验结果归纳在表1里。我们的网络top-1和top-5测试误差分别是37.5%和17.0%。在此之前ILSVRC- 2010数据集上的最好的比赛纪录是对在不同特征上训练的留个稀疏自编码器取平均，top-1和top-5测试误差分别是47.1%和28.2%【2】。</span></p> <p><span style="font-size:18px">之后，已出版的最佳结果是一种 对两个在不同取样密度的 费舍向量上训练的 分类器取平均的方法，结果是45.7%和25.7%【24】。</span></p> <p><span style="font-size:18px"><img src="https://images2.imgbox.com/4a/86/mQBW9Btg_o.png" alt=""><br> </span></p> <p></p> </td></tr><tr><td> <p><span style="font-size:18px">我们也让我们的模型参加了ILSVRC-2012的比赛，并在表2中展示了我们的结果。因为ILSVRC-2012测试集的标签并未公开，所以我们不能报告我们所有试过的模型的测试错误率。在这一段的余下部分，我们使用验证误差代替测试误差，因为根据我们的经验，它们的差距不会大于0.1%（见表2）。本文介绍的卷积神经网络达到了Top-5错误18.2%的水平。5个相同CNN平均TOP-5错误为16.4%。</span></p> <p><span style="font-size:18px">训练一个比之前说的五个卷积层还多一个卷积层的CNN去分类整个ImageNet Fall 2011数据集（1500万张图，22000个类别），然后对其进行调整，在ILSVRC-2012上可以达到16.6%的TOP-5错误。</span></p> <p><span style="font-size:18px">两个在ImageNet Fall 2011数据集上预训练的CNN，加上前面提到的五个CNN，平均TOP-5为15.3%。比赛的第二名达到了26.2%的TOP-5，他们用的是对 几个在特征取样密度不同的费舍向量上训练的分类器的预测结果取平均的方法【7】。</span></p> <p><span style="font-size:18px"><img src="https://images2.imgbox.com/62/63/2kxOTuIl_o.png" alt=""><br> </span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">6.1 定量分析</span></strong></p> <p><span style="font-size:18px">图3展示了网络的两个数据连接层学到的卷积内核。网络学到了一系列“频率+方向选择”【frequency-and orientation-selective】的内核，还有一系列色块。请注意两个GPU表现除出了不同的特性，这是3.5节介绍的限制互联方式的结果。GPU 1上的内核基本上不在意颜色，而GPU 2 上的内核就是色彩专家。这种专一性每次都会出现，与权值的随机初始化无关（GPU重新编号）。</span></p> </td></tr></tbody></table> 
<img src="https://images2.imgbox.com/f3/83/ZtIZ8LOT_o.png" alt=""> 
<br> 
<p></p> 
<p><span style="font-size:18px">图3：96个通过第一个卷积层学习224x224x3的图片得到的11x11x3的卷积内核。上面48个和下面48个分别由两个GPU学习得到，详见6.1.<br> </span></p> 
<p><span style="font-size:18px"><img src="https://images2.imgbox.com/00/ae/KXJ9oDGz_o.png" alt=""><br> </span></p> 
<p><span style="font-size:18px"></span> </p> 
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td> <p><span style="font-size:18px">在图四的左侧，我们定量地展示了对于8张图片网络所学习到的前五个预测。注意对于偏离中心的物体，比如左上角的那只螨虫，网络依然可以识别出来。大多数前五个标签看起来都比较合理，比如，只有其他类别的猫科动物才被判别是豹子的可能标签。在一些例子中，比如栅栏，樱桃，确实对于究竟该关注哪个物体存在歧义。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">另一个研究可视化网络所学知识的方法是考虑最后一个4096维隐层所激活的特征向量。</span></p> <p><span style="font-size:18px">如果两张图的向量欧氏距离很小，我们可以说很大程度上神经网络认为它们是相似的。图4展示了五张测试集中的图片，以及按照上述方法找出的分别与这五张图最相似的6张训练集图片。</span></p> <p><span style="font-size:18px">注意在像素尺度上，找出来的训练集图片不一定在L2【？】上和第一列的测试集图片很相似。比如，找出来的狗狗和大象摆出了不同的造型。我们用更多的测试集图片支持证明了这一观点。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">通过两个4096维的实数向量之间的欧氏距离来计算相似度显然效率很低，但可以通过训练一个自编码器去把这些向量压缩为二进制编码来提高效率。这应该能够产生一种比 对原始像素进行自编码【14】 更好的图像检索方法，因为（对原始像素进行自编码）用不到标签，因此它倾向于找出具有同样边缘模式的图片，而不是语义上相似的图。</span></p> </td></tr><tr><td> <p><strong><span style="font-size:18px">7 讨论</span></strong></p> <p><span style="font-size:18px">我们的结果显示一个大型深度卷积神经网络能够在一个极具挑战的数据集上进行破纪录的纯粹的监督学习。值得注意的是，如果把我们的网络去掉一层卷积层，表现就会变差。比如，去掉任意隐藏层会让top-1错误增加2%，所以深度对于我们的成功真的很重要。</span></p> </td></tr><tr><td> <p><span style="font-size:18px">为了简化我们的实验，我们并未使用非监督的预训练，即使我们知道这样会有帮助，特别是如果我们能够获得足够的计算力去大幅提升网络规模却不相应地增加标签数据的数量。至此，我们的结果已经通过增大我们的网络规模、进行更长时间的训练而得到优化。</span></p> <p><span style="font-size:18px">但我们还有很大的空间去优化网络使之能够像人类的视觉系统一样感知时序。</span></p> <p><span style="font-size:18px">最终我们希望在视频序列上使用极大极深的卷积神经网，因为视频序列的时序结构能够提供丰富的信息，这些信息在静态图片上丢失了，或远远没有那么明显。</span></p> </td></tr></tbody></table> 
<br> 
<p></p> 
<p><span style="font-size:18px">以上就是我全文翻译的AlexNet《ImageNet Classification with Deep Convolutional Neural Networks》。再次重申：因为这是我第一次全文翻译英文学术文章，加上英语水平和专业水平都有限，所以肯定有很多不准确的地方，所有标有【？】的位置是我自己还有疑问之处，欢迎各位批评指正！！！可以在博客上留言，也可以发邮件到motianchi@163.com与我交流。</span></p> 
<p><span style="font-size:18px"><br> </span></p> 
<p><span style="font-size:18px">预告：既然现在阿法狗很火，下一篇凑个热闹翻译GoogleNet好啦。</span></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2db1d77fc5ab30f1319255e2f04aee43/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">前端：客户端渲染 vs 服务器渲染</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/34f01bc6d26caaa1794101d6ad2627dd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Eclipse快速添加get、set方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>