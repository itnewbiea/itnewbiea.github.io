<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>百度Ai studio上运行pytorch和tensorflow（转载） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="百度Ai studio上运行pytorch和tensorflow（转载）" />
<meta property="og:description" content="转载 链接：https://www.zhihu.com/question/336485090/answer/1017905011
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
5.4.2020更新（要看正文的，直接跳到原始正文部分）：
大家去看看微软发布了一年半的牛逼项目 ： nni https://github.com/microsoft/nni/blob/master/README_zh_CN.md简单说，这个就是深度学习，神经网络的自动管理包。官方把中文做第二语言，中文的read me。但，重点：看它支持的框架………………什么叫主流？呵呵呵。一年半啦，没……浆。做企业，做开发……………………………………………………………………做人，都不能：自欺欺人啊！格局决定高度，高度决定未来！4.27.2020更新：果然，升级后，持续化环境，直接写在新开的book头部了，大家自己整。把我下面写的东东，扔进去就ok。其实对于kaggle，colab，那些大型的包一样可以这么整。但这个对于百度来说，尤其需要，为啥，因为咱的网络环境下，git和wget在他家是残疾的。附送大家一句尤其重要的代码（飞浆也置了顶，只是它只是写为了pip持续化，而没把核心原理讲清楚。另，.append这个写法我不喜欢，so）：import syssys.path &#43;=[’/usr/local/lib/python3.7/site-packages’,’/到/你的/工作目录/路径’]在最后这个路径文件夹下的所有包（不含路径名本身），都可以以层级关系 import XX，from XX import XX。虽说是python指定路径的常识，但我发现很多兄弟不明白怎么玩包，玩自定义程序。那你不是每次干程序，都得写一堆码？？？？？随着DL3的工业级部署成熟，框架HUB的时代来临了。我觉得很多小伙儿不要钻牛角尖，我们是使用灯泡的，不是研发生产灯泡的。人工智能的细化、分类，一定是越来越垂直。你只需要在你的那个分支够优秀，够深入就行，其他的，交给其他的兄弟们。飞浆的HUB中，我推荐语义分割接口，就是DL3-xp65-humanseg。对于图形图像CV类而言，真的省事儿。虽说这货基本完全复制pytorchhub，但是还真比对方还轻便快捷。哈哈哈。让我产生了一种幻觉，到底，谁高仿了谁？另：大家有能力的，去学习一下microsoft/onnxruntime。这货将来也是你三天两头会用的东东（飞浆里的中文教程挺简单的）。4.14.2020更新：github这段时间各种优秀，一堆兄弟疯了似的发新模，各个领域的。估计是疫情大伙儿都在家整研发了，没法出去混了，大家自己去看吧，我觉得量子位应该写不过来了。4.11.2020更新：百度4.5日起调整时长了，从10小时，变成2小时，白天基本上没任何可能开G机，说明社区这个项目问题大了去了。既封闭没法带来高效便利，又满足不了人工智能现阶段的大规模运算需要，这样它比拼其他平台的优势就彻底没了，运营模式上看来要调整了。全世界范围内，任何新的开源内容失去了大面积群体的验证，无论框架有多少优点，都是没有意义的，其发展思维是背离互联网发展需要的。我和一些搞研发的兄弟最近都只能不上去搞。有一次我想让他们把上传限制改一下（现在已经升级了），管理的兄弟告诉我，这个平台的定位是给大学生用的。于是我就纳闷了，为啥？因为，如果给大学生用，那你为啥限制其他框架？很多同学都刚刚开始接触，他们非常需要学习各种成熟的框架，需要最便捷的方式进入，然后一步步的深入，最好的方式是拉码学习，完成自己的功能实现。这一步是需要大量花时间看论文，看别人的git的……所以，当你限制时，就拒绝了大学生和新人。这不是摆在那里自相矛盾吗？
原始正文 我在这里写的方法是：从此，无论你重新开多少个新项目（核心中的核心哦），都不用重装，只需要5秒以内就搞定框架！！！！！！！！！！！！！！！！！！！！！ok，我索性把tensor和torch都全部补完把，先补充whl页面（先别急着下载，看完再下）：
tensorflow-gpu：https://pypi.tuna.tsinghua.edu.cn/simple/tensorflow-gpu/torch-gpu：https://download.pytorch.org/whl/torch_stable.html 虽然，我不喜欢百度的一些作风。但有一句说一句，飞浆有方便的地方，V100香也摆在那里。另，百度ai云端jupyter限制tensorflow和pytorch等框架，但终端至今能用，用的也流畅。我和一些兄弟在他们坛子里一直批评他们狭隘，说他们不支持这几个框架，不是说技术上完全不支持，是说理念上他们发展自己框架的这种策略不合适（为啥？因为很多新技术，往往处于最前沿的研究过程中。不少论文和项目，连作者自己的验证都不够完善（全球开源率不到6%），待全网开源探讨，这时，所有人的流程是，先用最顺手的框架搞定研究本身，再研究转移框架。转移的目的是为了更高效，而不是为了框架而框架，如果一种技术特别适合tensorflow，那就应该tensor，适合咖啡和妈差奶，就应该是咖啡和妈差奶，适合飞浆，那就应该飞浆，不能本末倒置）。百度的这个策略导致自身平台的使用效率降低，反而限制自身发展壮大，无法做活社区，导流过不来，成本就反而高居不下。
至于技术上：
请所有不明白的小伙伴，搞清楚免费版是CPU版，支持毛线的GPU版（colab的笔记本不是一样让你选硬件加速器吗？NONE\GPU\TPU，一个道理）？人那个算力卡版（其实也免费）是v100是gpu版好嘛！进去先nvcc -V再说支持不支持。（发现有小伙伴还是不明白cpu，gpu，cuda，cudnn，nvcc的关系。补：cpu，机器的心脏，处理一切资源的调度、供给分配。gpu，你就理解成显卡的心脏，图形处理器，咱平时说的nvidia就是产这个的。cuda是啥？一个平台，一种框架，一门‘语言’一个用来发挥gpu计算功能的东东。cudnn，函数集，深度学习的函数集子。nvcc就好比gcc，是编译器，编译啥，编译cuda这门‘语言’。大伙儿要再问，我就瞎了，好嘛！）为了防止还有更多小伙伴不理解，我写一下方法： A.torch 方法1（极简，不换虚拟环境，直接来）：进入终端（重点，别有了notebook忘了最基本的东西，linux 最香是终端！）:
1.0.0pip install torch1.0.0 torchvision0.2.1 -i http://pypi.douban.com/simple --trusted-host pypi.douban.com1.2.0pip install torch1.2.0&#43;cu92 torchvision0.4.0&#43;cu92 -f https://download.pytorch.org/whl/torch_stable.html成功。用！看明白没，九成的兄弟错在你装了cpu版的torch或tensor，你指望cuda gpu匹配，咋配，不false才怪！至于省下点算力卡安装，搞持续化目录，我就不写这里了，大家自己研究，免得大家越整越麻烦。个人体会，几个大包下载下来扔数据集后拉回home，然后改成官方标准文件名安装，最方便。另，torch和torchvision必须同时装（指网络安装，whl无视），分开的话，会丫的帮你升级torch到1.4。好，你喜欢折腾，迷恋conda（有时是非它不可），
方法2：
进入终端，然后：
step1，source activate -n python35-paddle120-env （楼上都写了）
step2，conda install -n python35-paddle120-env pytorch1.2.0 torchvision0.4.0 cudatoolkit==9.2 -c Tsinghua Open Source Mirror --yes；
上面那几个源的http地址，知乎显示成链接缩写了，请不要自己瞎编适配，务必去pytorch的官网和tensorflow的官网复制黏贴一下，都明明白白写着命令。pytorch也好，tensorflow也好，对自身版本，显卡驱动，cuda版本，cudnn版本的匹配要求很严格，尤其是gpu版本和cpu版本是完全不同的，在pip安装时尤其要注意。飞浆云端是cuda9.2（这是值得吐槽的，9.2是猴年马月的事情了。因为适配上他们做的效率低了，光想着飞浆怎么适配，用户可不会等你），我们可以升级成10.0以上，但显卡的驱动是要root的（也就是说，如果你开机拿到的是16G的gpu机子，你只能9.2，你运气好，开机拿到的是32G的机子，就可以10.0&#43;，这个看人品，是百度随机的，其模式和colab一样，人品不好：k80，呵呵），不像cuda可以随便改动，所以，9.2简单使用，10.0、10.2我写在下面tensorflow安装的例子里。conda安装源不顺，挤爆，讲个小技巧：除换源外，下载过程中遇到不顺的包，先跳过，终端ctrl&#43;c，ok，待会把conda install再执行一次，就ok了。当然，长期使用，有些大包，直接下下来，然后放进数据集，拉进home，从此清净！ok！贴图：
B.tensorflow-gpu（1.15.0为例）方法（只需要第一次这样装，以后极简，下文我全部写清楚的）： cuda10.2无root权限静默安装，飞浆开机home目录下：
wget http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run 下载cudnn-10.2.tgz（这个得你们注册下载，然后放到根目录）静默安装，就一句：
sh cuda_10." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/6c02c119d955e28588a6f561338388ba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-26T10:47:57+08:00" />
<meta property="article:modified_time" content="2020-10-26T10:47:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">百度Ai studio上运行pytorch和tensorflow（转载）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>转载</h2> 
<p>链接：<a href="https://www.zhihu.com/question/336485090/answer/1017905011" rel="nofollow">https://www.zhihu.com/question/336485090/answer/1017905011</a><br> 来源：知乎<br> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p> 
<p><strong>5.4.2020更新（要看正文的，直接跳到原始正文部分）：</strong><br> 大家去看看微软发布了一年半的牛逼项目 ： nni https://github.com/microsoft/nni/blob/master/README_zh_CN.md简单说，这个就是深度学习，神经网络的自动管理包。官方把中文做第二语言，中文的read me。但，重点：看它支持的框架………………什么叫主流？呵呵呵。一年半啦，没……浆。做企业，做开发……………………………………………………………………做人，都不能：自欺欺人啊！格局决定高度，高度决定未来！4.27.2020更新：果然，升级后，持续化环境，直接写在新开的book头部了，大家自己整。把我下面写的东东，扔进去就ok。其实对于kaggle，colab，那些大型的包一样可以这么整。但这个对于百度来说，尤其需要，为啥，因为咱的网络环境下，git和wget在他家是残疾的。附送大家一句尤其重要的代码（飞浆也置了顶，只是它只是写为了pip持续化，而没把核心原理讲清楚。另，.append这个写法我不喜欢，so）：import syssys.path +=[’/usr/local/lib/python3.7/site-packages’,’/到/你的/工作目录/路径’]在最后这个路径文件夹下的所有包（不含路径名本身），都可以以层级关系 import XX，from XX import XX。虽说是python指定路径的常识，但我发现很多兄弟不明白怎么玩包，玩自定义程序。那你不是每次干程序，都得写一堆码？？？？？随着DL3的工业级部署成熟，框架HUB的时代来临了。我觉得很多小伙儿不要钻牛角尖，我们是使用灯泡的，不是研发生产灯泡的。人工智能的细化、分类，一定是越来越垂直。你只需要在你的那个分支够优秀，够深入就行，其他的，交给其他的兄弟们。飞浆的HUB中，我推荐语义分割接口，就是DL3-xp65-humanseg。对于图形图像CV类而言，真的省事儿。虽说这货基本完全复制pytorchhub，但是还真比对方还轻便快捷。哈哈哈。让我产生了一种幻觉，到底，谁高仿了谁？另：大家有能力的，去学习一下microsoft/onnxruntime。这货将来也是你三天两头会用的东东（飞浆里的中文教程挺简单的）。4.14.2020更新：github这段时间各种优秀，一堆兄弟疯了似的发新模，各个领域的。估计是疫情大伙儿都在家整研发了，没法出去混了，大家自己去看吧，我觉得量子位应该写不过来了。4.11.2020更新：百度4.5日起调整时长了，从10小时，变成2小时，白天基本上没任何可能开G机，说明社区这个项目问题大了去了。既封闭没法带来高效便利，又满足不了人工智能现阶段的大规模运算需要，这样它比拼其他平台的优势就彻底没了，运营模式上看来要调整了。全世界范围内，任何新的开源内容失去了大面积群体的验证，无论框架有多少优点，都是没有意义的，其发展思维是背离互联网发展需要的。我和一些搞研发的兄弟最近都只能不上去搞。有一次我想让他们把上传限制改一下（现在已经升级了），管理的兄弟告诉我，这个平台的定位是给大学生用的。于是我就纳闷了，为啥？因为，如果给大学生用，那你为啥限制其他框架？很多同学都刚刚开始接触，他们非常需要学习各种成熟的框架，需要最便捷的方式进入，然后一步步的深入，最好的方式是拉码学习，完成自己的功能实现。这一步是需要大量花时间看论文，看别人的git的……所以，当你限制时，就拒绝了大学生和新人。这不是摆在那里自相矛盾吗？</p> 
<h2><a id="_7"></a><strong>原始正文</strong></h2> 
<p>我在这里写的方法是：从此，无论你重新开多少个新项目（核心中的核心哦），都不用重装，只需要5秒以内就搞定框架！！！！！！！！！！！！！！！！！！！！！ok，我索性把tensor和torch都全部补完把，先补充whl页面（先别急着下载，看完再下）：</p> 
<pre><code class="prism language-python">tensorflow<span class="token operator">-</span>gpu：https<span class="token punctuation">:</span><span class="token operator">//</span>pypi<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>simple<span class="token operator">/</span>tensorflow<span class="token operator">-</span>gpu<span class="token operator">/</span>torch<span class="token operator">-</span>gpu：https<span class="token punctuation">:</span><span class="token operator">//</span>download<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>org<span class="token operator">/</span>whl<span class="token operator">/</span>torch_stable<span class="token punctuation">.</span>html
</code></pre> 
<p>虽然，我不喜欢百度的一些作风。但有一句说一句，飞浆有方便的地方，V100香也摆在那里。另，百度ai云端jupyter限制tensorflow和pytorch等框架，但终端至今能用，用的也流畅。我和一些兄弟在他们坛子里一直批评他们狭隘，说他们不支持这几个框架，不是说技术上完全不支持，是说理念上他们发展自己框架的这种策略不合适（为啥？因为很多新技术，往往处于最前沿的研究过程中。不少论文和项目，连作者自己的验证都不够完善（全球开源率不到6%），待全网开源探讨，这时，所有人的流程是，先用最顺手的框架搞定研究本身，再研究转移框架。转移的目的是为了更高效，而不是为了框架而框架，如果一种技术特别适合tensorflow，那就应该tensor，适合咖啡和妈差奶，就应该是咖啡和妈差奶，适合飞浆，那就应该飞浆，不能本末倒置）。百度的这个策略导致自身平台的使用效率降低，反而限制自身发展壮大，无法做活社区，导流过不来，成本就反而高居不下。<br> 至于技术上：</p> 
<ol><li>请所有不明白的小伙伴，搞清楚免费版是CPU版，支持毛线的GPU版（colab的笔记本不是一样让你选硬件加速器吗？NONE\GPU\TPU，一个道理）？人那个算力卡版（其实也免费）是v100是gpu版好嘛！进去先nvcc -V再说支持不支持。（发现有小伙伴还是不明白cpu，gpu，cuda，cudnn，nvcc的关系。补：cpu，机器的心脏，处理一切资源的调度、供给分配。gpu，你就理解成显卡的心脏，图形处理器，咱平时说的nvidia就是产这个的。cuda是啥？一个平台，一种框架，一门‘语言’一个用来发挥gpu计算功能的东东。cudnn，函数集，深度学习的函数集子。nvcc就好比gcc，是编译器，编译啥，编译cuda这门‘语言’。大伙儿要再问，我就瞎了，好嘛！）</li><li>为了防止还有更多小伙伴不理解，我写一下方法：</li></ol> 
<h4><a id="Atorch_18"></a>A.torch</h4> 
<p><strong>方法1</strong>（极简，不换虚拟环境，直接来）：进入终端（重点，别有了notebook忘了最基本的东西，linux 最香是终端！）:<br> 1.0.0pip install torch<mark>1.0.0 torchvision</mark>0.2.1 -i http://pypi.douban.com/simple --trusted-host pypi.douban.com1.2.0pip install torch<mark>1.2.0+cu92 torchvision</mark>0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html<img src="https://images2.imgbox.com/d0/f7/TGQc8VUr_o.jpg" class="origin_image zh-lightbox-thumb" width="1487">成功。用！看明白没，九成的兄弟错在你装了cpu版的torch或tensor，你指望cuda gpu匹配，咋配，不false才怪！至于省下点算力卡安装，搞持续化目录，我就不写这里了，大家自己研究，免得大家越整越麻烦。个人体会，几个大包下载下来扔数据集后拉回home，然后改成官方标准文件名安装，最方便。另，torch和torchvision必须同时装（指网络安装，whl无视），分开的话，会丫的帮你升级torch到1.4。好，你喜欢折腾，迷恋conda（有时是非它不可），<br> <strong>方法2：</strong><br> 进入终端，然后：<br> step1，source activate -n python35-paddle120-env （楼上都写了）<br> step2，conda install -n python35-paddle120-env pytorch<mark>1.2.0 torchvision</mark>0.4.0 cudatoolkit==9.2 -c Tsinghua Open Source Mirror --yes；<br> 上面那几个源的http地址，知乎显示成链接缩写了，请不要自己瞎编适配，务必去pytorch的官网和tensorflow的官网复制黏贴一下，都明明白白写着命令。pytorch也好，tensorflow也好，对自身版本，显卡驱动，cuda版本，cudnn版本的匹配要求很严格，尤其是gpu版本和cpu版本是完全不同的，在pip安装时尤其要注意。飞浆云端是cuda9.2（这是值得吐槽的，9.2是猴年马月的事情了。因为适配上他们做的效率低了，光想着飞浆怎么适配，用户可不会等你），我们可以升级成10.0以上，但显卡的驱动是要root的（也就是说，如果你开机拿到的是16G的gpu机子，你只能9.2，你运气好，开机拿到的是32G的机子，就可以10.0+，这个看人品，是百度随机的，其模式和colab一样，人品不好：k80，呵呵），不像cuda可以随便改动，所以，9.2简单使用，10.0、10.2我写在下面tensorflow安装的例子里。conda安装源不顺，挤爆，讲个小技巧：除换源外，下载过程中遇到不顺的包，先跳过，终端ctrl+c，ok，待会把conda install再执行一次，就ok了。当然，长期使用，有些大包，直接下下来，然后放进数据集，拉进home，从此清净！ok！贴图：<img src="https://images2.imgbox.com/e6/45/EU6nJddY_o.jpg" class="origin_image zh-lightbox-thumb" width="1388"><img src="https://images2.imgbox.com/5e/cf/jFU9hz6q_o.jpg" class="origin_image zh-lightbox-thumb" width="1300"></p> 
<h4><a id="Btensorflowgpu1150_26"></a>B.tensorflow-gpu（1.15.0为例）方法（只需要第一次这样装，以后极简，下文我全部写清楚的）：</h4> 
<p>cuda10.2无root权限静默安装，飞浆开机home目录下：</p> 
<pre><code class="prism language-python"> wget http<span class="token punctuation">:</span><span class="token operator">//</span>developer<span class="token punctuation">.</span>download<span class="token punctuation">.</span>nvidia<span class="token punctuation">.</span>com<span class="token operator">/</span>compute<span class="token operator">/</span>cuda<span class="token operator">/</span><span class="token number">10.2</span><span class="token operator">/</span>Prod<span class="token operator">/</span>local_installers<span class="token operator">/</span>cuda_10<span class="token punctuation">.</span><span class="token number">2.</span>89_440<span class="token punctuation">.</span><span class="token number">33.</span>01_linux<span class="token punctuation">.</span>run
</code></pre> 
<p>下载cudnn-10.2.tgz（这个得你们注册下载，然后放到根目录）静默安装，就一句：<br> <code>sh cuda_10.2.89_440.33.01_linux.run --silent --toolkit --toolkitpath=$HOME/cuda_10.2 --installpath=$HOME/cuda_10.2</code>（针对百度数据集，文件上传时字数限制，我重命名为：cuda_10.2.89.run；然后一样</p> 
<pre><code class="prism language-python"> sh cuda_10<span class="token punctuation">.</span><span class="token number">2.89</span><span class="token punctuation">.</span>run <span class="token operator">-</span><span class="token operator">-</span>silent <span class="token operator">-</span><span class="token operator">-</span>toolkit <span class="token operator">-</span><span class="token operator">-</span>toolkitpath<span class="token operator">=</span>$HOME<span class="token operator">/</span>cuda_10<span class="token punctuation">.</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>installpath<span class="token operator">=</span>$HOME<span class="token operator">/</span>cuda_10<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> 
<p>接下来：</p> 
<pre><code class="prism language-python"> tar <span class="token operator">-</span>zxvf cudnn<span class="token operator">-</span><span class="token number">10.2</span><span class="token punctuation">.</span>tgzcp cuda<span class="token operator">/</span>include<span class="token operator">/</span>cudnn<span class="token punctuation">.</span>h cuda_10<span class="token punctuation">.</span><span class="token number">2</span><span class="token operator">/</span>include<span class="token operator">/</span>cp cuda<span class="token operator">/</span>lib64<span class="token operator">/</span>libcudnn<span class="token operator">*</span> cuda_10<span class="token punctuation">.</span><span class="token number">2</span><span class="token operator">/</span>lib64<span class="token operator">/</span>chmod a<span class="token operator">+</span>r cuda_10<span class="token punctuation">.</span><span class="token number">2</span><span class="token operator">/</span>include<span class="token operator">/</span>cudnn<span class="token punctuation">.</span>h <span class="token comment">#每次重启使用还需执行的语句，我们待会儿会一并写入shell，一劳永逸。</span>
</code></pre> 
<p><code>chmod a+r cuda_10.2/lib64/libcudnn* #每次重启使用还需执行的语句，我们待会儿会一并写入</code>shell，一劳永逸。在根目录新建一个名为‘环境变量’（你可以写你自己喜欢的名，呵呵）的文件方便长期执行export。然后，</p> 
<pre><code class="prism language-python"> echo <span class="token operator">-</span>e <span class="token string">'export PATH=$HOME/cuda_10.2/bin:$PATH\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/cuda_10.2/lib64'</span><span class="token operator">&gt;</span><span class="token operator">~</span><span class="token operator">/</span>环境变量
</code></pre> 
<pre><code class="prism language-python"> source <span class="token operator">~</span><span class="token operator">/</span>环境变量 <span class="token comment">#下次重启还需要执行的语句，我们待会儿会一并写入shell，一劳永逸。</span>
</code></pre> 
<p><code>pip install tensorflow-gpu==1.15.0</code>（由于源的速度不稳定，我建议下载whl包放在根目录执行，大家别忘记上传数据集后把名字改回成pip包原标准名），即<br> <code>pip install tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl</code>好了，用吧。<br> 测试：</p> 
<pre><code class="prism language-python"> <span class="token keyword">import</span> tensorflow 
 <span class="token keyword">as</span> tf sess <span class="token operator">=</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> 
 a <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> 
 b <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> 
 <span class="token keyword">print</span><span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>a<span class="token operator">+</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span> 
</code></pre> 
<p>结果: 32.0的tensor一样，10.2的cuda都支持。1.13.1等版本，对应的是cuda10.0，静默安装为（一个主文件48_linux后缀，一个补丁run后缀）：<br> <code>sh cuda_10.0.48_linux --silent --toolkit --toolkitpath=$HOME/cuda_10.0sh cuda_10.0.run --silent --installdir=$HOME/cuda_10.0 --accept-eula</code>(剩下cudnn复制和赋权和上面10.2版本一样，我就不写了）如图：<img src="https://images2.imgbox.com/b9/f3/w3QXUUuv_o.jpg" class="origin_image zh-lightbox-thumb" width="1920"><br> 写个shell，命名为<code>chmod_cuda102.sh</code>（名字随便起，如果你像我一样同时装多个版本cuda来回切换，那标注还是比较明确为好），shell文件内容如下<br> ：#!/bin/bash</p> 
<pre><code class="prism language-python"> chmod a<span class="token operator">+</span>r <span class="token operator">~</span><span class="token operator">/</span>cuda_10<span class="token punctuation">.</span><span class="token number">2</span><span class="token operator">/</span>include<span class="token operator">/</span>cudnn<span class="token punctuation">.</span>h
 chmod a<span class="token operator">+</span>r <span class="token operator">~</span><span class="token operator">/</span>cuda_10<span class="token punctuation">.</span><span class="token number">2</span><span class="token operator">/</span>lib64<span class="token operator">/</span>libcudnn<span class="token operator">*</span>source <span class="token operator">~</span><span class="token operator">/</span>环境变量
 pip install <span class="token operator">~</span><span class="token operator">/</span>tensorflow_gpu<span class="token operator">-</span><span class="token number">1.15</span><span class="token number">.0</span><span class="token operator">-</span>cp37<span class="token operator">-</span>cp37m<span class="token operator">-</span>manylinux2010_x86_64<span class="token punctuation">.</span>whlok
</code></pre> 
<p>，从此以后，重启进入平台系统，一劳永逸，就一句：<code>source chmod_cuda102.sh</code>好，核心中的核心来了，看好啦！！！！！！！你想每次开新项目都几秒搞定：zip -r 打包刚才你折腾出来的cuda文件夹，外加那个‘环境变量’文件和那个sh，然后下载后，上传你自己的数据集（这个过程第一次的时间较长，得花，值），以后想整哪个框架就哪个，想啥版本CudaTorchTensor就啥版本。我称这个方法为：cuda镜像大法！啊哈哈哈！没想到这么赖皮是吗？？？从今…………………………………………………………………………………………………………………………………………………………………………………………………………………6666666666666666666666666……………………以后，都………………几秒完！结束。这个方法的效率远大于conda install，无视百度内网的速度，无视安装源，极简。今儿在这里写这么多是因为在飞浆坛子里看到无数人在问，时间上竟然这个月还有，于是浪费我的算力卡写教程吧，希望能帮到大家，更希望伙伴们在人工智能领域能有所作为！另，我写这么一大段，是因为早年被折磨过，不是被飞浆，是被cuda折磨，当时很惨，我三大系统都得装，还5、6台机器……所以，祝大家能顺利！最后，呼应开篇，对于百度：坏的得骂，绝不留情，好的得赞，绝不吝啬！</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8a743d29145ac5b1565b63c99ee4b334/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">(主机)ARP表(ip/mac)、(交换机)交换/转发/MAC表(mac端口 同一网段)、(路由器)路由表(不同网段)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4a7f2b65bb342061f3bc850b969c3540/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">2020-10-26</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>