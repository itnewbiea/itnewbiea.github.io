<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《动手深度学习》线性回归简洁实现实例 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="《动手深度学习》线性回归简洁实现实例" />
<meta property="og:description" content="🎈 作者：Linux猿
🎈 简介：CSDN博客专家🏆，华为云享专家🏆，Linux、C/C&#43;&#43;、云计算、物联网、面试、刷题、算法尽管咨询我，关注我，有问题私聊！
🎈 欢迎小伙伴们点赞👍、收藏⭐、留言💬
本文是《动手深度学习》线性回归简洁实现实例的实现和分析，主要对代码进行详细讲解，有问题欢迎在评论区讨论交流。
一、代码实现 实现代码如下所示。
import torch from torch.utils import data # d2l包是李沐老师等人开发的动手深度学习配套的包, # 里面封装了很多有关与数据集定义，数据预处理，优化损失函数的包 from d2l import torch as d2l # nn 是神经网络 Neural Network 的缩写，提供了一系列的模块和类，实现创建、训练、保存、恢复神经网络 from torch import nn &#39;&#39;&#39; 1. 生成数据集，共 1000 条 true_w 和 true_b 是临时变量用于生成数据集 生成 X, y ：满足关系 y = Xw &#43; b &#43; noise &#39;&#39;&#39; true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = d2l.synthetic_data(true_w, true_b, 1000) &#39;&#39;&#39; 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/07d4a43a84d4aeba104f42310886679e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-31T08:00:00+08:00" />
<meta property="article:modified_time" content="2023-10-31T08:00:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《动手深度学习》线性回归简洁实现实例</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <hr> 
<p><strong>🎈 作者：</strong><a href="https://blog.csdn.net/nyist_zxp" title="Linux猿">Linux猿</a></p> 
<p><strong>🎈 简介：</strong>CSDN博客专家🏆，华为云享专家🏆，Linux、C/C++、云计算、物联网、面试、刷题、算法尽管咨询我，关注我，有问题私聊！</p> 
<p><strong>🎈 </strong>欢迎小伙伴们点赞👍、收藏⭐、留言💬</p> 
<hr> 
<p>本文是《动手深度学习》线性回归简洁实现实例的实现和分析，主要对代码进行详细讲解，有问题欢迎在评论区讨论交流。</p> 
<h2>一、代码实现</h2> 
<p>实现代码如下所示。</p> 
<pre><code class="language-python">import torch
from torch.utils import data
# d2l包是李沐老师等人开发的动手深度学习配套的包,
# 里面封装了很多有关与数据集定义，数据预处理，优化损失函数的包
from d2l import torch as d2l
# nn 是神经网络 Neural Network 的缩写，提供了一系列的模块和类，实现创建、训练、保存、恢复神经网络
from torch import nn

'''
1. 生成数据集，共 1000 条
true_w 和 true_b 是临时变量用于生成数据集
生成 X, y ：满足关系 y = Xw + b + noise
'''
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

'''
2. 构造循环读取数据集的迭代器
'''
def load_array(data_arrays, batch_size, is_train=True):  #@save
    # 构造一个 PyTorch 数据迭代器，对 tensor 进行打包，包装成 dataset。
    dataset = data.TensorDataset(*data_arrays)
    # 根据数据集构造一个迭代器
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

# 小批量数据
batch_size = 10
# 设置了一个数据读取的迭代器，每次读取 batch_size(10) 条
data_iter = load_array((features, labels), batch_size)

'''
3. 设置全连接层
'''
'''
# nn.Linear(in_features, out_features, bias=True)
# in_features : 输入向量的列数
# out_features : 输出向量的列数
# bias = True 是否包含偏置
执行线性变换：Yn*o = Xn*i Wi*o + b
其中：W 和 b 模型需要学习的参数
在本例中：n = 10，i = 2, o = 1
'''
net = nn.Sequential(nn.Linear(2, 1))
# 设置权重 w 和 偏置 b
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

'''
4. 定义损失函数
'''
# 均方误差，是预测值与真实值之差的平方和的平均值
loss = nn.MSELoss()
# lr 学习率 learning rate
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

'''
4. 训练数据
'''
# 超参数 设置批次
num_epochs = 3
for epoch in range(num_epochs): # 进行 num_epochs 个迭代周期
    for X, y in data_iter:
        l = loss(net(X) ,y) # 计算损失，net(X) 计算预测值 y1，loss(y1, y) 计算预测值和真实值之间的差距
        trainer.zero_grad() # 将所有模型参数的梯度置为 0
        l.backward() # 求梯度，不使用从零实现中 l.sum.backward 的原因是损失计算中使用了平均的 gard
        trainer.step() # 优化参数 w 和 b
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('b的估计误差：', true_b - b)</code></pre> 
<p></p> 
<h2>二、实现解析</h2> 
<p>针对实例中重要的函数解析如下。</p> 
<h3>2.1 Linear 函数</h3> 
<p>nn.Linear(in_features, out_features, bias=True)</p> 
<p>神经网络的线性层，也成为全连接层，进行 Y = XW + b 的线性变换。</p> 
<p><strong>参数：</strong></p> 
<p><strong>in_features : </strong>输入向量的列数</p> 
<p><strong>out_features :</strong> 输出向量的列数</p> 
<p><strong>bias = True</strong> 是否包含偏置</p> 
<p>in_features 和 out_features 是 W 的行和列。</p> 
<p>执行线性变换：Yn*o = Xn*i Wi*o + b</p> 
<p>其中：W 和 b 模型需要学习的参数</p> 
<p>在本例中：n = 10，i = 2, o = 1。</p> 
<h3>2.2 Sequential 函数</h3> 
<p>一个序列容器，用于搭建神经网络的模块，按照传入构造器的顺序添加到 nn.Sequential() 容器中。按照内部模块的顺序自动依次计算并输出结果。</p> 
<p></p> 
<h3>2.3 MSELoss 函数</h3> 
<p>均方误差，是预测值与真实值之差的平方和的平均值，即：</p> 
<p class="img-center"><img alt="" height="94" src="https://images2.imgbox.com/20/56/32rEb5Le_o.png" width="268"></p> 
<h3>2.4 TensorDataset 函数</h3> 
<p>用来对 tensor 进行打包，就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等. 另外：TensorDataset 中的参数必须是 tensor。可以参考如下例子：</p> 
<pre><code class="language-python">import torch
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader

# len = 12
a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9]])
# len = 12
b = torch.tensor([44, 55, 66, 44, 55, 66, 44, 55, 66, 44, 55, 66])
# 将 tensor a 和 b 压缩在一起
train_ids = TensorDataset(a, b)
# 输出
for x, y in train_ids:
    print(x, y)</code></pre> 
<p>输出如下：</p> 
<pre><code class="language-python">tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)
tensor([1, 2, 3]) tensor(44)
tensor([4, 5, 6]) tensor(55)
tensor([7, 8, 9]) tensor(66)</code></pre> 
<p></p> 
<h3>2.5 DataLoader 函数</h3> 
<p>DataLoader 是用来包装所使用的数据，每次抛出一批数据，下面来看一个例子。</p> 
<pre><code class="language-python">import torch
from torch.utils import data

# len = 12
a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6], [7, 8, 9]])
# len = 12
b = torch.tensor([44, 55, 66, 44, 55, 66, 44, 55, 66, 44, 55, 66])
# 将 tensor a 和 b 压缩在一起
train_ids = data.TensorDataset(a, b)
# 输出
#for x, y in train_ids:
#    print(x, y)

BATCH_SIZE = 4
loader = data.DataLoader(dataset=train_ids,
                         batch_size=BATCH_SIZE, # 每次取 BATCH_SIZE=4 个数据
                         shuffle=False, # 不打乱顺序,便于查看
                         num_workers=0)

for x, y in loader:
    print(x, y)
    break</code></pre> 
<p>输出如下：</p> 
<pre><code class="language-python">tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9],
        [1, 2, 3]]) tensor([44, 55, 66, 44])</code></pre> 
<p> 如上所示，输出第一个 BATCH_SIZE=4。</p> 
<p></p> 
<h3>2.6 zero_grad 函数</h3> 
<p>trainer.zero_grad() 是用来清空模型参数梯度的函数，它将模型参数的梯度缓存设置为 0。在进行反向传播时，梯度会累加，如果不清空梯度，会影响后续的梯度计算。</p> 
<p></p> 
<h3>2.7 backward 函数</h3> 
<p>对计算图进行梯度计算，求解计算图中所有节点的梯度。</p> 
<p></p> 
<h3>2.8 step 函数</h3> 
<p>根据 backward 函数计算出的梯度进行参数更新。</p> 
<p></p> 
<p>参考链接：</p> 
<p><a href="https://blog.csdn.net/weixin_45360119/article/details/123346305" title="线性回归的实现学习_data.tensordataset_带刺的厚崽的博客-CSDN博客">线性回归的实现学习_data.tensordataset_带刺的厚崽的博客-CSDN博客</a></p> 
<p><a href="https://blog.csdn.net/Just_do_myself/article/details/124195393" title="nn.Sequential()_一颗磐石的博客-CSDN博客">nn.Sequential()_一颗磐石的博客-CSDN博客</a></p> 
<p><a href="https://blog.csdn.net/zfhsfdhdfajhsr/article/details/115637954" title="【Pytorch基础】torch.nn.MSELoss损失函数_一穷二白到年薪百万的博客-CSDN博客">【Pytorch基础】torch.nn.MSELoss损失函数_一穷二白到年薪百万的博客-CSDN博客</a></p> 
<p><a href="https://blog.csdn.net/yuebowhu/article/details/118099124" title="pytorch之trainer.zero_grad()_FibonacciCode的博客-CSDN博客">pytorch之trainer.zero_grad()_FibonacciCode的博客-CSDN博客</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/622690174" rel="nofollow" title="清空模型参数梯度的函数 - 知乎">清空模型参数梯度的函数 - 知乎</a></p> 
<p><a href="https://blog.csdn.net/sinat_28731575/article/details/90342082" title="pytorch中backward()函数详解_backward函数_Camlin_Z的博客-CSDN博客">pytorch中backward()函数详解_backward函数_Camlin_Z的博客-CSDN博客</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/445009191" rel="nofollow" title="理解Pytorch的loss.backward()和optimizer.step() - 知乎">理解Pytorch的loss.backward()和optimizer.step() - 知乎</a></p> 
<hr> 
<p><strong>🎈 </strong>感觉有帮助记得<span style="color:#fe2c24;"><strong>「一键三连</strong><strong>」</strong></span>支持下哦！有问题可在评论区留言💬，感谢大家的一路支持！🤞猿哥将持续输出<span style="color:#fe2c24;"><strong>「优质文章</strong><strong>」</strong></span>回馈大家！🤞🌹🌹🌹🌹🌹🌹🤞</p> 
<hr> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d298ec6bcdaac0bee9c96914e3726949/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">centos7.4安装tcpreplay显示No package tcpreplay available.</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/44a40e1d63e61f52361289a874666c42/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【问题分析解决】git添加.gitignore后不生效问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>