<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch中的归一化函数 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch中的归一化函数" />
<meta property="og:description" content="在 PyTorch 的 nn 模块中，有一些常见的归一化函数，用于在深度学习模型中进行数据的标准化和归一化。以下是一些常见的归一化函数：
nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d：
这些函数用于批量归一化 (Batch Normalization) 操作。它们可以应用于一维、二维和三维数据，通常用于卷积神经网络中。批量归一化有助于加速训练过程，提高模型的稳定性。
nn.LayerNorm：
Layer Normalization 是一种归一化方法，通常用于自然语言处理任务中。它对每个样本的每个特征进行归一化，而不是对整个批次进行归一化。nn.LayerNorm可用于一维数据。
nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d：
Instance Normalization 也是一种归一化方法，通常用于图像处理任务中。它对每个样本的每个通道进行归一化，而不是对整个批次进行归一化。这些函数分别适用于一维、二维和三维数据。
nn.GroupNorm：
Group Normalization 是一种介于批量归一化和 Instance Normalization 之间的方法。它将通道分成多个组，然后对每个组进行归一化。这个函数可以用于一维、二维和三维数据。
nn.SyncBatchNorm：
SyncBatchNorm 是一种用于分布式训练的归一化方法，它扩展了 Batch Normalization 并支持多 GPU 训练。
这些归一化函数可以根据具体的任务和模型选择使用，以帮助模型更快地收敛，提高训练稳定性，并改善模型的泛化性能。选择哪种归一化方法通常取决于数据的特点和任务的需求。在使用时，可以在 PyTorch 的模型定义中包含这些归一化层，以将它们集成到模型中。
本文主要包括以下内容： 1.归一化函数的函数构成（1）nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d（2）nn.LayerNorm（3）nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d（4） nn.GroupNorm（5）nn.SyncBatchNorm 2.归一化函数的用法（1）nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.BatchNorm3d（2）nn.LayerNorm（3）nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d（4）nn.GroupNorm（5）nn.SyncBatchNorm 3.归一化函数在神经网络中的应用示例（1）Batch Normalization (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)（2） Layer Normalization (nn.LayerNorm)（3）Instance Normalization (nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d) 1.归一化函数的函数构成 PyTorch中的归一化函数都是通过nn模块中的不同类来实现的。这些类都是继承自PyTorch的nn.Module类，它们具有共同的构造函数和一些通用的方法，同时也包括了归一化特定的计算。以下是这些归一化函数的一般函数构成：
（1）nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d 构造函数：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/fd111c912b74bcff76d0ff113c2ea17c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-13T20:57:38+08:00" />
<meta property="article:modified_time" content="2023-10-13T20:57:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch中的归一化函数</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>在 PyTorch 的 <code>nn</code> 模块中，有一些常见的归一化函数，用于在深度学习模型中进行数据的标准化和归一化。以下是一些常见的归一化函数：</p> 
<ol><li> <p><code>nn.BatchNorm1d</code>, <code>nn.BatchNorm2d</code>, <code>nn.BatchNorm3d</code>：<br> 这些函数用于批量归一化 (Batch Normalization) 操作。它们可以应用于一维、二维和三维数据，通常用于卷积神经网络中。批量归一化有助于加速训练过程，提高模型的稳定性。</p> </li><li> <p><code>nn.LayerNorm</code>：<br> Layer Normalization 是一种归一化方法，通常用于自然语言处理任务中。它对每个样本的每个特征进行归一化，而不是对整个批次进行归一化。<code>nn.LayerNorm</code>可用于一维数据。</p> </li><li> <p><code>nn.InstanceNorm1d</code>, <code>nn.InstanceNorm2d</code>, <code>nn.InstanceNorm3d</code>：<br> Instance Normalization 也是一种归一化方法，通常用于图像处理任务中。它对每个样本的每个通道进行归一化，而不是对整个批次进行归一化。这些函数分别适用于一维、二维和三维数据。</p> </li><li> <p><code>nn.GroupNorm</code>：<br> Group Normalization 是一种介于批量归一化和 Instance Normalization 之间的方法。它将通道分成多个组，然后对每个组进行归一化。这个函数可以用于一维、二维和三维数据。</p> </li><li> <p><code>nn.SyncBatchNorm</code>：<br> SyncBatchNorm 是一种用于分布式训练的归一化方法，它扩展了 Batch Normalization 并支持多 GPU 训练。</p> </li></ol> 
<p>这些归一化函数可以根据具体的任务和模型选择使用，以帮助模型更快地收敛，提高训练稳定性，并改善模型的泛化性能。选择哪种归一化方法通常取决于数据的特点和任务的需求。在使用时，可以在 PyTorch 的模型定义中包含这些归一化层，以将它们集成到模型中。</p> 
<p></p> 
<div class="toc"> 
 <h4>本文主要包括以下内容：</h4> 
 <ul><li><a href="#1_20" rel="nofollow">1.归一化函数的函数构成</a></li><li><ul><li><a href="#1nnBatchNorm1d_nnBatchNorm2d_nnBatchNorm3d_23" rel="nofollow">（1）nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d</a></li><li><a href="#2nnLayerNorm_36" rel="nofollow">（2）nn.LayerNorm</a></li><li><a href="#3nnInstanceNorm1d_nnInstanceNorm2d_nnInstanceNorm3d_46" rel="nofollow">（3）nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d</a></li><li><a href="#4_nnGroupNorm_58" rel="nofollow">（4） nn.GroupNorm</a></li><li><a href="#5nnSyncBatchNorm_69" rel="nofollow">（5）nn.SyncBatchNorm</a></li></ul> 
  </li><li><a href="#2_73" rel="nofollow">2.归一化函数的用法</a></li><li><ul><li><a href="#1nnBatchNorm1d_nnBatchNorm2d_nnBatchNorm3d_76" rel="nofollow">（1）nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.BatchNorm3d</a></li><li><a href="#2nnLayerNorm_94" rel="nofollow">（2）nn.LayerNorm</a></li><li><a href="#3nnInstanceNorm1d_nnInstanceNorm2d_nnInstanceNorm3d_112" rel="nofollow">（3）nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d</a></li><li><a href="#4nnGroupNorm_130" rel="nofollow">（4）nn.GroupNorm</a></li><li><a href="#5nnSyncBatchNorm_148" rel="nofollow">（5）nn.SyncBatchNorm</a></li></ul> 
  </li><li><a href="#3_168" rel="nofollow">3.归一化函数在神经网络中的应用示例</a></li><li><ul><li><a href="#1Batch_Normalization_nnBatchNorm1d_nnBatchNorm2d_nnBatchNorm3d_172" rel="nofollow">（1）Batch Normalization (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)</a></li><li><a href="#2_Layer_Normalization_nnLayerNorm_205" rel="nofollow">（2） Layer Normalization (nn.LayerNorm)</a></li><li><a href="#3Instance_Normalization_nnInstanceNorm1d_nnInstanceNorm2d_nnInstanceNorm3d_233" rel="nofollow">（3）Instance Normalization (nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d)</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="1_20"></a>1.归一化函数的函数构成</h2> 
<p>PyTorch中的归一化函数都是通过<code>nn</code>模块中的不同类来实现的。这些类都是继承自PyTorch的<code>nn.Module</code>类，它们具有共同的构造函数和一些通用的方法，同时也包括了归一化特定的计算。以下是这些归一化函数的一般函数构成：</p> 
<h3><a id="1nnBatchNorm1d_nnBatchNorm2d_nnBatchNorm3d_23"></a>（1）nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d</h3> 
<p>构造函数：</p> 
<pre><code class="prism language-python">nn<span class="token punctuation">.</span>BatchNorm<span class="token operator">*</span>d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> track_running_stats<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li><code>*</code>：1，2，3</li><li><code>num_features</code>：输入数据的通道数或特征数。</li><li><code>eps</code>：防止除以零的小值。</li><li><code>momentum</code>：用于计算运行时统计信息的动量。</li><li><code>affine</code>：一个布尔值，表示是否应用仿射变换。</li><li><code>track_running_stats</code>：一个布尔值，表示是否跟踪运行时的统计信息。</li></ul> 
<h3><a id="2nnLayerNorm_36"></a>（2）nn.LayerNorm</h3> 
<p>构造函数：</p> 
<pre><code class="prism language-python">nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>normalized_shape<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li><code>normalized_shape</code>：输入数据的形状，通常是一个整数或元组。</li><li><code>eps</code>：防止除以零的小值。</li><li><code>elementwise_affine</code>：一个布尔值，表示是否应用元素级别的仿射变换。</li></ul> 
<h3><a id="3nnInstanceNorm1d_nnInstanceNorm2d_nnInstanceNorm3d_46"></a>（3）nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d</h3> 
<p>构造函数：</p> 
<pre><code class="prism language-python">nn<span class="token punctuation">.</span>InstanceNorm<span class="token operator">*</span>d<span class="token punctuation">(</span>num_features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> track_running_stats<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li><code>*</code>：1，2，3</li><li><code>num_features</code>：输入数据的通道数或特征数。</li><li><code>eps</code>：防止除以零的小值。</li><li><code>affine</code>：一个布尔值，表示是否应用仿射变换。</li><li><code>track_running_stats</code>：一个布尔值，表示是否跟踪运行时的统计信息。</li></ul> 
<h3><a id="4_nnGroupNorm_58"></a>（4） nn.GroupNorm</h3> 
<p>构造函数：</p> 
<pre><code class="prism language-python">nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">(</span>num_groups<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li><code>num_groups</code>：将通道分成的组数。</li><li><code>num_channels</code>：输入数据的通道数。</li><li><code>eps</code>：防止除以零的小值。</li><li><code>affine</code>：一个布尔值，表示是否应用仿射变换。</li></ul> 
<h3><a id="5nnSyncBatchNorm_69"></a>（5）nn.SyncBatchNorm</h3> 
<ul><li>这个归一化函数通常在分布式训练中使用，它与<code>nn.BatchNorm*d</code>具有相似的构造函数，但还支持分布式计算。</li></ul> 
<p>这些归一化函数的构造函数参数可能会有所不同，但它们都提供了一种方便的方式来创建不同类型的归一化层，以用于深度学习模型中。一旦创建了这些层，您可以将它们添加到模型中，然后通过前向传播计算归一化的输出。</p> 
<h2><a id="2_73"></a>2.归一化函数的用法</h2> 
<p>这些函数都是 PyTorch 中用于规范化（Normalization）的函数，它们用于在深度学习中处理输入数据以提高训练稳定性和模型性能。</p> 
<h3><a id="1nnBatchNorm1d_nnBatchNorm2d_nnBatchNorm3d_76"></a>（1）nn.BatchNorm1d<code>, </code>nn.BatchNorm2d<code>, </code>nn.BatchNorm3d</h3> 
<p>这是批标准化（Batch Normalization）的函数，用于规范化输入数据。它在训练深度神经网络时有助于加速收敛，提高稳定性。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 以二维输入为例（2D图像数据）</span>
input_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有4个样本，每个样本是3通道的32x32图像</span>

<span class="token comment"># 创建 Batch Normalization 层</span>
batch_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>

<span class="token comment"># 对输入数据进行规范化</span>
output <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="2nnLayerNorm_94"></a>（2）nn.LayerNorm</h3> 
<p>层标准化（Layer Normalization）通常用于自然语言处理（NLP）中，用于规范化神经网络中的层级数据。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 以二维输入为例</span>
input_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有4个样本，每个样本有3个特征</span>

<span class="token comment"># 创建 Layer Normalization 层</span>
layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>

<span class="token comment"># 对输入数据进行规范化</span>
output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="3nnInstanceNorm1d_nnInstanceNorm2d_nnInstanceNorm3d_112"></a>（3）nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d</h3> 
<p>实例标准化（Instance Normalization）通常用于风格迁移等任务，逐样本规范化数据。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 以二维输入为例</span>
input_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有4个样本，每个样本是3通道的32x32图像</span>

<span class="token comment"># 创建 Instance Normalization 层</span>
instance_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>

<span class="token comment"># 对输入数据进行规范化</span>
output <span class="token operator">=</span> instance_norm<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="4nnGroupNorm_130"></a>（4）nn.GroupNorm</h3> 
<p>分组标准化（Group Normalization）是一种替代 Batch Normalization 的规范化方法，它将通道分成多个组，并在每个组内进行规范化。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 以二维输入为例</span>
input_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有4个样本，每个样本有6个通道的32x32图像</span>

<span class="token comment"># 创建 Group Normalization 层</span>
group_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span>

<span class="token comment"># 对输入数据进行规范化</span>
output <span class="token operator">=</span> group_norm<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="5nnSyncBatchNorm_148"></a>（5）nn.SyncBatchNorm</h3> 
<p>同步批标准化（SyncBatchNorm）是一种多 GPU 训练时用于保持 Batch Normalization 的统计一致性的方法。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 以二维输入为例</span>
input_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有4个样本，每个样本是3通道的32x32图像</span>

<span class="token comment"># 创建 SyncBatchNorm 层</span>
sync_batch_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>

<span class="token comment"># 对输入数据进行规范化</span>
output <span class="token operator">=</span> sync_batch_norm<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span>
</code></pre> 
<p>这些规范化方法可以在神经网络中用于处理不同类型的数据和任务，以提高训练和收敛的稳定性。我们可以根据具体任务和模型需求选择合适的规范化方法。</p> 
<h2><a id="3_168"></a>3.归一化函数在神经网络中的应用示例</h2> 
<p>当使用 PyTorch 中的不同归一化函数时，您通常会首先创建一个归一化层实例，然后将其添加到您的神经网络模型中。以下是一些不同类型的归一化函数的示例用法：</p> 
<h3><a id="1Batch_Normalization_nnBatchNorm1d_nnBatchNorm2d_nnBatchNorm3d_172"></a>（1）Batch Normalization (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)</h3> 
<p>Batch Normalization 用于对输入数据进行批量归一化。以下是一个示例，演示如何在一个卷积神经网络中使用 Batch Normalization：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义一个简单的卷积神经网络</span>
<span class="token keyword">class</span> <span class="token class-name">CNNWithBatchNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CNNWithBatchNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span> <span class="token operator">*</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span> <span class="token operator">*</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">16</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token comment"># 创建模型实例</span>
model <span class="token operator">=</span> CNNWithBatchNorm<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 将模型添加到优化器等代码中进行训练</span>
</code></pre> 
<h3><a id="2_Layer_Normalization_nnLayerNorm_205"></a>（2） Layer Normalization (nn.LayerNorm)</h3> 
<p>Layer Normalization 通常用于自然语言处理任务。以下是一个示例，演示如何在一个循环神经网络中使用 Layer Normalization：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义一个简单的循环神经网络</span>
<span class="token keyword">class</span> <span class="token class-name">RNNWithLayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>RNNWithLayerNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ln<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 取最后一个时间步的输出</span>
        <span class="token keyword">return</span> x

<span class="token comment"># 创建模型实例</span>
model <span class="token operator">=</span> RNNWithLayerNorm<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">)</span>

<span class="token comment"># 将模型添加到优化器等代码中进行训练</span>
</code></pre> 
<h3><a id="3Instance_Normalization_nnInstanceNorm1d_nnInstanceNorm2d_nnInstanceNorm3d_233"></a>（3）Instance Normalization (nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d)</h3> 
<p>Instance Normalization 通常用于图像处理任务。以下是一个示例，演示如何在一个卷积神经网络中使用 Instance Normalization：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义一个简单的卷积神经网络</span>
<span class="token keyword">class</span> <span class="token class-name">CNNWithInstanceNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CNNWithInstanceNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span> <span class="token operator">*</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>in1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span> <span class="token operator">*</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">16</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token comment"># 创建模型实例</span>
model <span class="token operator">=</span> CNNWithInstanceNorm<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 将模型添加到优化器等代码中进行训练</span>
</code></pre> 
<p>nn.SyncBatchNorm。nn.SyncBatchNorm是在多GPU分布式训练环境中使用的同步批标准化方法，用于确保不同GPU上的批标准化参数保持同步，不再举例。</p> 
<p>这些示例演示了如何在不同类型的神经网络中使用不同的归一化函数，具体用法可以根据任务和模型的需求进行调整。不同的归一化函数适用于不同的场景，可帮助加速训练过程，提高模型的稳定性，并改善模型的泛化性能。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5770dea4a6545f8745e7a9f463d60933/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">安装docker以及nvidia-container-toolkit</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/257a9d721912def1b957b61939d987e6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【数据结构】顺序表(SeqList)（增、删、查、改）详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>