<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>2020年，关于事件相机(event camera)的最新研究论文总结 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="2020年，关于事件相机(event camera)的最新研究论文总结" />
<meta property="og:description" content="本文仅用于学习过程中的一些总结记录。2020年发表的关于事件相机在一些领域的总结。
1. 运动估计 Motion Estimation 有不少关于运动估计的论文，基本是围绕之前最大化事件积累图对比度的方式进行。
最小化Entropy [1] 采用最小化熵函数的方式，对运动参数进行估计。文章指出最大的优点是，这种方法不需要将空间中的事件投影到平面上去最大化对比度。然而数学内容太复杂真的看不懂。
全局最优解计算 [2] [3] 两篇论文，都是解决运动参数估计时陷入局部最优解的问题。如果在使用最大对比度函数进行计算运动参数时，需要有一个较好的参数初值，否则对于图像问题在优化时会陷入局部最优解。如下图所示，左下角的运动参数就是局部最优解。
论文采用了分支定界（Branch-and-bound）的方法进行全局搜索，提出了某个区域的近似上界和下界，加速运算。
运动参数时间连续性 [4] 这篇论文提出，相机的运动应该是连续的，之前的方法只是针对每个积累图进行一次优化得到运动参数，并没有考虑与前后时刻的运动参数是否一致。
论文将一段儿时间内的运动参数设置为时变的，但这样难以优化，所以不得不再次切片成多个更短的时间离散化，分别优化运动参数。同时增加一阶导数作为惩罚项避免过拟合，保证参数变化的连续性。如下图，待优化的运动参数是时变的 θ t \theta_t θt​，最后一项是参数的一阶导。
这个想法让人眼前一亮，从实验结果上看，增加噪声以后，考虑了时间上的连续性情况下的运动曲线更加平滑。
2. 建图、场景识别与VIO Frame&#43;event的深度估计 [5] 使用了事件相机&#43;传统相机进行深度估计。作者指出这是第一个关于二者融合的深度估计，二者结合得到了时间上连续的稠密深度图。
方法架构如上图。stereo相机数据过来以后进行稠密重建，然后根据运动参数推测和收到的events在两帧图像之间对depth图进行更新。由于利用到了视差的信息，论文提到这种方法更适合于平行于平面运动，但仍然不失为一种不错的尝试。毕竟这种方法补全了两帧之间的深度图，还是挺有趣的。
基于事件的场景识别 [6] 是我读到的第一个event-based的场景识别的论文。时空窗口的选择方式常见的有定事件数量或定时间，但无论如何事件只归属于一个窗口。而这篇论文采用了多种尺度的时间窗口，一个事件同时隶属于多个窗口。然后通过已有的E2VID方法恢复到图像，再进行匹配。
如上图的流程所示，第一个图示意某一个时刻某一个事件可能同时归属于6个窗口（3种时间间隔、3种数量上限），之后所有的窗格恢复images，再分别提取特征，与之前关键位置的images进行比对，形成Ensemble matrix。最近的那个就是成功匹配。这种方法给出了一些近似方法，降低了比对的复杂度。这个方法还是比较有趣的，虽然感觉有点儿暴力。
利用线特征的VIO [7] 这篇论文提出了一个IMU&#43;DVS的VIO。大致看了一下主要是优化的点到线距离。
4. 信号处理领域相关研究 降噪
2020降噪的研究有不少在我之前的一篇博客中有所总结：【事件相机整理】信号处理、噪声与滤波。这里就不展开介绍。
数据编码与压缩
之前没有注意，最近发现在github的event camera的汇总中出现了“Compression”相关的研究，以为是2020年才开始的，结果打开论文发现，[10][11]中已经指出了不少的研究。想简单看一看发现实在是太深入了，之前研究传统图像编码与压缩的方法，大量的用在event上面。真的是新的应用场景又养活了一批人。
压缩与编码目的：降低数据量，总线传输更快，降低存储空间，IoT领域降低传输数据量
数据编码与压缩的主要衡量指标：压缩率、编解码速度、编解码延时
方法比较的结果：压缩率：LZMA（静态），Spike（动态）； 编解码速度：LZ4和Snappy；综合性能：Brotli算法
5. 图像/视频恢复 Reconstruction 我对重建部分了解不多。传统方法是基于一些假设前提，例如亮度一致性，或利用泊松积分等方式进行恢复。现在更多地采用learning的方法。
包含了超分辨率与降噪的重建 [12] 这篇工作，采用事件对图像进行增强，得到了降噪、超分的图像。多说一句，event的超分工作也很少，这篇论文的作者表示只遇到了一篇相关论文。这篇论文的视频上传到了bilibili，可以看一看。干杯~https://www.bilibili.com/video/av838383543/
E2VID 视频恢复 E2VID基本可以算是最常对比的一种方法了，也衍生了许多改进方案。[9] 这篇论文是rpg组Henri Rebecq的工作，好像是在18年基础上的改进，并公开了代码。event数据流直接通过learning后转成视频。看着效果还是不错的，应该比之前基于泊松的一些重建要方便许多。
基于光度一致性的自监督重建 E2VID的采用了仿真生成了带有标签的数据进行的训练，[13] 这工作不需要真值，直接自监督。思路如下图，利用FlowNet和ReconNet两个网络生成的数据，和事件积累图采用光度一致性，确定误差，反向传播到两个网络进行调整。
基于cycle-GAN的重建 [14] 采用cycle-GAN进行重建。对GAN我也不懂，说到这些又想到了伤心的过去。算了不说了。
基于representation learning 和 domain adaption的learning 这个也是好新的内容啊，完全不懂。大概理解，[15] 的主要解决的问题是，在黑暗环境下的重建可以利用光照条件良好的数据进行学习，经过domin adaption把光照良好的一些domain-invariant representations迁移到光照不好的情况下。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/6dd1120604410a37f6b5868e0df62905/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-16T22:14:36+08:00" />
<meta property="article:modified_time" content="2020-11-16T22:14:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">2020年，关于事件相机(event camera)的最新研究论文总结</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>本文仅用于学习过程中的一些总结记录。2020年发表的关于事件相机在一些领域的总结。</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span></p> 
<h2><a id="1__Motion_Estimation_3"></a>1. 运动估计 Motion Estimation</h2> 
<p>有不少关于运动估计的论文，基本是围绕之前最大化事件积累图对比度的方式进行。</p> 
<h4><a id="Entropy_5"></a>最小化Entropy</h4> 
<p>[1] 采用最小化熵函数的方式，对运动参数进行估计。文章指出最大的优点是，这种方法不需要将空间中的事件投影到平面上去最大化对比度。然而数学内容太复杂真的看不懂。<br> <img src="https://images2.imgbox.com/b5/88/zzmC8Jpn_o.png" alt="在这里插入图片描述" width="720"></p> 
<h4><a id="_8"></a>全局最优解计算</h4> 
<p>[2] [3] 两篇论文，都是解决运动参数估计时陷入局部最优解的问题。如果在使用最大对比度函数进行计算运动参数时，需要有一个较好的参数初值，否则对于图像问题在优化时会陷入局部最优解。如下图所示，左下角的运动参数就是局部最优解。<br> <img src="https://images2.imgbox.com/e2/6d/VxDJKmpX_o.png" alt="在这里插入图片描述" width="480">论文采用了分支定界（Branch-and-bound）的方法进行全局搜索，提出了某个区域的近似上界和下界，加速运算。</p> 
<h4><a id="_12"></a>运动参数时间连续性</h4> 
<p>[4] 这篇论文提出，相机的运动应该是连续的，之前的方法只是针对每个积累图进行一次优化得到运动参数，并没有考虑与前后时刻的运动参数是否一致。</p> 
<p>论文将一段儿时间内的运动参数设置为时变的，但这样难以优化，所以不得不再次切片成多个更短的时间离散化，分别优化运动参数。同时增加一阶导数作为惩罚项避免过拟合，保证参数变化的连续性。如下图，待优化的运动参数是时变的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          θ 
         
        
          t 
         
        
       
      
        \theta_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span style="margin-right: 0.02778em;" class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: -0.02778em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，最后一项是参数的一阶导。</p> 
<p><img src="https://images2.imgbox.com/05/34/vtCTbs59_o.png" alt="在这里插入图片描述" width="480"><br> 这个想法让人眼前一亮，从实验结果上看，增加噪声以后，考虑了时间上的连续性情况下的运动曲线更加平滑。<br> <img src="https://images2.imgbox.com/6b/e7/ijIt7q01_o.png" alt="在这里插入图片描述" width="1080"><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span></p> 
<h2><a id="2_VIO_21"></a>2. 建图、场景识别与VIO</h2> 
<h4><a id="Frameevent_22"></a>Frame+event的深度估计</h4> 
<p>[5] 使用了事件相机+传统相机进行深度估计。作者指出这是第一个关于二者融合的深度估计，二者结合得到了时间上连续的稠密深度图。</p> 
<p><img src="https://images2.imgbox.com/cb/20/eEttmtko_o.png" alt="在这里插入图片描述" width="720">方法架构如上图。stereo相机数据过来以后进行稠密重建，然后根据运动参数推测和收到的events在两帧图像之间对depth图进行更新。由于利用到了视差的信息，论文提到这种方法更适合于平行于平面运动，但仍然不失为一种不错的尝试。毕竟这种方法补全了两帧之间的深度图，还是挺有趣的。<br> <img src="https://images2.imgbox.com/2a/69/jAs1oNOS_o.png" alt="在这里插入图片描述" width="480"></p> 
<h4><a id="_28"></a>基于事件的场景识别</h4> 
<p>[6] 是我读到的第一个event-based的场景识别的论文。时空窗口的选择方式常见的有定事件数量或定时间，但无论如何事件只归属于一个窗口。而这篇论文采用了多种尺度的时间窗口，一个事件同时隶属于多个窗口。然后通过已有的E2VID方法恢复到图像，再进行匹配。<br> <img src="https://images2.imgbox.com/a0/e9/uvBMQXmS_o.png" alt="在这里插入图片描述" width="1080">如上图的流程所示，第一个图示意某一个时刻某一个事件可能同时归属于6个窗口（3种时间间隔、3种数量上限），之后所有的窗格恢复images，再分别提取特征，与之前关键位置的images进行比对，形成Ensemble matrix。最近的那个就是成功匹配。这种方法给出了一些近似方法，降低了比对的复杂度。这个方法还是比较有趣的，虽然感觉有点儿暴力。</p> 
<h4><a id="VIO_32"></a>利用线特征的VIO</h4> 
<p>[7] 这篇论文提出了一个IMU+DVS的VIO。大致看了一下主要是优化的点到线距离。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span></p> 
<h2><a id="4__37"></a>4. 信号处理领域相关研究</h2> 
<p><strong>降噪</strong><br> 2020降噪的研究有不少在我之前的一篇博客中有所总结：<a href="https://blog.csdn.net/tfb760/article/details/105970156">【事件相机整理】信号处理、噪声与滤波</a>。这里就不展开介绍。</p> 
<p><strong>数据编码与压缩</strong><br> 之前没有注意，最近发现在github的event camera的汇总中出现了“Compression”相关的研究，以为是2020年才开始的，结果打开论文发现，[10][11]中已经指出了不少的研究。想简单看一看发现实在是太深入了，之前研究传统图像编码与压缩的方法，大量的用在event上面。真的是新的应用场景又养活了一批人。</p> 
<p><img src="https://images2.imgbox.com/94/ed/IEaJ27Dg_o.png" alt="在这里插入图片描述" width="480"><br> 压缩与编码目的：降低数据量，总线传输更快，降低存储空间，IoT领域降低传输数据量<br> 数据编码与压缩的主要衡量指标：压缩率、编解码速度、编解码延时<br> 方法比较的结果：压缩率：LZMA（静态），Spike（动态）； 编解码速度：LZ4和Snappy；综合性能：Brotli算法</p> 
<p><img src="https://images2.imgbox.com/c8/97/jEDsJc4w_o.png" alt="在这里插入图片描述" width="720"><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span></p> 
<h2><a id="5__Reconstruction_51"></a>5. 图像/视频恢复 Reconstruction</h2> 
<p>我对重建部分了解不多。传统方法是基于一些假设前提，例如亮度一致性，或利用泊松积分等方式进行恢复。现在更多地采用learning的方法。</p> 
<h4><a id="_54"></a>包含了超分辨率与降噪的重建</h4> 
<p>[12] 这篇工作，采用事件对图像进行增强，得到了降噪、超分的图像。多说一句，event的超分工作也很少，这篇论文的作者表示只遇到了一篇相关论文。这篇论文的视频上传到了bilibili，可以看一看。干杯~<a href="https://www.bilibili.com/video/av838383543/" rel="nofollow">https://www.bilibili.com/video/av838383543/</a></p> 
<h4><a id="E2VID__59"></a>E2VID 视频恢复</h4> 
<p>E2VID基本可以算是最常对比的一种方法了，也衍生了许多改进方案。[9] 这篇论文是rpg组Henri Rebecq的工作，好像是在18年基础上的改进，并公开了代码。event数据流直接通过learning后转成视频。看着效果还是不错的，应该比之前基于泊松的一些重建要方便许多。<br> <img src="https://images2.imgbox.com/87/25/jwYM24GY_o.png" alt="在这里插入图片描述" width="720"></p> 
<h4><a id="_63"></a>基于光度一致性的自监督重建</h4> 
<p>E2VID的采用了仿真生成了带有标签的数据进行的训练，[13] 这工作不需要真值，直接自监督。思路如下图，利用FlowNet和ReconNet两个网络生成的数据，和事件积累图采用光度一致性，确定误差，反向传播到两个网络进行调整。</p> 
<p><img src="https://images2.imgbox.com/31/cf/J6XkXUS7_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="cycleGAN_67"></a>基于cycle-GAN的重建</h4> 
<p>[14] 采用cycle-GAN进行重建。对GAN我也不懂，说到这些又想到了伤心的过去。算了不说了。</p> 
<h4><a id="representation_learning__domain_adaptionlearning_71"></a>基于representation learning 和 domain adaption的learning</h4> 
<p>这个也是好新的内容啊，完全不懂。大概理解，[15] 的主要解决的问题是，在黑暗环境下的重建可以利用光照条件良好的数据进行学习，经过domin adaption把光照良好的一些domain-invariant representations迁移到光照不好的情况下。</p> 
<h4><a id="_76"></a>二维码重建与识别</h4> 
<p>终于碰到了一个自己熟悉的工作。[16] 这篇论文研究了将高速运动的QR码恢复后解码。主要有两步：由定位图案大致确定运动参数，之后优化目标函数使运动参数、仿射变换与重建的误差最小。</p> 
<p><img src="https://images2.imgbox.com/c9/0b/rRSDmX1v_o.png" alt="在这里插入图片描述" width="640"><br> 创新点之处是，并没有直接在原图像尺寸恢复二维码，而是通过了一个仿射变换，将原尺寸的二维码降采样到25x25的平面，从而极大程度的降低了参数的维度。所优化的目标函数如下所示，让事件经过运动参数warp后与图像尽可能相同。<br> <img src="https://images2.imgbox.com/99/3f/vOn9DymU_o.png" alt="在这里插入图片描述" width="480">然而我实际做过二维码识别解码的相关工作，认为这篇论文还有许多地方没有说清楚。例如用PCA确定运动参数时，定位模块的pattern具有噪声不知道是怎么找的；时间长度如何选取最合适等这些都没有研究。</p> 
<p>同时我又有了一个大胆的想法。event的二维码工作这是我看到的第一篇，只停留在相机固定二维码单方向运动，没有背景干扰，速度恒定，QR码。如果是相机运动、地面有干扰、6dof、不匀速、datamatrxi/QR多种码，等，能够搞出来一大票的工作。</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span></p> 
<h2><a id="n__88"></a>n. 其他暂未分类</h2> 
<h4><a id="SNN_90"></a>基于SNN的角速度估计</h4> 
<p>[8] 这篇论文采用SNN对3DoF的角速度进行估计。在此之前，SNN基本只用与做Event的分类问题，而没有做过回归（角速度估计是回归问题）。这篇论文表示做回归问题也是可行的，同时比其他ANN的方法要好。<br> <img src="https://images2.imgbox.com/c1/e1/Q7JVVMYR_o.png" alt="在这里插入图片描述" width="480"></p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span></p> 
<h2><a id="_96"></a>未完待续</h2> 
<p>有时间继续补充，未完待续……</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
        
       
     </span><span class="katex-html"></span></span></span></span></p> 
<h2><a id="_103"></a>参考文献</h2> 
<p>[1]. Urbano Miguel Nunes: Entropy Minimisation Framework for Event-based Vision Model Estimation.</p> 
<p>[2]. Xin Peng: Globally-Optimal Event Camera Motion Estimation.</p> 
<p>[3]. Liu, Daqi; Parra, Álvaro; Chin, Tat-Jun (2020): Globally Optimal Contrast Maximisation for Event-based Motion Estimation. Available online at http://arxiv.org/pdf/2002.10686v3.</p> 
<p>[4]. Xu, Jie; Jiang, Meng; Yu, Lei; Yang, Wen; Wang, Wenwei (2020): Robust Motion Compensation for Event Cameras With Smooth Constraint. In IEEE Trans. Comput. Imaging 6, pp. 604–614. DOI: 10.1109/TCI.2020.2964255.</p> 
<p>[5]. Antea Hadviger: Stereo Dense Depth Tracking Based on Optical Flow using Frames and Events.</p> 
<p>[6]. Fischer, Tobias; Milford, Michael (2020): Event-Based Visual Place Recognition With Ensembles of Temporal Windows. In IEEE Robot. Autom. Lett. 5 (4), pp. 6924–6931. DOI: 10.1109/LRA.2020.3025505.</p> 
<p>[7]. Le Gentil, Cedric; Tschopp, Florian; Alzugaray, Ignacio; Vidal-Calleja, Teresa; Siegwart, Roland; Nieto, Juan (2020): IDOL: A Framework for IMU-DVS Odometry using Lines. Available online at http://arxiv.org/pdf/2008.05749v1.</p> 
<p>[8]. Gehrig, Mathias; Shrestha, Sumit Bam; Mouritzen, Daniel; Scaramuzza, Davide: Event-Based Angular Velocity Regression with Spiking Networks. In IEEE International Conference on Robotics and Automation (ICRA). Available online at http://arxiv.org/pdf/2003.02790v1.</p> 
<p>[9]. Rebecq, Henri; Ranftl, Rene; Koltun, Vladlen; Scaramuzza, Davide (2019): High Speed and High Dynamic Range Video with an Event Camera. In IEEE transactions on pattern analysis and machine intelligence PP. DOI: 10.1109/TPAMI.2019.2963386.</p> 
<p>[10]. Khan, Nabeel; Iqbal, Khurram; Martini, Maria G. (2020): Lossless Compression of Data From Static and Mobile Dynamic Vision Sensors-Performance and Trade-Offs. In IEEE Access 8, pp. 103149–103163. DOI: 10.1109/ACCESS.2020.2996661.</p> 
<p>[11]. Banerjee, Srutarshi; Wang, Zihao W.; Chopp, Henry H.; Cossairt, Oliver; Katsaggelos, Aggelos (2020): Quadtree Driven Lossy Event Compression. Available online at http://arxiv.org/pdf/2005.00974v1.</p> 
<p>[12]. Bishan Wang∗: Event Enhanced High-Quality Image Recovery.</p> 
<p>[13]. Paredes-Vallés, F.; Croon, G. C. H. E. de (2020): Back to Event Basics: Self-Supervised Learning of Image Reconstruction for Event Cameras via Photometric Constancy. Available online at http://arxiv.org/pdf/2009.08283v1.</p> 
<p>[14]. Binyi Su (2019): Event-based High Frame-rate Video Reconstruction with a Novel Cycle-event Network. Available online at http://arxiv.org/pdf/1906.07165v1.</p> 
<p>[15]. Song Zhang: Learning to See in the Dark with Events.</p> 
<p>[16]. Jun Nagata; Yusuke Sekikawa; Kosuke Hara; Teppei Suzuki; Yoshimitsu Aoki: QR-code Reconstruction from Event Data via Optimization in Code Subspace.</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b90028ace6765e7886dfd837f46cb7b8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">京训钉怎么快速看完_钉眼补腻子的方法技巧和注意事项</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f5a8f3bfbf5459888e91766a7f91b9b1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">什么是 ill-posed 问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>