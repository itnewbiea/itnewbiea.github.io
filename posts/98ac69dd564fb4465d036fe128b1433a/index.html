<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【机器学习】卷积神经网络（四）-实现细节（Caffe框架为例） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【机器学习】卷积神经网络（四）-实现细节（Caffe框架为例）" />
<meta property="og:description" content="六、 实现细节
都有哪些开源的卷积神经网络实现
caffe中卷积神经网络各个层（卷积层、全连接层、池化层、激活函数层、内基层、损失层等）
Caffe2 与caffe 对比
caffe2 开源吗
使用caffe的c&#43;&#43; 版本和python版本，分别适用于哪些场景
6.1 卷积层
在进行预测时，训练时 正向传播区别
采用矩阵乘法的优势
6.2 激活函数
在神经网络中，激活函数通常是将向量中的每个元素独立地映射到一个新的值。这种映射是逐元素（element-wise）的，也就是说，每个元素的新值只取决于该元素的原值。
例如，ReLU（Rectified Linear Unit）激活函数就是一个常见的逐元素映射。它将输入向量中的每个元素x映射到max(0, x)，即如果x大于0，就保留x，否则就将x置为0。同样，Sigmoid激活函数也是一个逐元素映射。它将输入向量中的每个元素x映射到1 / (1 &#43; exp(-x))，这样可以将x的值压缩到0和1之间。
6.3 内积层
反向传播是同时对多个样本进行计算的。
6.4 损失层
Multinomial Logistic Loss
Infogain Loss - a generalization of MultinomialLogisticLossLayer.
Softmax with Loss - computes the multinomial logistic loss of the softmax of its inputs. It&#39;s conceptually identical to a softmax layer followed by a multinomial logistic loss layer, but provides a more numerically stable gradient." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/98ac69dd564fb4465d036fe128b1433a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-02T00:15:52+08:00" />
<meta property="article:modified_time" content="2024-01-02T00:15:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【机器学习】卷积神经网络（四）-实现细节（Caffe框架为例）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p><strong>六、 实现细节</strong></p> 
 <p><strong>都有哪些开源的卷积神经网络实现</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/44/6d/LKr7Ns8j_o.png" alt="bc97a0d3a404991bab619c3d2c0b07fa.png"></p> 
 <p><strong>caffe中卷积神经网络各个层（卷积层、全连接层、池化层、激活函数层、内基层、损失层等）</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/17/1c/CPTI5xdB_o.png" alt="98a1d2c68000b723009f0f4db584fab3.png"></p> 
 <p><strong>Caffe2 与caffe 对比</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bc/10/uWfr53Xr_o.png" alt="38a5b5314dfbccfd5c94d8acc3f4287a.png"></p> 
 <p><strong>caffe2 开源吗</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/36/1a/udE4SNKq_o.png" alt="e8213a181f2929579c397409e21937d2.png"></p> 
 <p><strong>使用caffe的c++ 版本和python版本，分别适用于哪些场景</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/cf/23/UsjYQWhb_o.png" alt="b8d9c0e8671dfde11c760cc136ff78ae.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ab/36/EbzZVFOZ_o.png" alt="abd5915605685b9845357afc73b58092.png"></p> 
 <p><strong>6.1 卷积层</strong><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0f/f9/gb2iboY8_o.png" alt="8e01c9bfe3b8d9bdb5ec554d46c74ca7.png"></p> 
 <p><strong>在进行预测时，训练时 正向传播区别</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bb/ae/lB4oTrHx_o.png" alt="c8a4fc911133e03b22bb18a03b828906.png"></p> 
 <p><strong>采用矩阵乘法的优势</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4d/e0/dkxS4MqS_o.png" alt="2e58c220d95da62efee7eb3e6bbeb407.png"></p> 
 <p><strong>6.2 激活函数</strong><br></p> 
 <p>在神经网络中，激活函数通常是将向量中的每个元素独立地映射到一个新的值。这种映射是逐元素（element-wise）的，也就是说，每个元素的新值只取决于该元素的原值。</p> 
 <p>例如，ReLU（Rectified Linear Unit）激活函数就是一个常见的逐元素映射。它将输入向量中的每个元素x映射到max(0, x)，即如果x大于0，就保留x，否则就将x置为0。同样，Sigmoid激活函数也是一个逐元素映射。它将输入向量中的每个元素x映射到1 / (1 + exp(-x))，这样可以将x的值压缩到0和1之间。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c2/35/QkmaHZEi_o.png" alt="b47c1ec9ed8271199e8c1cdb3c799f69.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/22/26/FUHJ1iu6_o.png" alt="e727ed3b98125019df08893e020a3a25.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e3/10/uKGae35s_o.png" alt="5021477b494eac5400bf5d5061b94d00.png"><img src="https://images2.imgbox.com/7b/87/0S1Uussw_o.png" alt="6ff3dc37ee7c4d210d0fbd184e668e15.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/46/31/Y5b5oFRu_o.png" alt="7bddbb7be4d37583ed9ca4f35c86a6ce.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ac/ba/nMAbkV8B_o.png" alt="cbd3775091f6483b1441816b88d92147.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/95/58/pxpDuAsX_o.png" alt="2f290e06c6afdd20b99cfa22ce608db2.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1d/b5/LHrqhDSM_o.png" alt="45d35af735476895fbd69f9335f2e82a.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/84/1f/Fy4TTw54_o.png" alt="09ee625d115a6819621f0e306b01ad50.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/78/17/kkjL9G4V_o.png" alt="863f111d44715a5eba39423a508bda1e.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9f/c1/6vCANEQl_o.png" alt="1d94d5e177dea07a56e67cedda9b40c9.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4c/25/p5ql26sF_o.png" alt="255e1a50bcc731ebba7bee56aaa8c372.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f6/07/9MUS3sCb_o.png" alt="e543448b5c4dcec7bde6459677ea619c.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/34/4f/YpKe1O2B_o.png" alt="572f78671f5837c5469508287b111c80.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bc/26/BcbK4ngK_o.png" alt="26290a3176a6585c6b7afb3bb04a05d3.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/26/b1/twTiF8Gc_o.png" alt="283349238f21a338cc511a4873b6b691.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/cc/2c/ftil1NDJ_o.png" alt="aa8c24d64b383a249c493fa0b75166c1.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/47/26/xCA4WR7g_o.png" alt="b70254fe10cce28a0ade28736bd76470.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/69/97/JwrN3kwq_o.png" alt="eab389d1934760231eb1adec49785935.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1e/2f/ddTTLsEN_o.png" alt="82e1f8176ae9de660c571068c3609f65.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4e/c5/XpOWR8v5_o.png" alt="958fdb028c2038a148bf02b2ff7ef047.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5c/d8/l9iPBhd4_o.png" alt="68579c8fa38af2c30003000f8e9b6e1d.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f9/1b/kBr1kpsb_o.png" alt="0a98298ef76301d618628739bde3ac53.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ff/18/WEhKd3lU_o.png" alt="d2351444ee56c0404a10dcaa519693cc.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/54/75/AaG5S8xL_o.png" alt="7c24d39a8c78858f3d993dffc1a75fb1.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/12/83/hIcjmJli_o.png" alt="d2f99c1071cbb58dfae52a82a035661e.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/45/9c/zb6VyS9A_o.png" alt="fa932574b3dad70d4fc2bd04801d8bd4.png"><img src="https://images2.imgbox.com/c1/6e/K7YqPk6B_o.png" alt="d4e1b596708c5f657598dd2b4a25a84d.png"></p> 
 <p><strong>6.3 内积层</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/45/14/MQA6mdgH_o.png" alt="27a4854db4a4ecb6ec6fe08bc77c0d96.png"></p> 
 <p>反向传播是同时对多个样本进行计算的。<br></p> 
 <p><strong>6.4 损失层</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/cf/6c/Iy9K4YjT_o.png" alt="431083fdefb976175cfaaf2901959a69.png"></p> 
 <ul><li><p>Multinomial Logistic Loss</p></li><li><p>Infogain Loss - a generalization of MultinomialLogisticLossLayer.</p></li><li><p>Softmax with Loss - computes the multinomial logistic loss of the softmax of its inputs. It's conceptually identical to a softmax layer followed by a multinomial logistic loss layer, but provides a more numerically stable gradient.</p></li><li><p>Sum-of-Squares / Euclidean - computes the sum of squares of differences of its two inputs,</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4c/98/C9qi5na6_o.png" alt="bb91b21da3691e13e8f3a1dd7a46c8bf.png"></p> 
 <ul><li><p>Hinge / Margin - The hinge loss layer computes a one-vs-all hinge (L1) or squared hinge loss (L2).</p></li><li><p>Sigmoid Cross-Entropy Loss - computes the cross-entropy (logistic) loss, often used for predicting targets interpreted as probabilities.</p></li><li><p>Accuracy / Top-k layer - scores the output as an accuracy with respect to target -- it is not actually a loss and has no backward step.</p></li><li><p>Contrastive Loss</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/05/b9/s6jinloI_o.png" alt="c6e2de0e91984032b018ee8f760b4870.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ab/0e/l3QkHHci_o.png" alt="5280a9d7c4dd0ca250a97e3c5ee72d50.png"></p> 
 <p><strong>caffe支持的各种损失函数都适用于哪些场景</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b9/0d/hrUp5OlG_o.png" alt="549ded6bb6e05ffe478ecefd8d366386.png"></p> 
 <p><strong>6.5 求解器</strong><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/24/65/KXfRQBr2_o.png" alt="ea06837a30ae4428563cfc793e2073b4.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0a/b5/iKH2IWqx_o.png" alt="249e9586f1e32ed20cc4c7afaab58baf.png"></p> 
 <p><strong>Caffe 求解器支持哪些优化算法</strong></p> 
 <p>Stochastic Gradient Descent (type: "SGD"),<br>AdaDelta (type: "AdaDelta"),<br>Adaptive Gradient (type: "AdaGrad"),<br>Adam (type: "Adam"),<br>Nesterov’s Accelerated Gradient (type: "Nesterov") and<br>RMSprop (type: "RMSProp")</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ee/e7/j9JGF2EW_o.png" alt="14467a79ef3c9453d4d02e92c49d0c8d.png"></p> 
 <p><strong>详细介绍caffe支持的优化算法:</strong></p> 
 <p style="text-align:center;">回答A</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/02/e8/H2FDOGm9_o.png" alt="91ce9e2c8f10b34cf48aff2d6eebff5d.png"></p> 
 <p style="text-align:center;">回答B</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/cf/f4/BZUFFAJQ_o.png" alt="71e6d025f3ac093ad92dd24856001aaa.png"></p> 
 <p style="text-align:center;">回答C<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/14/0a/K5JqegvB_o.png" alt="b83946621c936a611108828a747effa5.png"></p> 
 <p><strong>caffe支持的优化算法分别适用于哪些场景</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b4/01/8RfCEZao_o.png" alt="97163d4b2e1c4ac01918bdfa839841ee.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/23/61/2ZLbvVDW_o.png" alt="e188a0eb561ce53d047871fa0a3086d2.png"></p> 
 <p><strong>神经网络的优化中一般使用梯度下降法，而不使用牛顿法这样的二阶优化技术的原因是什么？</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/7b/30/kmCTnu0f_o.png" alt="a11850d61686ff4313b5ee496db738a9.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9b/8a/J8NBnlsj_o.png" alt="142a22ec3ff9d18db6e1030ad15b4ac4.png"></p> 
 <p><strong>分析参数初始化和动量项对算法的收敛行的影响</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e3/14/qr8hNNKE_o.png" alt="0549417afc185de447b356d365a4bc38.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9b/35/8QuO2ac5_o.png" alt="58d8704a80d3c56619efd2280f9217ee.png"></p> 
 <p><strong>caffe支持的学习率计算策略</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/8f/6b/khKoRtn9_o.png" alt="380321ce381f2902506b5a3da8d0e3bd.png"></p> 
 <p>学习率是优化算法中的一个重要的超参数，它决定了每次更新权重的步长。学习率的大小会影响优化的速度和效果，一般来说，学习率不能太大，否则会导致优化不稳定或者发散，也不能太小，否则会导致优化过慢或者停滞。因此，合适的学习率计算策略是优化算法的关键。</p> 
 <p>Caffe 支持以下几种学习率计算策略：</p> 
 <ul><li><p>固定（fixed）：这是最简单的学习率计算策略，它使用一个固定的学习率，不随着迭代的进行而改变。这种策略适用于那些目标函数比较平滑，不需要动态调整学习率的情况。</p></li><li><p>步长（step）：这是一种常用的学习率计算策略，它根据一个预设的步长，每隔一定的迭代次数，就将学习率乘以一个衰减因子。这种策略适用于那些目标函数比较复杂，需要逐渐降低学习率的情况。</p></li><li><p>指数（exp）：这是一种较为灵活的学习率计算策略，它根据一个预设的指数，每次迭代，就将学习率乘以一个衰减因子的指数。这种策略适用于那些目标函数比较陡峭，需要快速降低学习率的情况。</p></li><li><p>逆时针（inv）：这是一种较为稳定的学习率计算策略，它根据一个预设的幂，每次迭代，就将学习率除以一个增长因子的幂。这种策略适用于那些目标函数比较平缓，需要缓慢降低学习率的情况。</p></li><li><p>多项式（poly）：这是一种较为精确的学习率计算策略，它根据一个预设的幂，每次迭代，就将学习率乘以一个多项式函数的值。这种策略适用于那些目标函数比较复杂，需要根据迭代的进度调整学习率的情况。</p></li><li><p>S型（sigmoid）：这是一种较为平滑的学习率计算策略，它根据一个预设的幂，每次迭代，就将学习率乘以一个 S 型函数的值。这种策略适用于那些目标函数比较复杂，需要在初始阶段快速降低学习率，然后在后期阶段缓慢降低学习率的情况。</p></li></ul> 
 <p><strong>参考网址：</strong></p> 
 <p>https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/ Visualising Activation Functions in Neural Networks - dashee87.github.io</p> 
 <p>https://caffe.berkeleyvision.org/tutorial/layers.html Caffe | Layer Catalogue --- Caffe | 层目录 (berkeleyvision.org)</p> 
 <p>https://caffe2.ai/docs/caffe-migration.html   What is Caffe2? | Caffe2</p> 
 <p>https://github.com/pytorch/pytorch pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration (github.com)</p> 
 <p>https://github.com/pytorch/examples</p> 
 <p>https://blog.csdn.net/lliming2006/article/details/76636329 </p> 
 <p>http://caffe.berkeleyvision.org/doxygen/namespacecaffe.html Caffe中的损失函数_caffe设置损失函数-CSDN博客</p> 
 <p>https://cloud.tencent.com/developer/article/1670389  caffe详解之损失函数-腾讯云开发者社区-腾讯云 (tencent.com)</p> 
 <p>https://blog.csdn.net/wuqingshan2010/article/details/71156236  Caffe中求解器（Solver）介绍_solver.compute-CSDN博客</p> 
 <p>https://zhuanlan.zhihu.com/p/24087905 Caffe入门与实践-简介 - 知乎 (zhihu.com)</p> 
 <p>https://caffe.berkeleyvision.org/tutorial/solver.html 求解器</p> 
 <hr> 
 <p style="text-align:center;">The End</p> 
 <p>正传播向传播的规律</p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/01348bfabccbee47eb4c5a00ff5c620f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">关于“Python”的核心知识点整理大全55</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/89b4df727d8760c37ccf006ac3847e59/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python多环境管理工具——pyenv-win安装与使用教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>