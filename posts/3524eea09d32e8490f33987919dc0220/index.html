<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>sigmoid、Dice、ReLu激活函数简单介绍 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="sigmoid、Dice、ReLu激活函数简单介绍" />
<meta property="og:description" content="----摘自chatgpt的回答
sigmoid函数 sigmoid函数是一种常用的神经网络激活函数，也称为Logistic函数。sigmoid函数的数学表达式如下：
f ( x ) = 1 1 &#43; e − x f(x) = \frac{1}{1&#43;e^{-x}} f(x)=1&#43;e−x1​
其中， x x x表示输入， f ( x ) f(x) f(x)表示输出。sigmoid函数的输出在 ( 0 , 1 ) (0,1) (0,1)之间，具有单调性和可导性。
sigmoid函数的优点：具有平滑的S形曲线，能够将输入映射到 ( 0 , 1 ) (0,1) (0,1)之间的概率值，常用于二分类问题的输出层。此外，sigmoid函数的导数在 x = 0 x=0 x=0处取得最大值 1 4 \frac{1}{4} 41​，能够有效地加速神经网络的训练速度。
但是，sigmoid函数也存在一些问题：
首先，当输入 x x x较大或较小时，sigmoid函数的输出会接近于0或1，造成梯度消失问题，从而影响模型的性能。
其次，sigmoid函数的输出不是以0为中心的，会导致梯度更新时的偏移问题，从而影响模型的收敛速度。
因此，在实际应用中，人们通常使用其他的激活函数，如Relu、Leaky Relu、ELU等，这些激活函数能够更好地解决神经网络中的梯度消失和过拟合问题，具有更好的性能和稳定性。
Dice激活函数 Dice激活函数是一种用于神经网络的非线性激活函数，由阿里巴巴提出。Dice激活函数主要用于解决梯度消失和过拟合问题，能够有效地提高模型的鲁棒性和泛化能力。Dice激活函数的数学表达式如下：
D i c e ( x ) = 2 1 &#43; e − β x − 1 \mathrm{Dice}(x) = \frac{2}{1&#43;e^{-\beta x}} - 1 Dice(x)=1&#43;e−βx2​−1" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/3524eea09d32e8490f33987919dc0220/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-23T11:46:46+08:00" />
<meta property="article:modified_time" content="2023-03-23T11:46:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">sigmoid、Dice、ReLu激活函数简单介绍</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>----摘自chatgpt的回答</p> 
<h4><a id="sigmoid_2"></a>sigmoid函数</h4> 
<p>sigmoid函数是一种常用的神经网络激活函数，也称为Logistic函数。sigmoid函数的数学表达式如下：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          f 
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          = 
         
         
         
           1 
          
          
          
            1 
           
          
            + 
           
           
           
             e 
            
            
            
              − 
             
            
              x 
             
            
           
          
         
        
       
         f(x) = \frac{1}{1+e^{-x}} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.0908em; vertical-align: -0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6973em;"><span class="" style="top: -2.989em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.7693em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>表示输入，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
      
        f(x) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>表示输出。sigmoid函数的输出在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         0 
        
       
         , 
        
       
         1 
        
       
         ) 
        
       
      
        (0,1) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>之间，具有单调性和可导性。<br> <strong>sigmoid函数的优点</strong>：具有平滑的S形曲线，能够将输入映射到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         0 
        
       
         , 
        
       
         1 
        
       
         ) 
        
       
      
        (0,1) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>之间的概率值，常用于二分类问题的输出层。此外，sigmoid函数的导数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         = 
        
       
         0 
        
       
      
        x=0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>处取得最大值<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          1 
         
        
          4 
         
        
       
      
        \frac{1}{4} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>，能够有效地加速神经网络的训练速度。<br> <strong>但是，sigmoid函数也存在一些问题：</strong></p> 
<ul><li> <p>首先，当输入<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           x 
          
         
        
          x 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>较大或较小时，sigmoid函数的输出会接近于0或1，造成梯度消失问题，从而影响模型的性能。</p> </li><li> <p>其次，sigmoid函数的输出不是以0为中心的，会导致梯度更新时的偏移问题，从而影响模型的收敛速度。</p> <p>因此，在实际应用中，人们通常使用其他的激活函数，如Relu、Leaky Relu、ELU等，这些激活函数能够更好地解决神经网络中的梯度消失和过拟合问题，具有更好的性能和稳定性。</p> </li></ul> 
<h4><a id="Dice_21"></a>Dice激活函数</h4> 
<p>Dice激活函数是一种用于神经网络的非线性激活函数，由阿里巴巴提出。Dice激活函数主要用于解决<strong>梯度消失和过拟合</strong>问题，能够有效地提高模型的鲁棒性和泛化能力。Dice激活函数的数学表达式如下：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           D 
          
         
           i 
          
         
           c 
          
         
           e 
          
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          = 
         
         
         
           2 
          
          
          
            1 
           
          
            + 
           
           
           
             e 
            
            
            
              − 
             
            
              β 
             
            
              x 
             
            
           
          
         
        
          − 
         
        
          1 
         
        
       
         \mathrm{Dice}(x) = \frac{2}{1+e^{-\beta x}} - 1 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathrm">Dice</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.0908em; vertical-align: -0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7751em;"><span class="" style="top: -2.989em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right: 0.0528em;">β</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.7693em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">1</span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>表示输入，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         β 
        
       
      
        \beta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span></span></span></span></span>表示一个可学习的参数，通常初始化为0。Dice激活函数的<strong>输出值在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          [ 
         
        
          − 
         
        
          1 
         
        
          , 
         
        
          1 
         
        
          ] 
         
        
       
         [-1,1] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>之间</strong>，具有单调性和可导性。<br> <strong>Dice激活函数的特点是</strong>：将sigmoid函数进行了平移和缩放，使其更加平滑，同时保留了sigmoid函数的单调性和可导性。Dice激活函数的另一个重要特点是<strong>引入了Dropout机制</strong>，用于随机丢弃一部分神经元，从而减少模型的过拟合风险。具体来说，Dice激活函数在每个batch中，都会随机生成一个掩码向量，用于控制哪些神经元需要被保留，哪些神经元需要被丢弃。这种Dropout机制能够有效地提高模型的鲁棒性和泛化能力，从而使得模型更加健壮。<br> 总之，Dice激活函数是一种比较新颖的非线性激活函数，能够有效地解决神经网络中的梯度消失和过拟合问题，具有良好的性能和可解释性。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Dice</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    :Description: 激活函数，改造后的prelu
    :param epsilon: 介绍某个类全局变量的功能
    :type epsilon: string //指定该变量的类型
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e-3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        :param epsilon: dice中的超参
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Dice<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> epsilon
        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        :param x: 输入数据
        :return: dice后的结果
        """</span>
        <span class="token comment"># x: N * num_neurons</span>
        avg <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># N</span>
        avg <span class="token operator">=</span> avg<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># N * 1</span>
        var <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>x <span class="token operator">-</span> avg<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>epsilon  <span class="token comment"># N * num_neurons</span>
        var <span class="token operator">=</span> var<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># N * 1</span>

        ps <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> avg<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>var<span class="token punctuation">)</span>  <span class="token comment"># N * 1</span>

        ps <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>ps<span class="token punctuation">)</span>  <span class="token comment"># N * 1</span>
        <span class="token keyword">return</span> ps <span class="token operator">*</span> x <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> ps<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>alpha <span class="token operator">*</span> x

</code></pre> 
<h4><a id="Relu_66"></a>Relu激活函数</h4> 
<p>Relu（Rectified Linear Unit）激活函数是一种常用的神经网络激活函数，用于解决神经网络中的非线性问题。Relu激活函数的数学表达式如下：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          f 
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          = 
         
        
          max 
         
        
          ⁡ 
         
        
          ( 
         
        
          0 
         
        
          , 
         
        
          x 
         
        
          ) 
         
        
       
         f(x) = \max(0, x) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>表示输入，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
      
        f(x) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>表示输出。当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &gt; 
        
       
         0 
        
       
      
        x&gt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
         = 
        
       
         x 
        
       
      
        f(x)=x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>；当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
         = 
        
       
         0 
        
       
      
        f(x)=0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>。Relu激活函数的输出在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         [ 
        
       
         0 
        
       
         , 
        
       
         + 
        
       
         ∞ 
        
       
         ) 
        
       
      
        [0,+\infty) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">+</span><span class="mord">∞</span><span class="mclose">)</span></span></span></span></span>之间，具有单调性和可导性。<br> <strong>Relu激活函数的主要优点</strong>：是计算速度快、具有稀疏性和非线性性。与sigmoid、tanh等激活函数相比，Relu激活函数的导数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时为0，在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &gt; 
        
       
         0 
        
       
      
        x&gt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时为1，不会出现梯度消失的问题，能够有效地加速神经网络的训练速度。此外，Relu激活函数的输出值在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时为0，具有稀疏性，能够有效地减少网络中的冗余信息，从而提高模型的泛化能力。<br> <strong>但是，Relu激活函数也存在一些问题</strong>：</p> 
<ul><li> <p>首先，当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           x 
          
         
           &lt; 
          
         
           0 
          
         
        
          x&lt;0 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时，Relu激活函数的导数为0，会出现“死亡神经元”的问题，即某些神经元永远无法被激活，从而影响模型的性能。</p> </li><li> <p>其次，当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           x 
          
         
        
          x 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span>较大时，Relu激活函数的导数也会变得很大，从而可能导致梯度爆炸的问题。<br> 因此，为了克服Relu激活函数的缺陷，人们提出了一系列改进版本的Relu激活函数，如Leaky Relu、PRelu、ELU等，这些改进版本的Relu激活函数能够更好地解决神经网络中的梯度消失和过拟合问题，具有更好的性能和稳定性。</p> </li></ul> 
<h4><a id="Leaky_Relu_84"></a>Leaky Relu激活函数</h4> 
<p>Leaky Relu（Leaky Rectified Linear Unit）是一种改进版本的Relu激活函数，用于解决Relu激活函数“死亡神经元”的问题。与Relu激活函数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         f 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
         = 
        
       
         max 
        
       
         ⁡ 
        
       
         ( 
        
       
         0 
        
       
         , 
        
       
         x 
        
       
         ) 
        
       
      
        f(x)=\max(0,x) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>不同，Leaky Relu激活函数的数学表达式如下：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          f 
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          = 
         
         
         
           { 
          
          
           
            
             
              
              
                x 
               
              
                , 
               
              
             
            
            
             
              
              
                x 
               
              
                &gt; 
               
              
                0 
               
              
             
            
           
           
            
             
              
              
                a 
               
              
                x 
               
              
                , 
               
              
             
            
            
             
              
              
                x 
               
              
                ≤ 
               
              
                0 
               
              
             
            
           
          
         
        
       
         f(x) = \begin{cases} x, &amp; x &gt; 0 \\ ax, &amp; x \leq 0 \end{cases} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 3em; vertical-align: -1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size4">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69em;"><span class="" style="top: -3.69em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mpunct">,</span></span></span><span class="" style="top: -2.25em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.19em;"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69em;"><span class="" style="top: -3.69em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord">0</span></span></span><span class="" style="top: -2.25em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.19em;"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<p>其中x<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         表示输入， 
        
       
      
        表示输入， 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord cjk_fallback">表示输入，</span></span></span></span></span>a表示一个小的正数，通常取0.01。Leaky Relu激活函数的输出在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         − 
        
       
         ∞ 
        
       
         , 
        
       
         + 
        
       
         ∞ 
        
       
         ) 
        
       
      
        (-\infty,+\infty) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">∞</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">+</span><span class="mord">∞</span><span class="mclose">)</span></span></span></span></span>之间，具有单调性和可导性。<br> 相比于Relu激活函数，Leaky Relu激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时的导数不为0，从而避免了“死亡神经元”的问题，能够更好地解决神经网络中的梯度消失问题。此外，Leaky Relu激活函数的输出值在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时也不为0，能够保留更多的信息，从而提高模型的泛化能力。<br> 除了Leaky Relu之外，还有一些其他的改进版本的Relu激活函数，如Parametric Relu（PRelu）、Exponential Linear Unit（ELU）等，这些激活函数也能够更好地解决神经网络中的梯度消失和过拟合问题，具有更好的性能和稳定性。</p> 
<h4><a id="PRelu_99"></a>PRelu</h4> 
<p>Parametric Relu（PRelu）是一种改进版本的Relu激活函数，与Leaky Relu激活函数不同的是，PRelu激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时会引入一个可学习的参数，从而更加灵活地调整激活函数的输出。PRelu激活函数的数学表达式如下：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          f 
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          = 
         
         
         
           { 
          
          
           
            
             
              
              
                x 
               
              
                , 
               
              
             
            
            
             
              
              
                x 
               
              
                &gt; 
               
              
                0 
               
              
             
            
           
           
            
             
              
              
                a 
               
              
                x 
               
              
                , 
               
              
             
            
            
             
              
              
                x 
               
              
                ≤ 
               
              
                0 
               
              
             
            
           
          
         
        
       
         f(x) = \begin{cases} x, &amp; x &gt; 0 \\ ax, &amp; x \leq 0 \end{cases} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 3em; vertical-align: -1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size4">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69em;"><span class="" style="top: -3.69em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mpunct">,</span></span></span><span class="" style="top: -2.25em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.19em;"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69em;"><span class="" style="top: -3.69em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord">0</span></span></span><span class="" style="top: -2.25em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.19em;"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<p>其中 x表<u>示输入</u>， <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         a 
        
       
      
        a 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span></span>表示一个可学习的参数。<br> 与Leaky Relu激活函数相比，PRelu激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时引入了一个<strong>可学习的参数</strong>，能够更加灵活地调整激活函数的输出，从而提高模型的性能和泛化能力。此外，PRelu激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时的导数也不为0，避免了“死亡神经元”的问题。<br> 在实际应用中，PRelu激活函数常常用于卷积神经网络（CNN）中，能够更好地提高模型的性能和稳定性。</p> 
<h4><a id="Exponential_Linear_UnitELU_114"></a>Exponential Linear Unit（ELU）</h4> 
<p>Exponential Linear Unit（ELU）是一种改进版本的激活函数，可以在一定程度上解决神经网络中的梯度消失和过拟合问题。ELU激活函数的数学表达式如下：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          f 
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          = 
         
         
         
           { 
          
          
           
            
             
              
              
                x 
               
              
                , 
               
              
             
            
            
             
              
              
                x 
               
              
                &gt; 
               
              
                0 
               
              
             
            
           
           
            
             
              
              
                α 
               
              
                ( 
               
               
               
                 e 
                
               
                 x 
                
               
              
                − 
               
              
                1 
               
              
                ) 
               
              
                , 
               
              
             
            
            
             
              
              
                x 
               
              
                ≤ 
               
              
                0 
               
              
             
            
           
          
         
        
       
         f(x) = \begin{cases} x, &amp; x &gt; 0 \\ \alpha(e^x-1), &amp; x \leq 0 \end{cases} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 3em; vertical-align: -1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size4">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69em;"><span class="" style="top: -3.69em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mpunct">,</span></span></span><span class="" style="top: -2.25em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6644em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.19em;"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69em;"><span class="" style="top: -3.69em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord">0</span></span></span><span class="" style="top: -2.25em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.19em;"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<p>其中 x表示输入，alpha是一个可选的超参数，通常取1。<br> 与Relu激活函数不同的是，ELU激活函数在x&lt;0时不是线性的，而是指数级的，能够更好地处理负数部分，从而避免了“死亡神经元”的问题，提高了模型的稳定性和泛化能力。此外，ELU激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时的导数也不为0，能够更好地传递梯度，进一步缓解了梯度消失问题。<br> 虽然ELU激活函数具有很好的性能和稳定性，但是计算复杂度较高，特别是在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
         &lt; 
        
       
         0 
        
       
      
        x&lt;0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时需要进行指数运算，相比于Relu激活函数而言计算速度较慢。</p> 
<h4><a id="_129"></a>怎么选取激活函数？</h4> 
<p>在神经网络中，不同的激活函数适用于不同的场景，没有哪一种激活函数是绝对好的，需要根据具体的应用场景来选择合适的激活函数。下面简单总结一下几种常用的激活函数的特点和适用场景：</p> 
<ol><li>Relu激活函数<br> Relu激活函数简单有效，能够快速收敛，特别适用于深度神经网络。但是，Relu激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
          &lt; 
         
        
          0 
         
        
       
         x&lt;0 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时输出为0，会导致一些神经元“死亡”，影响模型的性能和泛化能力。</li><li>Leaky Relu激活函数<br> Leaky Relu激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
          &lt; 
         
        
          0 
         
        
       
         x&lt;0 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时引入了一个可学习的参数，能够更好地解决神经元“死亡”的问题，提高模型的泛化能力。但是，Leaky Relu激活函数仍然存在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
          &lt; 
         
        
          0 
         
        
       
         x&lt;0 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时输出较小的问题，可能会影响模型的性能。</li><li>PRelu激活函数<br> PRelu激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
          &lt; 
         
        
          0 
         
        
       
         x&lt;0 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时引入了一个可学习的参数，能够更加灵活地调整激活函数的输出，从而提高模型的性能和泛化能力。但是，PRelu激活函数的计算复杂度较高，需要学习参数，容易过拟合。</li><li>ELU激活函数<br> ELU激活函数在<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
          &lt; 
         
        
          0 
         
        
       
         x&lt;0 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>时不是线性的，而是指数级的，能够更好地处理负数部分，避免了“死亡神经元”的问题，提高了模型的稳定性和泛化能力。但是，ELU激活函数的计算复杂度较高，需要进行指数运算，相比于Relu激活函数而言计算速度较慢。</li></ol> 
<p>综上所述，选择哪种激活函数应该根据具体的应用场景来选择，需要综合考虑模型的性能、泛化能力、计算速度等多个因素。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1436bb588b935a81d2606ab244bfd7f2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux进程的睡眠和唤醒简析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d6a00c5e56f2c9161733d016a96cac06/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">KVM（虚拟化平台）概念及部署</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>