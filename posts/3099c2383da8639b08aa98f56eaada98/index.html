<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>手把手教你本地CPU环境部署清华大模型ChatGLM-6B，利用量化模型，本地即可开始智能聊天，达到ChatGPT的80% - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="手把手教你本地CPU环境部署清华大模型ChatGLM-6B，利用量化模型，本地即可开始智能聊天，达到ChatGPT的80%" />
<meta property="og:description" content="大家好，我是微学AI，今天教你们本地CPU环境部署清华大ChatGLM-6B模型，利用量化模型，每个人都能跑动大模型。ChatGLM-6B是一款出色的中英双语对话模型，拥有超过62亿个参数，可高效地处理日常对话场景。与GLM-130B模型相比，ChatGLM-6B在对话场景处理能力方面表现更加卓越。此外，在使用体验方面，ChatGLM-6B采用了模型量化技术和本地部署技术，为用户提供更加便利和灵活的使用方式。值得一提的是，该模型还能够在单张消费级显卡上顺畅运行，速度较快，是一款非常实用的对话模型。
ChatGLM-6B是清华开发的中文对话大模型的小参数量版本，目前已经开源了，可以单卡部署在个人电脑上，利用 INT4 量化还可以最低部署到 6G 显存的电脑上，在 CPU 也可以运行起来的。
项目地址：mirrors / THUDM / chatglm-6b · GitCode
第1步：下载：
git clone https://gitcode.net/mirrors/THUDM/chatglm-6b.git 第2步：进入ChatGLM-6B-main目录下，安装相关依赖
pip install -r requirements.txt 其中 torch安装CPU版本即可。
第3步：打开ChatGLM-6B-main目录的web_demo.py文件，源代码：
from transformers import AutoModel, AutoTokenizer import gradio as gr import mdtex2html tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True) model = AutoModel.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True).half().cuda() model = model.eval() 这个是在GPU版本下的代码，现在改为CPU版本下的代码：
from transformers import AutoModel, AutoTokenizer import gradio as gr import mdtex2html tokenizer = AutoTokenizer.from_pretrained(&#34;THUDM/chatglm-6b-int4&#34;, trust_remote_code=True) model = AutoModel.from_pretrained(&#34;THUDM/chatglm-6b-int4&#34;, trust_remote_code=True).float() model = model." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/3099c2383da8639b08aa98f56eaada98/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-03T06:32:03+08:00" />
<meta property="article:modified_time" content="2023-05-03T06:32:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">手把手教你本地CPU环境部署清华大模型ChatGLM-6B，利用量化模型，本地即可开始智能聊天，达到ChatGPT的80%</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>大家好，我是微学AI，今天教你们本地CPU环境部署清华大ChatGLM-6B模型，利用量化模型，每个人都能跑动大模型。ChatGLM-6B是一款出色的中英双语对话模型，拥有超过62亿个参数，可高效地处理日常对话场景。与GLM-130B模型相比，ChatGLM-6B在对话场景处理能力方面表现更加卓越。此外，在使用体验方面，ChatGLM-6B采用了模型量化技术和本地部署技术，为用户提供更加便利和灵活的使用方式。值得一提的是，该模型还能够在单张消费级显卡上顺畅运行，速度较快，是一款非常实用的对话模型。</p> 
<p>ChatGLM-6B是清华开发的中文对话大模型的小参数量版本，目前已经开源了，可以单卡部署在个人电脑上，利用 <a href="https://huggingface.co/THUDM/chatglm-6b-int4" rel="nofollow" title="INT4 量化">INT4 量化</a>还可以最低部署到 <strong>6G 显存</strong>的电脑上，在 CPU 也可以运行起来的。</p> 
<p>项目地址：<a href="https://gitcode.net/mirrors/THUDM/chatglm-6b?utm_source=csdn_github_accelerator" rel="nofollow" title="mirrors / THUDM / chatglm-6b ·  GitCode">mirrors / THUDM / chatglm-6b · GitCode</a></p> 
<p><strong>第1步：下载：</strong></p> 
<pre><code>git clone https://gitcode.net/mirrors/THUDM/chatglm-6b.git</code></pre> 
<p><strong>第2步：</strong>进入ChatGLM-6B-main目录下，安装相关依赖</p> 
<pre><code>pip install -r requirements.txt</code></pre> 
<p>其中 torch安装CPU版本即可。</p> 
<p><strong>第3步：</strong>打开ChatGLM-6B-main目录的web_demo.py文件，源代码：</p> 
<pre><code>from transformers import AutoModel, AutoTokenizer
import gradio as gr
import mdtex2html

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
model = model.eval()
</code></pre> 
<p>这个是在GPU版本下的代码，现在改为CPU版本下的代码：</p> 
<pre><code>from transformers import AutoModel, AutoTokenizer
import gradio as gr
import mdtex2html

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).float()
model = model.eval()
</code></pre> 
<p>模型下载改成THUDM/chatglm-6b-int4，也就是int4量化版本。模型量化到int4是一种将神经网络模型中的参数从浮点数格式调整为4位精度的整数格式的技术，可以显著提高硬件设备的效率和速度，并且适用于需要在低功耗设备上运行的场景。</p> 
<p>INT4量化的预训练文件下载地址：https://huggingface.co/THUDM/chatglm-6b-int4/tree/main</p> 
<p> <strong>第4步：</strong>kernel的编译</p> 
<p>CPU版本的安装还需要安装好C/C++的编译环境。这里大家可以安装TDM-GCC。</p> 
<p>下载地址：https://jmeubank.github.io/tdm-gcc/，大家选择选取<a href="https://jmeubank.github.io/tdm-gcc/articles/2021-05/10.3.0-release" rel="nofollow" title="TDM-GCC 10.3.0 release">TDM-GCC 10.3.0 release</a>下载安装。<strong>特别注意：</strong>安装的时候在选项gcc选项下方，勾选<strong>openmp</strong>，这个很重要，踩过坑，直接安装的话后续会报错。</p> 
<p><img src="https://images2.imgbox.com/ea/c2/ZjiCTgz0_o.png" alt="d44f6eb455ef4737b7868b9996bbad31.png"></p> 
<p><img src="https://images2.imgbox.com/19/70/xUhxAs9E_o.png" alt="1d3078b2262049028df6ae52a6370e55.png"></p> 
<p> 安装完在cmd中运行”gcc -v”测试是否成功即可。</p> 
<p><img src="https://images2.imgbox.com/5c/4f/TXsDS3cT_o.png" alt="8e3df00558a64933ba2a3fe0eafef770.png"></p> 
<p>安装gcc的目的是为了编译c++文件,<code>quantization_kernels.c</code>和<code>quantization_kernels_parallel.c</code><img src="https://images2.imgbox.com/c4/08/JHnzU9aE_o.png" alt="00ef846c3a714f9d91963cfdbd04e221.png"></p> 
<p> <code>quantization_kernels.c文件：</code></p> 
<pre><code>void compress_int4_weight(void *weight, void *out, int n, int m)
{
    for(int i=0;i&lt;n*m;i++)
    {
        (*(unsigned char*)(out)) = ((*(unsigned char*)(weight)) &lt;&lt; 4);
        weight += sizeof(char);
        (*(unsigned char*)(out)) |= ((*(unsigned char*)(weight)) &amp; 15);
        weight += sizeof(char);
        out += sizeof(char);
    }
}

void extract_int8_weight_to_float(void *weight, void *scale_list, void *out, int n, int m)
{
	for(int i=0;i&lt;n;i++)
        for(int j=0;j&lt;m;j++)
            (*(float*)(out + sizeof(float) * (i * m + j))) = (*(float*)(scale_list + sizeof(float) * i)) * (*(char*)(weight + sizeof(char) * (i * m + j)));
}

void extract_int4_weight_to_float(void *weight, void *scale_list, void *out, int n, int m)
{
	for(int i=0;i&lt;n;i++)
    {
        for(int j=0;j&lt;m;j++)
        {
            (*(float*)(out)) = (*(float*)(scale_list)) * ((*(char*)(weight)) &gt;&gt; 4);
            out += sizeof(float);
            (*(float*)(out)) = (*(float*)(scale_list)) * (((char)((*(unsigned char*)(weight)) &lt;&lt; 4))&gt;&gt; 4);
            out += sizeof(float);
            weight += sizeof(char);
        }
        scale_list += sizeof(float);
    }
}</code></pre> 
<p>以上C++程序对于每个8位的输入权重值，都会被压缩成一个4位的输出权重值，并存储到指定的输出数组中。这种权重量化方式可以有效减小模型的内存占用，提高模型的推理速度。</p> 
<p> <strong>第5步：</strong>运行web_demo.py文件</p> 
<p><strong>注意</strong>：如果大家在运行中遇到了错误提示，说明两个文件编译出问题。我们可以手动去编译这两个文件：即在上面下载的D<code>:..\</code>chatglm-6b-int4本地目录下进入cmd，运行两个编译命令：</p> 
<pre><code>gcc -fPIC -pthread -fopenmp -std=c99 quantization_kernels.c -shared -o quantization_kernels.so
gcc -fPIC -pthread -fopenmp -std=c99 quantization_kernels_parallel.c -shared -o quantization_kernels_parallel.so</code></pre> 
<p>没有报错说明运行成功，目录下看到下面两个新的文件：<code>quantization_kernels_parallel.so</code>和<code>quantization_kernels.so</code>。说明编译成功，后面我们手动载入,这里要多加一行代码</p> 
<pre><code>model = model.quantize(bits=4, kernel_file="D:..\\chatglm-6b-int4\\quantization_kernels.so")</code></pre> 
<p>如果原来代码没有错可以去掉这行。</p> 
<p> <strong>第6步：</strong>web_demo.py文件运行成功</p> 
<p><img src="https://images2.imgbox.com/22/d2/U2oCKlZu_o.png" alt="ab52cf9cb92a4d068a85ee8c28d08d16.png"></p> 
<p> 出现地址就大功告成了。</p> 
<p> <strong>第7步：</strong>测试问题</p> 
<p><strong>1.鲁迅和周树人是同一个人吗？</strong></p> 
<p>ChatGLM的结果：</p> 
<p><img src="https://images2.imgbox.com/8e/27/sqRTtuo8_o.png" alt="83f95aa7655b452aa24cb5d4994f866e.png"></p> 
<p> ChatGPT的结果：</p> 
<p><img src="https://images2.imgbox.com/b2/60/pW1IveDf_o.png" alt="50dca051e3f44c6fb527176ad3691009.png"></p> 
<p><strong> 2.树上9只鸟，用枪打掉1只，还剩几只？</strong></p> 
<p>ChatGLM的结果：</p> 
<p><img src="https://images2.imgbox.com/a5/19/r3flXP1Z_o.png" alt="e459de5f6fc84faabe16bdc1e76c96f4.png"></p> 
<p> ChatGPT的结果：</p> 
<p><img src="https://images2.imgbox.com/9d/df/pDStmdAf_o.png" alt="79a7eb5644204547a3649ae398fa1137.png"></p> 
<p>ChatGLM在某些中文问题和常识问题上超过ChatGPT，但是总体上是不如ChatGPT，他在总结任务上，代码编写上不如ChatGPT，总体达到ChatGPT的80%左右，可以做简单的任务。</p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/680b5387d17ceed2aac340d7ca1cb73a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">前端三剑客之HTML】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/70346344b326ead61da57f7e07f4d60b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">抖音和tiktok的逆向开发</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>