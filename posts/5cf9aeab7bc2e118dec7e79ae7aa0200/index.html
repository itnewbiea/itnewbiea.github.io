<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【事件相机与计算机视觉】论文分享--End-to-End Learning of Representations for Asynchronous Event-Based Data - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【事件相机与计算机视觉】论文分享--End-to-End Learning of Representations for Asynchronous Event-Based Data" />
<meta property="og:description" content=" .前言 事件相机作为一种新型的图像数据形式，由于其时间分辨率高、高动态范围、低功耗的优点，在许多CV问题上有着比传统相机更好的表现，但要想将其产生的数据流用于现有深度网络中，需要将数据转化为网络可以处理的形式。笔者在之前的综述分享中介绍了许多事件相机的数据转化方式，但是由于它们都压缩了某个维度，造成了信息丢失，因此都具有一定缺陷。
本次笔者分享的是一篇ICCV的论文，作者提出了一种综合了所有信息的事件数据表示方式EST。在这种方式下，压缩某个维度即可得到例如Event-frame、Voxel等目前常见的表达方式，EST因此可以说是事件数据表示的集大成者。同时，作者更是在变换中提出了一种可训练的核函数，使得数据表示可以通过端对端训练来更加“适应”任务。
论文原文：
End-to-End Learning of Representations for Asynchronous Event-Based Data
若不了解事件相机，关于事件相机的综述导读可以阅读笔者的文章：
Event-based Vision: A Survey——从这篇综述开始了解事件相机（一）
1. 论文总览 本篇论文解决了事件相机用于深度学习的一个最基本但也是最关键的问题：将事件流数据处理为深度网络可以接收的“网格”数据。目前，常用的事件流表示法有Event-Frame（Single or Two Channel）、Voxel、Time Surface等，它们已经在许多领域大放异彩，表现出超越传统相机的训练结果；哪怕是最简单的Event-Frame形式，在汽车转角预测、图像重建、视频去模糊等问题上也能有优于传统灰度图的结果。但正如笔者多次提到的那样，这几种数据表示法可以说都“浪费”了一部分事件流的信息，在一些问题上也许够用，但还是有很大提升的空间以获得更精确的结果。
因此文中提出了上图所示的Event Spike Tensor（简称EST）的事件数据表示方法，对于一个四元组事件，EST能够涵盖其位置、时间、极性等所有信息，因此能够充分利用事件流的时间分辨率、动态范围、运动捕捉等优势。同时，比起笔者经常处理事件流的方式（对事件的极性进行简单的叠加、用极性来表示每个像素点的值），作者还增加了测量函数、核变换等流程，使得生成的EST能够包含更多信息，并且更具有“意义”（笔者认为此处的“意义”在于合理的核变换能够使得事件的信息更加充分地体现，即变换后网络更能提取出某些特征），甚至该核函数可以用多层感知机（MLP）替代，作为学习的一部分，让网络自己去选择核。
本文的亮点在于：
1）：提出了一种综合的事件数据表示法EST。包含了事件流所有的信息，同时处理过程中引入了测量函数和核函数的变换，使得事件流的信息能被更有效地提取；
2）：提出了一种端到端学习的事件表示方法。用MLP取代核函数，使得EST具有可学习型，实现事件流—&gt;网络输出的端到端学习。
2. 数学模型 2.1 Event Spike Tensor —EST 建立 事件相机中DVS模块输出四元组事件流：
上述事件流表示了四元空间中的点，引入Event Field这一概念，通过狄拉克脉冲函数将这些事件进行整合，Event Field表示了特定像素点、时间戳处，事件产生的不同极性的脉冲（可以理解为原本事件是点集，现在用函数将其表达出来）：
深度卷积网络显然处理不了这样的脉冲（SNN专门处理这种脉冲），因此作者引入了一个定义在事件域的、连续的测量函数来具体化每个事件。这个可以是事件极性（），可以是事件数，也可以是平均时间戳，或者是其他任何包含事件流某种信息的值。（该部分笔者这样理解：在事件域这一包含空间像素和时间戳的坐标系中，每个事件原本都是脉冲，通过来将脉冲替换为一个具体的量，若是极性，便是Voxel、若是时间戳再压缩，便是Time Surface）
为了让事件表示方式更加具有“意义”，作者对上述S与一个核变换函数进行了卷积操作，只要核函数选取得合适，那么得到的事件表示就有可能让卷积网络学到更多特征。核函数内是包含狄拉克脉冲函数的，可以看作它是针对狄拉克函数的一种变化，得到一种有“意义”的函数来将事件具体化。常用的核函数有：指数核函数；alpha核函数：
；三元线性投票核函数：。当然，核函数没有绝对的优劣，对于特定的问题，有的核函数能使事件数据表现出更多可学习的特征。核函数变换表达式如下：
最后，为了使数据“网格化”，以便输入进卷积网络（卷积网络接受数据可以看作以像素为单位的网格），作者对核变换后的事件进行了取样，取样是在时间维度进行的，根据事件密度通过选取一个合适的时间间隔，将连续的事件离散化，最终得到EST模型：
其实笔者对上述数学模型理解并不是很透彻，仅仅是理解了一个大意。笔者看来，事件一直都存在于上图所示的三维空间中，极性这第四维可以看作是空间中每个点的标记；测试函数为空间中的事件点赋予有意义的值；核函数卷积（可以看作是对3D图像进行3D卷积操作）对整个空间进行某种运算得到另一个三维空间（依然是坐标和时间三维，但是每个点的值因卷积运算改变了）；最后再将连续的三维空间离散化变为网格得到EST模型。
对EST的某个维度进行叠加，即可得到我们常见的几种事件表示（笔者认为对于Voxel这些表达方式来说，可以不加核变换），在此不再展示具体公式。
2.2 端对端学习 本篇论文另一个精妙之处在于实现了可端对端学习的事件表示方法。对于核函数，与其费尽心思去寻找一个合适的，不如直接引入一个充满参数的卷积核，让网络自己决定自己想要什么样的数据。这就好比卷积网络中，卷积操作实为一种特征提取器，而卷积神经网络则是将特征提取器全设为参数，让网络自己学习需要提取的特征。 因此，作者选择采用多层感知机（MLP）来替代核函数，事件的坐标和时间戳作为输入，以此生成激活映射（取代核变换的作用）。
笔者自己绘制的学习流程如上。作者实现了事件流到输出的端对端学习系统，根据系统得到的输出与目标值的损失，进行反向传播，其中MLP的参数也是需要进行梯度下降更新的，这使得任务本身对事件的表示方式也有了选择权，自然也能有比人工选择核函数更好的训练效果。
3. 实验结果 作者在物体识别和光流估计这两个具体问题上进行了实验分析，选用了多种事件表示方式进行了对比。同时对于不同的事件表示方式，作者还对比了测量函数与核函数对结果的影响（端对端学习属于不同核函数的选取），最终都能发现选用MLP进行端对端学习的方式表现得更为出色。由于笔者对光流计算了解甚少，此处只展示物体识别的结果，望读者见谅。
作者分别在N-Cars以及N-Caltech101这两个开放数据集上进行了测试，采用了Resnet-34的网络结构，先在ImageNet上进行了预训练，选用了交叉熵损失函数和ADAM优化算法，初始学习率为1e-5，每10000次迭代就减少2倍，N-Cars与N-Caltech101采用的batch分别为60和100。识别任务结果如下：
从上表可以看出，采用时间戳作为测量函数效果最好，这也不难理解，毕竟这样事件信息更加丰富（即便是EST也压缩了部分时间，时间戳作为测量函数能够补充一定的时间信息）；而在核函数选取方面，对于固定的核函数来说，三元线性投票函数的效果最佳，但也比不上端对端学习的方式。综合来看，采用时间戳作为测量函数，将核变换替换为MLP进行端对端学习的事件表示方式，在物体识别这个任务上表现得最佳，即便不引入MLP，EST相较于其他常用表示方法来说也具有优势，这也证明了这篇论文提出的EST的有效性。
4. 小结 本篇论文可谓是所有事件数据表示方法的集大成者，给出了一种综合的事件表示模型EST，构建了一种事件数据转化的通用框架，涵盖了事件流几乎所有的有用信息，同时还能经过叠加转化为Voxel等我们常用的类型，更不用说引入MLP实现了端对端学习。
当然，本文对笔者来说，最大的意义在于充分了解了每一种事件数据的表示方法，以便针对不同问题去使用，毕竟即便是效果最差的Event Frame，相较于传统灰度图来说都是极具优势的，因此没必要在每个问题上都要采取EST进行端对端学习（EST数据量较大，使得训练消耗的时间更多）。但作者提出的数据表示框架还是极具价值的，如果能够正确复现并广泛使用，那么将节省很多数据处理的功夫，我认为这也是事件相机领域发展成熟的关键一步。
最后，谢谢大家读到最后，文中许多细节笔者未作太多阐述，若想复现该篇论文，读者还需更加深入阅读，请见谅。 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/5cf9aeab7bc2e118dec7e79ae7aa0200/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-26T23:35:55+08:00" />
<meta property="article:modified_time" content="2021-08-26T23:35:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【事件相机与计算机视觉】论文分享--End-to-End Learning of Representations for Asynchronous Event-Based Data</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>.前言</h2> 
<p style="text-align:justify;">        事件相机作为一种新型的图像数据形式，由于其时间分辨率高、高动态范围、低功耗的优点，在许多CV问题上有着比传统相机更好的表现，但要想将其产生的数据流用于现有深度网络中，需要将数据转化为网络可以处理的形式。笔者在之前的综述分享中介绍了许多事件相机的数据转化方式，但是由于它们都压缩了某个维度，造成了信息丢失，因此都具有一定缺陷。</p> 
<p style="text-align:justify;">        本次笔者分享的是一篇ICCV的论文，作者提出了一种<strong>综合了所有信息的事件数据表示方式EST。</strong>在这种方式下，压缩某个维度即可得到例如Event-frame、Voxel等目前常见的表达方式，EST因此可以说是事件数据表示的集大成者。同时，作者更是在变换中提出了一种<strong>可训练的核函数</strong>，使得数据表示可以通过端对端训练来更加“适应”任务。</p> 
<p style="text-align:justify;">论文原文：</p> 
<p style="text-align:justify;"><a href="https://arxiv.org/pdf/1904.08245.pdf" rel="nofollow">End-to-End Learning of Representations for Asynchronous Event-Based Data</a></p> 
<p>若不了解事件相机，关于事件相机的综述导读可以阅读笔者的文章：</p> 
<p><a href="https://blog.csdn.net/weixin_47805362/article/details/119568659?spm=1001.2014.3001.5501">Event-based Vision: A Survey——从这篇综述开始了解事件相机（一）</a></p> 
<h2 style="text-align:justify;">1. 论文总览</h2> 
<p>        本篇论文解决了事件相机用于深度学习的一个最基本但也是最关键的问题：<strong>将事件流数据处理为深度网络可以接收的“网格”数据</strong>。目前，常用的事件流表示法有Event-Frame（Single or Two Channel）、Voxel、Time Surface等，它们已经在许多领域大放异彩，表现出超越传统相机的训练结果；哪怕是最简单的Event-Frame形式，在汽车转角预测、图像重建、视频去模糊等问题上也能有优于传统灰度图的结果。但正如笔者多次提到的那样，这几种数据表示法可以说都“浪费”了一部分事件流的信息，在一些问题上也许够用，但还是有很大提升的空间以获得更精确的结果。</p> 
<p style="text-align:center;"><img alt="" height="337" src="https://images2.imgbox.com/f6/5c/JlqGiOLE_o.png" width="254"></p> 
<p>         因此文中提出了上图所示的<strong>Event Spike Tensor（简称EST）</strong>的事件数据表示方法，对于一个四元组事件<img alt="" height="23" src="https://images2.imgbox.com/a6/58/jwFU6Sik_o.png" width="166">，EST能够涵盖其位置、时间、极性等所有信息，因此能够充分利用事件流的时间分辨率、动态范围、运动捕捉等优势。同时，比起笔者经常处理事件流的方式（对事件的极性进行简单的叠加、用极性来表示每个像素点的值），作者还增加了<strong>测量函数、核变换</strong>等流程，使得生成的EST能够包含更多信息，并且更具有“意义”（笔者认为此处的“意义”在于合理的核变换能够使得事件的信息更加充分地体现，即变换后网络更能提取出某些特征），甚至该核函数可以用<strong>多层感知机（MLP）</strong>替代，作为学习的一部分，让网络自己去选择核。</p> 
<p>        本文的亮点在于：</p> 
<p>1）：提出了一种<strong>综合的事件数据表示法EST</strong>。包含了事件流所有的信息，同时处理过程中引入了测量函数和核函数的变换，使得事件流的信息能被更有效地提取；</p> 
<p>2）：提出了一种端到端学习的事件表示方法。用MLP取代核函数，使得EST具有可学习型，实现<strong>事件流—&gt;网络输出的端到端学习</strong>。</p> 
<h2>2. 数学模型 </h2> 
<h3>2.1 Event Spike Tensor —EST 建立</h3> 
<p>        事件相机中DVS模块输出四元组事件流：</p> 
<p style="text-align:center;"><img alt="" height="25" src="https://images2.imgbox.com/6e/a8/xhgHclix_o.png" width="229"></p> 
<p>        上述事件流表示了四元空间中的点，引入<strong>Event Field</strong>这一概念，通过狄拉克脉冲函数<img alt="\delta" class="mathcode" src="https://images2.imgbox.com/6f/b0/cXW7IER0_o.png">将这些事件进行整合，Event Field表示了特定像素点、时间戳处，事件产生的不同极性的脉冲（可以理解为原本事件是点集，现在用函数将其表达出来）：</p> 
<p style="text-align:center;"><img alt="" height="33" src="https://images2.imgbox.com/55/82/P3t199Ql_o.png" width="246"></p> 
<p style="text-align:center;"><img alt="" height="87" src="https://images2.imgbox.com/17/f3/Pk0LBTSD_o.png" width="184"></p> 
<p>         深度卷积网络显然处理不了这样的脉冲（SNN专门处理这种脉冲），因此作者引入了一个定义在事件域的、连续的<strong>测量函数<img alt="f" class="mathcode" src="https://images2.imgbox.com/83/9f/VHF3PtWb_o.png"></strong>来具体化每个事件。这个<img alt="f" class="mathcode" src="https://images2.imgbox.com/08/66/c7KQmEA6_o.png">可以是事件极性（<img alt="\pm 1" class="mathcode" src="https://images2.imgbox.com/50/d3/cnGk3dkT_o.png">），可以是事件数，也可以是平均时间戳，或者是其他任何包含事件流某种信息的值。（该部分笔者这样理解：在事件域这一包含空间像素和时间戳的坐标系中，每个事件原本都是脉冲，通过<img alt="f" class="mathcode" src="https://images2.imgbox.com/a5/15/N6BdKGr8_o.png">来将脉冲替换为一个具体的量，若<img alt="f" class="mathcode" src="https://images2.imgbox.com/1b/42/qk1oBvAk_o.png">是极性，便是Voxel、若是时间戳再压缩，便是Time Surface）</p> 
<p style="text-align:center;"><img alt="" height="34" src="https://images2.imgbox.com/90/b0/T201kp6A_o.png" width="335"></p> 
<p style="text-align:center;"><img alt="" height="87" src="https://images2.imgbox.com/ba/09/mHxcSj7R_o.png" width="191"></p> 
<p>         为了让事件表示方式更加具有“意义”，作者对上述S与一个<strong>核变换函数</strong>进行了卷积操作，只要核函数选取得合适，那么得到的事件表示就有可能让卷积网络学到更多特征。核函数<img alt="k" class="mathcode" src="https://images2.imgbox.com/fc/8e/bKTXzYN1_o.png">内是包含狄拉克脉冲函数的，可以看作它是针对狄拉克函数的一种变化，得到一种有“意义”的函数来将事件具体化。常用的核函数有：指数核函数<img alt="" height="21" src="https://images2.imgbox.com/0e/78/jdyiejTf_o.png" width="208">；alpha核函数：<img alt="" height="23" src="https://images2.imgbox.com/41/d4/dDwR8lv7_o.png" width="92"></p> 
<p> <img alt="" height="22" src="https://images2.imgbox.com/ba/3c/yKp25zw4_o.png" width="129">；三元线性投票核函数：<img alt="" height="21" src="https://images2.imgbox.com/03/c7/XiC8hOBH_o.png" width="205">。当然，核函数没有绝对的优劣，对于特定的问题，有的核函数能使事件数据表现出更多可学习的特征。核函数变换表达式如下：</p> 
<p style="text-align:center;"><img alt="" height="64" src="https://images2.imgbox.com/14/ce/XSdXROMd_o.png" width="297"></p> 
<p style="text-align:center;"><img alt="" height="116" src="https://images2.imgbox.com/27/eb/GoaYZbRC_o.png" width="206"></p> 
<p>         最后，为了使数据“网格化”，以便输入进卷积网络（卷积网络接受数据可以看作以像素为单位的网格），作者对核变换后的事件进行了<strong>取样</strong>，取样是在时间维度进行的，根据事件密度通过选取一个合适的时间间隔，将连续的事件<strong>离散化</strong>，最终得到<strong>EST模型</strong>：</p> 
<p style="text-align:center;"><img alt="" height="65" src="https://images2.imgbox.com/92/fb/mOiCnYsL_o.png" width="333"></p> 
<p style="text-align:center;"><img alt="" height="105" src="https://images2.imgbox.com/24/71/GFt5qvi4_o.png" width="124"></p> 
<p style="text-align:justify;">        其实笔者对上述数学模型理解并不是很透彻，仅仅是理解了一个大意。<strong>笔者看来</strong>，事件一直都存在于上图所示的三维空间中，极性这第四维可以看作是空间中每个点的标记；测试函数<img alt="f" class="mathcode" src="https://images2.imgbox.com/38/4e/sYeXzt0e_o.png">为空间中的事件点赋予有意义的值；核函数卷积（可以看作是对3D图像进行3D卷积操作）对整个空间进行某种运算得到另一个三维空间（依然是坐标和时间三维，但是每个点的值因卷积运算改变了）；最后再将连续的三维空间离散化变为网格得到EST模型。</p> 
<p style="text-align:justify;">        对EST的某个维度进行叠加，即可得到我们常见的几种事件表示（笔者认为对于Voxel这些表达方式来说，可以不加核变换），在此不再展示具体公式。</p> 
<h3> 2.2 端对端学习</h3> 
<p>        本篇论文另一个精妙之处在于实现了<strong>可端对端学习的事件表示方法</strong>。对于核函数<img alt="k" class="mathcode" src="https://images2.imgbox.com/34/cd/fYH8BWTA_o.png">，与其费尽心思去寻找一个合适的，不如直接引入一个充满参数的卷积核，让网络自己决定自己想要什么样的数据。这就好比卷积网络中，卷积操作实为一种特征提取器，而卷积神经网络则是将特征提取器全设为参数，让网络自己学习需要提取的特征。 </p> 
<p>        因此，作者选择采用<strong>多层感知机（MLP）</strong>来替代核函数，事件的坐标和时间戳作为输入，以此生成激活映射（取代核变换的作用）。</p> 
<p><img alt="" height="177" src="https://images2.imgbox.com/63/41/Xz0MyNEE_o.png" width="970">        笔者自己绘制的学习流程如上。作者实现了事件流到输出的端对端学习系统，根据系统得到的输出与目标值的损失，进行反向传播，其中MLP的参数也是需要进行梯度下降更新的，这使得任务本身对事件的表示方式也有了选择权，自然也能有比人工选择核函数更好的训练效果。</p> 
<h2>3. 实验结果</h2> 
<p>         作者在<strong>物体识别</strong>和<strong>光流估计</strong>这两个具体问题上进行了实验分析，选用了多种事件表示方式进行了对比。同时对于不同的事件表示方式，作者还对比了测量函数与核函数对结果的影响（端对端学习属于不同核函数的选取），最终都能发现选用MLP进行端对端学习的方式表现得更为出色。由于笔者对光流计算了解甚少，此处只展示物体识别的结果，望读者见谅。</p> 
<p>        作者分别在N-Cars以及N-Caltech101这两个开放数据集上进行了测试，采用了Resnet-34的网络结构，先在ImageNet上进行了预训练，选用了交叉熵损失函数和ADAM优化算法，初始学习率为1e-5，每10000次迭代就减少2倍，N-Cars与N-Caltech101采用的batch分别为60和100。识别任务结果如下：</p> 
<p style="text-align:center;"><img alt="" height="263" src="https://images2.imgbox.com/b2/1e/pc1r0a4H_o.png" width="416"></p> 
<p>         从上表可以看出，采用<strong>时间戳作为测量函数<img alt="f" class="mathcode" src="https://images2.imgbox.com/27/4d/WLWKE5DL_o.png">效果最好</strong>，这也不难理解，毕竟这样事件信息更加丰富（即便是EST也压缩了部分时间，时间戳作为测量函数能够补充一定的时间信息）；而在核函数选取方面，对于固定的核函数来说，三元线性投票函数的效果最佳，但也比不上<strong>端对端学习</strong>的方式。综合来看，<strong>采用时间戳作为测量函数，将核变换替换为MLP进行端对端学习的事件表示方式，在物体识别这个任务上表现得最佳</strong>，即便不引入MLP，EST相较于其他常用表示方法来说也具有优势，这也证明了这篇论文提出的EST的有效性。</p> 
<h2>4. 小结</h2> 
<p>        本篇论文可谓是所有事件数据表示方法的集大成者，给出了一种综合的事件表示模型EST，构建了一种事件数据转化的通用框架，涵盖了事件流几乎所有的有用信息，同时还能经过叠加转化为Voxel等我们常用的类型，更不用说引入MLP实现了端对端学习。</p> 
<p>        当然，本文对笔者来说，最大的意义在于充分了解了每一种事件数据的表示方法，以便针对不同问题去使用，毕竟即便是效果最差的Event Frame，相较于传统灰度图来说都是极具优势的，因此没必要在每个问题上都要采取EST进行端对端学习（EST数据量较大，使得训练消耗的时间更多）。但作者提出的数据表示框架还是极具价值的，如果能够正确复现并广泛使用，那么将节省很多数据处理的功夫，我认为这也是事件相机领域发展成熟的关键一步。</p> 
<p>        最后，谢谢大家读到最后，文中许多细节笔者未作太多阐述，若想复现该篇论文，读者还需更加深入阅读，请见谅。 </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/811d02b22616456174040bb8ceec2e9e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">常考基础知识</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/53b08d542057f0c564c35ec7576bf1c3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">单元测试，报java.lang.NoClassDefFoundError:org/springframework/test/content/TestContesxtAnnotationUtils</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>