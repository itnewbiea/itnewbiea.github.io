<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>k8s的ceph - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="k8s的ceph" />
<meta property="og:description" content="ceph安装
地址：https://rook.io/docs/rook/v1.8/quickstart.html
特性丰富
1，支持三种存储接口：块存储、文件存储、对象存储。
2，支持自定义接口，支持多种语言驱动。
基本概念
Ceph OSD
Object Storage Device是ceph的核心组件，用于存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向Ceph Monitors提供一些监控信息。ceph集群每台机器上的每块盘需要运行一个OSD进程。
每个OSD有自己日志，生产环境一般要求OSD的journal日志单独放在ssd上，数据对象放在机械硬盘，以提升性能。而OSD使用日志的原因有二：速度和一致性。
1，速度： 日志使得OSD可以快速地提交小块数据的写入，Ceph把小片、随机IO依次写入日志，这样，后端文件系统就有可能归并写入动作，并最终提升并发承载力。因此，使用OSD日志能展现出优秀的突发写性能，实际上数据还没有写入OSD，因为文件系统把它们捕捉到了日志。
2，一致性： Ceph的OSD守护进程需要一个能保证原子操作的文件系统接口。OSD把一个操作的描述写入日志，然后把操作应用到文件系统，这需要原子更新一个对象（例如归置组元数据）。每隔一段 filestore max sync interval 和 filestore min sync interval之间的时间，OSD停止写入、把日志同步到文件系统，这样允许 OSD 修整日志里的操作并重用空间。若失败， OSD 从上个同步点开始重放日志。
Monitor
Ceph Monitor维护着展示集群状态的各种图表，包括监视器图、 OSD图、归置组（ PG ）图、和 CRUSH 图。 Ceph 保存着发生在Monitors、OSD和PG上的每一次状态变更的历史信息（称为epoch ）。Monitor支持高可用部署，可以运行多个Monitor组成一个集群，一个监视器集群确保了当某个监视器失效时的高可用性。存储集群客户端向Ceph监视器索取集群运行图的最新副本，而集群通过Paxos算法就集群当前状态保持一致。
MDS
Ceph Metadata Server为Ceph文件系统存储元数据（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。Metadata Server使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 ls、find 等基本命令。
CRUSH
CRUSH 是 Ceph 使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。Ceph 客户端和 OSD 守护进程都用 CRUSH 算法来计算对象的位置信息，而不是依赖于一个中心化的查询表。与以往方法相比， CRUSH 的数据管理机制更好，它很干脆地把工作分配给集群内的所有客户端和 OSD 来处理，因此具有极大的伸缩性。 CRUSH 用智能数据复制确保弹性，更能适应超大规模存储。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/2519d118f51a7cfd4ef8729ed9b06a24/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-30T11:51:53+08:00" />
<meta property="article:modified_time" content="2022-03-30T11:51:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">k8s的ceph</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>ceph安装<br> 地址：https://rook.io/docs/rook/v1.8/quickstart.html</p> 
<p>特性丰富<br> 1，支持三种存储接口：块存储、文件存储、对象存储。<br> 2，支持自定义接口，支持多种语言驱动。</p> 
<p>基本概念<br> Ceph OSD<br> Object Storage Device是ceph的核心组件，用于存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向Ceph Monitors提供一些监控信息。ceph集群每台机器上的每块盘需要运行一个OSD进程。</p> 
<p>每个OSD有自己日志，生产环境一般要求OSD的journal日志单独放在ssd上，数据对象放在机械硬盘，以提升性能。而OSD使用日志的原因有二：速度和一致性。<br> 1，速度： 日志使得OSD可以快速地提交小块数据的写入，Ceph把小片、随机IO依次写入日志，这样，后端文件系统就有可能归并写入动作，并最终提升并发承载力。因此，使用OSD日志能展现出优秀的突发写性能，实际上数据还没有写入OSD，因为文件系统把它们捕捉到了日志。<br> 2，一致性： Ceph的OSD守护进程需要一个能保证原子操作的文件系统接口。OSD把一个操作的描述写入日志，然后把操作应用到文件系统，这需要原子更新一个对象（例如归置组元数据）。每隔一段 filestore max sync interval 和 filestore min sync interval之间的时间，OSD停止写入、把日志同步到文件系统，这样允许 OSD 修整日志里的操作并重用空间。若失败， OSD 从上个同步点开始重放日志。</p> 
<p>Monitor<br> Ceph Monitor维护着展示集群状态的各种图表，包括监视器图、 OSD图、归置组（ PG ）图、和 CRUSH 图。 Ceph 保存着发生在Monitors、OSD和PG上的每一次状态变更的历史信息（称为epoch ）。Monitor支持高可用部署，可以运行多个Monitor组成一个集群，一个监视器集群确保了当某个监视器失效时的高可用性。存储集群客户端向Ceph监视器索取集群运行图的最新副本，而集群通过Paxos算法就集群当前状态保持一致。</p> 
<p>MDS<br> Ceph Metadata Server为Ceph文件系统存储元数据（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。Metadata Server使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 ls、find 等基本命令。</p> 
<p>CRUSH<br> CRUSH 是 Ceph 使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。Ceph 客户端和 OSD 守护进程都用 CRUSH 算法来计算对象的位置信息，而不是依赖于一个中心化的查询表。与以往方法相比， CRUSH 的数据管理机制更好，它很干脆地把工作分配给集群内的所有客户端和 OSD 来处理，因此具有极大的伸缩性。 CRUSH 用智能数据复制确保弹性，更能适应超大规模存储。</p> 
<p>Object<br> Ceph 最底层的存储单元是 Object 对象，每个 Object 包含元数据和原始数据。</p> 
<p>PG<br> PG 全称 Placement Grouops，是一个逻辑的概念，一个 PG 包含多个Object。引入PG这一层其实是为了更好的分配数据和定位数据。</p> 
<p>存储pool<br> Ceph 存储系统支持“池”概念，它是存储对象的逻辑分区，集群部署起来之后会有一个rdb的默认存储池。一个存储池可以设置PG的数量，CRUSH规则集，访问控制等。</p> 
<p>RADOS<br> RADOS 全称 Reliable Autonomic Distributed Object Store，是 Ceph 集群的核心，用户实现数据分配、Failover 等集群操作。</p> 
<p>Libradio<br> Librados 是 RADOS 的接口库，因为 RADOS 是协议很难直接访问，因此上层的 RBD、RGW 和 CephFS 都是通过 librados 访问的。Ceph的客户端通过一套名为librados的接口进行集群的访问，这里的访问包括对集群的整体访问和对象的访问两类接口。这套接口（API）包括C、C++和Python常见语言的实现，接口通过网络实现对Ceph集群的访问。在用户层面，可以在自己的程序中调用该接口，从而集成Ceph集群的存储功能，或者在监控程序中实现对Ceph集群状态的监控。</p> 
<p>RBD<br> RBD 全称 RADOS block device，是 Ceph 对外提供的块设备服务。Ceph块设备是精简配置的、大小可调且将数据条带化存储到集群内的多个 OSD 。 Ceph 块设备利用 RADOS 的多种能力，如快照、复制和一致性。 Ceph 的 RADOS 块设备（ RBD ）使用内核模块或 librbd 库与 OSD 交互。</p> 
<p>RGW<br> RGW 全称 RADOS gateway，是 Ceph 对外提供的对象存储服务，接口与 S3 和 Swift 兼容。</p> 
<p>CephFS<br> CephFS 全称 Ceph File System，是Ceph对外提供的文件系统服务。</p> 
<p><br> 一：<span style="color:#fe2c24;">k8s集群节点都可以新加一块没有格式化的磁盘，三个节点都新加一块磁盘20G。只是测试而已。</span><br> # 扫描 SCSI总线并添加 SCSI 设备<br> for host in $(ls /sys/class/scsi_host) ; do echo "- - -"&gt;/sys/class/scsi_host/$host/scan; done </p> 
<p># 查看已添加的磁盘 sdb<br> [root@master01 ceph]# lsblk <br> NAME                              MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT<br> sdb                                8:16   0   20G  0   disk  <br> sr0                                11:0   1  4.5G  0   rom  <br> sda                                8:0    0  100G  0   disk <br> ├─sda2                             8:2    0   99G  0   part <br> │ ├─centos-swap                    253:1  0    2G  0   lvm  <br> │ ├─centos-home                    253:2  0   47G  0   lvm   /home<br> │ └─centos-root                    253:0  0   50G  0   lvm   /<br> └─sda1                             8:1    0    1G  0   part  /boot<br>  </p> 
<p>1：<span style="color:#fe2c24;">新磁盘清零，防止磁盘有数据导致pvc创建有问题</span><br> [root@master01 kubernetes]# dd if=/dev/zero of=/dev/sdb bs=1M status=progress</p> 
<p>2，下载rook<br> https://github.com/rook/rook/tree/v1.5.5</p> 
<p>3.解压，构建<br> [root@master01 ~]# cd rook/<br> [root@master01 rook]# unzip rook-1.5.5.zip<br> [root@master01 rook]# cd rook-1.5.5/cluster/examples/kubernetes/ceph/</p> 
<p><br> #<span style="color:#fe2c24;">所有节点加载rdb模块</span><br> [root@master01 ~]# modprobe rbd<br> [root@master01 ~]# cat &gt; /etc/sysconfig/modules/rbd.modules &lt;&lt; EOF<br> modprobe rbd<br> EOF</p> 
<p>#加载模块<br> [root@master01 ~]# chmod 755 /etc/sysconfig/modules/rbd.modules<br> [root@master01 ~]# source /etc/sysconfig/modules/rbd.modules<br> [root@master01 ~]# source /etc/sysconfig/modules/rbd.modules<br> [root@master01 ~]# lsmod |grep rbd</p> 
<p>#<span style="color:#fe2c24;">部署ceph集群服务osd，mon，mgr节点打上标签</span><br> [root@master01 ceph]# kubectl label nodes {master01,node01,node02} ceph-osd=enabled<br> [root@master01 ceph]# kubectl label nodes {master01,node01,node02} ceph-mon=enabled<br> [root@master01 ceph]# kubectl label nodes {node01,node02} ceph-mgr=enabled   #只有一个mgr使用，但是就怕node01挂了，mgr的pod无法应用到k8s节点。</p> 
<p>#创建rbac，serviceaccount，clusterrole，clusterrolebinding等资源<br> [root@master01 ceph]# kubectl apply -f common.yaml </p> 
<p>#查看<br> [root@master01 ceph]# kubectl get sa -n rook-ceph</p> 
<p>[root@master01 ceph]# kubectl get clusterrole -n rook-ceph</p> 
<p>[root@master01 ceph]# kubectl get clusterrolebinding -n rook-ceph</p> 
<p>[root@master01 ceph]# kubectl get role -n rook-ceph</p> 
<p>#<span style="color:#fe2c24;">创建自定义扩展资源对象</span><br> [root@master01 ceph]# kubectl apply -f crds.yaml </p> 
<p>#<span style="color:#fe2c24;">镜像下载脚本.推送到私有仓库harbor.od.com</span><br> [root@master01 ceph]# vi ceph-images-download.sh<br> #/bin/bash</p> 
<p>#下面镜像要根据ceph的operator.yaml文件要求<br> image_list=(<br>     ceph:v15.2.8<br>     rook-ceph:v1.5.5<br>     cephcsi:v3.2.0<br>     csi-node-driver-registrar:v2.0.1<br>     csi-resizer:v1.0.0<br>     csi-provisioner:v2.0.0<br>     csi-snapshotter:v3.0.0<br>     csi-attacher:v3.0.0<br> )</p> 
<p>aliyuncs="registry.aliyuncs.com/it00021hot"<br> google_gcr="k8s.gcr.io/sig-storage"<br> harbor="harbor.od.com/ceph"<br> for image in ${image_list[*]}<br> do<br>    docker image pull ${aliyuncs}/${image}</p> 
<p>   #下面这行下载镜像后，不需要修改operator.yaml文件<br>    # docker image tag ${aliyuncs}/${image} ${google_gcr}/${image}</p> 
<p>   docker image tag ${aliyuncs}/${image} ${harbor}/${image}<br>    docker image rm ${aliyuncs}/${image}<br>    docker push ${harbor}/${image}<br>    echo "${aliyuncs}/${image} ${google_gcr}/${image} downloaded"<br> done</p> 
<p>#下载镜像<br> [root@yunwei ~]# sh ceph-images-download.sh </p> 
<p>#修改<span style="color:#fe2c24;">operator.yaml文件，主要是下面两个地方</span>。<br> [root@master01 ceph]# vi operator.yaml <br>  spec:<br>       serviceAccountName: rook-ceph-system<br>       containers:<br>       - name: rook-ceph-operator<br>         image: harbor.od.com/ceph/rook-ceph:v1.5.5  <span style="color:#fe2c24;">#这里也要修改</span><br>         args: ["ceph", "operator"]<br>         volumeMounts:<br>         - mountPath: /var/lib/rook<br>           name: rook-config<br>         - mountPath: /etc/ceph<br>           name: default-config-dir<br> .............................................................................<br> ...............................................................................          <br>   # ROOK_CSI_PROVISIONER_IMAGE: "k8s.gcr.io/sig-storage/csi-provisioner:v2.0.0"<br>   # ROOK_CSI_SNAPSHOTTER_IMAGE: "k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.0"<br>   # ROOK_CSI_ATTACHER_IMAGE: "k8s.gcr.io/sig-storage/csi-attacher:v3.0.0"</p> 
<p>#<span style="color:#fe2c24;">下面添加内容，要注意缩进</span><br>   ROOK_CSI_CEPH_IMAGE: "harbor.od.com/ceph/cephcsi:v3.2.0"<br>   ROOK_CSI_REGISTRAR_IMAGE: "harbor.od.com/ceph/csi-node-driver-registrar:v2.0.1"<br>   ROOK_CSI_RESIZER_IMAGE: "harbor.od.com/ceph/csi-resizer:v1.0.0"<br>   ROOK_CSI_PROVISIONER_IMAGE: "harbor.od.com/ceph/csi-provisioner:v2.0.0"<br>   ROOK_CSI_SNAPSHOTTER_IMAGE: "harbor.od.com/ceph/csi-snapshotter:v3.0.0"<br>   ROOK_CSI_ATTACHER_IMAGE: "harbor.od.com/ceph/csi-attacher:v3.0.0"</p> 
<p><br> #创建operator.yaml <br> [root@master01 ceph]# kubectl apply -f operator.yaml <br> configmap/rook-ceph-operator-config created<br> deployment.apps/rook-ceph-operator created</p> 
<p>[root@master01 ceph]# kubectl get pods -n rook-ceph<br> NAME                                  READY   STATUS    RESTARTS   AGE<br> rook-ceph-operator-64cbcf46cb-5hd9f   1/1     Running   0          12s</p> 
<p>[root@master01 ceph]# kubectl get deployment -n rook-ceph<br> NAME                 READY   UP-TO-DATE   AVAILABLE   AGE<br> rook-ceph-operator   1/1     1            1           27s</p> 
<p>#查看<span style="color:#fe2c24;">operator容器</span><br> [root@master01 ceph]# kubectl get pods -n rook-ceph<br> NAME                                  READY   STATUS            RESTARTS   AGE<br> rook-ceph-operator-64cbcf46cb-5hd9f   1/1     Running           0          13m</p> 
<p>#查看<span style="color:#fe2c24;">operator容器日志，可以查看ceph初始化</span>。<br> [root@master01 ceph]# kubectl logs -f  rook-ceph-operator-64cbcf46cb-5hd9f -n rook-ceph</p> 
<p>#由于<span style="color:#fe2c24;">只有三个节点，所以需要打标签。在cluster.yaml配置亲和性调到pod</span><br> [root@master01 ceph]# kubectl label nodes {master01,node02,node01} ceph-mon=enabled</p> 
<p>[root@master01 ceph]# kubectl label nodes {master01,node02,node01} ceph-osd=enabled</p> 
<p>[root@master01 ceph]# kubectl label nodes master01 ceph-mgr=enabled</p> 
<p><br> #创建ceph的集群,下面是主要配置信息<br> apiVersion: ceph.rook.io/v1<br> kind: CephCluster<br> metadata:<br> # 命名空间的名字，同一个命名空间只支持一个集群<br>   name: rook-ceph<br>   namespace: rook-ceph<br> spec:<br> # ceph版本说明<br> # v13 is mimic, v14 is nautilus, and v15 is octopus.<br>   cephVersion:<br> #修改ceph镜像，加速部署时间<br>     image: harbor.foxchan.com/google_containers/ceph/ceph:v15.2.5<br> # 是否允许不支持的ceph版本<br>     allowUnsupported: false<br> #指定rook数据在节点的保存路径<br>   dataDirHostPath: /data/rook<br> # 升级时如果检查失败是否继续<br>   skipUpgradeChecks: false<br> # 从1.5开始，mon的数量必须是奇数<br>   mon:<br>     count: 3<br> # 是否允许在单个节点上部署多个mon pod<br>     allowMultiplePerNode: false<br>   mgr:<br>     modules:<br>     - name: pg_autoscaler<br>       enabled: true<br> # 开启dashboard，禁用ssl，指定端口是7000，你可以默认https配置。我是为了ingress配置省事。<br>   dashboard:<br>     enabled: true<br>     port: 7000<br>     ssl: false<br> # 开启prometheusRule<br>   monitoring:<br>     enabled: true<br> # 部署PrometheusRule的命名空间，默认此CR所在命名空间<br>     rulesNamespace: rook-ceph<br> # 开启网络为host模式，解决无法使用cephfs pvc的bug<br>   network:<br>     provider: host<br> # 开启crash collector，每个运行了Ceph守护进程的节点上创建crash collector pod<br>   crashCollector:<br>     disable: false<br> # 设置node亲缘性，指定节点安装对应组件<br>   placement:<br>     mon:<br>       nodeAffinity:<br>         requiredDuringSchedulingIgnoredDuringExecution:<br>           nodeSelectorTerms:<br>           - matchExpressions:<br>             - key: ceph-mon<br>               operator: In<br>               values:<br>               - enabled<br>         <br>     osd:<br>       nodeAffinity:<br>         requiredDuringSchedulingIgnoredDuringExecution:<br>           nodeSelectorTerms:<br>           - matchExpressions:<br>             - key: ceph-osd<br>               operator: In<br>               values:<br>               - enabled</p> 
<p>    mgr:<br>       nodeAffinity:<br>         requiredDuringSchedulingIgnoredDuringExecution:<br>           nodeSelectorTerms:<br>           - matchExpressions:<br>             - key: ceph-mgr<br>               operator: In<br>               values:<br>               - enabled <br> # 存储的设置，默认都是true，意思是会把集群所有node的设备清空初始化。<br>   storage: # cluster level storage configuration and selection<br>     useAllNodes: false     #关闭使用所有Node<br>     useAllDevices: false   #关闭使用所有设备<br>     nodes:<br>     - name: "192.168.1.162"  #指定存储节点主机<br>       devices:<br>       - name: "nvme0n1p1"    #指定磁盘为nvme0n1p1<br>     - name: "192.168.1.163"<br>       devices:<br>       - name: "nvme0n1p1"<br>     - name: "192.168.1.164"<br>       devices:<br>       - name: "nvme0n1p1"<br>     - name: "192.168.1.213"<br>       devices:<br>       - name: "nvme0n1p1"</p> 
<p><br> #修改<span style="color:#fe2c24;">cluster.yaml配置文件,注意缩进</span><br> [root@master01 ceph]# cat cluster.yaml|grep -v '#'<br> apiVersion: ceph.rook.io/v1<br> kind: CephCluster<br> metadata:<br>   name: rook-ceph<br> spec:<br>   cephVersion:<br>     image: ceph/ceph:v15.2.8      #镜像<br>     allowUnsupported: false<br>   dataDirHostPath: /var/lib/rook   #rook集群的配置数据存在主机上路径<br>   skipUpgradeChecks: false<br>   continueUpgradeAfterChecksEvenIfNotHealthy: false<br>   mon:<br>     count: 3<br>     allowMultiplePerNode: false<br>   mgr:<br>     modules:<br>     - name: pg_autoscaler<br>       enabled: true<br>   dashboard:<br>     enabled: true<br>     ssl: true<br>   monitoring:<br>     enabled: false<br>     rulesNamespace: rook-ceph<br>   network:<br>   crashCollector:<br>     disable: false<br>   cleanupPolicy:<br>     confirmation: ""<br>     sanitizeDisks:<br>       method: quick<br>       dataSource: zero<br>       iteration: 1<br>     allowUninstallWithVolumes: false<br>   placement:<br>     mon:<br>       nodeAffinity:<br>         requiredDuringSchedulingIgnoredDuringExecution:<br>           nodeSelectorTerms:<br>           - matchExpressions:<br>             - key: ceph-mon<br>               operator: In<br>               values:<br>               - enabled<br>       tolerations:              ##<span style="color:#fe2c24;">容忍master污点，可以部署pod</span><br>       - key: node-role.kubernetes.io/master<br>         effect: NoSchedule<br>       - key: ceph-mon     #<span style="color:#fe2c24;">部署节点有标签为 ceph-mon节点上</span><br>         operator: Exists<br>         <br>     osd:<br>       nodeAffinity:<br>         requiredDuringSchedulingIgnoredDuringExecution:<br>           nodeSelectorTerms:<br>           - matchExpressions:<br>             - key: ceph-osd<br>               operator: In<br>               values:<br>               - enabled<br>       tolerations:<br>       - key: node-role.kubernetes.io/master<br>         effect: NoSchedule<br>       - key: ceph-osd<br>         operator: Exists<br>         <br>     mgr:<br>       nodeAffinity:            #<span style="color:#fe2c24;">配置节点亲和度</span><br>         requiredDuringSchedulingIgnoredDuringExecution:<br>           nodeSelectorTerms:<br>           - matchExpressions:<br>             - key: ceph-mgr<br>               operator: In<br>               values:<br>               - enabled<br>       tolerations:              #<span style="color:#fe2c24;">容忍master污点，可以部署pod</span><br>       - key: node-role.kubernetes.io/master<br>         effect: NoSchedule<br>       - key: ceph-mgr<br>         operator: Exists<br>         <br>   annotations:<br>   labels:<br>   resources:<br>   removeOSDsIfOutAndSafeToRemove: false<br>   storage:<br>     useAllNodes: false        #<span style="color:#fe2c24;">关闭使用所有Node</span><br>     useAllDevices: false      #<span style="color:#fe2c24;">关闭使用所有设备</span>  <br>     config:<br>       osdsPerDevice: "1"<br>     nodes:<br>     - name: "master01"    #<span style="color:#fe2c24;">指定节点主机，这个名称是根据节点标签labels的</span> kubernetes.io/hostname=master01<br>       config:<br>         storeType: bluestore                               #指定类型为裸磁盘<br>       devices:                                             #指定磁盘为sdb<br>       - name: "sdb"    <br>     - name: "node01"           <br>       config:<br>         storeType: bluestore   #<span style="color:#fe2c24;">指定类型为裸磁盘</span><br>       devices: <br>       - name: "sdb"            <span style="color:#fe2c24;">#指定磁盘名称</span><br>     - name: "node02"<br>       config:<br>         storeType: bluestore<br>       devices:<br>       - name: "sdb"</p> 
<p>  disruptionManagement:<br>     managePodBudgets: false<br>     osdMaintenanceTimeout: 30<br>     pgHealthCheckTimeout: 0<br>     manageMachineDisruptionBudgets: false<br>     machineDisruptionBudgetNamespace: openshift-machine-api</p> 
<p>  healthCheck:<br>     daemonHealth:<br>       mon:<br>         disabled: false<br>         interval: 45s<br>       osd:<br>         disabled: false<br>         interval: 60s<br>       status:<br>         disabled: false<br>         interval: 60s<br>     livenessProbe:<br>       mon:<br>         disabled: false<br>       mgr:<br>         disabled: false<br>       osd:<br>         disabled: false<br>               <br>               <br> #创建ceph的集群              <br> [root@master01 ceph]# kubectl apply -f cluster.yaml </p> 
<p>#csi-rbdplugin，csi-cephfsplugin 是<span style="color:#fe2c24;">daemonset控制器，每个节点都要部署</span>，但是他们都只有两个pod，master节点无法部署，需要下面修改配置，部署到master节点。  <br> [root@master01 ceph]# kubectl get pods -n rook-ceph -o wide<br> NAME                                                 READY   STATUS      RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES<br> csi-cephfsplugin-8v7hs                               3/3     Running     0          40m     10.4.7.51     node02     &lt;none&gt;           &lt;none&gt;<br> csi-cephfsplugin-cfq78                               3/3     Running     0          40m     10.4.7.50     node01     &lt;none&gt;           &lt;none&gt;<br> csi-cephfsplugin-provisioner-745fcb7868-mqrhd        6/6     Running     0          40m     10.244.2.14   node02     &lt;none&gt;           &lt;none&gt;<br> csi-cephfsplugin-provisioner-745fcb7868-x8sww        6/6     Running     0          40m     10.244.1.11   node01     &lt;none&gt;           &lt;none&gt;<br> csi-rbdplugin-bwjgr                                  3/3     Running     0          41m     10.4.7.51     node02     &lt;none&gt;           &lt;none&gt;<br> csi-rbdplugin-gg28f                                  3/3     Running     0          41m     10.4.7.50     node01     &lt;none&gt;           &lt;none&gt;<br> csi-rbdplugin-provisioner-7fdb4675dc-7m7gf           6/6     Running     0          41m     10.244.2.13   node02     &lt;none&gt;           &lt;none&gt;<br> csi-rbdplugin-provisioner-7fdb4675dc-krt8d           6/6     Running     0          41m     10.244.1.10   node01     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-crashcollector-master01-5f7dbf46fc-lfzkf   1/1     Running     0          9m37s   10.244.0.18   master01   &lt;none&gt;           &lt;none&gt;<br> rook-ceph-crashcollector-node01-7ffdfd64c8-2cksj     1/1     Running     0          11m     10.244.1.27   node01     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-crashcollector-node02-8889897f4-x6lcp      1/1     Running     0          11m     10.244.2.26   node02     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-mgr-a-5dcb79d55d-zzpms                     1/1     Running     0          12m     10.244.1.23   node01     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-mon-a-68fb45ddb-9ltqt                      1/1     Running     0          13m     10.244.2.22   node02     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-mon-b-749978f45f-nhxqv                     1/1     Running     0          13m     10.244.0.15   master01   &lt;none&gt;           &lt;none&gt;<br> rook-ceph-mon-c-59fff7cfc5-tn6gt                     1/1     Running     0          13m     10.244.1.21   node01     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-operator-d459696cf-x8zq9                   1/1     Running     0          64m     10.244.1.9    node01     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-osd-0-68585fc65-2xvj2                      1/1     Running     0          11m     10.244.2.25   node02     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-osd-1-6b65c4f6c4-s2rrq                     1/1     Running     0          11m     10.244.1.26   node01     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-osd-2-78788d78b6-vjsgb                     1/1     Running     0          9m37s   10.244.0.17   master01   &lt;none&gt;           &lt;none&gt;<br> rook-ceph-osd-prepare-master01-7wm87                 0/1     Completed   0          12m     10.244.0.16   master01   &lt;none&gt;           &lt;none&gt;<br> rook-ceph-osd-prepare-node01-mbjbj                   0/1     Completed   0          12m     10.244.1.25   node01     &lt;none&gt;           &lt;none&gt;<br> rook-ceph-osd-prepare-node02-gfkpq                   0/1     Completed   0          12m     10.244.2.24   node02     &lt;none&gt;           &lt;none&gt;</p> 
<p><br> 2、检查csi-rbdplugin，csi-cephfsplugin，csi-rbdplugin-provisioner，csi-cephfsplugin-provisioner的调度控制器</p> 
<p>[root@master01 ~]# kubectl get deployment,daemonset,statefulset -n rook-ceph  | grep csi-rbdplugin<br> deployment.apps/csi-rbdplugin-provisioner           2/2     2            2           23h<br> daemonset.apps/csi-rbdplugin      3         3         2       1            2           &lt;none&gt;          23h</p> 
<p>[root@master01 ~]# kubectl get deployment,daemonset,statefulset -n rook-ceph  | grep csi-cephfsplugin<br> deployment.apps/csi-cephfsplugin-provisioner        2/2     2            2           23h<br> daemonset.apps/csi-cephfsplugin   3         3         2       3            2           &lt;none&gt;          23h</p> 
<p>注解：<br>    检查发现csi-rbdplugin-provisioner，csi-cephfsplugin-provisioner是<span style="color:#fe2c24;">deployment</span>控制器，<br> csi-rbdplugin，csi-cephfsplugin 是<span style="color:#fe2c24;">daemonset</span>控制器，daemonset控制器可以保障每个可调度节点运行一份副本，<br> 做为节点csi驱动daemonset相比deployment更适合一些，那么csi-rbdplugin更有可能是csi驱动管理pod，<br> 那么我们就先将它调度到master节点试试。</p> 
<p>1.修改<span style="color:#fe2c24;">csi-rbdplugin的daemonset配置，可以部署到master节点。</span><br> [root@master01 ~]# kubectl edit daemonset csi-rbdplugin -n rook-ceph<br> 。。。。。。。。。。。<br> spec:<br>   revisionHistoryLimit: 10<br>   selector:<br>     matchLabels:<br>       app: csi-rbdplugin<br>   template:<br>     metadata:<br>       creationTimestamp: null<br>       labels:<br>         app: csi-rbdplugin<br>         contains: csi-rbdplugin-metrics<br>     spec:<br>       affinity:<br>         nodeAffinity: {}<br>       tolerations:    <span style="color:#fe2c24;"> #添三行，pod部署到master</span><br>       - key: node-role.kubernetes.io/master<br>         effect: NoSchedule<br>       containers:<br>       - args:<br>         - --v=0<br>         - --csi-address=/csi/csi.sock</p> 
<p>2.修改<span style="color:#fe2c24;">csi-cephfsplugin </span><br> [root@master01 ~]# kubectl edit daemonset csi-cephfsplugin -n rook-ceph<br> spec:<br>   revisionHistoryLimit: 10<br>   selector:<br>     matchLabels:<br>       app: csi-cephfsplugin<br>   template:<br>     metadata:<br>       creationTimestamp: null<br>       labels:<br>         app: csi-cephfsplugin<br>         contains: csi-cephfsplugin-metrics<br>     spec:<br>       affinity:<br>         nodeAffinity: {}<br>       tolerations:     <span style="color:#fe2c24;">#添三行，pod部署到master</span><br>       - key: node-role.kubernetes.io/master<br>         effect: NoSchedule<br>       containers:<br>       - args:<br>         - --v=0<br>         - --csi-address=/csi/csi.sock</p> 
<p>#查看修改后pod，<span style="color:#fe2c24;">csi-rbdplugin，csi-cephfsplugin都是三个pod</span>，<br><span style="color:#fe2c24;">csi-cephfsplugin-provisioner，csi-rbdplugin-provisioner的deployment配置文件定义是2个副本</span>，所以是对的。</p> 
<p>[root@master01 template]# kubectl get pods -n rook-ceph -o wide<br> NAME                                                 READY   STATUS      RESTARTS   AGE    IP            NODE       <br> csi-cephfsplugin-5mll4                               3/3     Running     0          25m    10.4.7.50     node01    <br> csi-cephfsplugin-bz6v6                               3/3     Running     0          25m    10.4.7.49     master01  <br> csi-cephfsplugin-hgx4v                               3/3     Running     0          25m    10.4.7.51     node02     <br> csi-cephfsplugin-provisioner-745fcb7868-mqrhd        6/6     Running     37         24h    10.244.2.38   node02    <br> csi-cephfsplugin-provisioner-745fcb7868-x8sww        6/6     Running     49         24h    10.244.1.38   node01    <br> csi-rbdplugin-4mm7q                                  3/3     Running     0          26m    10.4.7.51     node02    <br> csi-rbdplugin-9bv9k                                  3/3     Running     0          56m    10.4.7.49     master01   <br> csi-rbdplugin-f5wrx                                  3/3     Running     0          26m    10.4.7.50     node01    <br> csi-rbdplugin-provisioner-7fdb4675dc-7m7gf           6/6     Running     42         24h    10.244.2.40   node02   <br> csi-rbdplugin-provisioner-7fdb4675dc-krt8d           6/6     Running     43         24h    10.244.1.34   node01    <br> rook-ceph-crashcollector-master01-5f7dbf46fc-lfzkf   1/1     Running     2          23h    10.244.0.29   master01 <br> rook-ceph-crashcollector-node01-7ffdfd64c8-2cksj     1/1     Running     1          23h    10.244.1.39   node01    <br> rook-ceph-crashcollector-node02-8889897f4-x6lcp      1/1     Running     1          23h    10.244.2.43   node02     <br> rook-ceph-mgr-a-5dcb79d55d-zzpms                     1/1     Running     1          23h    10.244.1.36   node01    <br> rook-ceph-mon-a-68fb45ddb-9ltqt                      1/1     Running     1          23h    10.244.2.39   node02     <br> rook-ceph-mon-b-749978f45f-nhxqv                     1/1     Running     2          23h    10.244.0.30   master01   <br> rook-ceph-mon-c-59fff7cfc5-tn6gt                     1/1     Running     1          23h    10.244.1.35   node01    <br> rook-ceph-operator-d459696cf-x8zq9                   1/1     Running     1          24h    10.244.1.40   node01    <br> rook-ceph-osd-0-68585fc65-2xvj2                      1/1     Running     1          23h    10.244.2.42   node02     <br> rook-ceph-osd-1-6b65c4f6c4-s2rrq                     1/1     Running     1          23h    10.244.1.37   node01    <br> rook-ceph-osd-2-78788d78b6-vjsgb                     1/1     Running     3          23h    10.244.0.34   master01   <br> rook-ceph-osd-prepare-master01-kl2gp                 0/1     Completed   0          101m   10.244.0.36   master01   <br> rook-ceph-osd-prepare-node01-mxnn6                   0/1     Completed   0          101m   10.244.1.41   node01    <br> rook-ceph-osd-prepare-node02-4sm6n                   0/1     Completed   0          101m   10.244.2.44   node02    </p> 
<p>#<span style="color:#fe2c24;">查看主机上的磁盘是否被调用，如果没有，就要查看下cluster.yaml配置</span><br> [root@master01 static]# lsblk -f<br> NAME                                                                                    FSTYPE      LABEL           UUID                                   MOUNTPOINT<br> sdb                                                                                     LVM2_member                 3PQAnX-rehr-fxu0-bIMJ-fYWS-Zcaz-ublUvk <br> └─ceph--4ce439e6--f251--4fc3--9e75--375ed61d6d8b-osd--block--4e99fb5a--9c32--482d--897f--6b689f60c946<br>                                                                <br> #查看查看rook-ceph-operatorpod的log<br> [root@master01 ceph]# kubectl logs -f rook-ceph-operator-d459696cf-x8zq9 -n rook-ceph</p> 
<p>#查看osd<br> [root@master01 ceph]# kubectl logs -f rook-ceph-osd-prepare-master01-7wm87 -n rook-ceph provision</p> 
<p>#<span style="color:#fe2c24;">查看ceph集群的svc，其中rook-ceph-mgr-dashboard服务是后面配置ingress，外面域名可以访问的。</span><br> [root@master01 ceph]# kubectl get svc -n rook-ceph<br> NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE<br> csi-cephfsplugin-metrics   ClusterIP   10.103.24.98    &lt;none&gt;        8080/TCP,8081/TCP   93m<br> csi-rbdplugin-metrics      ClusterIP   10.108.163.76   &lt;none&gt;        8080/TCP,8081/TCP   93m<br> rook-ceph-mgr              ClusterIP   10.97.146.102   &lt;none&gt;        9283/TCP            90m<br> rook-ceph-mgr-dashboard    ClusterIP   10.106.30.224   &lt;none&gt;        8443/TCP            90m  <br> rook-ceph-mon-a            ClusterIP   10.107.214.89   &lt;none&gt;        6789/TCP,3300/TCP   93m<br> rook-ceph-mon-b            ClusterIP   10.98.9.202     &lt;none&gt;        6789/TCP,3300/TCP   92m<br> rook-ceph-mon-c            ClusterIP   10.97.87.235    &lt;none&gt;        6789/TCP,3300/TCP   91m</p> 
<p><br> 11、安装Toolbox<br> toolbox是一个rook的工具集容器，该容器中的命令可以用来调试、测试Rook，对Ceph临时测试的操作一般在这个容器内执行。<br> [root@master01 ceph]# kubectl apply -f toolbox.yaml </p> 
<p>#测试 T<span style="color:#fe2c24;">oolbox</span> 是否生成<br> [root@master01 ceph]#  kubectl -n rook-ceph get pod -l "app=rook-ceph-tools"<br> NAME                               READY   STATUS    RESTARTS   AGE<br> rook-ceph-tools-58d7dbc69f-d5kz4   1/1     Running   0          6m41s</p> 
<p>#<span style="color:#fe2c24;">进入rook-tools 容器</span><br> [root@master01 ceph]#  kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') sh<br> kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.<br> sh-4.4# <br> sh-4.4# ceph status  #<span style="color:#fe2c24;">查看ceph集群状态</span><br>   cluster:<br>     id:     4dc66671-6fe6-4dee-9a09-cdb24bc481e9<br>     health: HEALTH_OK<br>  <br>   services:<br>     mon: 3 daemons, quorum a,b,c (age 72m)<br>     mgr: a(active, since 71m)<br>     osd: 3 osds: 3 up (since 69m), 3 in (since 73m)<br>  <br>   data:<br>     pools:   1 pools, 1 pgs<br>     objects: 0 objects, 0 B<br>     usage:   3.0 GiB used, 57 GiB / 60 GiB avail<br>     pgs:     1 active+clean<br>  <br> sh-4.4# <br> sh-4.4# ceph osd status  #<span style="color:#fe2c24;">查看osd状态</span><br> ID  HOST       USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      <br>  0  node02    1030M  18.9G      0        0       0        0   exists,up  <br>  1  node01    1030M  18.9G      0        0       0        0   exists,up  <br>  2  master01  1030M  18.9G      0        0       0        0   exists,up  <br> sh-4.4# <br> sh-4.4# rados df   <span style="color:#fe2c24;"># 查看ceph集群的存储情况</span><br> POOL_NAME              USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS   RD  WR_OPS   WR  USED COMPR  UNDER COMPR<br> device_health_metrics   0 B        0       0       0                   0        0         0       0  0 B       0  0 B         0 B          0 B</p> 
<p>total_objects    0<br> total_used       3.0 GiB<br> total_avail      57 GiB<br> total_space      60 GiB</p> 
<p>sh-4.4# ceph auth ls   #<span style="color:#fe2c24;">ceph集群各个服务的认证信息</span><br> installed auth entries:</p> 
<p>osd.0<br>         key: AQDML0FikQLhCRAAUw9YbHN0X22HktxtC4k0lA==<br>         caps: [mgr] allow profile osd<br>         caps: [mon] allow profile osd<br>         caps: [osd] allow *<br> osd.1<br>         key: AQDTL0FiQ1WQNRAAdo77BUT8aV8KjIHz3SCPhA==<br>         caps: [mgr] allow profile osd<br>         caps: [mon] allow profile osd<br>         caps: [osd] allow *<br> osd.2<br>         key: AQC7MkFihCAfBxAAlD/ik5g9Xm1dZNgvpDCaIQ==<br>         caps: [mgr] allow profile osd<br>         caps: [mon] allow profile osd<br>         caps: [osd] allow *<br> client.admin<br>         key: AQALL0Fi8fnyIhAAIrUfMZiAcJ3Y2QNhozEAbA==<br>         caps: [mds] allow *<br>         caps: [mgr] allow *<br>         caps: [mon] allow *<br>         caps: [osd] allow *<br> client.bootstrap-mds<br>         key: AQAuL0FiXA+SBxAA2O4l5/BkZyXsFSRou4iF5A==<br>         caps: [mon] allow profile bootstrap-mds<br> client.bootstrap-mgr<br>         key: AQAuL0FitCqSBxAAUlOJxEngZisvyN0pL3dFiQ==<br>         caps: [mon] allow profile bootstrap-mgr<br> client.bootstrap-osd<br>         key: AQAuL0FiCUCSBxAAsOu8GxJH6/8Ea+7EAzMRDA==<br>         caps: [mon] allow profile bootstrap-osd<br> client.bootstrap-rbd<br>         key: AQAuL0FiN1eSBxAAcPauOXaUXogYXx3dgKwNKw==<br>         caps: [mon] allow profile bootstrap-rbd<br> client.bootstrap-rbd-mirror<br>         key: AQAuL0FioGySBxAAaApeNBU/JgyLnHyYjtPGhQ==<br>         caps: [mon] allow profile bootstrap-rbd-mirror<br> client.bootstrap-rgw<br>         key: AQAuL0FixIGSBxAAWhnt7ozxhd/DyDsLU+4Alw==<br>         caps: [mon] allow profile bootstrap-rgw<br> client.crash<br>         key: AQCnL0Fi+43UDxAAKkcYOsbycCtXAvxqmGPIqA==<br>         caps: [mgr] allow profile crash<br>         caps: [mon] allow profile crash<br> client.csi-cephfs-node<br>         key: AQCmL0Fiw5W5LhAAMINFmCHXKmofF7Cf50SpdQ==<br>         caps: [mds] allow rw<br>         caps: [mgr] allow rw<br>         caps: [mon] allow r<br>         caps: [osd] allow rw tag cephfs *=*<br> client.csi-cephfs-provisioner<br>         key: AQCmL0FiJCvXEhAA9a3JKSNZRsTamAq37CnDBw==<br>         caps: [mgr] allow rw<br>         caps: [mon] allow r<br>         caps: [osd] allow rw tag cephfs metadata=*<br> client.csi-rbd-node<br>         key: AQClL0Fic1evMBAAksit07nE0srJCj4jTgR+bg==<br>         caps: [mgr] allow rw<br>         caps: [mon] profile rbd<br>         caps: [osd] profile rbd<br> client.csi-rbd-provisioner<br>         key: AQClL0FiUKncGRAA4C4iw9FoNgZPVHq0ygrltw==<br>         caps: [mgr] allow rw<br>         caps: [mon] profile rbd<br>         caps: [osd] profile rbd<br> mgr.a<br>         key: AQCoL0FiRTfqDhAA5YtsWGFA/AJHuV4Cdw857w==<br>         caps: [mds] allow *<br>         caps: [mon] allow profile mgr<br>         caps: [osd] allow *</p> 
<p><br> 12.部署ceph的dashboard</p> 
<p>#<span style="color:#fe2c24;">创建自签tls公私秘钥对</span><br> [root@master01 ceph]# mkdir tls<br> [root@master01 ceph]# cd tls/<br> [root@master01 tls]# openssl genrsa -out tls.key 2048<br> [root@master01 tls]# openssl req -new -x509 -days 3650 -key tls.key -out tls.crt -subj /C=CN/ST=Beijing/O=DevOps/CN=*.od.com<br> [root@master01 tls]# ll<br> total 8<br> -rw-r--r-- 1 root root 1220 Mar 28 14:37 tls.crt<br> -rw-r--r-- 1 root root 1679 Mar 28 14:37 tls.key</p> 
<p>#<span style="color:#fe2c24;">创建ceph的dashboard的ingress的secret，由于ceph在rook-ceph名称空间里，所以创建secret也要在这个名称空间里。</span><br> [root@master01 tls]# kubectl create secret tls ceph-ingress-secret --cert=tls.crt --key=tls.key -n rook-ceph<br> secret/ceph-ingress-secret created</p> 
<p>#修改<span style="color:#fe2c24;">ceph的dashboard文件</span><br> [root@master01 tls]# cd ..<br> [root@master01 ceph]# cat dashboard-ingress-https.yaml<br> #<br> # This example is for Kubernetes running an ngnix-ingress<br> # and an ACME (e.g. Let's Encrypt) certificate service<br> #<br> # The nginx-ingress annotations support the dashboard<br> # running using HTTPS with a self-signed certificate<br> #<br> apiVersion: extensions/v1beta1<br> kind: Ingress<br> metadata:<br>   name: rook-ceph-mgr-ingress  #定<span style="color:#fe2c24;">义ceph的ingress名字，</span><br>   namespace: rook-ceph        # <span style="color:#fe2c24;">namespace:cluster，名称空间</span><br>   annotations:                 #<span style="color:#fe2c24;">注解，一定要有，否则会有问题</span><br>     kubernetes.io/ingress.class: "nginx"<br>     nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"<br>     nginx.ingress.kubernetes.io/server-snippet: |<br>       proxy_ssl_verify off;<br> spec:<br>   tls:<br>    - hosts:<br>      - rook-ceph.od.com       <span style="color:#fe2c24;">#自签tls的CN，建议一致跟下面的rules的host名字</span><br>      secretName: ceph-ingress-secret   <span style="color:#fe2c24;">#定义上面对od.com泛域名创建的secret的名字</span><br>   rules:<br>   - host: rook-ceph.od.com   <span style="color:#fe2c24;">#定义访问的域名</span><br>     http:<br>       paths:<br>       - path: /<br>         backend:<br>           serviceName: rook-ceph-mgr-dashboard   <span style="color:#fe2c24;">#这是mgr的dashboard服务名字，前面在创建ceph集群的时候就已经创建好了。</span><br>           servicePort: 8443    <span style="color:#fe2c24;">#rook-ceph-mgr-dashboard服务端口</span></p> 
<p>#创建ceph的dashboard的ingress<br> [root@master01 ceph]# kubectl apply -f dashboard-ingress-https.yaml<br>     <br> [root@master01 ceph]# kubectl get ingress -n rook-ceph<br> NAME                    CLASS    HOSTS              ADDRESS   PORTS     AGE<br> rook-ceph-mgr-ingress   &lt;none&gt;   rook-ceph.od.com             80, 443   29m</p> 
<p>[root@master01 ceph]# kubectl describe ingress rook-ceph-mgr-ingress -n rook-ceph<br> Name:             rook-ceph-mgr-ingress<br> Namespace:        rook-ceph<br> Address:          <br> Default backend:  default-http-backend:80 (&lt;none&gt;)<br> TLS:<br>   ceph-ingress-secret terminates rook-ceph.od.com<br> Rules:<br>   Host              Path  Backends<br>   ----              ----  --------<br>   rook-ceph.od.com  <br>                     /   rook-ceph-mgr-dashboard:8443 (10.244.1.23:8443)<br> Annotations:        kubernetes.io/ingress.class: nginx<br>                     kubernetes.io/tls-acme: true<br>                     nginx.ingress.kubernetes.io/backend-protocol: HTTPS<br>                     nginx.ingress.kubernetes.io/server-snippet: proxy_ssl_verify off;<br> Events:<br>   Type    Reason  Age   From                      Message<br>   ----    ------  ----  ----                      -------<br>   Normal  CREATE  29m   nginx-ingress-controller  Ingress rook-ceph/rook-ceph-mgr-ingress<br>   <br> #配置dns，<span style="color:#fe2c24;">#由于ingress-collector服务是hostnetwork，而且通过标签部署pod在master01，<br> 如果有三台master，可以指定vip，高可用</span></p> 
<p>[root@master01 tls]# vi /var/named/od.com.zone <br> $ORIGIN od.com.<br> $TTL 600        ; 10 minutes<br> @               IN SOA  dns.od.com. dnsadmin.od.com. (<br>                                 2020011201 ; serial<br>                                 10800      ; refresh (3 hours)<br>                                 900        ; retry (15 minutes)<br>                                 604800     ; expire (1 week)<br>                                 86400      ; minimum (1 day)<br>                                 )<br>                                 NS   dns.od.com.<br> $TTL 60 ; 1 minute<br> dns                A    10.4.7.49<br> harbor             A    10.4.7.53<br> demo               A    10.4.7.49<br> rook-ceph          A    10.4.7.49      </p> 
<p>#加载一下named服务<br> [root@master01 tls]# service named restart</p> 
<p>#另外一种方式就直接修改 <span style="color:#fe2c24;">rook-ceph-mgr-dashboard </span>的service，<br> [root@master01 ceph]# kubectl get svc -n rook-ceph<br> NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE<br> csi-cephfsplugin-metrics   ClusterIP   10.100.248.219   &lt;none&gt;        8080/TCP,8081/TCP   58m<br> csi-rbdplugin-metrics      ClusterIP   10.109.65.11     &lt;none&gt;        8080/TCP,8081/TCP   58m<br> rook-ceph-mgr              ClusterIP   10.100.72.151    &lt;none&gt;        9283/TCP            53m<br> rook-ceph-mgr-dashboard    ClusterIP    10.111.104.197   &lt;none&gt;        8443/TCP      53m<br> rook-ceph-mon-a            ClusterIP   10.101.3.133     &lt;none&gt;        6789/TCP,3300/TCP   58m<br> rook-ceph-mon-b            ClusterIP   10.111.38.134    &lt;none&gt;        6789/TCP,3300/TCP   55m<br> rook-ceph-mon-c            ClusterIP   10.104.208.191   &lt;none&gt;        6789/TCP,3300/TCP   54m</p> 
<p>#修改<span style="color:#fe2c24;">type</span>类型，默认是ClusterIp，改为<span style="color:#fe2c24;">NodePort</span><br> [root@master01 tls]# kubectl edit svc  rook-ceph-mgr-dashboard -n rook-ceph</p> 
<p>#<span style="color:#fe2c24;">查看rook-ceph-mgr-dashboard 的service服务type为NodePort</span><br> [root@master01 tls]# kubectl get svc -n rook-ceph<br> NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE<br> csi-cephfsplugin-metrics   ClusterIP   10.100.248.219   &lt;none&gt;        8080/TCP,8081/TCP   64m<br> csi-rbdplugin-metrics      ClusterIP   10.109.65.11     &lt;none&gt;        8080/TCP,8081/TCP   64m<br> rook-ceph-mgr              ClusterIP   10.100.72.151    &lt;none&gt;        9283/TCP            59m<br> rook-ceph-mgr-dashboard    NodePort    10.111.104.197   &lt;none&gt;        8443:31004/TCP      59m<br> rook-ceph-mon-a            ClusterIP   10.101.3.133     &lt;none&gt;        6789/TCP,3300/TCP   63m<br> rook-ceph-mon-b            ClusterIP   10.111.38.134    &lt;none&gt;        6789/TCP,3300/TCP   60m<br> rook-ceph-mon-c            ClusterIP   10.104.208.191   &lt;none&gt;        6789/TCP,3300/TCP   59m</p> 
<p>#浏览器直接用node节点ip:31004访问</p> 
<p>15、查看登录密码<br> [root@master01 ceph]# kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath='{.data.password}'  |  base64 --decode &amp;&amp; echo<br> 8ynOK:Wb${/)/;0SY[9(</p> 
<p>#修改密码为：admin123，但是重启后，密码又变了</p> 
<p>16.浏览器登录 https://rook-ceph.od.com<br> 用户：admin<br> 密码：8ynOK:Wb${/)/;0SY[9(</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/05/17/QGoNbtyU_o.png"></p> 
<p></p> 
<p></p> 
<p><br> #卸载ceph集群<br> 1.删除之前执行的yaml文件<br> [root@master01 ceph]# kubectl delete -f crds.yaml -f common.yaml -f operator.yaml  -f cluster.yaml</p> 
<p>2.删除ceph集群的data目录<br> [root@master01 ceph]# rm /var/lib/rook/ -rf</p> 
<p>3：查看资源<br> [root@master01 tmp]# kubectl -n rook-ceph get cephcluster</p> 
<p>4.删除rook-ceph名称空间<br> [root@master01 ceph]# kubectl get ns rook-ceph -o json &gt;/tmp/tmp.json<br> [root@master01 ceph]# kubectl delete namespace rook-ceph</p> 
<p>5.删除名称空间rook-ceph时，出现Terminating，开启临时一个服务<br> [root@master01 ~]# kubectl proxy --port=9098<br> Starting to serve on 127.0.0.1:9098</p> 
<p>6.开启另外一个终端，删除tmp.json文件里的spec内容<br> [root@master01 ceph]# cd /tmp/<br> [root@master01 tmp]# vi tmp.json <br> [root@master01 tmp]# curl -k -H "Content-Type: application/json" -X PUT --data-binary @tmp.json http://127.0.0.1:9098/api/v1/namespaces/rook-ceph/finalize</p> 
<p>7.再次查看集群状态<br> [root@master01 tmp]# kubectl -n rook-ceph get cephcluster</p> 
<p>8.之前的磁盘清零<br> [root@master01 ceph]# dd if=/dev/zero of=/dev/sdb bs=1M status=progress</p> 
<p>17.ceph配置rdb，一般是配置有状态服务statefulset，<br> [root@master01 rbd]# cd /root/rook/rook-1.5.5/cluster/examples/kubernetes/ceph/csi/rbd<br> [root@master01 rbd]# cat storageclass.yaml <br> apiVersion: ceph.rook.io/v1<br> kind: CephBlockPool   #块地址池<br> metadata:<br>   name: replicapool   #<span style="color:#fe2c24;">定义块地址池名称</span><br>   namespace: rook-ceph  #<span style="color:#fe2c24;">在rook-ceph名称空间创建块存储池。</span><br> spec:<br>   failureDomain: osd   #<span style="color:#fe2c24;">容灾模式host或者osd</span>，<br>   replicated:<br>     size: 3  <span style="color:#fe2c24;">#数据副本个数，一个节点挂了，不会丢失数据，另外还有两个副本</span>。<br>     # Disallow setting pool with replica 1, this could lead to data loss without recovery.<br>     # Make sure you're *ABSOLUTELY CERTAIN* that is what you want<br>     requireSafeReplicaSize: true<br>     # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool<br>     # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size<br>     #targetSizeRatio: .5<br> ---<br> apiVersion: storage.k8s.io/v1<br> kind: StorageClass    <span style="color:#fe2c24;">#储存类型</span><br> metadata:<br>    name: rook-ceph-block  <span style="color:#fe2c24;">#创建一个块存储名字，储存是针对集群的，不针对名称空间的</span><br> # Change "rook-ceph" provisioner prefix to match the operator namespace if needed<br> provisioner: rook-ceph.rbd.csi.ceph.com   <span style="color:#fe2c24;">#提供rdb块存储的供应商。</span><br> parameters:<br>     # clusterID is the namespace where the rook cluster is running<br>     # If you change this namespace, also change the namespace below where the secret namespaces are defined<br>     clusterID: rook-ceph # namespace:cluster</p> 
<p>    # If you want to use erasure coded pool with RBD, you need to create<br>     # two pools. one erasure coded and one replicated.<br>     # You need to specify the replicated pool here in the `pool` parameter, it is<br>     # used for the metadata of the images.<br>     # The erasure coded pool must be set as the `dataPool` parameter below.<br>     #dataPool: ec-data-pool<br>     pool: replicapool   #<span style="color:#fe2c24;">指定上面创建的块储存池名称</span></p> 
<p>    # (optional) mapOptions is a comma-separated list of map options.<br>     # For krbd options refer<br>     # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options<br>     # For nbd options refer<br>     # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options<br>     # mapOptions: lock_on_read,queue_depth=1024</p> 
<p>    # (optional) unmapOptions is a comma-separated list of unmap options.<br>     # For krbd options refer<br>     # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options<br>     # For nbd options refer<br>     # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options<br>     # unmapOptions: force</p> 
<p>    # RBD image format. Defaults to "2".<br>     imageFormat: "2"</p> 
<p>    # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.<br>     imageFeatures: layering</p> 
<p>    # The secrets contain Ceph admin credentials. These are generated automatically by the operator<br>     # in the same namespace as the cluster.<br>     csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner<br>     csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster<br>     csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner<br>     csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster<br>     csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node<br>     csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster<br>     # Specify the filesystem type of the volume. If not specified, csi-provisioner<br>     # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock<br>     # in hyperconverged settings where the volume is mounted on the same node as the osds.<br>     csi.storage.k8s.io/fstype: ext4<br> # uncomment the following to use rbd-nbd as mounter on supported nodes<br> # **IMPORTANT**: If you are using rbd-nbd as the mounter, during upgrade you will be hit a ceph-csi<br> # issue that causes the mount to be disconnected. You will need to follow special upgrade steps<br> # to restart your application pods. Therefore, this option is not recommended.<br> #mounter: rbd-nbd<br> allowVolumeExpansion: true<br> reclaimPolicy: Delete</p> 
<p>#可以默认参数，<br> [root@master01 rbd]# kubectl apply -f storageclass.yaml <br> cephblockpool.ceph.rook.io/replicapool created<br> storageclass.storage.k8s.io/rook-ceph-block created</p> 
<p>#<span style="color:#fe2c24;">查看创建的块池</span><br> [root@master01 rbd]# kubectl get cephblockpool -A<br> NAMESPACE   NAME          AGE<br> rook-ceph   replicapool   92m</p> 
<p>#<span style="color:#fe2c24;">查看刚才创建的存储类名称，也叫存储驱动</span>，<br> [root@master01 rbd]# kubectl get sc <br> NAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE<br> rook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   27m</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/42/2c/P1rClaCM_o.png"></p> 
<p></p> 
<p></p> 
<p>#<span style="color:#fe2c24;">在创建一个rook-cephfs的共享文件存储驱动</span><br> [root@master01 ~]# cd /root/rook/rook-1.5.5/cluster/examples/kubernetes/ceph/</p> 
<p>#<span style="color:#fe2c24;">修改容灾模式为osd</span><br> [root@master01 ceph]# sed -i 's/failureDomain: host/failureDomain: osd/g' filesystem.yaml</p> 
<p>#创建文件系统<br> [root@master01 ceph]# cat filesystem.yaml<br> #############################################################################<br> # Create a filesystem with settings with replication enabled for a production environment.<br> # A minimum of 3 OSDs on different nodes are required in this example.<br> #  kubectl create -f filesystem.yaml<br> #############################################################################</p> 
<p>apiVersion: ceph.rook.io/v1<br> kind: CephFilesystem<br> metadata:<br>   name: myfs  #创建fs的名字<br>   namespace: rook-ceph # namespace:cluster<br> spec:<br>   # The metadata pool spec. Must use replication.<br>   metadataPool:<br>     replicated:<br>       size: 3<br>       requireSafeReplicaSize: true<br>     parameters:<br>       # Inline compression mode for the data pool<br>       # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression<br>       compression_mode: none<br>         # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool<br>       # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size<br>       #target_size_ratio: ".5"<br>   # The list of data pool specs. Can use replication or erasure coding.<br>   dataPools:<br>     - failureDomain: osd<br>       replicated:<br>         size: 3<br>         # Disallow setting pool with replica 1, this could lead to data loss without recovery.<br>         # Make sure you're *ABSOLUTELY CERTAIN* that is what you want<br>         requireSafeReplicaSize: true<br>       parameters:<br>         # Inline compression mode for the data pool<br>         # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression<br>         compression_mode: none<br>           # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool<br>         # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size<br>         #target_size_ratio: ".5"<br>   # Whether to preserve filesystem after CephFilesystem CRD deletion<br>   preserveFilesystemOnDelete: true<br>   # The metadata service (mds) configuration<br>   metadataServer:<br>     # The number of active MDS instances<br>     activeCount: 1<br>     # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.<br>     # If false, standbys will be available, but will not have a warm cache.<br>     activeStandby: true<br>     # The affinity rules to apply to the mds deployment<br>     placement:<br>     #  nodeAffinity:<br>     #    requiredDuringSchedulingIgnoredDuringExecution:<br>     #      nodeSelectorTerms:<br>     #      - matchExpressions:<br>     #        - key: role<br>     #          operator: In<br>     #          values:<br>     #          - mds-node<br>     #  topologySpreadConstraints:<br>     #  tolerations:<br>     #  - key: mds-node<br>     #    operator: Exists<br>     #  podAffinity:<br>        podAntiAffinity:<br>           requiredDuringSchedulingIgnoredDuringExecution:<br>           - labelSelector:<br>               matchExpressions:<br>               - key: app<br>                 operator: In<br>                 values:<br>                 - rook-ceph-mds<br>             # topologyKey: kubernetes.io/hostname will place MDS across different hosts<br>             topologyKey: kubernetes.io/hostname<br>           preferredDuringSchedulingIgnoredDuringExecution:<br>           - weight: 100<br>             podAffinityTerm:<br>               labelSelector:<br>                 matchExpressions:<br>                 - key: app<br>                   operator: In<br>                   values:<br>                   - rook-ceph-mds<br>               # topologyKey: */zone can be used to spread MDS across different AZ<br>               # Use &lt;topologyKey: failure-domain.beta.kubernetes.io/zone&gt; in k8s cluster if your cluster is v1.16 or lower<br>               # Use &lt;topologyKey: topology.kubernetes.io/zone&gt;  in k8s cluster is v1.17 or upper<br>               topologyKey: topology.kubernetes.io/zone<br>     # A key/value list of annotations<br>     annotations:<br>     #  key: value<br>     # A key/value list of labels<br>     labels:<br>     #  key: value<br>     resources:<br>     # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory<br>     #  limits:<br>     #    cpu: "500m"<br>     #    memory: "1024Mi"<br>     #  requests:<br>     #    cpu: "500m"<br>     #    memory: "1024Mi"<br>     # priorityClassName: my-priority-class<br>     <br> #创建 CephFilesystem<br> [root@master01 ceph]# kubectl apply -f filesystem.yaml</p> 
<p>[root@master01 cephfs]# kubectl get CephFilesystem -n rook-ceph<br> NAME   ACTIVEMDS   AGE<br> myfs   1           72s    </p> 
<p>#这个时候会运行mds两个pod<br> [root@master01 ceph]# kubectl -n rook-ceph get pod -l app=rook-ceph-mds<br> NAME                                    READY   STATUS    RESTARTS   AGE<br> rook-ceph-mds-myfs-a-6d69895657-bl5cs   1/1     Running   0          4m28s<br> rook-ceph-mds-myfs-b-b6d546f87-pjm7p    1/1     Running   0          4m25s</p> 
<p>#创建<span style="color:#fe2c24;">cephfs文件共享存储驱动</span><br> [root@master01 cephfs]# cd /root/rook/rook-1.5.5/cluster/examples/kubernetes/ceph/csi/cephfs<br> [root@master01 cephfs]# vi storageclass.yaml<br> apiVersion: storage.k8s.io/v1<br> kind: StorageClass<br> metadata:<br>   name: rook-cephfs<br> provisioner: rook-ceph.cephfs.csi.ceph.com # driver:namespace:operator<br> parameters:<br>   # clusterID is the namespace where operator is deployed.<br>   clusterID: rook-ceph # namespace:cluster</p> 
<p>  # CephFS filesystem name into which the volume shall be created<br>   fsName: myfs   #<span style="color:#fe2c24;">要指定已经创建好的CephFilesystem</span></p> 
<p>  # Ceph pool into which the volume shall be created<br>   # Required for provisionVolume: "true"<br>   pool: myfs-data0</p> 
<p>  # Root path of an existing CephFS volume<br>   # Required for provisionVolume: "false"<br>   # rootPath: /absolute/path</p> 
<p>  # The secrets contain Ceph admin credentials. These are generated automatically by the operator<br>   # in the same namespace as the cluster.<br>   csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner<br>   csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster<br>   csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner<br>   csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster<br>   csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node<br>   csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster</p> 
<p>  # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)<br>   # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse<br>   # or by setting the default mounter explicitly via --volumemounter command-line argument.<br>   mounter: kernel<br> reclaimPolicy: Delete<br> allowVolumeExpansion: true<br> mountOptions:<br>   # uncomment the following line for debugging<br>   #- debug</p> 
<p>#创建rook-cephfs存储驱动<br> [root@master01 cephfs]# kubectl apply -f storageclass.yaml<br> storageclass.storage.k8s.io/rook-cephfs created</p> 
<p>#查看已经创建了两个存储驱动。需要存储可以调用这些驱动，自动创建pvc<br> [root@master01 cephfs]# kubectl get sc<br> NAME              PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE<br> rook-ceph-block   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   19h<br> rook-cephfs       rook-ceph.cephfs.csi.ceph.com   Delete          Immediate           true                   5s<br>  </p> 
<p>#测试rbd只支持有状态服务，<br> 1.创建nginx的无状态应用<br> [root@master01 ceph]# cat /root/nginx-dp.yaml<br> apiVersion: apps/v1<br> kind: Deployment<br> metadata:<br>   name: test-nginx<br>   labels:<br>     app: nginx<br> spec:<br>   selector:<br>     matchLabels:<br>       app: nginx<br>   replicas: 1           # tells deployment to run 2 pods matching the template<br>   template:<br>     metadata:<br>       labels:<br>         app: nginx<br>     spec:<br>       containers:<br>       - name: nginx<br>         image: nginx:1.16<br>         ports:<br>         - containerPort: 80<br>         volumeMounts:<br>         - name: localtime<br>           mountPath: /etc/localtime<br>         - name: nginx-html-storage<br>           mountPath: /usr/share/nginx/html<br>       volumes:<br>        - name: localtime<br>          hostPath:<br>            path: /usr/share/zoneinfo/Asia/Shanghai<br>        - name: nginx-html-storage<br>          persistentVolumeClaim:<br>            claimName: nginx-pv-claim   #指定名为nginx-pv-claim的pvc</p> 
<p>---<br> apiVersion: v1<br> kind: PersistentVolumeClaim<br> metadata:<br>   name: nginx-pv-claim    #创建名为nginx-pv-claim的pvc<br>   labels:<br>     app: nginx<br> spec:<br>   storageClassName: rook-ceph-block  #指定上面创建存储引擎名为 rook-ceph-block <br>   accessModes:<br>     - ReadWriteMany    #访问模式为多节点同时访问<br>   resources:<br>     requests:<br>       storage: 1Gi   #容量为1Gi</p> 
<p>#创建nginx的应用，<br> [root@master01 ~]# kubectl apply -f nginx-dp.yaml<br> deployment.apps/test-nginx created<br> persistentvolumeclaim/nginx-pv-claim created</p> 
<p>#查看pvc，状态为pending，肯定不正常。<br> [root@master01 ~]# kubectl get pvc<br> NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE<br> nginx-pv-claim   Pending                                      rook-ceph-block   10s</p> 
<p>2,。查看创建的pvc，状态为pending，肯定不正常。<br> [root@master01 ceph]# kubectl get pvc<br> NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE<br> nginx-pv-claim   Pending                                      rook-ceph-block   4m48s</p> 
<p>3.查看pvc，报错了<br> [root@master01 ~]# kubectl describe pvc nginx-pv-claim</p> 
<p># failed to provision volume with StorageClass "rook-ceph-block": rpc <br> error: code = InvalidArgument desc = multi node access modes are <br> only supported on rbd `block` type volumes<br> 意思是说：意思是不推荐在ceph rbd模式下使用RWX访问控制，如果应用层没有访问锁机制，可能会造成数据损坏</p> 
<p></p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/75/c8/bjDpBV0j_o.png"></p> 
<p></p> 
<p><br> #修改定义的pvc的访问模式为<span style="color:#fe2c24;">ReadWriteOnce</span><br> 1.修改配置<br> [root@master01 ceph]# cat /root/nginx-dp.yaml<br> apiVersion: apps/v1<br> kind: Deployment<br> metadata:<br>   name: test-nginx<br>   labels:<br>     app: nginx<br> spec:<br>   selector:<br>     matchLabels:<br>       app: nginx<br>   replicas: 1           # tells deployment to run 2 pods matching the template<br>   template:<br>     metadata:<br>       labels:<br>         app: nginx<br>     spec:<br>       containers:<br>       - name: nginx<br>         image: nginx:1.16<br>         ports:<br>         - containerPort: 80<br>         volumeMounts:<br>         - name: localtime<br>           mountPath: /etc/localtime<br>         - name: nginx-html-storage<br>           mountPath: /usr/share/nginx/html<br>       volumes:<br>        - name: localtime<br>          hostPath:<br>            path: /usr/share/zoneinfo/Asia/Shanghai<br>        - name: nginx-html-storage<br>          persistentVolumeClaim:<br>            claimName: nginx-pv-claim   #<span style="color:#fe2c24;">指定名为nginx-pv-claim的pvc,或者已经存在的pvc的名字</span><br>            readOnly: false<br> ---<br> apiVersion: v1<br> kind: PersistentVolumeClaim<br> metadata:<br>   name: nginx-pv-claim    #<span style="color:#fe2c24;">创建名为nginx-pv-claim的pvc</span><br>   labels:<br>     app: nginx<br> spec:<br>   storageClassName: rook-ceph-block  #<span style="color:#fe2c24;">指定上面创建存储引擎名为 rook-ceph-block</span> <br>   accessModes:<br>     - ReadWriteOnce    #<span style="color:#fe2c24;">访问模式为单节点访问</span><br>   resources:<br>     requests:<br>       storage: 1Gi   #容量为1Gi<br>       <br> 2.创建pvc，nginx应用并查看pvc跟pod<br> [root@master01 ~]# kubectl apply -f nginx-dp.yaml<br> deployment.apps/test-nginx created<br> persistentvolumeclaim/nginx-pv-claim created</p> 
<p>[root@master01 ~]# kubectl get pvc<br> NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE<br> nginx-pv-claim   Bound    pvc-6d2d0264-48fc-4edf-9433-875b0512ec5f   1Gi      RWO            rook-ceph-block   6s  </p> 
<p>3.查看pod<br> [root@master01 ~]# kubectl get pods<br> NAME                         READY   STATUS    RESTARTS   AGE<br> test-nginx-6f6584b4b-zgksm   1/1     Running   0          61s<br>  </p> 
<p><br> # <span style="color:#fe2c24;">使用cephfs的存储驱动来存储共享文件cpch-cephFS测试</span><br> 案例一:多容器共享同一个数据目录,部署多个私有仓库共享同一个数据目录进行测试<br> [root@master01 cephfs]# cd /root/rook/rook-1.5.5/cluster/examples/kubernetes/ceph/csi/cephfs <br> [root@master01 cephfs]# ls<br> kube-registry.yaml  pod.yaml  pvc-clone.yaml  pvc-restore.yaml  pvc.yaml  rook-cephfs.yaml  snapshotclass.yaml  snapshot.yaml  storageclass.yaml</p> 
<p>#创建test的名称空间<br> [root@master01 cephfs]# kubectl create ns test</p> 
<p>#在test名称空间创建2个pod<br> [root@master01 cephfs]# cat kube-registry.yaml <br> apiVersion: v1<br> kind: PersistentVolumeClaim<br> metadata:<br>   name: cephfs-pvc     #<span style="color:#fe2c24;">通过下面的cephfs存储驱动为rook-cephfs来创建pvc</span><br>   namespace: test      #<span style="color:#fe2c24;">pvc是针对名称空间的</span><br> spec:<br>   accessModes:<br>   - ReadWriteMany        #<span style="color:#fe2c24;">访问模式为多节点读写访问</span><br>   resources:<br>     requests:<br>       storage: 1Gi<br>   storageClassName: rook-cephfs    #<span style="color:#fe2c24;">指定存储驱动名为：rook-cephfs，这个存储驱动是cephfs驱动。</span><br> ---<br> apiVersion: apps/v1<br> kind: Deployment<br> metadata:<br>   name: kube-registry<br>   namespace: test           #<span style="color:#fe2c24;">要挂载test的pvc，所以就需要在test名称空间创建pod，才可以识别</span><br>   labels:<br>     k8s-app: kube-registry<br>     kubernetes.io/cluster-service: "true"<br> spec:<br>   replicas: 2   #<span style="color:#fe2c24;">用两个副本</span><br>   selector:<br>     matchLabels:<br>       k8s-app: kube-registry<br>   template:<br>     metadata:<br>       labels:<br>         k8s-app: kube-registry<br>         kubernetes.io/cluster-service: "true"<br>     spec:<br>       containers:<br>       - name: registry<br>         image: registry:2<br>         imagePullPolicy: Always<br>         resources:<br>           limits:<br>             cpu: 100m<br>             memory: 100Mi<br>         env:<br>         # Configuration reference: https://docs.docker.com/registry/configuration/<br>         - name: REGISTRY_HTTP_ADDR<br>           value: :5000<br>         - name: REGISTRY_HTTP_SECRET<br>           value: "Ple4seCh4ngeThisN0tAVerySecretV4lue"<br>         - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY<br>           value: /var/lib/registry<br>         volumeMounts:<br>         - name: image-store    #<span style="color:#fe2c24;">这里指定挂载逻辑名字，意思就是把名为：cephfs-pvc的pvc挂载到容器里，</span><br>           mountPath: /var/lib/registry   #<span style="color:#fe2c24;">挂载到容器这个目录下</span><br>         ports:<br>         - containerPort: 5000<br>           name: registry<br>           protocol: TCP<br>         livenessProbe:<br>           httpGet:<br>             path: /<br>             port: registry<br>         readinessProbe:<br>           httpGet:<br>             path: /<br>             port: registry<br>       volumes:<br>       - name: image-store    #<span style="color:#fe2c24;">定义挂载逻辑名字</span><br>         persistentVolumeClaim:<br>           claimName: cephfs-pvc  #<span style="color:#fe2c24;">定义上面创建的pvc</span><br>           readOnly: false<br>           <br>           <br> #<span style="color:#fe2c24;">创建挂载共享文件存储pod</span><br> [root@master01 cephfs]# kubectl apply -f kube-registry.yaml <br>   <br> [root@master01 cephfs]# kubectl get pvc -n test<br> NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br> cephfs-pvc   Bound    pvc-933ab84b-f298-4c42-946d-5560a367caf1   1Gi        RWX            rook-cephfs    19s</p> 
<p>[root@master01 cephfs]# kubectl get pods -n test<br> NAME                             READY   STATUS    RESTARTS   AGE<br> kube-registry-58659ff99b-bvd52   1/1     Running   0          49s<br> kube-registry-58659ff99b-gvcbz   1/1     Running   0          49s</p> 
<p>#<span style="color:#fe2c24;">进入其中一个pod。</span><br> [root@master01 cephfs]# kubectl exec -it kube-registry-58659ff99b-bvd52 -n test sh<br> kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.<br> / # <br> / # cd /var/lib/registry/<br> /var/lib/registry # echo "abc" &gt; test.txt <br> /var/lib/registry # cat test.txt <br> abc</p> 
<p>#进<span style="color:#fe2c24;">入另外一个pod</span><br> [root@master01 cephfs]# kubectl exec -it kube-registry-58659ff99b-gvcbz -n test sh<br> kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.<br> / # <br> / # cat /var/lib/registry/test.txt <br> abc</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b982ad7f2b51240c28296d65a4c8a998/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Uncaught (in promise) DOMException: Failed to load because no supported source was found.</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/773003110acd6327bdc4771207fd4b43/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">续之前微信小程序的部分代码</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>