<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 21), which maintains... - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 21), which maintains..." />
<meta property="og:description" content="问题描述 org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 21), which maintains the block data to fetch is dead.
最近在做Spark的性能优化，测试使用不同CPU核数和内存对计算性能的影响，由于是在测试集群进行测试的，硬件配置比生产上面的要少和低，遇到了不少的问题，其中一个值得说一下的就是org.apache.spark.shuffle.FetchFailedException:Failed to connect to /xxx:43301
一、运行环境 1.1 硬件 3台测试服务器，分别为A,B,C，每台4核，16GB内存
每台部署HDFS的DataNode和Spark的Worker
其中A同时部署了HDFS的NameNode
其中B同时部署了Spark的Master
其中C是Spark的Driver
1.2 软件 HDFS 2.7.3，集群
Spark 2.1.0，标准集群模式
Java 1.8.0_131
二、Spark启动参数 2.1 测试1 2.1.1 测试参数 spark.driver.cores 没有配置，默认使用1
spark.driver.maxResultSize 配置2g，默认是1g
spark.driver.memory 配置3g，默认是1g
spark.executor.memory 配置8g，默认是1g
spark.executor.cores 没有配置，默认使用Worker全部核数，这里是4
2.1.2 测试结果 Spark集群每个Worker创建了1个Executor，每个Executor使用了4核和8g内存，可以得出结果，耗时2小时
2.2 测试2 2.2.1 测试参数 spark.driver.cores 没有配置，默认使用1
spark.driver.maxResultSize 配置2g，默认是1g
spark.driver.memory 配置3g，默认是1g
因为想每个Worker创建多于1个Executor，测试多个Executors是否能提高性能，所以修改以下参数：
spark.executor.memory 配置4g，是测试1的一半
spark.executor.cores 配置2，是测试1的一半" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/c15f21e54d7b702ef068d1b9289176e6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-05T09:37:00+08:00" />
<meta property="article:modified_time" content="2023-07-05T09:37:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 21), which maintains...</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown" style="font-size: 16px;"> 
 <h2 id="问题描述">问题描述</h2> 
 <p>org.apache.spark.shuffle.FetchFailedException: The relative remote executor(Id: 21), which maintains the block data to fetch is dead.<br> 最近在做Spark的性能优化，测试使用不同CPU核数和内存对计算性能的影响，由于是在测试集群进行测试的，硬件配置比生产上面的要少和低，遇到了不少的问题，其中一个值得说一下的就是org.apache.spark.shuffle.FetchFailedException:Failed to connect to /xxx:43301</p> 
 <h2 id="一运行环境">一、运行环境</h2> 
 <h3 id="11-硬件">1.1 硬件</h3> 
 <p>3台测试服务器，分别为A,B,C，每台4核，16GB内存<br> 每台部署HDFS的DataNode和Spark的Worker<br> 其中A同时部署了HDFS的NameNode<br> 其中B同时部署了Spark的Master<br> 其中C是Spark的Driver</p> 
 <h3 id="12-软件">1.2 软件</h3> 
 <p>HDFS 2.7.3，集群<br> Spark 2.1.0，标准集群模式<br> Java 1.8.0_131</p> 
 <h2 id="二spark启动参数">二、Spark启动参数</h2> 
 <h3 id="21-测试1">2.1 测试1</h3> 
 <h4 id="211-测试参数">2.1.1 测试参数</h4> 
 <p>spark.driver.cores 没有配置，默认使用1<br> spark.driver.maxResultSize 配置2g，默认是1g<br> spark.driver.memory 配置3g，默认是1g<br> spark.executor.memory 配置8g，默认是1g<br> spark.executor.cores 没有配置，默认使用Worker全部核数，这里是4</p> 
 <h4 id="212-测试结果">2.1.2 测试结果</h4> 
 <p>Spark集群每个Worker创建了1个Executor，每个Executor使用了4核和8g内存，可以得出结果，耗时2小时</p> 
 <h3 id="22-测试2">2.2 测试2</h3> 
 <h4 id="221-测试参数">2.2.1 测试参数</h4> 
 <p>spark.driver.cores 没有配置，默认使用1<br> spark.driver.maxResultSize 配置2g，默认是1g<br> spark.driver.memory 配置3g，默认是1g</p> 
 <p>因为想每个Worker创建多于1个Executor，测试多个Executors是否能提高性能，所以修改以下参数：</p> 
 <p>spark.executor.memory 配置4g，是测试1的一半<br> spark.executor.cores 配置2，是测试1的一半</p> 
 <h4 id="222-测试结果">2.2.2 测试结果</h4> 
 <p>Spark集群每个Worker创建了2个Executors(spark.cores.max/spark.executor.cores=4/2=2)，每个Executor使用了2核和4g内存，总使用资源和测试1是一样的，也就是每台服务器的2个Executors总共使用了4核和8g内存，但是遇到以下异常：</p> 
 <pre class="has"><code>[WARN][TaskSetManager] Lost task 6.0 in stage 4.0 (TID 307, xxx, executor 0): FetchFailed(BlockManagerId(1, xxx, 33557, None), shuffleId=0, mapId=7, reduceId=6, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /xxx:43301
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:357)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:332)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:154)
	at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:85)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)</code></pre> 
 <h2 id="三异常分析">三、异常分析</h2> 
 <p>因为Executor可用的资源减少了一半，shuffle执行的时间变长，内存使用过多导致无响应心跳，超过默认的spark.network.timeout=120s，对应的Executor会被移除，任务丢失：</p> 
 <pre class="has"><code>[WARN][HeartbeatReceiver] Removing executor 5 with no recent heartbeats: 120504 ms exceeds timeout 120000 ms
[ERROR][TaskSchedulerImpl] Lost executor 5 on xxx: Executor heartbeat timed out after 120504 ms
[WARN][TaskSetManager] Lost task 8.0 in stage 4.0 (TID 309, xxx, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 122504 ms</code></pre> 
 <p>Spark的DAGScheduler会尝试提交失败的task到其它的Executors，但是由于其它的Executors也是使用同样的配置资源，最终的任务还是会失败。</p> 
 <h2 id="四解决方案">四、解决方案</h2> 
 <p>减少使用触发shuffle的操作，例如reduceByKey，从而减少使用内存<br> 增大spark.network.timeout，从而允许有更多时间去等待心跳响应<br> 增加spark.executor.cores，从而减少创建的Executor数量，使得总使用内存减少<br> 同时增大spark.executor.memory，保证每个Executor有足够的可用内存<br> 增大spark.shuffle.memoryFraction，默认为0.2(需要spark.memory.useLegacyMode配置为true，适用于1.5或更旧版本，已经deprecated)</p> 
 <p>另外可以参考官方的shuffle配置参数：<a href="http://spark.apache.org/docs/latest/configuration.html#shuffle-behavior" rel="noopener noopener noreferrer" target="_blank">http://spark.apache.org/docs/latest/configuration.html#shuffle-behavior</a></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fcf128e93c02461c2b95c5a98d6baa7f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">虚拟机防火墙以及端口操作</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/518efadbdd3697f48e50f0b2c3f45f87/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">什么是Ajax，Ajax的原理是什么？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>