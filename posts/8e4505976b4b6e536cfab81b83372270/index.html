<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Error: loaded state dict contains a parameter group that doesn’t match the size of optimizer’s group - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Error: loaded state dict contains a parameter group that doesn’t match the size of optimizer’s group" />
<meta property="og:description" content="ValueError: loaded state dict contains a parameter group that doesn&#39;t match the size of optimizer&#39;s group 错误日志：
Traceback (most recent call last): File &#34;train.py&#34;, line 128, in &lt;module&gt; optimizer.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;]) File &#34;/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py&#34;, line 115, in load_state_dict raise ValueError(&#34;loaded state dict contains a parameter group &#34; ValueError: loaded state dict contains a parameter group that doesn&#39;t match the size of optimizer&#39;s group 这个错误通常是由于加载的模型权重和当前模型的结构不一致导致的。解决方法通常有以下几种：
确认模型结构是否一致，如果不一致需要手动修改代码或者将加载的权重进行转换。确认优化器的参数组是否和模型参数一致，如果不一致需要手动调整优化器代码或者将加载的权重进行转换。确认加载的权重是否是正确的，可以将加载的权重打印出来，与当前模型的权重进行对比。可能是 optimizer 的 state_dict 和加载的 checkpoint 的 state_dict 尺寸不匹配导致的。 以下是一个代码示例，可以帮助你更好地理解如何解决该错误：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/8e4505976b4b6e536cfab81b83372270/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-24T16:09:57+08:00" />
<meta property="article:modified_time" content="2023-11-24T16:09:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Error: loaded state dict contains a parameter group that doesn’t match the size of optimizer’s group</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group</h4> 
</div> 
<p></p> 
<p>错误日志：</p> 
<pre><code class="prism language-bash">Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span>:
  File <span class="token string">"train.py"</span>, line <span class="token number">128</span>, <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>
    optimizer.load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
  File <span class="token string">"/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py"</span>, line <span class="token number">115</span>, <span class="token keyword">in</span> load_state_dict
    raise ValueError<span class="token punctuation">(</span><span class="token string">"loaded state dict contains a parameter group "</span>
ValueError: loaded state dict contains a parameter group that doesn<span class="token string">'t match the size of optimizer'</span>s group
</code></pre> 
<p>这个错误通常是由于加载的模型权重和当前模型的结构不一致导致的。解决方法通常有以下几种：</p> 
<ol><li>确认模型结构是否一致，如果不一致需要手动修改代码或者将加载的权重进行转换。</li><li>确认优化器的参数组是否和模型参数一致，如果不一致需要手动调整优化器代码或者将加载的权重进行转换。</li><li>确认加载的权重是否是正确的，可以将加载的权重打印出来，与当前模型的权重进行对比。</li><li>可能是 optimizer 的 state_dict 和加载的 checkpoint 的 state_dict 尺寸不匹配导致的。</li></ol> 
<p>以下是一个代码示例，可以帮助你更好地理解如何解决该错误：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token comment"># 定义一个模型</span>
<span class="token keyword">class</span> <span class="token class-name">Model</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Model<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token comment"># 定义一个优化器</span>
model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token comment"># 加载模型权重</span>
state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"checkpoint.pth"</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>

<span class="token comment"># 尝试进行优化</span>
optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>运行以上代码会出现该错误，我们可以通过打印出模型参数和优化器参数来查看具体的问题所在：</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">"state"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出结果如下：</p> 
<pre><code>odict_keys(['conv.weight', 'bn.weight', 'bn.bias', 'bn.running_mean', 'bn.running_var', 'fc.weight', 'fc.bias'])
dict_keys(['param_groups', 'state'])
</code></pre> 
<p>通过对比可以发现，模型的权重中没有 <code>conv.bias</code>，而优化器中的参数组却包含了 <code>conv.bias</code>，因此需要手动去除这个偏差参数：</p> 
<pre><code class="prism language-python"><span class="token comment"># 加载模型权重</span>
state_dict <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"checkpoint.pth"</span><span class="token punctuation">)</span>
new_state_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
<span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> state_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">"conv.bias"</span> <span class="token keyword">not</span> <span class="token keyword">in</span> k<span class="token punctuation">:</span>
        new_state_dict<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> v
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>new_state_dict<span class="token punctuation">)</span>

<span class="token comment"># 尝试进行优化</span>
optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>通过手动去除偏差参数后，我们可以成功地进行优化。</p> 
<ul><li>查看 optimizer 的 state_dict 和加载的 checkpoint 的 state_dict 尺寸是否匹配：</li></ul> 
<pre><code class="prism language-python"><span class="token comment"># 创建一个 optimizer</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>

<span class="token comment"># 加载 checkpoint</span>
checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>

<span class="token comment"># 检查 optimizer 的 state_dict 和 checkpoint 的 state_dict 尺寸是否匹配</span>
<span class="token keyword">if</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'param_groups'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'param_groups'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token comment"># 加载 optimizer 的 state_dict</span>
    optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Size of optimizer's group doesn't match the checkpoint's group!"</span><span class="token punctuation">)</span>
</code></pre> 
<p>在这个示例中，我们检查了 optimizer 的 state_dict 和 checkpoint 的 state_dict 尺寸是否匹配。如果它们匹配，我们就可以使用 <code>optimizer.load_state_dict(checkpoint['optimizer'])</code> 来加载 optimizer 的 state_dict。否则，我们打印一个错误信息来指示尺寸不匹配。您需要根据您的实际场景来修改示例代码。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bad05934a5cc0efff81b40c05b48d836/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JVM 内存分析工具 MAT及实践</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ca9ced42301aba6549d352ab15d22db8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">线程信息分析，生产环境问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>