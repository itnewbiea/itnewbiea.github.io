<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>数据分析(7)-如何使用Python与Hadoop生态系统进行交互(译) - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="数据分析(7)-如何使用Python与Hadoop生态系统进行交互(译)" />
<meta property="og:description" content="我们都知道hadoop主要使用java实现的，那么如何使用python与hadoop生态圈进行交互呢，我看到一篇很好的文章，结合google翻译和自己的认识分享给大家。
您将学习如何从Hadoop Distributed Filesystem直接加载文件内存等信息。将文件从本地移动到HDFS或设置Spark。
from pathlib import Path import pandas as pd import numpy as np spark 安装 首先，安装findspark，以及pyspark，以防您在本地计算机上工作。如果您在Hadoop集群中关注本教程，可以跳过pyspark install。为简单起见，我将使用conda虚拟环境管理器（专业提示：在开始之前创建虚拟环境，不要破坏系统Python安装！）。
!conda install -c conda-forge findspark -y !conda install -c conda-forge pyspark -y 使用findspark进行Spark设置 import findspark # Local Spark # findspark.init(&#39;/home/cloudera/miniconda3/envs/jupyter/lib/python3.7/site-packages/pyspark/&#39;) # Cloudera cluster Spark findspark.init(spark_home=&#39;/opt/cloudera/parcels/SPARK2-2.3.0.cloudera4-1.cdh5.13.3.p0.611179/lib/spark2/&#39;) 进入pyspark shell
from pyspark.sql import SparkSession spark = SparkSession.builder.appName(&#39;example_app&#39;).master(&#39;local[*]&#39;).getOrCreate() 让我们获得现有的数据库。我假设您熟悉Spark DataFrame API及其方法：
spark.sql(&#34;show databases&#34;).show() ±-----------&#43;
|databaseName|
±-----------&#43;
| __ibis_tmp|
| analytics|
| db1|
| default|" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/7045eb89d6a041d004711ec31b4821ca/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-06-30T23:34:14+08:00" />
<meta property="article:modified_time" content="2019-06-30T23:34:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">数据分析(7)-如何使用Python与Hadoop生态系统进行交互(译)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>我们都知道hadoop主要使用java实现的，那么如何使用python与hadoop生态圈进行交互呢，我看到一篇<a href="https://thegurus.tech/posts/2019/05/hadoop-python/" rel="nofollow">很好的文章</a>，结合google翻译和自己的认识分享给大家。<br> 您将学习如何从Hadoop Distributed Filesystem直接加载文件内存等信息。将文件从本地移动到HDFS或设置Spark。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
</code></pre> 
<p> </p> 
<h2><a id="spark__11"></a>spark 安装</h2> 
<p>首先，安装findspark，以及pyspark，以防您在本地计算机上工作。如果您在Hadoop集群中关注本教程，可以跳过pyspark install。为简单起见，我将使用conda虚拟环境管理器（专业提示：在开始之前创建虚拟环境，不要破坏系统Python安装！）。</p> 
<pre><code class="prism language-python">!conda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge findspark <span class="token operator">-</span>y
!conda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge pyspark <span class="token operator">-</span>y
</code></pre> 
<h4><a id="findsparkSpark_18"></a>使用findspark进行Spark设置</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> findspark
<span class="token comment"># Local Spark</span>
<span class="token comment"># findspark.init('/home/cloudera/miniconda3/envs/jupyter/lib/python3.7/site-packages/pyspark/')</span>

<span class="token comment"># Cloudera cluster Spark</span>
findspark<span class="token punctuation">.</span>init<span class="token punctuation">(</span>spark_home<span class="token operator">=</span><span class="token string">'/opt/cloudera/parcels/SPARK2-2.3.0.cloudera4-1.cdh5.13.3.p0.611179/lib/spark2/'</span><span class="token punctuation">)</span>
</code></pre> 
<p>进入pyspark shell</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession
spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'example_app'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>让我们获得现有的数据库。我假设您熟悉Spark DataFrame API及其方法：</p> 
<pre><code class="prism language-python">spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span><span class="token string">"show databases"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>±-----------+<br> |databaseName|<br> ±-----------+<br> | __ibis_tmp|<br> | analytics|<br> | db1|<br> | default|<br> | fhadoop|<br> | juan|<br> ±-----------+</p> 
<h4><a id="pandas__spark_51"></a>pandas -&gt; spark</h4> 
<p>第一个集成是关于如何将数据从pandas库（即用于执行内存数据操作的Python标准库）移动到Spark。首先，让我们加载一个pandas DataFrame。这个是关于马德里的空气质量（只是为了满足您的好奇心，但对于将数据从一个地方移动到另一个地方并不重要）。你可以在这里下载。确保安装pytables以读取hdf5数据。</p> 
<pre><code class="prism language-python">air_quality_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_hdf<span class="token punctuation">(</span><span class="token string">'data/air_quality/air-quality-madrid/madrid.h5'</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token string">'28079008'</span><span class="token punctuation">)</span>
air_quality_df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<div class="output_html rendered_html output_subarea output_execute_result"> 
 <div> 
 </div> 
</div> 
<table border="1" class="dataframe"><thead><tr><th></th><th><span class="caps">BEN</span></th><th><span class="caps">CH4</span></th><th><span class="caps">CO</span></th><th><span class="caps">EBE</span></th><th><span class="caps">NMHC</span></th><th><span class="caps">NO</span></th><th>NO_2</th><th>NOx</th><th>O_3</th><th><span class="caps">PM10</span></th><th><span class="caps">PM25</span></th><th>SO_2</th><th><span class="caps">TCH</span></th><th><span class="caps">TOL</span></th></tr><tr><th>date</th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>2001-07-01 01:00:00</th><td>30.65</td><td>NaN</td><td>6.91</td><td>42.639999</td><td>NaN</td><td>NaN</td><td>381.299988</td><td>1017.000000</td><td>9.010000</td><td>158.899994</td><td>NaN</td><td>47.509998</td><td>NaN</td><td>76.050003</td></tr><tr><th>2001-07-01 02:00:00</th><td>29.59</td><td>NaN</td><td>2.59</td><td>50.360001</td><td>NaN</td><td>NaN</td><td>209.500000</td><td>409.200012</td><td>23.820000</td><td>104.800003</td><td>NaN</td><td>20.950001</td><td>NaN</td><td>84.900002</td></tr><tr><th>2001-07-01 03:00:00</th><td>4.69</td><td>NaN</td><td>0.76</td><td>25.570000</td><td>NaN</td><td>NaN</td><td>116.400002</td><td>143.399994</td><td>31.059999</td><td>48.470001</td><td>NaN</td><td>11.270000</td><td>NaN</td><td>20.980000</td></tr><tr><th>2001-07-01 04:00:00</th><td>4.46</td><td>NaN</td><td>0.74</td><td>22.629999</td><td>NaN</td><td>NaN</td><td>116.199997</td><td>149.300003</td><td>23.780001</td><td>47.500000</td><td>NaN</td><td>10.100000</td><td>NaN</td><td>14.770000</td></tr><tr><th>2001-07-01 05:00:00</th><td>2.18</td><td>NaN</td><td>0.57</td><td>11.920000</td><td>NaN</td><td>NaN</td><td>100.900002</td><td>124.800003</td><td>29.530001</td><td>49.689999</td><td>NaN</td><td>7.680000</td><td>NaN</td><td>8.970000</td></tr></tbody></table> 让我们对这个DataFrame进行一些更改，比如重置datetime索引，以便在加载到Spark时不会丢失信息。由于Spark在处理日期时遇到了一些问题（与系统区域设置，时区等相关），因此日期时间也将转换为字符串。 
<pre><code class="prism language-python">air_quality_df<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
air_quality_df<span class="token punctuation">[</span><span class="token string">'date'</span><span class="token punctuation">]</span> <span class="token operator">=</span> air_quality_df<span class="token punctuation">[</span><span class="token string">'date'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dt<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token string">'%Y-%m-%d %H:%M:%S'</span><span class="token punctuation">)</span>
</code></pre> 
<p>我们可以简单地从pandas加载到Spark createDataFrame：</p> 
<pre><code class="prism language-python">air_quality_sdf <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>air_quality_df<span class="token punctuation">)</span>
air_quality_sdf<span class="token punctuation">.</span>dtypes
</code></pre> 
<p>将DataFrame加载到Spark（如此air_quality_sdf处）后，可以使用PySpark方法轻松操作：</p> 
<pre><code class="prism language-python">air_quality_sdf<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">'date'</span><span class="token punctuation">,</span> <span class="token string">'NOx'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>

</code></pre> 
<p>±------------------±-----------------+<br> | date| NOx|<br> ±------------------±-----------------+<br> |2001-07-01 01:00:00| 1017.0|<br> |2001-07-01 02:00:00|409.20001220703125|<br> |2001-07-01 03:00:00|143.39999389648438|<br> |2001-07-01 04:00:00| 149.3000030517578|<br> |2001-07-01 05:00:00|124.80000305175781|<br> ±------------------±-----------------+<br> only showing top 5 rows</p> 
<h4><a id="pandas__spark__hive_230"></a>pandas -&gt; spark -&gt; hive</h4> 
<p>要将Spark DataFrame持久保存到HDFS中，可以使用默认的Hadoop SQL引擎（Hive）进行查询，一个简单的策略（不是唯一的策略）是从该DataFrame创建时间视图：</p> 
<pre><code class="prism language-python">air_quality_sdf<span class="token punctuation">.</span>createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"air_quality_sdf"</span><span class="token punctuation">)</span>

</code></pre> 
<p>创建时态视图后，可以使用Spark SQL引擎创建实时表create table as select。在创建此表之前，我将创建一个名为analytics存储它的新数据库</p> 
<pre><code class="prism language-python">sql_drop_table <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
drop table if exists analytics.pandas_spark_hive
"""</span>

sql_drop_database <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
drop database if exists analytics cascade
"""</span>

sql_create_database <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
create database if not exists analytics
location '/user/cloudera/analytics/'
"""</span>

sql_create_table <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
create table if not exists analytics.pandas_spark_hive
using parquet
as select to_timestamp(date) as date_parsed, *
from air_quality_sdf
"""</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"dropping database..."</span><span class="token punctuation">)</span>
result_drop_db <span class="token operator">=</span> spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>sql_drop_database<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"creating database..."</span><span class="token punctuation">)</span>
result_create_db <span class="token operator">=</span> spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>sql_create_database<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"dropping table..."</span><span class="token punctuation">)</span>
result_droptable <span class="token operator">=</span> spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>sql_drop_table<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"creating table..."</span><span class="token punctuation">)</span>
result_create_table <span class="token operator">=</span> spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>sql_create_table<span class="token punctuation">)</span>

borrando bb<span class="token punctuation">.</span>dd<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
creando bb<span class="token punctuation">.</span>dd<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
borrando tabla<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
creando tabla<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

</code></pre> 
<p>可以使用Spark SQL引擎检查结果，例如选择臭氧污染物浓度随时间变化：</p> 
<pre><code>spark.sql("select * from analytics.pandas_spark_hive").select("date_parsed", "O_3").show(5)

</code></pre> 
<p>±------------------±-----------------+<br> | date_parsed| O_3|<br> ±------------------±-----------------+<br> |2001-07-01 01:00:00| 9.010000228881836|<br> |2001-07-01 02:00:00| 23.81999969482422|<br> |2001-07-01 03:00:00|31.059999465942383|<br> |2001-07-01 04:00:00|23.780000686645508|<br> |2001-07-01 05:00:00|29.530000686645508|<br> ±------------------±-----------------+<br> only showing top 5 rows<br>  <br>  <br>  </p> 
<h2><a id="Apache_Arrow_297"></a>Apache Arrow</h2> 
<p>Apache Arrow是一种内存中的柱状数据格式，用于支持大数据环境中的高性能操作（可以将其视为内存等效的parquet格式）。它是用C ++开发的，但它的Python API很棒，你现在可以看到，但首先请安装它：</p> 
<pre><code>!conda install pyarrow -y
</code></pre> 
<p>为了与HDFS建立本地通信，我将使用pyarrow中包含的接口。只有要求是设置一个指向其位置的环境变量libhdfs。请记住，我们处于Cloudera环境中。如果你正在使用Horton必须找到合适的位置（相信我，它存在）。</p> 
<h4><a id="_304"></a>建立连接</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> pyarrow <span class="token keyword">as</span> pa
<span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'ARROW_LIBHDFS_DIR'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/opt/cloudera/parcels/CDH-5.14.4-1.cdh5.14.4.p0.3/lib64/'</span>
hdfs_interface <span class="token operator">=</span> pa<span class="token punctuation">.</span>hdfs<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'localhost'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">8020</span><span class="token punctuation">,</span> user<span class="token operator">=</span><span class="token string">'cloudera'</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="HDFS_313"></a>在HDFS中列出文件</h4> 
<p>让我们列出Spark之前保存的文件。请记住，这些文件先前已从本地文件加载到pandas DataFrame中，然后加载到Spark DataFrame中。Spark默认使用分区为大量snappy压缩文件的文件。在HDFS路径中，您可以标识数据库名称（analytics）和表名称（pandas_spark_hive）：</p> 
<pre><code class="prism language-python">hdfs_interface<span class="token punctuation">.</span>ls<span class="token punctuation">(</span><span class="token string">'/user/cloudera/analytics/pandas_spark_hive/'</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token string">'/user/cloudera/analytics/pandas_spark_hive/_SUCCESS'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/pandas_spark_hive/part-00000-b4371c8e-0f5c-4d20-a136-a65e56e97f16-c000.snappy.parquet'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/pandas_spark_hive/part-00001-b4371c8e-0f5c-4d20-a136-a65e56e97f16-c000.snappy.parquet'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/pandas_spark_hive/part-00002-b4371c8e-0f5c-4d20-a136-a65e56e97f16-c000.snappy.parquet'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/pandas_spark_hive/part-00003-b4371c8e-0f5c-4d20-a136-a65e56e97f16-c000.snappy.parquet'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/pandas_spark_hive/part-00004-b4371c8e-0f5c-4d20-a136-a65e56e97f16-c000.snappy.parquet'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/pandas_spark_hive/part-00005-b4371c8e-0f5c-4d20-a136-a65e56e97f16-c000.snappy.parquet'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/pandas_spark_hive/part-00006-b4371c8e-0f5c-4d20-a136-a65e56e97f16-c000.snappy.parquet'</span><span class="token punctuation">,</span>
 '<span class="token operator">/</span>user<span class="token operator">/</span>cloudera<span class="token operator">/</span>analytics<span class="token operator">/</span>pandas_spark_hive<span class="token operator">/</span>part<span class="token operator">-</span><span class="token number">00007</span><span class="token operator">-</span>b4371c8e<span class="token operator">-</span><span class="token number">0f5</span>
</code></pre> 
<h4><a id="Reading_parquet_files_directly_from_HDFS_329"></a>Reading parquet files directly from HDFS</h4> 
<p>要直接从HDFS读取representing文件（或充满表示文件的文件的文件夹），我将使用之前创建的PyArrow HDFS界面：</p> 
<pre><code class="prism language-python">table <span class="token operator">=</span> hdfs_interface<span class="token punctuation">.</span>read_parquet<span class="token punctuation">(</span><span class="token string">'/user/cloudera/analytics/pandas_spark_hive/'</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="HDFS__pandas_336"></a>HDFS -&gt; pandas</h4> 
<p>一旦parquetPyArrow HDFS接口读取文件，就会创建一个Table对象。我们可以通过方法轻松回到pandas 使用 to_pandas：</p> 
<pre><code class="prism language-python">table_df <span class="token operator">=</span> table<span class="token punctuation">.</span>to_pandas<span class="token punctuation">(</span><span class="token punctuation">)</span>
table_df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">/</span>home<span class="token operator">/</span>cloudera<span class="token operator">/</span>miniconda3<span class="token operator">/</span>envs<span class="token operator">/</span>jupyter<span class="token operator">/</span>lib<span class="token operator">/</span>python3<span class="token punctuation">.</span><span class="token number">6</span><span class="token operator">/</span>site<span class="token operator">-</span>packages<span class="token operator">/</span>pyarrow<span class="token operator">/</span>pandas_compat<span class="token punctuation">.</span>py<span class="token punctuation">:</span><span class="token number">752</span><span class="token punctuation">:</span> FutureWarning<span class="token punctuation">:</span> <span class="token punctuation">.</span>labels was deprecated <span class="token keyword">in</span> version <span class="token number">0.24</span><span class="token number">.0</span><span class="token punctuation">.</span> Use <span class="token punctuation">.</span>codes instead<span class="token punctuation">.</span>
  labels<span class="token punctuation">,</span> <span class="token operator">=</span> index<span class="token punctuation">.</span>labels
</code></pre> 
<div class="output_html rendered_html output_subarea output_execute_result"> 
 <div> 
 </div> 
</div> 
<table border="1" class="dataframe"><thead><tr><th></th><th>date_parsed</th><th>date</th><th><span class="caps">BEN</span></th><th><span class="caps">CH4</span></th><th><span class="caps">CO</span></th><th><span class="caps">EBE</span></th><th><span class="caps">NMHC</span></th><th><span class="caps">NO</span></th><th>NO_2</th><th>NOx</th><th>O_3</th><th><span class="caps">PM10</span></th><th><span class="caps">PM25</span></th><th>SO_2</th><th><span class="caps">TCH</span></th><th><span class="caps">TOL</span></th></tr></thead><tbody><tr><th>0</th><td>2001-06-30 23:00:00</td><td>2001-07-01 01:00:00</td><td>30.65</td><td>NaN</td><td>6.91</td><td>42.639999</td><td>NaN</td><td>NaN</td><td>381.299988</td><td>1017.000000</td><td>9.010000</td><td>158.899994</td><td>NaN</td><td>47.509998</td><td>NaN</td><td>76.050003</td></tr><tr><th>1</th><td>2001-07-01 00:00:00</td><td>2001-07-01 02:00:00</td><td>29.59</td><td>NaN</td><td>2.59</td><td>50.360001</td><td>NaN</td><td>NaN</td><td>209.500000</td><td>409.200012</td><td>23.820000</td><td>104.800003</td><td>NaN</td><td>20.950001</td><td>NaN</td><td>84.900002</td></tr><tr><th>2</th><td>2001-07-01 01:00:00</td><td>2001-07-01 03:00:00</td><td>4.69</td><td>NaN</td><td>0.76</td><td>25.570000</td><td>NaN</td><td>NaN</td><td>116.400002</td><td>143.399994</td><td>31.059999</td><td>48.470001</td><td>NaN</td><td>11.270000</td><td>NaN</td><td>20.980000</td></tr><tr><th>3</th><td>2001-07-01 02:00:00</td><td>2001-07-01 04:00:00</td><td>4.46</td><td>NaN</td><td>0.74</td><td>22.629999</td><td>NaN</td><td>NaN</td><td>116.199997</td><td>149.300003</td><td>23.780001</td><td>47.500000</td><td>NaN</td><td>10.100000</td><td>NaN</td><td>14.770000</td></tr><tr><th>4</th><td>2001-07-01 03:00:00</td><td>2001-07-01 05:00:00</td><td>2.18</td><td>NaN</td><td>0.57</td><td>11.920000</td><td>NaN</td><td>NaN</td><td>100.900002</td><td>124.800003</td><td>29.530001</td><td>49.689999</td><td>NaN</td><td>7.680000</td><td>NaN</td><td>8.970000</td></tr></tbody></table> 这就是我们开始的基础，关闭循环Python - &gt; Hadoop - &gt; Python。 
<h4><a id="HDFS_485"></a>上传本地文件到HDFS</h4> 
<p>使用PyArrow HDFS接口支持所有类型的HDFS操作，例如，将一堆本地文件上传到HDFS：</p> 
<pre><code class="prism language-python">cwd <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">'./data/'</span><span class="token punctuation">)</span>
destination_path <span class="token operator">=</span> <span class="token string">'/user/cloudera/analytics/data/'</span>

<span class="token keyword">for</span> f <span class="token keyword">in</span> cwd<span class="token punctuation">.</span>rglob<span class="token punctuation">(</span><span class="token string">'*.*'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'uploading {f.name}'</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>f<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f_upl<span class="token punctuation">:</span>
        hdfs_interface<span class="token punctuation">.</span>upload<span class="token punctuation">(</span>destination_path <span class="token operator">+</span> f<span class="token punctuation">.</span>name<span class="token punctuation">,</span> f_upl<span class="token punctuation">)</span>
uploading sandp500<span class="token punctuation">.</span><span class="token builtin">zip</span>
uploading stations<span class="token punctuation">.</span>csv
uploading madrid<span class="token punctuation">.</span>h5
uploading diamonds_train<span class="token punctuation">.</span>csv
uploading diamonds_test<span class="token punctuation">.</span>csv
</code></pre> 
<p>让我们检查文件是否已正确上传，列出目标路径中的文件：</p> 
<pre><code class="prism language-python">hdfs_interface<span class="token punctuation">.</span>ls<span class="token punctuation">(</span>destination_path<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token string">'/user/cloudera/analytics/data/diamonds_test.csv'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/data/diamonds_train.csv'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/data/madrid.h5'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/data/sandp500.zip'</span><span class="token punctuation">,</span>
 <span class="token string">'/user/cloudera/analytics/data/stations.csv'</span><span class="token punctuation">]</span>
</code></pre> 
<h4><a id="Reading_arbitrary_files_not_parquet_from_HDFS_HDFS__pandas_example_514"></a>Reading arbitrary files (not parquet) from HDFS (HDFS -&gt; pandas example</h4> 
<p>例如，.csv可以使用方法和标准pandas函数将文件从HDFS直接加载到pandas DataFrame中open，read_csv该函数可以获取缓冲区作为输入：</p> 
<pre><code class="prism language-python">diamonds_train <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>hdfs_interface<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'/user/cloudera/analytics/data/diamonds_train.csv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
diamonds_train<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<div> 
</div> 
<table border="1" class="dataframe"><thead><tr><th></th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>depth</th><th>table</th><th>price</th><th>x</th><th>y</th><th>z</th></tr></thead><tbody><tr><th>0</th><td>1.21</td><td>Premium</td><td>J</td><td><span class="caps">VS2</span></td><td>62.4</td><td>58.0</td><td>4268</td><td>6.83</td><td>6.79</td><td>4.25</td></tr><tr><th>1</th><td>0.32</td><td>Very Good</td><td>H</td><td><span class="caps">VS2</span></td><td>63.0</td><td>57.0</td><td>505</td><td>4.35</td><td>4.38</td><td>2.75</td></tr><tr><th>2</th><td>0.71</td><td>Fair</td><td>G</td><td><span class="caps">VS1</span></td><td>65.5</td><td>55.0</td><td>2686</td><td>5.62</td><td>5.53</td><td>3.65</td></tr><tr><th>3</th><td>0.41</td><td>Good</td><td>D</td><td><span class="caps">SI1</span></td><td>63.8</td><td>56.0</td><td>738</td><td>4.68</td><td>4.72</td><td>3.00</td></tr><tr><th>4</th><td>1.02</td><td>Ideal</td><td>G</td><td><span class="caps">SI1</span></td><td>60.5</td><td>59.0</td><td>4882</td><td>6.55</td><td>6.51</td><td>3.95</td></tr></tbody></table> 
<p>如果您对该库具有的所有方法和可能性感兴趣，请访问：https：<a href="//arrow.apache.org/docs/python/filesystems.html#hdfs-api" rel="nofollow">//arrow.apache.org/docs/python/filesystems.html#hdfs-api</a><br>  <br>  </p> 
<h3><a id="WebHDFS_625"></a>WebHDFS</h3> 
<p>有时无法访问libhdfs本机HDFS库（例如，从不属于群集的计算机执行分析）。在这种情况下，我们可以依赖WebHDFS（HDFS服务REST API），它速度较慢，不适合繁重的大数据负载，但在轻量级工作负载的情况下是一个有趣的选择。让我们安装一个WebHDFS Python API：</p> 
<pre><code class="prism language-python">!conda install <span class="token operator">-</span>c conda<span class="token operator">-</span>forge python<span class="token operator">-</span>hdfs <span class="token operator">-</span>y
Collecting package metadata<span class="token punctuation">:</span> done
Solving environment<span class="token punctuation">:</span> done

<span class="token comment">## Package Plan ##</span>

  environment location<span class="token punctuation">:</span> <span class="token operator">/</span>home<span class="token operator">/</span>cloudera<span class="token operator">/</span>miniconda3<span class="token operator">/</span>envs<span class="token operator">/</span>jupyter

  added <span class="token operator">/</span> updated specs<span class="token punctuation">:</span>
    <span class="token operator">-</span> python<span class="token operator">-</span>hdfs


The following packages will be downloaded<span class="token punctuation">:</span>

    package                    <span class="token operator">|</span>            build
    <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">|</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
    certifi<span class="token operator">-</span><span class="token number">2019.3</span><span class="token number">.9</span>           <span class="token operator">|</span>           py36_0         <span class="token number">149</span> KB  conda<span class="token operator">-</span>forge
    <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
                                           Total<span class="token punctuation">:</span>         <span class="token number">149</span> KB

The following packages will be UPDATED<span class="token punctuation">:</span>

  ca<span class="token operator">-</span>certificates    pkgs<span class="token operator">/</span>main<span class="token punctuation">:</span><span class="token punctuation">:</span>ca<span class="token operator">-</span>certificates<span class="token operator">-</span><span class="token number">2019.1</span><span class="token number">.23</span><span class="token operator">-</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span> conda<span class="token operator">-</span>forge<span class="token punctuation">:</span><span class="token punctuation">:</span>ca<span class="token operator">-</span>certificates<span class="token operator">-</span><span class="token number">2019.3</span><span class="token number">.9</span><span class="token operator">-</span>hecc5488_0

The following packages will be SUPERSEDED by a higher<span class="token operator">-</span>priority channel<span class="token punctuation">:</span>

  certifi                                         pkgs<span class="token operator">/</span>main <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span> conda<span class="token operator">-</span>forge
  openssl              pkgs<span class="token operator">/</span>main<span class="token punctuation">:</span><span class="token punctuation">:</span>openssl<span class="token operator">-</span><span class="token number">1.1</span><span class="token punctuation">.</span><span class="token number">1b</span><span class="token operator">-</span>h7b6447c_1 <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span> conda<span class="token operator">-</span>forge<span class="token punctuation">:</span><span class="token punctuation">:</span>openssl<span class="token operator">-</span><span class="token number">1.1</span><span class="token punctuation">.</span><span class="token number">1b</span><span class="token operator">-</span>h14c3975_1



Downloading <span class="token operator">and</span> Extracting Packages
certifi<span class="token operator">-</span><span class="token number">2019.3</span><span class="token number">.9</span>     <span class="token operator">|</span> <span class="token number">149</span> KB    <span class="token operator">|</span> <span class="token comment">##################################### | 100% </span>
Preparing transaction<span class="token punctuation">:</span> done
Verifying transaction<span class="token punctuation">:</span> done
Executing transaction<span class="token punctuation">:</span> done
</code></pre> 
<h4><a id="WebHDFS_669"></a>建立WebHDFS连接</h4> 
<p>建立连接</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> hdfs <span class="token keyword">import</span> InsecureClient

web_hdfs_interface <span class="token operator">=</span> InsecureClient<span class="token punctuation">(</span><span class="token string">'http://localhost:50070'</span><span class="token punctuation">,</span> user<span class="token operator">=</span><span class="token string">'cloudera'</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="List_files_in_HDFS_678"></a>List files in HDFS</h4> 
<p>列表文件类似于使用PyArrow接口，只需使用list方法和HDFS 路径：</p> 
<pre><code class="prism language-python">web_hdfs_interface<span class="token punctuation">.</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token string">'/user/cloudera/analytics/data'</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token string">'diamonds_test.csv'</span><span class="token punctuation">,</span>
 <span class="token string">'diamonds_train.csv'</span><span class="token punctuation">,</span>
 <span class="token string">'madrid.h5'</span><span class="token punctuation">,</span>
 <span class="token string">'sandp500.zip'</span><span class="token punctuation">,</span>
 <span class="token string">'stations.csv'</span><span class="token punctuation">]</span>
</code></pre> 
<h4><a id="HDFSWebHDFS_691"></a>上传本地文件到HDFS采用WebHDFS</h4> 
<pre><code class="prism language-python">cwd <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">'./data/'</span><span class="token punctuation">)</span>
destination_path <span class="token operator">=</span> <span class="token string">'/user/cloudera/analytics/data_web_hdfs/'</span>

<span class="token keyword">for</span> f <span class="token keyword">in</span> cwd<span class="token punctuation">.</span>rglob<span class="token punctuation">(</span><span class="token string">'*.*'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'uploading {f.name}'</span><span class="token punctuation">)</span>
    web_hdfs_interface<span class="token punctuation">.</span>upload<span class="token punctuation">(</span>destination_path <span class="token operator">+</span> f<span class="token punctuation">.</span>name<span class="token punctuation">,</span> 
                              <span class="token builtin">str</span><span class="token punctuation">(</span>f<span class="token punctuation">)</span><span class="token punctuation">,</span>
                              overwrite<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
uploading sandp500<span class="token punctuation">.</span><span class="token builtin">zip</span>
uploading stations<span class="token punctuation">.</span>csv
uploading madrid<span class="token punctuation">.</span>h5
uploading diamonds_train<span class="token punctuation">.</span>csv
uploading diamonds_test<span class="token punctuation">.</span>csv
</code></pre> 
<p>让我们检查上传是否正确：</p> 
<pre><code class="prism language-python">web_hdfs_interface<span class="token punctuation">.</span><span class="token builtin">list</span><span class="token punctuation">(</span>destination_path<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token string">'diamonds_test.csv'</span><span class="token punctuation">,</span>
 <span class="token string">'diamonds_train.csv'</span><span class="token punctuation">,</span>
 <span class="token string">'madrid.h5'</span><span class="token punctuation">,</span>
 <span class="token string">'sandp500.zip'</span><span class="token punctuation">,</span>
 <span class="token string">'stations.csv'</span><span class="token punctuation">]</span>
</code></pre> 
<p>HDFS也可以处理更大的文件（有一些限制）。这些文件来自Kaggle Microsoft恶意软件竞赛， 每个重量为几GB：</p> 
<pre><code class="prism language-python">web_hdfs_interface<span class="token punctuation">.</span>upload<span class="token punctuation">(</span>destination_path <span class="token operator">+</span> <span class="token string">'train.parquet'</span><span class="token punctuation">,</span> <span class="token string">'/home/cloudera/analytics/29_03_2019/notebooks/data/microsoft/train.pq'</span><span class="token punctuation">,</span> overwrite<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
web_hdfs_interface<span class="token punctuation">.</span>upload<span class="token punctuation">(</span>destination_path <span class="token operator">+</span> <span class="token string">'test.parquet'</span><span class="token punctuation">,</span> <span class="token string">'/home/cloudera/analytics/29_03_2019/notebooks/data/microsoft/test.pq'</span><span class="token punctuation">,</span> overwrite<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

</code></pre> 
<h4><a id="WebHDFS_HDFSHDFS___pandas_726"></a>使用WebHDFS 从HDFS读取文件（HDFS - &gt; pandas示例）¶</h4> 
<p>在这种情况下，使用PyArrow parquet模块并传递缓冲区来创建Table对象很有用。之后，可以使用to_pandas方法从Table对象轻松创建pandas DataFrame ：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> pyarrow <span class="token keyword">import</span> parquet <span class="token keyword">as</span> pq
<span class="token keyword">from</span> io <span class="token keyword">import</span> BytesIO

<span class="token keyword">with</span> web_hdfs_interface<span class="token punctuation">.</span>read<span class="token punctuation">(</span>destination_path <span class="token operator">+</span> <span class="token string">'train.parquet'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> reader<span class="token punctuation">:</span>
    microsoft_train <span class="token operator">=</span> pq<span class="token punctuation">.</span>read_table<span class="token punctuation">(</span>BytesIO<span class="token punctuation">(</span>reader<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to_pandas<span class="token punctuation">(</span><span class="token punctuation">)</span>
microsoft_train<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<div class="output_html rendered_html output_subarea output_execute_result"> 
 <div> 
 </div> 
</div> 
<table border="1" class="dataframe"><thead><tr><th></th><th>MachineIdentifier</th><th>ProductName</th><th>EngineVersion</th><th>AppVersion</th><th>AvSigVersion</th><th>IsBeta</th><th>RtpStateBitfield</th><th>IsSxsPassiveMode</th><th>DefaultBrowsersIdentifier</th><th>AVProductStatesIdentifier</th><th>…</th><th>Census_FirmwareVersionIdentifier</th><th>Census_IsSecureBootEnabled</th><th>Census_IsWIMBootEnabled</th><th>Census_IsVirtualDevice</th><th>Census_IsTouchEnabled</th><th>Census_IsPenCapable</th><th>Census_IsAlwaysOnAlwaysConnectedCapable</th><th>Wdft_IsGamer</th><th>Wdft_RegionIdentifier</th><th>HasDetections</th></tr></thead><tbody><tr><th>0</th><td>0000028988387b115f69f31a3bf04f09</td><td>win8defender</td><td>1.1.15100.1</td><td>4.18.1807.18075</td><td>1.273.1735.0</td><td>0</td><td>7.0</td><td>0</td><td>NaN</td><td>53447.0</td><td>…</td><td>36144.0</td><td>0</td><td>NaN</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>10.0</td><td>0</td></tr><tr><th>1</th><td>000007535c3f730efa9ea0b7ef1bd645</td><td>win8defender</td><td>1.1.14600.4</td><td>4.13.17134.1</td><td>1.263.48.0</td><td>0</td><td>7.0</td><td>0</td><td>NaN</td><td>53447.0</td><td>…</td><td>57858.0</td><td>0</td><td>NaN</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>8.0</td><td>0</td></tr><tr><th>2</th><td>000007905a28d863f6d0d597892cd692</td><td>win8defender</td><td>1.1.15100.1</td><td>4.18.1807.18075</td><td>1.273.1341.0</td><td>0</td><td>7.0</td><td>0</td><td>NaN</td><td>53447.0</td><td>…</td><td>52682.0</td><td>0</td><td>NaN</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>3.0</td><td>0</td></tr><tr><th>3</th><td>00000b11598a75ea8ba1beea8459149f</td><td>win8defender</td><td>1.1.15100.1</td><td>4.18.1807.18075</td><td>1.273.1527.0</td><td>0</td><td>7.0</td><td>0</td><td>NaN</td><td>53447.0</td><td>…</td><td>20050.0</td><td>0</td><td>NaN</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>3.0</td><td>1</td></tr><tr><th>4</th><td>000014a5f00daa18e76b81417eeb99fc</td><td>win8defender</td><td>1.1.15100.1</td><td>4.18.1807.18075</td><td>1.273.1379.0</td><td>0</td><td>7.0</td><td>0</td><td>NaN</td><td>53447.0</td><td>…</td><td>19844.0</td><td>0</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1</td></tr></tbody></table> 
<p>5 rows × 83 columns</p> 
<p> <br>  </p> 
<h2><a id="Hive__Impala_910"></a>Hive + Impala</h2> 
<p>Hive和Impala是Hadoop的两个SQL引擎。一个是基于MapReduce（Hive），而Impala是Cloudera创建和开源的更现代，更快速的内存实现。两个引擎都可以使用其多个API之一从Python中充分利用。在这种情况下，我将向您展示impyla，它支持两个引擎。让我们使用conda安装它，不要忘记安装thrift_sasl0.2.1版本（是的，必须是这个特定的版本，否则它将无法工作）：</p> 
<pre><code class="prism language-python">!conda install impyla thrift_sasl<span class="token operator">=</span><span class="token number">0.2</span><span class="token number">.1</span> <span class="token operator">-</span>y
<span class="token comment">## Package Plan ##</span>

  environment location<span class="token punctuation">:</span> <span class="token operator">/</span>home<span class="token operator">/</span>cloudera<span class="token operator">/</span>miniconda3<span class="token operator">/</span>envs<span class="token operator">/</span>jupyter

  added <span class="token operator">/</span> updated specs<span class="token punctuation">:</span>
    <span class="token operator">-</span> impyla
    <span class="token operator">-</span> thrift_sasl<span class="token operator">=</span><span class="token number">0.2</span><span class="token number">.1</span>


The following packages will be downloaded<span class="token punctuation">:</span>

    package                    <span class="token operator">|</span>            build
    <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">|</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
    certifi<span class="token operator">-</span><span class="token number">2019.3</span><span class="token number">.9</span>           <span class="token operator">|</span>           py36_0         <span class="token number">155</span> KB
    <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>
                                           Total<span class="token punctuation">:</span>         <span class="token number">155</span> KB

The following packages will be SUPERSEDED by a higher<span class="token operator">-</span>priority channel<span class="token punctuation">:</span>

  ca<span class="token operator">-</span>certificates    conda<span class="token operator">-</span>forge<span class="token punctuation">:</span><span class="token punctuation">:</span>ca<span class="token operator">-</span>certificates<span class="token operator">-</span><span class="token number">2019.3</span><span class="token number">.9</span><span class="token operator">~</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span> pkgs<span class="token operator">/</span>main<span class="token punctuation">:</span><span class="token punctuation">:</span>ca<span class="token operator">-</span>certificates<span class="token operator">-</span><span class="token number">2019.1</span><span class="token number">.23</span><span class="token operator">-</span><span class="token number">0</span>
  certifi                                       conda<span class="token operator">-</span>forge <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span> pkgs<span class="token operator">/</span>main
  openssl            conda<span class="token operator">-</span>forge<span class="token punctuation">:</span><span class="token punctuation">:</span>openssl<span class="token operator">-</span><span class="token number">1.1</span><span class="token punctuation">.</span><span class="token number">1b</span><span class="token operator">-</span>h14c3975_1 <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span> pkgs<span class="token operator">/</span>main<span class="token punctuation">:</span><span class="token punctuation">:</span>openssl<span class="token operator">-</span><span class="token number">1.1</span><span class="token punctuation">.</span><span class="token number">1b</span><span class="token operator">-</span>h7b6447c_1



Downloading <span class="token operator">and</span> Extracting Packages
certifi<span class="token operator">-</span><span class="token number">2019.3</span><span class="token number">.9</span>     <span class="token operator">|</span> <span class="token number">155</span> KB    <span class="token operator">|</span> <span class="token comment">##################################### | 100% </span>
Preparing transaction<span class="token punctuation">:</span> done
Verifying transaction<span class="token punctuation">:</span> done
Executing transaction<span class="token punctuation">:</span> done

</code></pre> 
<h3><a id="_948"></a>建立连接</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> impala<span class="token punctuation">.</span>dbapi <span class="token keyword">import</span> connect
<span class="token keyword">from</span> impala<span class="token punctuation">.</span>util <span class="token keyword">import</span> as_pandas
Hive <span class="token punctuation">(</span>Hive <span class="token operator">-</span><span class="token operator">&gt;</span> pandas example<span class="token punctuation">)</span>¶
</code></pre> 
<p>API遵循经典的ODBC标准，您可能对此很熟悉。impyla包括一个名为的实用程序函数as_pandas，可以轻松地将结果（元组列表）解析为pandas DataFrame。谨慎使用它，它存在某些类型的数据问题，并且对大数据工作负载效率不高。以两种方式获取结果：</p> 
<pre><code class="prism language-python">hive_conn <span class="token operator">=</span> connect<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'localhost'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> database<span class="token operator">=</span><span class="token string">'analytics'</span><span class="token punctuation">,</span> auth_mechanism<span class="token operator">=</span><span class="token string">'PLAIN'</span><span class="token punctuation">)</span>

<span class="token keyword">with</span> hive_conn<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> c<span class="token punctuation">:</span>
    c<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'SELECT * FROM analytics.pandas_spark_hive LIMIT 100'</span><span class="token punctuation">)</span>
    results <span class="token operator">=</span> c<span class="token punctuation">.</span>fetchall<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
<span class="token keyword">with</span> hive_conn<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> c<span class="token punctuation">:</span>
    c<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'SELECT * FROM analytics.pandas_spark_hive LIMIT 100'</span><span class="token punctuation">)</span>
    results_df <span class="token operator">=</span> as_pandas<span class="token punctuation">(</span>c<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Impala_Impala__pandas_example_969"></a>Impala (Impala -&gt; pandas example)</h4> 
<p>使用Impala遵循与Hive相同的模式，只需确保连接到正确的端口，在这种情况下默认为21050：</p> 
<pre><code class="prism language-python">impala_conn <span class="token operator">=</span> connect<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'localhost'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">21050</span><span class="token punctuation">)</span>

<span class="token keyword">with</span> impala_conn<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> c<span class="token punctuation">:</span>
    c<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'show databases'</span><span class="token punctuation">)</span>
    result_df <span class="token operator">=</span> as_pandas<span class="token punctuation">(</span>c<span class="token punctuation">)</span>
</code></pre> 
<div class="output_html rendered_html output_subarea output_execute_result"> 
 <div> 
 </div> 
</div> 
<table border="1" class="dataframe"><thead><tr><th></th><th>name</th><th>comment</th></tr></thead><tbody><tr><th>0</th><td>__ibis_tmp</td><td></td></tr><tr><th>1</th><td>_impala_builtins</td><td>System database for Impala builtin functions</td></tr><tr><th>2</th><td>analytics</td><td></td></tr><tr><th>3</th><td>db1</td><td></td></tr><tr><th>4</th><td>default</td><td>Default Hive database</td></tr><tr><th>5</th><td>fhadoop</td><td></td></tr><tr><th>6</th><td>juan</td><td></td></tr></tbody></table> 
<h3><a id="Ibis_Framework_1043"></a>Ibis Framework</h3> 
<p>另一种选择是Ibis Framework，它是一个相对庞大的数据源集合的高级API，包括HDFS和Impala。它是围绕使用Python对象和方法对这些源执行操作的想法构建的。让我们以与其他库相同的方式安装它：</p> 
<pre><code>!conda install ibis-framework -y
</code></pre> 
<p>让我们创建一个HDFS和Impala接口（impala需要在Ibis中使用hdfs接口对象）：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> ibis

hdfs_ibis <span class="token operator">=</span> ibis<span class="token punctuation">.</span>hdfs_connect<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'localhost'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">50070</span><span class="token punctuation">)</span>
impala_ibis <span class="token operator">=</span> ibis<span class="token punctuation">.</span>impala<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'localhost'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">21050</span><span class="token punctuation">,</span> hdfs_client<span class="token operator">=</span>hdfs_ibis<span class="token punctuation">,</span> user<span class="token operator">=</span><span class="token string">'cloudera'</span><span class="token punctuation">)</span>

</code></pre> 
<p>创建接口后，可以执行调用方法的操作，无需编写更多SQL。如果您熟悉ORM（对象关系映射器），这不完全相同，但基本思想非常相似。</p> 
<pre><code class="prism language-python">impala_ibis<span class="token punctuation">.</span>invalidate_metadata<span class="token punctuation">(</span><span class="token punctuation">)</span>
impala_ibis<span class="token punctuation">.</span>list_databases<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>[’__ibis_tmp’,<br> ‘_impala_builtins’,<br> ‘analytics’,<br> ‘db1’,<br> ‘default’,<br> ‘fhadoop’,<br> ‘juan’]</p> 
<h4><a id="Impala__pandas_1072"></a>Impala -&gt; pandas</h4> 
<p>ibis本地工作于pandas，因此无需进行转换。读表返回一个pandas DataFrame对象：</p> 
<pre><code class="prism language-python">table <span class="token operator">=</span> impala_ibis<span class="token punctuation">.</span>table<span class="token punctuation">(</span><span class="token string">'pandas_spark_hive'</span><span class="token punctuation">,</span> database<span class="token operator">=</span><span class="token string">'analytics'</span><span class="token punctuation">)</span>
table_df <span class="token operator">=</span> table<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token punctuation">)</span>
table_df<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<h4><a id="pandasImpala_1082"></a>pandas–&gt;Impala</h4> 
<p>从pandas到Impala可以使用Ibis使用Impala接口选择数据库，设置权限（取决于您的群集设置）并使用该方法create，将pandas DataFrame对象作为参数传递：</p> 
<pre><code class="prism language-python">analytics_db<span class="token punctuation">.</span>table<span class="token punctuation">(</span><span class="token string">'diamonds'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>

</code></pre> 
<div class="output_html rendered_html output_subarea output_execute_result"> 
 <div> 
 </div> 
</div> 
<table border="1" class="dataframe"><thead><tr><th></th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>depth</th><th>table</th><th>price</th><th>x</th><th>y</th><th>z</th></tr></thead><tbody><tr><th>0</th><td>1.21</td><td>Premium</td><td>J</td><td><span class="caps">VS2</span></td><td>62.4</td><td>58.0</td><td>4268</td><td>6.83</td><td>6.79</td><td>4.25</td></tr><tr><th>1</th><td>0.32</td><td>Very Good</td><td>H</td><td><span class="caps">VS2</span></td><td>63.0</td><td>57.0</td><td>505</td><td>4.35</td><td>4.38</td><td>2.75</td></tr><tr><th>2</th><td>0.71</td><td>Fair</td><td>G</td><td><span class="caps">VS1</span></td><td>65.5</td><td>55.0</td><td>2686</td><td>5.62</td><td>5.53</td><td>3.65</td></tr><tr><th>3</th><td>0.41</td><td>Good</td><td>D</td><td><span class="caps">SI1</span></td><td>63.8</td><td>56.0</td><td>738</td><td>4.68</td><td>4.72</td><td>3.00</td></tr><tr><th>4</th><td>1.02</td><td>Ideal</td><td>G</td><td><span class="caps">SI1</span></td><td>60.5</td><td>59.0</td><td>4882</td><td>6.55</td><td>6.51</td><td>3.95</td></tr></tbody></table> 最后希望翻译这篇文章对你有所帮助谢谢！
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/afa04ceaeff12e7b231115239ffcb973/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">c&#43;&#43;四种cast的原理和用途</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6925a3605e97b1f5a77f7d7233234956/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据分析(8)--matplotlib 数据可视化</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>