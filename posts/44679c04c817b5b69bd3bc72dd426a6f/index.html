<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>视觉SLAM技术简述，一文了解视觉SLAM - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="视觉SLAM技术简述，一文了解视觉SLAM" />
<meta property="og:description" content="如今科技发展日新月异，随着机器人、AR/VR等人工智能领域的不断发展，视觉SLAM也取得了惊人的发展。本文就视觉SLAM的定义、研究分类、模块、工作原理及应用方向等方面做一个视觉SLAM的技术简述。
视觉SLAM是什么？
SLAM的英文全称是Simultaneous Localization and Mapping，中文称作「即时定位与地图创建」，而视觉SLAM就是用摄像头来完成环境的感知工作。
当一个自主移动的机器人处在一个未知的环境，它要通过摄像头获取的信息数据对自身以及周围环境进行一个预估，在机器人移动过程中根据位置估计和摄像头对感知的数据进行自身的定位，同时不断地建造和更新地图并且规划自身的路径。
视觉SLAM研究分类
视觉SLAM研究主要分为三大类:单目、双目、RGBD。
单目SLAM只用一支摄像头就可以完成SLAM。最大的优点是传感器简单且成本低，但同时也有个大问题，就是不能通过单张图片得到深度信息（距离），存在尺寸不确定的现象，比如丢失深度信息的相片中，我们能见到“手捏太阳”“借位拍照”这样的现象。
单目相机无法依靠一张图像获得图像中物体离自己的相对距离。为了估计这个相对深度，只能利用移动相机之后进行三角化，测量像素的距离。即是说，它的轨迹和地图，只有在相机运动之后才能收敛，如果相机不进行运动时，就无法得知像素的位置。同时，相机运动还不能是纯粹的旋转，这就给单目SLAM的应用带来了一些麻烦。
双目SLAM利用左右目的视差计算像素的距离，从而实现自身的定位。立体视觉既可以在运动时估计深度，亦可在静止时估计，消除了单目视觉的无法得到深度信息的麻烦。目前市面常见的双目相机包括INDEMIND双目视觉惯性模组等。不过通过双目图像计算像素距离，计算量大，而且在特征少的白墙或暗光环境易丢失目标。
RGBD相机是2010年左右新兴的一种相机，它最大的特点是可以通过红外结构光或Time-of-Flight原理，直接测出图像中各像素离相机的距离。因此，它比传统相机能够提供更丰富的信息，也不必像单目或双目那样费时费力地计算深度。目前常用的RGBD相机包括Kinect/Kinect V2等。不过，现在多数RGBD相机还存在测量范围窄、噪声大、视野小等诸多问题。出于量程的限制，主要用于室内SLAM。
视觉SLAM框架解读
一般的视觉SLAM系统分为五个模块：传感器数据、视觉里程计、后端、建图、回环检测。
1.传感器数据
在视觉SLAM中主要为相机图像信息的读取和预处理。如果在机器人中，还可能有码盘，惯性传感器等信息的读取和同步。
2.视觉里程计
视觉里程计（VO）也称前端。它根据相邻图像的信息，估计出粗略的相机运动，给后端提供较好的 初始值。VO 的实现方法，按是否需要提取特征，分为特征点法的前端以及不提特征的直 接法前端。基于特征点法的前端，长久以来（直到现在）被认为是视觉里程计的主流方法。 它运行稳定，对光照、动态物体不敏感，是目前比较成熟的解决方案。
3.后端优化
后端优化主要是处理slam过程中噪声的问题。任何传感器都有噪声，所以除了要处理“如何从图像中估计出相机运动”，还要关心这个估计带有多大的噪声。
前端给后端提供待优化的数据，以及这些数据的初始值，而后端负责整体的优化过程，它往往面对的只有数据，不必关系这些数据来自哪里。在视觉slam中，前端和计算接视觉研究领域更为相关，比如图像的特征提取与匹配等，后端则主要是滤波和非线性优化算法。
4.回环检测
回环检测也可以称为闭环检测，是指机器人识别曾到达场景的能力。回环检测提供了当前数据与所有历史数据的关联，在跟踪算法丢失之后，我们还可以利用回环检测进行重定位。因此，回环检测对整个SLAM系统精度与鲁棒性的提升，是非常明显的。
5.建图
建图主要是根据估计的轨迹建立与任务要求对应的地图，在机器人学中，地图的表示主要有栅格地图、直接表征法、拓扑地图以及特征点地图这4种。而特征点地图是用有关的几何特征（如点、直线、面）表示环境，常见于视觉SLAM技术中。
视觉SLAM工作原理
大多数视觉SLAM系统的工作方式是通过连续的相机帧，跟踪设置关键点，以三角算法定位其3D位置，同时使用此信息来逼近推测相机自己的姿态。简单来说，这些系统的目标是绘制与自身位置相关的环境地图。这个地图可以用于机器人系统在该环境中导航作用。与其他形式的SLAM技术不同，只需一个3D视觉摄像头，就可以做到这一点。
通过跟踪摄像头视频帧中足够数量的关键点，可以快速了解传感器的方向和周围物理环境的结构。所有视觉SLAM系统都在不断的工作，以使重新投影误差(Reprojection Error)或投影点与实际点之间的差异最小化，通常是通过一种称为Bundle Adjustment(BA)的算法解决方案。VSLAM系统需要实时操作，这涉及到大量的运算，因此位置数据和映射数据经常分别进行Bundle Adjustment，但同时进行，便于在最终合并之前加快处理速度。
目前，视觉SLAM主要被运用于无人机、无人驾驶、机器人、AR、智能家居等领域，同时涌现出了一大批视觉SLAM优秀的公司和产品。如百度的自动驾驶软件、仙途智能的智能环卫车、科沃斯的扫地机器人、INDEMIND的机器人视觉导航定位解决方案、穿戴计算解决方案等等。
随着城市物联网和智能系统的完善，视觉SLAM的前景是非常广阔的。尤其是视觉SLAM得天独厚的语义地图优势，能很好地满足人与人工智能之间的交互，非常符合人工智能的发展趋势。
虽然目前视觉SLAM技术方面还存在着一些问题，但这些都会随着消费刺激和产业链的发展逐步解决、趋于完善。
参考文献
《视觉SLAM十四讲》高翔" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/44679c04c817b5b69bd3bc72dd426a6f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-14T18:02:43+08:00" />
<meta property="article:modified_time" content="2020-05-14T18:02:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">视觉SLAM技术简述，一文了解视觉SLAM</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>如今科技发展日新月异，随着机器人、AR/VR等人工智能领域的不断发展，视觉SLAM也取得了惊人的发展。本文就视觉SLAM的定义、研究分类、模块、工作原理及应用方向等方面做一个视觉SLAM的技术简述。</p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;"><strong><strong>视觉SLAM是什么？</strong></strong></p> 
<p style="margin-left:0pt;">SLAM的英文全称是Simultaneous Localization and Mapping，中文称作「即时定位与地图创建」，而视觉SLAM就是用摄像头来完成环境的感知工作。</p> 
<p style="margin-left:0pt;">当一个自主移动的机器人处在一个未知的环境，它要通过摄像头获取的信息数据对自身以及周围环境进行一个预估，在机器人移动过程中根据位置估计和摄像头对感知的数据进行自身的定位，同时不断地建造和更新地图并且规划自身的路径。</p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;"><strong><strong>视觉SLAM研究分类</strong></strong></p> 
<p style="margin-left:0pt;">视觉SLAM研究主要分为三大类:单目、双目、RGBD。</p> 
<p style="margin-left:0pt;">单目SLAM只用一支摄像头就可以完成SLAM。最大的优点是传感器简单且成本低，但同时也有个大问题，就是不能通过单张图片得到深度信息（距离），存在尺寸不确定的现象，比如丢失深度信息的相片中，我们能见到“手捏太阳”“借位拍照”这样的现象。</p> 
<p style="text-align:center;"><img alt="" height="218" src="https://images2.imgbox.com/22/0f/oqMnIhAO_o.jpg" width="214"></p> 
<p style="margin-left:0pt;">单目相机无法依靠一张图像获得图像中物体离自己的相对距离。为了估计这个相对深度，只能利用移动相机之后进行三角化，测量像素的距离。即是说，它的轨迹和地图，只有在相机运动之后才能收敛，如果相机不进行运动时，就无法得知像素的位置。同时，相机运动还不能是纯粹的旋转，这就给单目SLAM的应用带来了一些麻烦。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/08/aa/fp9r1qwu_o.jpg"></p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;">双目SLAM利用左右目的视差计算像素的距离，从而实现自身的定位。立体视觉既可以在运动时估计深度，亦可在静止时估计，消除了单目视觉的无法得到深度信息的麻烦。目前市面常见的双目相机包括INDEMIND双目视觉惯性模组等。不过通过双目图像计算像素距离，计算量大，而且在特征少的白墙或暗光环境易丢失目标。</p> 
<p style="text-align:center;"><img alt="" height="134" src="https://images2.imgbox.com/15/13/WtEbKn9N_o.jpg" width="324"></p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;">RGBD相机是2010年左右新兴的一种相机，它最大的特点是可以通过红外结构光或Time-of-Flight原理，直接测出图像中各像素离相机的距离。因此，它比传统相机能够提供更丰富的信息，也不必像单目或双目那样费时费力地计算深度。目前常用的RGBD相机包括Kinect/Kinect V2等。不过，现在多数RGBD相机还存在测量范围窄、噪声大、视野小等诸多问题。出于量程的限制，主要用于室内SLAM。</p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;"><strong><strong>视觉SLAM框架解读</strong></strong></p> 
<p style="margin-left:0pt;">一般的视觉SLAM系统分为五个模块：传感器数据、视觉里程计、后端、建图、回环检测。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c4/17/KLsZ9jTi_o.jpg"></p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;">1.传感器数据</p> 
<p style="margin-left:0pt;">在视觉SLAM中主要为相机图像信息的读取和预处理。如果在机器人中，还可能有码盘，惯性传感器等信息的读取和同步。</p> 
<p style="margin-left:0pt;">2.视觉里程计</p> 
<p style="margin-left:0pt;">视觉里程计（VO）也称前端。它根据相邻图像的信息，估计出粗略的相机运动，给后端提供较好的 初始值。VO 的实现方法，按是否需要提取特征，分为特征点法的前端以及不提特征的直 接法前端。基于特征点法的前端，长久以来（直到现在）被认为是视觉里程计的主流方法。 它运行稳定，对光照、动态物体不敏感，是目前比较成熟的解决方案。</p> 
<p style="margin-left:0pt;">3.后端优化</p> 
<p style="margin-left:0pt;">后端优化主要是处理slam过程中噪声的问题。任何传感器都有噪声，所以除了要处理“如何从图像中估计出相机运动”，还要关心这个估计带有多大的噪声。</p> 
<p style="margin-left:0pt;">前端给后端提供待优化的数据，以及这些数据的初始值，而后端负责整体的优化过程，它往往面对的只有数据，不必关系这些数据来自哪里。在视觉slam中，前端和计算接视觉研究领域更为相关，比如图像的特征提取与匹配等，后端则主要是滤波和非线性优化算法。</p> 
<p style="margin-left:0pt;">4.回环检测</p> 
<p style="margin-left:0pt;">回环检测也可以称为闭环检测，是指机器人识别曾到达场景的能力。回环检测提供了当前数据与所有历史数据的关联，在跟踪算法丢失之后，我们还可以利用回环检测进行重定位。因此，回环检测对整个SLAM系统精度与鲁棒性的提升，是非常明显的。</p> 
<p style="margin-left:0pt;">5.建图</p> 
<p style="margin-left:0pt;">建图主要是根据估计的轨迹建立与任务要求对应的地图，在机器人学中，地图的表示主要有栅格地图、直接表征法、拓扑地图以及特征点地图这4种。而特征点地图是用有关的几何特征（如点、直线、面）表示环境，常见于视觉SLAM技术中。</p> 
<p style="margin-left:0pt;"><br><strong><strong>视觉SLAM工作原理</strong></strong></p> 
<p style="margin-left:0pt;">大多数视觉SLAM系统的工作方式是通过连续的相机帧，跟踪设置关键点，以三角算法定位其3D位置，同时使用此信息来逼近推测相机自己的姿态。简单来说，这些系统的目标是绘制与自身位置相关的环境地图。这个地图可以用于机器人系统在该环境中导航作用。与其他形式的SLAM技术不同，只需一个3D视觉摄像头，就可以做到这一点。</p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;">通过跟踪摄像头视频帧中足够数量的关键点，可以快速了解传感器的方向和周围物理环境的结构。所有视觉SLAM系统都在不断的工作，以使重新投影误差(Reprojection Error)或投影点与实际点之间的差异最小化，通常是通过一种称为Bundle Adjustment(BA)的算法解决方案。VSLAM系统需要实时操作，这涉及到大量的运算，因此位置数据和映射数据经常分别进行Bundle Adjustment，但同时进行，便于在最终合并之前加快处理速度。</p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;">目前，视觉SLAM主要被运用于无人机、无人驾驶、机器人、AR、智能家居等领域，同时涌现出了一大批视觉SLAM优秀的公司和产品。如百度的自动驾驶软件、仙途智能的智能环卫车、科沃斯的扫地机器人、INDEMIND的机器人视觉导航定位解决方案、穿戴计算解决方案等等。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/97/71/R6SaybQW_o.jpg"></p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;">随着城市物联网和智能系统的完善，视觉SLAM的前景是非常广阔的。尤其是视觉SLAM得天独厚的语义地图优势，能很好地满足人与人工智能之间的交互，非常符合人工智能的发展趋势。</p> 
<p style="margin-left:0pt;">虽然目前视觉SLAM技术方面还存在着一些问题，但这些都会随着消费刺激和产业链的发展逐步解决、趋于完善。</p> 
<p style="margin-left:0pt;"> </p> 
<p style="margin-left:0pt;">参考文献</p> 
<p style="margin-left:0pt;">《视觉SLAM十四讲》高翔</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/53276d77aef124dfe61a3b8e5e941d6f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue轮播图简单实现</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d36e00da1000e553d7d3c7c6eb05a323/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">弄它！！！静态路由原理与配置实验（华为ensp软件上手动配置静态路由）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>