<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud【翻译】 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud【翻译】" />
<meta property="og:description" content="Abstract 本文提出 PointRCNN 用于从原始点云进行 3D 对象检测。
两个阶段
stage-1 用于自下而上的 3D proposal生成
我们的 stage-1 子网络不是像以前的方法那样从 RGB 图像生成proposal或将点云投影到鸟瞰图或体素，而是通过分割以自下而上的方式直接从点云生成少量高质量的 3D proposal将整个场景的点云分为前景点和背景。
stage-2 用于在规范坐标中细化proposal以获得最终检测结果
stage-2 子网络将每个proposal 的池化点转换为规范坐标，以学习更好的局部空间特征，并结合stage-1 中学习的每个点的全局语义特征进行准确的框细化和置信度预测。
数据集：KITTI 数据集
仅使用点云作为输入，以显着的优势优于最先进的方法（现在不是了）。
1. Introduction 虽然最近开发的 2D 检测算法能够处理图像中大量的视点变化和背景杂乱，但由于三维目标的不规则数据格式和6自由度大的搜索空间，对具有点云特征的三维目标的检测仍然面临着巨大的挑战。
在自动驾驶中，最常用的 3D 传感器是 LiDAR 传感器，它可以生成 3D 点云来捕捉场景的 3D 结构。 基于点云的3D目标检测的难点主要在于点云的不规则性。 最先进的 3D 检测方法要么利用成熟的 2D 检测框架，将点云投影到鸟瞰图 [14、42、17]（参见图 1（a））到正面图 [4 , 38] 或常规 3D 体素 [34, 43]，它们不是最优的，并且在量化过程中会遭受信息丢失。
图 1. 与最先进方法的比较。 我们的方法不是从鸟瞰图和前视图的融合特征图 [14] 或 RGB 图像 [25] 生成提案，而是直接从原始点云以自下而上的方式生成 3D 提案。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/d93ec7cc4a531ae27632de59a5f63d39/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-03T16:36:28+08:00" />
<meta property="article:modified_time" content="2022-03-03T16:36:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud【翻译】</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>Abstract</h2> 
<p>本文提出 <span style="background-color:#fbd4d0;">PointRCNN </span>用于从原始点云进行 3D 对象检测。</p> 
<p><strong><span style="background-color:#f9eda6;">两个阶段</span></strong></p> 
<p><strong><span style="background-color:#f9eda6;">stage-1 用于自下而上的 3D proposal生成</span></strong></p> 
<p>我们的 stage-1 子网络不是像以前的方法那样从 RGB 图像生成<span style="background-color:#fbd4d0;">proposal</span>或将点云投影到鸟瞰图或体素，而是通<strong>过<span style="background-color:#f9eda6;">分割</span>以<span style="background-color:#f9eda6;">自下而上</span>的方式直接从点云生成少量高质量的 3D proposal将整个场景的点云分为前景点和背景。</strong></p> 
<p><strong><span style="background-color:#f9eda6;">stage-2 用于在规范坐标中细化proposal以获得最终检测结果</span></strong></p> 
<p>stage-2 子网络将每个<span style="background-color:#fbd4d0;">proposal </span>的<strong>池化点</strong>转换为<strong>规范坐标</strong>，以学习更好的局部空间特征，并结合stage-1 中学习的每个点的全局语义特征进行准确的<strong>框细化</strong>和<strong>置信度预测</strong>。</p> 
<p><span style="background-color:#f9eda6;">数据集</span>：KITTI 数据集</p> 
<p>仅使用点云作为输入，以显着的优势优于最先进的方法（现在不是了）。</p> 
<hr> 
<h2>1. Introduction</h2> 
<p>虽然最近开发的 2D 检测算法能够处理图像中大量的视点变化和背景杂乱，但由于三维目标的<span style="color:#38d8f0;">不规则数据格式</span>和<span style="color:#38d8f0;">6自由度大的搜索空间</span>，对具有点云特征的三维目标的检测仍然面临着<strong>巨大的挑战</strong>。</p> 
<p>在自动驾驶中，最常用的 3D 传感器是 <span style="background-color:#ffd7b9;">LiDAR 传感器</span>，它可以生成 3D 点云来捕捉场景的 <span style="background-color:#ffd7b9;">3D 结构</span>。 基于点云的3D目标检测的<strong>难点主要在于点云的不规则性</strong>。 最先进的 3D 检测方法要么利用成熟的 2D 检测框架，将点云投影到鸟瞰图 [14、42、17]（参见图<span style="background-color:#fe2c24;"> 1（a）</span>）到正面图 [4  , 38] 或常规 3D 体素 [34, 43]，<strong>它们不是最优的，并且在量化过程中会遭受信息丢失</strong>。</p> 
<p style="text-align:center;"><img alt="" height="480" src="https://images2.imgbox.com/b1/8f/1x7JLSxR_o.png" width="528"></p> 
<p> <span style="background-color:#fe2c24;">图 1</span>. 与最先进方法的比较。 我们的方法不是从鸟瞰图和前视图的融合特征图 [14] 或 RGB 图像 [25] 生成提案，而是直接从原始点云以自下而上的方式生成 3D 提案。</p> 
<p>[26, 28] 提出了 <span style="background-color:#fbd4d0;">PointNet</span>，用于直接从点云数据中学习 3D 表示，用于点云分类和分割。 如图 <span style="background-color:#fe2c24;">1 (b)</span> 所示，他们的后续工作 [25] 将 <span style="background-color:#fbd4d0;">PointNet </span>应用于 3D 对象检测，基于从 2D RGB 检测结果中裁剪的截锥体点云来估计 3D 边界框。 然而，<strong>该方法的性能在很大程度上依赖于 2D 检测性能，不能利用 3D 信息的优势来生成鲁棒的边界框proposal。</strong></p> 
<p>与 2D 图像中的对象检测不同，<strong>自动驾驶场景中的 3D 对象由带注释的 3D 边界框自然且良好地分离</strong>。 换句话说，用于 3D 对象检测的训练数据直接为 3D 对象分割提供了语义掩码。 这是 3D 检测和 2D 检测训练数据之间的关键区别。 在 2D 对象检测中，边界框只能为语义分割提供弱监督 [5]。</p> 
<p>基于这一观察，我们提出了一种<strong><span style="background-color:#f9eda6;">新颖的两阶段 3D 对象检测框架</span></strong>，名为 <span style="background-color:#fbd4d0;">PointRCNN</span>，它直接对 3D 点云进行操作，并实现了鲁棒和准确的 3D 检测性能（见图 1（c））。</p> 
<p>提出的框架由两个阶段组成，<span style="background-color:#f9eda6;">第一阶段</span><span style="color:#ff9900;">旨在以自下而上的方案生成 3D 边界框提案</span>。 通过利用 3D 边界框生成真实分割掩码，第一阶段分割前景点并同时从分割点生成少量边界框建议。 这种策略避免了在整个 3D 空间中使用大量的 3D 锚框，就像以前的方法 [43,14,4] 所做的那样，并且节省了大量的计算量。</p> 
<p><span style="background-color:#fbd4d0;">PointRCNN </span>的<span style="background-color:#f9eda6;">第二阶段</span><span style="color:#ff9900;">进行规范的 3D 框细化</span>。 在生成 3D proposal后，采用点云区域池化操作来池化来自阶段 1 的学习点表示。 与现有的直接估计全局框坐标的 3D 方法不同，池化的 3D 点被转换为规范坐标，并与池化点特征以及来自 stage-1 的分割掩码相结合，用于学习相对坐标细化。 该策略充分利用了我们强大的第一阶段分割和提议子网络提供的所有信息。 为了学习更有效的坐标细化，<strong>我们还提出了完整的基于 bin 的 3D 框回归损失用于提案生成和细化，</strong>并且消融实验表明它比其他 3D 框回归损失收敛更快并实现更高的召回率。</p> 
<p><strong>我们的贡献可以概括为三个方面。</strong></p> 
<p>(1) 我们提出了一种新颖的基于自下而上点云的 3D 边界框建议生成算法，该算法通过将点云分割为前景对象和背景来生成少量高质量的 3D 建议。从分割中学习到的点表示不仅擅长生成提案，而且有助于后期的框细化。</p> 
<p>(2) 提出的规范 3D 边界框细化利用了我们从阶段 1 生成的高召回率框提议，并学习在规范坐标中预测框坐标细化，并具有鲁棒的基于 bin 的损失。</p> 
<p>(3) 我们提出的 3D 检测框架 PointRCNN 在仅使用点云作为输入的情况下，在 KITTI 的 3D 检测测试板上，在截至 2018 年 11 月 16 日的所有已发表作品中以显着的优势优于 state-of-theart 方法，并且在所有已发表的作品中排名第一。</p> 
<h2>2. Related Work</h2> 
<p><strong>3D object detection from 2D images.</strong></p> 
<p>目前已有从图像中估计三维边界框的工作。<strong>由于缺乏深度信息，这些作品只能生成粗略的3D检测结果，并且会受到外观变化的显著影响。</strong></p> 
<p><strong>3D object detection from point clouds.</strong></p> 
<p>最先进的三维目标检测方法提出了从稀疏的三维点云中学习鉴别特征的各种方法。然而，由于数据量化，鸟瞰投影和体素化遭受信息丢失，并且3D CNN既没有存储效率，也没有计算效率。基于2D图像的提案生成可能会在一些只有在3D空间才能很好观察到的挑战性案例中失败。这种故障无法通过3D框估计步骤恢复。相比之下，<strong>我们自下而上的3Dproposal生成方法直接从点云生成健壮的3Dproposal，既高效又无量化。</strong></p> 
<p><strong>Learning point cloud representations.</strong></p> 
<p>Qi 等人没有将点云表示为体素 [22、33、35] 或多视图格式 [27、36、37]。 [26]提出了<span style="background-color:#fbd4d0;">PointNet</span>架构，<strong>直接从原始点云中学习点特征</strong>，大大提高了点云分类和分割的速度和准确性。后续工作 [28, 12] 通过考虑<strong>点云中的局部结构</strong>进一步提高了提取的特征质量。<strong>我们的工作将基于点的特征提取器扩展到基于 3D 点云的对象检测，从而产生了一种新颖的两阶段 3D 检测框架，该框架直接从原始点云生成 3D 框proposal和检测结果。</strong></p> 
<h2><strong>3. PointRCNN for Point Cloud 3D Detection</strong></h2> 
<p>在本节中，我们将介绍我们提出的两阶段检测框架 PointRCNN，用于从不规则点云中检测 3D 对象。整体结构如图 <span style="background-color:#fe2c24;">2</span> 所示，由<strong>自下而上的 3D proposal生成阶段</strong>和<strong>规范边界框细化阶段</strong>组成。</p> 
<p><img alt="" height="632" src="https://images2.imgbox.com/66/23/r6jPUTeH_o.png" width="1180"></p> 
<h3> 3.1. Bottom-up 3D proposal generation via point cloud segmentation</h3> 
<p>现有的二维目标检测方法可分为一阶段方法和两阶段方法。</p> 
<p>一阶段方法[19,21,31,30,29]<strong>通常较快，但直接估计物体边界框，而不加细化。</strong></p> 
<p>两阶段方法[10,18,32,8]<strong>首先产生建议，并在第二阶段进一步完善proposal和保密。</strong></p> 
<p>然而，由于三维搜索空间的巨大和点云格式的不规则性，两阶段方法从2d 直接扩展到3d 并不是一件简单的事情。Avod [14]在3d 空间中放置80-100k 锚框，并在多个视图中为每个锚框放置池化特性，以生成proposal。Fpointnet [25]根据2d 图像生成2d proposal，并根据2d 区域的3d 点估计3d 框，这可能会漏掉那些只能从3d 空间清晰观察到的难度较大的物体。</p> 
<p>我们提出了一种精确且鲁棒的3D提案生成算法，作为基于点云分割的第一阶段子网络。我们观察到3D场景中的对象自然分离，彼此不重叠。所有3D对象的分割掩码都可以通过其3D边界框标注直接获得，即3D框内的3D点被视为前景点。</p> 
<p>因此，我们建议以<strong>自下而上的方式生成3Dproposal</strong>。具体来说，<strong>我们学习了点特征来分割原始点云，并同时从分割的前景点生成3Dproposal。基于这种自下而上的策略，我们的方法避免了在3D空间中使用大量预定义的3D框，并显著限制了3Dproposal生成的搜索空间。</strong>实验表明，与基于3D锚的proposal生成方法相比，我们提出的3D框proposal生成方法具有更高的召回率。</p> 
<p><strong>Learning point cloud representations.</strong></p> 
<p>为了了解用于描述原始点云的<span style="background-color:#fbd4d0;">point-wise</span>特征，我们使用具有<strong>多尺度分组的PointNet++</strong>[28]作为主干网络。还有其他几种可选的点云网络结构，例如[26,13]或具有稀疏卷积的体素网[43]，它们也可以用作我们的主干网络。</p> 
<p><strong>Foreground point segmentation.</strong></p> 
<p>前景点提供了丰富的信息来预测其关联对象的位置和方向。通过学习对前景点进行分割，使点云网络获取上下文信息，进行准确的点预测，有利于三维框生成。设计了自下而上的三维proposal生成方法，直接从前景点生成三维proposal，即前景分割和三维方proposal生成同时进行。</p> 
<p>给定由主干点云网络编码的point-wise特征，我们附加一个分割头来估计前景掩码和一个框回归头来生成 3D proposal。对于点分割，ground-truth分割掩码自然由3Dground-truth框提供。对于大型户外场景，前景点的数量通常比背景点的数量小得多。因此，我们使用<strong>焦点损失</strong> [19] 来处理类不平衡问题：</p> 
<p style="text-align:center;"><img alt="" height="76" src="https://images2.imgbox.com/fe/f6/hCyj8NtX_o.png" width="433"></p> 
<p>在训练点云分割过程中，我们保持默认设置 αt =0.25 和 γ =2 作为原始论文。</p> 
<p><strong>Bin-based 3D bounding box generation.</strong></p> 
<p>正如我们上面提到的，还附加了一个框回归头，用于同时生成自下而向上的3Dproposal。在训练过程中，我们只需要框回归头从前景点回归三维边界框位置。请注意，尽管框没有从背景点回归，但由于点云网络的接受域，这些点也为生成框提供了支持信息。</p> 
<p>三维边界框在激光雷达坐标系中表示为（x，y，z，h，w，l，θ），其中（x，y，z）是对象中心位置，（h，w，l）是对象大小，θ是对象从鸟瞰角度的方向。<strong>为了约束生成的 3D 框proposal，我们提出了基于 bin 的回归损失来估计对象的 3D 边界框。</strong></p> 
<p style="text-align:center;"><img alt="" height="323" src="https://images2.imgbox.com/cf/3d/MvsBAAoL_o.png" width="487"></p> 
<p></p> 
<p>为了估计对象的中心位置，如图 <span style="background-color:#fe2c24;">3</span> 所示，我们将每个前景点的周围区域沿 X 轴和 Z 轴拆分为一系列离散的 bin。具体来说，我们为当前前景点的每个 X 轴和 Z 轴设置了一个搜索范围 S，每个 1D 搜索范围被划分为长度一致的 δ 的 bin 来表示 X-Z 平面上的不同对象中心（x，z）。我们观察到，使用基于 bin 的分类和 X 轴和 Z 轴的交叉熵损失，而不是使用平滑 L1 损失的直接回归，可以实现更准确和鲁棒的中心定位。</p> 
<p>X 或 Z 轴的定位损失由两项组成，一项用于沿每个 X 和 Z 轴的 bin 分类，另一项用于分类 bin 内的residual回归。对于沿垂直 Y 轴的中心位置 y，我们直接利用平滑 L1 损失进行回归，因为大多数对象的 y 值都在非常小的范围内。 使用 L1 损失足以获得准确的 y 值。</p> 
<p>因此，定位目标可以表述为</p> 
<p style="text-align:center;"><img alt="" height="160" src="https://images2.imgbox.com/7d/72/xxiQi7Vw_o.png" width="502"></p> 
<p> 其中 (x(p),y(p),z(p)) 是前景兴趣点的坐标，(xp,yp,zp) 是其对应对象的中心坐标，bin(p) x 和 bin  (p) z 是沿 X 和 Z 轴的 ground-truth bin 分配，res(p) x 和 res(p) z 是用于在指定 bin 内进行进一步位置细化的 ground-truth 残差，C 是用于归一化的 bin 长度 。</p> 
<p>方向 θ 和大小 (h,w,l) 估计的目标与 [25] 中的目标相似。 我们将方向 2π 划分为 n 个 bin，并按照与 x 或 z 预测相同的方式计算 bin 分类目标 bin(p) θ 和残差回归目标 res(p) θ。 对象大小 (h,w, l) 通过计算残差 (res(p) h , res(p) w , res(p) l ) w.r.t 直接回归。 整个训练集中每个类的平均对象大小。</p> 
<p>在推理阶段，对于基于bin的预测参数x，z，θ，我们首先选择预测置信度最高的bin中心，并添加预测残差，得到精化参数。 对于其他直接回归的参数，包括 y、h、w 和 l，我们将预测残差添加到它们的初始值。</p> 
<p>然后可以将具有不同训练损失项的整体 3D 边界框回归损失 Lreg 表示为</p> 
<p style="text-align:center;"><img alt="" height="178" src="https://images2.imgbox.com/8d/9f/Q5HjhAtZ_o.png" width="465"></p> 
<p> 其中<img alt="" height="20" src="https://images2.imgbox.com/11/76/b5tP1rIn_o.png" width="26">是前景点的数量，<img alt="" height="22" src="https://images2.imgbox.com/2d/a9/kaOIPHei_o.png" width="33">和是预测的 bin 分配和前景点 p 的残差，<img alt="" height="21" src="https://images2.imgbox.com/a4/2a/rIva0wSe_o.png" width="100"> 是ground-truth 目标，<span style="background-color:#d4e9d5;">Fcls </span>表示交叉熵分类损失，<span style="background-color:#d4e9d5;">Freg </span>表示平滑 L1 损失。</p> 
<p>为了去除多余的proposal，我们从鸟瞰的角度进行基于有向 IoU 的非极大值抑制 (NMS)，以生成少量高质量的proposal。对于训练，我们使用 0.85 作为鸟瞰 IoU 阈值，在 NMS 之后，我们保留前 300 个proposal用于训练阶段 2 子网络。对于推理，我们使用 IoU 阈值为 0.8 的有向 NMS，并且只保留前 100 个proposal用于细化阶段 2 子网络。</p> 
<h3>3.2. Point cloud region pooling</h3> 
<p>在获得 3D 边界框proposal后，我们的目标是根据先前生成的框proposal来细化框的位置和方向。为了了解每个proposal的更具体的局部特征，我们建议根据每个 3D proposal的位置从阶段 1 中池化3D 点及其对应的点特征。</p> 
<p>对于每一个3D 框proposal，<img alt="" height="24" src="https://images2.imgbox.com/92/97/U1F4mUvq_o.png" width="213"><img alt="" height="22" src="https://images2.imgbox.com/cf/0c/eN37Re6X_o.png" width="47">我们稍微放大它以创建一个新的3D 框<img alt="" height="17" src="https://images2.imgbox.com/32/a3/F9oxTaMm_o.png" width="150"><img alt="" height="17" src="https://images2.imgbox.com/e2/c6/DmIDGJaU_o.png" width="112">从上下文中编码附加信息，其中 η 是用于扩大框大小的常数值。</p> 
<p>对于每个点<img alt="" height="18" src="https://images2.imgbox.com/b0/11/xQSBQqoC_o.png" width="151">进行内部/外部测试以确定点 p 是否在放大的边界框提议<img alt="" height="22" src="https://images2.imgbox.com/c3/c6/aLKLLQvA_o.png" width="21">内。如果是这样，则将保留该点及其特征以改进框<span style="background-color:#d4e9d5;"> bi</span>。与内部点 p 相关的特征包括其 3D 点坐标 <img alt="" height="20" src="https://images2.imgbox.com/a9/2f/D4U6cmkW_o.png" width="145">，其激光反射强度<img alt="" height="18" src="https://images2.imgbox.com/c3/0d/9lXSbmCe_o.png" width="64">其预测分割掩码<img alt="" height="19" src="https://images2.imgbox.com/2c/bd/0ksOAa3L_o.png" width="37">∈{0,1}从阶段1，和C-dimensional学到点特性表示<img alt="" height="19" src="https://images2.imgbox.com/11/1c/ja18Yeua_o.png" width="69">从阶段1。</p> 
<p>我们包括分割掩码 <img alt="" height="18" src="https://images2.imgbox.com/cd/a5/R3tAW05m_o.png" width="36"> 以区分放大框内的预测前景/背景点<img alt="" height="20" src="https://images2.imgbox.com/ca/0d/p4pviRRf_o.png" width="19"> 。学习到的点特征<img alt="" height="20" src="https://images2.imgbox.com/f6/73/J8UVLudE_o.png" width="28">通过学习分割和proposal生成来编码有价值的信息，因此也包括在内。我们将在下一阶段删除没有内部点的proposal。</p> 
<h3> 3.3. Canonical 3D bounding box reﬁnement</h3> 
<p> 如图 <span style="background-color:#fe2c24;">2 (b)</span> 所示，每个proposal的池化点及其相关特征（参见第 3.2 节）被输入到我们的第 2 阶段子网络，以细化 3D 框位置以及前景对象置信度。</p> 
<p><strong>Canonical transformation.</strong></p> 
<p> 为了利从第一阶段的高召回率框proposal并仅估计proposal框参数的残差，我们将属于每个proposal的池化点转换为相应 3D proposal的规范坐标系。如图<span style="background-color:#fe2c24;"> 4</span> 所示，一个 3D proposal 的规范坐标系表示：（1）原点位于 box proposal 的中心；（2）局部X' 轴和Z' 轴近似平行于地平面，X' 指向proposal的头部方向，另一个Z' 轴垂直于X' ；（3） Y轴与激光雷达坐标系保持相同。box proposal 的所有池化点的坐标 p 应通过适当的旋转和平移转换为规范坐标系为 ~p。使用提出的规范坐标系使框细化阶段能够为每个proposal学习更好的局部空间特征。</p> 
<p style="text-align:center;"><img alt="" height="321" src="https://images2.imgbox.com/4e/3f/6xbt7Fke_o.png" width="496"></p> 
<p><strong> Feature learning for box proposal reﬁnement.</strong></p> 
<p> 正如我们在第3.2节中提到的，细化子网络结合了转换后的局部空间点(特征)p及其来自阶段1的全局语义特征<span style="background-color:#d4e9d5;">f(p)</span>，以进行进一步的框和置信度细化。虽然规范变换能够实现鲁棒的局部空间特征学习，但它不可避免地会丢失每个对象的深度信息。例如，由于激光雷达传感器的固定角度扫描分辨率，远处物体的点通常比附近物体少得多。为了补偿损失的深度信息，我们将到传感器的距离d  (p) =  <img alt="" height="21" src="https://images2.imgbox.com/1f/0b/ga51iZGG_o.png" width="197">包括到点p的特征中。</p> 
<p>对于每个proposal，其关联点的局部空间特征和额外特征<img alt="" height="20" src="https://images2.imgbox.com/2b/7e/tnvjCIpI_o.png" width="118">首先被连接并输入到几个完全连接的层，以将它们的局部特征编码到全局特征<span style="background-color:#d4e9d5;">f (p) </span>的相同维度。然后将局部特征和全局特征连接起来，并按照[28]的结构输入到网络中，以获得用于后续置信度分类和框细化的判别特征向量。</p> 
<p><strong>Losses for box proposal reﬁnement.</strong></p> 
<p>我们采用类似的基于 bin 的回归损失来进行proposal细化。 如果它们的 3D IoU 大于 0.55，则将ground-truth框分配给 3D 框proposal用于学习框细化。3D proposal及其相应的 3D ground-truth框都被转换为规范坐标系，这意味着 3D 提议 bi = <img alt="" height="19" src="https://images2.imgbox.com/11/11/sN6EXKkU_o.png" width="169">和 3D ground-truth框<img alt="" height="18" src="https://images2.imgbox.com/6f/f4/cDVa2QXo_o.png" width="38"><img alt="" height="20" src="https://images2.imgbox.com/4f/34/PizeDjpL_o.png" width="160">将被转换为</p> 
<p style="text-align:center;"><img alt="" height="59" src="https://images2.imgbox.com/6c/1e/bqf1fQbT_o.png" width="435"></p> 
<p>第 i 个 box proposal 的中心位置的训练目标，<img alt="" height="18" src="https://images2.imgbox.com/21/b4/EPdFJXif_o.png" width="237">以与等式（2）相同的方式设置。除了我们使用较小的搜索范围 <span style="background-color:#d4e9d5;">S</span> 来细化 3D proposal的位置。我们仍然直接回归大小residual<img alt="" height="18" src="https://images2.imgbox.com/2a/5b/vfd9Bd3l_o.png" width="124">w.r.t.由于合并的稀疏点，训练集中每个类的平均对象大小通常不能提供足够的proposal大小的信息<img alt="" height="20" src="https://images2.imgbox.com/2a/3d/zjpNlJhb_o.png" width="69">。</p> 
<p> 为了细化方位，我们假设ground-truth方位的角度差<img alt="" height="21" src="https://images2.imgbox.com/4f/af/OJQJJFLz_o.png" width="58">在范围 [- π/4，π/4] 内，基于proposal和它们的ground-truth方位框之间的3 D IoU至少为0.55的事实。因此，我们将 π/ 2 划分为具有 bin 大小 ω 的离散 bin，并将基于 bin 的方向目标预测为</p> 
<p style="text-align:center;"><img alt="" height="116" src="https://images2.imgbox.com/f9/eb/FKYJ2del_o.png" width="461"></p> 
<p> 因此，阶段2子网的总损失可表示为</p> 
<p style="text-align:center;"><img alt="" height="147" src="https://images2.imgbox.com/da/18/vzhFHl7k_o.png" width="463"></p> 
<p style="text-align:center;"><img alt="" height="146" src="https://images2.imgbox.com/a9/21/uDUC4KZ9_o.png" width="464"></p> 
<h2> 4. Experiments</h2> 
<p> 在KITTI数据集[7]的具有挑战性的三维目标检测基准上对PointRCNN进行评估。我们首先在4.1节介绍了PointRCNN的实现细节。在第4.2节中，我们与最先进的3D检测方法进行了比较。最后，我们在4.3节中进行了大量的消融实验来分析PointRCNN。</p> 
<h3>4.1. Implementation Details</h3> 
<p><strong>Network Architecture.</strong></p> 
<p>对于训练集中的每个3D点云场景，我们从每个场景中抽取16384个点作为输入。对于点数小于16384的场景，我们随机重复点数以获得16384点。对于第一阶段的子网络，我们遵循[28]的网络结构，其中使用四个具有多尺度分组的集合抽象层，将点分为大小为4096、1024、256、64的组。然后使用四个特征传播层获得每点特征向量，用于分割和proposal生成。</p> 
<p>对于框proposal细化子网络，我们从每个proposal的池化区域中随机抽取 512 个点作为细化子网络的输入。具有单尺度分组的三个集合抽象层 [28]（组大小为 128、32、1）用于生成用于对象置信度分类和proposal位置细化的单个特征向量。</p> 
<p><strong>The training scheme.</strong></p> 
<p>在这里我们报告汽车类别的训练细节，因为它在 KITTI 数据集中拥有大部分样本，并且可以从发布的代码中找到行人和骑自行车的人的超参数。</p> 
<p>对于第一阶段的子网络，3D ground-truth框内的所有点都被视为前景点，其他点被视为背景点。 在训练期间，我们通过在对象的每一侧将 3D ground-truth框放大 0.2m 来忽略对象边界附近的背景点，以实现鲁棒的分割，因为 3D ground-truth框可能有小的变化。对于基于 bin 的提案生成，超参数设置为搜索范围 S =3m，bin 大小 δ =0.5m 和方向 bin 编号 n =12。</p> 
<p>为了训练第 2 阶段的子网络，我们随机增加 3D proposal的小变化以增加proposal的多样性。 对于训练框分类头，如果proposal与 ground-truth 框的最大 3D IoU 高于 0.6，则将其视为正，如果其最大 3D IoU 低于 0.45，则将其视为负。 我们使用 3D IoU 0.55 作为proposal框回归头训练的最小阈值。 对于基于 bin 的proposal细化，搜索范围为 S =1.5m，定位 bin 大小为 δ =0.5m，方向 bin 大小为 ω =10◦。 点云池化的上下文长度为 η =1.0m。</p> 
<p>PointRCNN 的两个阶段子网络是分开训练的。 阶段 1 的子网络训练了 200 个 epoch，batch 大小为 16，学习率为 0.002，而 stage-2 子网络训练了 50 个 epoch，batch 大小为 256，学习率为 0.002。 在训练期间，我们对随机翻转进行数据增强，使用从 [0.95, 1.05] 采样的比例因子进行缩放，并在 [-10, 10] 度之间围绕垂直 Y 轴旋转。 受 [40] 的启发，为了模拟具有各种环境的对象，我们还通过随机选择不重叠的框将几个新的ground-truth框及其内部点从其他场景放置到当前训练场景的相同位置，这种增强表示为 在以下部分中作为 <span style="background-color:#fbd4d0;">GT-AUG</span>。</p> 
<h3>4.2. 3D Object Detection on KITTI</h3> 
<p>KITTI 的 3D 物体检测基准包含 7481 个训练样本和 7518 个测试样本（测试拆分）。 我们按照 [4] 中提到的常用 train/val split 将训练样本分为 train split（3712 个样本）和 val split（3769 个样本）。 我们在 KITTI 数据集的 val split 和 test split 上将 PointRCNN 与最先进的 3D 对象检测方法进行比较。 所有模型都在 train split 上进行训练，并在 test split 和 val split 上进行评估。</p> 
<p><strong>Evaluation of 3D object detection.</strong></p> 
<p>我们在 KITTI 测试服务器的 3D 检测基准上评估我们的方法，结果显示在表<span style="background-color:#fe2c24;">1</span>中。  对于汽车和骑自行车的人的 3D 检测，我们的方法在所有三个难度上都优于以前的最先进方法，并且在所有已发表的作品中均在 KITTI 测试板上排名第一。 尽管大多数先前的方法都使用 RGB 图像和点云作为输入，但我们的方法通过仅使用点云作为输入，以高效的架构实现了更好的性能。 对于行人检测，与以前的仅 LiDAR 方法相比，我们的方法取得了更好或相当的结果。 但是，它的性能比具有多个传感器的方法稍差。 我们认为这是因为我们的方法仅使用稀疏点云作为输入，但行人尺寸较小，并且图像可以比点云捕获更多的行人细节来帮助 3D 检测。</p> 
<p><img alt="" height="319" src="https://images2.imgbox.com/a3/0d/xWPxKFmK_o.png" width="1181"></p> 
<p>对于最重要的汽车类别，我们还报告了val分割的3D检测结果的性能，如表2所示。我们的方法优于以前的最先进的方法，在 val 拆分上具有较大的边距。 特别是在困难难度下，我们的方法比之前最好的 AP 有 8.28% 的 AP 改进，这证明了所提出的 PointRCNN 的有效性。</p> 
<p><strong>Evaluation of 3D proposal generation.</strong></p> 
<p>我们的自下而上proposal生成网络的性能通过计算具有不同proposal数量和 3D IoU 阈值的 3D 边界框的召回率来评估。 如表<span style="background-color:#fe2c24;">3</span>中所示。我们的方法（没有 GT-AUG）比以前的方法实现了显着更高的召回率。 只有 50 个proposal，我们的方法在 IoU 阈值为 0.5 时在中等难度的汽车类中获得了 <strong>96.01%</strong> 的召回率，在相同的proposal数量下，这比 AVOD [14] 的 91% 召回率高出 5.01%，请注意，后一种方法同时使用 2D 图像和点云用于proposal生成，而我们只使用点云作为输入。 当使用 300 个proposal时，我们的方法在 IoU 阈值为 0.5 时进一步实现了<strong> 98.21% </strong>的召回率。 由于我们的方法已经在 IoU 阈值 0.5 处获得了高召回率，因此增加proposal的数量是没有意义的。 相比之下，如表<span style="background-color:#fe2c24;">3</span>中所示我们报告了 IoU 阈值为 0.7 的 3D 边界框的召回率，以供参考。 通过 300 个proposal，我们的方法在 IoU 阈值 0.7 时实现了 <strong>82.29%</strong> 的召回率。 尽管proposal的召回与最终的 3D 对象检测性能松散地相关 [11, 8]，但出色的召回仍然表明我们自下而上的提案生成网络的鲁棒性和准确性。</p> 
<p style="text-align:center;"><img alt="" height="337" src="https://images2.imgbox.com/68/91/eqS9yMOV_o.png" width="502"></p> 
<h3> 4.3. Ablation Study</h3> 
<p>在本节中，我们进行了广泛的消融实验，以分析 PointRCNN 不同组件的有效性。 所有实验都在没有 GT-AUG 的 train split 上进行训练，并在 car class1 的 val split 上进行评估。</p> 
<p><strong>Different inputs for the reﬁnement sub-network.</strong></p> 
<p>如第3.3节，细化子网的输入由每个池化点的规范转换坐标和集合特征组成。我们通过删除一个并保持所有其他部分不变来分析每种类型的特征对细化子网络的影响。所有实验共享相同的固定第一阶段子网络，以便进行公平比较。结果显示在表<span style="background-color:#fe2c24;">4</span>中。</p> 
<p style="text-align:center;"><img alt="" height="289" src="https://images2.imgbox.com/32/cd/1xjvQxsp_o.png" width="539"></p> 
<p> 没有提出的正则变换情况下，细化子网络的性能显著下降，这表明将其转换为正则坐标系可以极大地消除旋转和位置变化，并提高阶段2的特征学习效率。我们还看到，去除从点云分割和提案生成中学习到的第一阶段特征<span style="background-color:#d4e9d5;"> f(p) </span>在中等难度下将 mAP 降低了 2.71%，这证明了在第一阶段学习语义分割的优势。标签<span style="background-color:#fe2c24;">4</span> 还显示了 3D 点 p 的相机深度信息 <span style="background-color:#d4e9d5;">d(p) </span>和分割掩码<span style="background-color:#d4e9d5;"> m(p)</span> 对最终性能的贡献很小，因为相机深度完成了在规范变换期间消除的距离信息，并且分割掩码表示前景池化区域中的点。</p> 
<p><strong>Context-aware point cloud pooling.</strong></p> 
<p>在 3.2 节中，我们引入了将proposal框<span style="background-color:#d4e9d5;"> bi </span>扩大一个边距 <span style="background-color:#d4e9d5;">η </span>以创建 <span style="background-color:#d4e9d5;">be i </span>来为每个proposal的置信度估计和位置回归池化更多的上下文点。 标签。 图 <span style="background-color:#fe2c24;">5 </span>显示了不同池化上下文宽度 <span style="background-color:#d4e9d5;">η </span>的影响。 <span style="background-color:#d4e9d5;"> η</span> =1.0m 导致我们提出的框架中的最佳性能。 我们注意到，当没有池化上下文信息时，准确率，尤其是困难难度下的准确率，会显着下降。 由于对象可能被遮挡或远离传感器，困难的情况通常在proposal中的点较少，这需要更多的上下文信息来进行分类和proposal细化。 如表5中所示。所示，太大的<span style="background-color:#d4e9d5;">η</span>也会导致性能下降，因为当前proposal的池化区域可能包括其他对象的噪声前景点。</p> 
<p style="text-align:center;"><img alt="" height="236" src="https://images2.imgbox.com/51/76/4rYUMW2n_o.png" width="492"></p> 
<p style="text-align:center;"><img alt="" height="376" src="https://images2.imgbox.com/aa/9a/lbgVFTmw_o.png" width="487"></p> 
<p style="text-align:center;"> </p> 
<p><strong>Losses of 3D bounding box regression.</strong></p> 
<p>在第3.1节中，我们建议基于 bin 的位置地化损失来生成3d box 提案。在这一部分中，我们评估了我们的 stage-1子网在使用不同类型的3D 框回归损耗时的性能，包括基于剩余损耗(RB-loss)[43]、基于剩余 cos-based 损耗(RCB-loss)、基于角点损耗(CN-loss)[4,14]、基于部分bin损耗(PBB-loss)[25]和full bin-based loss (BB-loss)。为了消除角度回归的模糊性，将剩余损失的<img alt="" height="19" src="https://images2.imgbox.com/a5/d6/SXcTggvQ_o.png" width="31">编码<img alt="" height="21" src="https://images2.imgbox.com/5a/dd/sHsWxaA9_o.png" width="120">。</p> 
<p> 最终召回（IoU阈值0.5和0.7）和第1阶段的100个proposal用作评估指标，如图 <span style="background-color:#fe2c24;">5 </span>所示。该图显示了我们基于完整 bin 的 3D 边界框回归损失的有效性。具体来说，具有我们基于 bin 的完整损失函数的第一阶段子网络比所有其他损失函数实现了更高的召回率和收敛速度，这得益于使用先验知识约束目标，尤其是定位。基于部分 bin 的损失实现了类似的召回，但收敛速度比我们的慢得多。 完全和部分基于 bin 的损失比其他损失函数具有显着更高的召回率，尤其是在 IoU 阈值 0.7 时。 通过改进角度回归目标，改进的基于残差的损失也比基于残差的损失获得更好的召回率。通过改进角度回归目标，改进的residual-cos-based loss也比residual-based loss获得更好的召回率。</p> 
<h2> 5. Conclusion</h2> 
<p>我们提出了 PointRCNN，一种新颖的 3D 对象检测器，用于从原始点云中检测 3D 对象。所提出的 stage-1 网络以自下而上的方式直接从点云生成 3D proposal，与以前的proposal生成方法相比，它实现了更高的召回率。第二阶段网络通过结合语义特征和局部空间特征来细化规范坐标中的proposal。此外，新提出的bin-based 的损失已经证明了其对 3D 边界框回归的效率和有效性。实验表明，PointRCNN 在具有挑战性的 KITTI 数据集 3D 检测基准上优于以前的最先进方法，具有显着的优势。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6c53e0cbe958e8c9a93d668ace65a3a2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">没有webpack.config.js如何配置less吗 安装配置less必看</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2ffddad3e4b2277f52ab57fc6db76c7a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用泰勒展开解释梯度下降方法参数更新过程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>