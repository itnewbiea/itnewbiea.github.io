<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Transform环境搭建与代码调试——Attention Is All Y ou Need - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Transform环境搭建与代码调试——Attention Is All Y ou Need" />
<meta property="og:description" content="1、源代码 2、环境搭建 conda create -n transform python=3.8 -y conda activate transform cd /media/lhy/Transforms/annotatedtransformer pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple # # Uncomment for colab pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil -i https://pypi.tuna.tsinghua.edu.cn/simple python -m spacy download de_core_news_sm python -m spacy download en_core_web_sm #或者离线下载 pip install de_core_news_sm-3.2.0-py3-none-any.whl pip install en_core_web_sm-3.2.0-py3-none-any.whl 3、构建Teamsform模型（Model Architecture） 1、编码器、解码器以及预测部分 class EncoderDecoder(nn.Module): &#34;&#34;&#34; A standard Encoder-Decoder architecture. Base for this and many other models. &#34;&#34;&#34; def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/4d8a62f3efa02e448cfd3718fa6bcd3c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-28T11:18:52+08:00" />
<meta property="article:modified_time" content="2023-12-28T11:18:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Transform环境搭建与代码调试——Attention Is All Y ou Need</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="1httpsnlpseasharvardeduannotatedtransformer_0"></a>1、<a href="https://nlp.seas.harvard.edu/annotated-transformer/" rel="nofollow">源代码</a></h3> 
<h3><a id="2_1"></a>2、环境搭建</h3> 
<pre><code class="prism language-bash">conda create <span class="token parameter variable">-n</span> transform <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span> <span class="token parameter variable">-y</span>
conda activate transform
<span class="token builtin class-name">cd</span> /media/lhy/Transforms/annotatedtransformer

pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt <span class="token parameter variable">-i</span> https://pypi.tuna.tsinghua.edu.cn/simple
<span class="token comment"># # Uncomment for colab</span>

pip <span class="token function">install</span> <span class="token parameter variable">-q</span> <span class="token assign-left variable">torchdata</span><span class="token operator">==</span><span class="token number">0.3</span>.0 <span class="token assign-left variable">torchtext</span><span class="token operator">==</span><span class="token number">0.12</span> <span class="token assign-left variable">spacy</span><span class="token operator">==</span><span class="token number">3.2</span> altair GPUtil  <span class="token parameter variable">-i</span> https://pypi.tuna.tsinghua.edu.cn/simple
python <span class="token parameter variable">-m</span> spacy download de_core_news_sm
python <span class="token parameter variable">-m</span> spacy download en_core_web_sm
<span class="token comment">#或者离线下载</span>
pip <span class="token function">install</span> de_core_news_sm-3.2.0-py3-none-any.whl
pip <span class="token function">install</span> en_core_web_sm-3.2.0-py3-none-any.whl
</code></pre> 
<h3><a id="3TeamsformModel_Architecture_18"></a>3、构建Teamsform模型（Model Architecture）</h3> 
<h4><a id="1_19"></a>1、编码器、解码器以及预测部分</h4> 
<pre><code class="prism language-bash">class EncoderDecoder<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">""</span>"
    A standard Encoder-Decoder architecture. Base <span class="token keyword">for</span> this and many
    other models.
    <span class="token string">""</span>"

    def __init__<span class="token punctuation">(</span>self, encoder, decoder, src_embed, tgt_embed, generator<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>EncoderDecoder, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.encoder <span class="token operator">=</span> encoder
        self.decoder <span class="token operator">=</span> decoder
        self.src_embed <span class="token operator">=</span> src_embed
        self.tgt_embed <span class="token operator">=</span> tgt_embed
        self.generator <span class="token operator">=</span> generator

    def forward<span class="token punctuation">(</span>self, src, tgt, src_mask, tgt_mask<span class="token punctuation">)</span>:
        <span class="token string">"Take in and process masked src and target sequences."</span>
        <span class="token builtin class-name">return</span> self.decode<span class="token punctuation">(</span>self.encode<span class="token punctuation">(</span>src, src_mask<span class="token punctuation">)</span>, src_mask, tgt, tgt_mask<span class="token punctuation">)</span>

    def encode<span class="token punctuation">(</span>self, src, src_mask<span class="token punctuation">)</span>:
        <span class="token builtin class-name">return</span> self.encoder<span class="token punctuation">(</span>self.src_embed<span class="token punctuation">(</span>src<span class="token punctuation">)</span>, src_mask<span class="token punctuation">)</span>

    def decode<span class="token punctuation">(</span>self, memory, src_mask, tgt, tgt_mask<span class="token punctuation">)</span>:
        <span class="token builtin class-name">return</span> self.decoder<span class="token punctuation">(</span>self.tgt_embed<span class="token punctuation">(</span>tgt<span class="token punctuation">)</span>, memory, src_mask, tgt_mask<span class="token punctuation">)</span>

class Generator<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Define standard linear + softmax generation step."</span>

    def __init__<span class="token punctuation">(</span>self, d_model, vocab<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>Generator, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.proj <span class="token operator">=</span> nn.Linear<span class="token punctuation">(</span>d_model, vocab<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, x<span class="token punctuation">)</span>:
        <span class="token builtin class-name">return</span> log_softmax<span class="token punctuation">(</span>self.proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>, <span class="token assign-left variable">dim</span><span class="token operator">=</span>-1<span class="token punctuation">)</span>
</code></pre> 
<p>Transformer遵循这个整体架构，使用堆叠的自关注层和点方向层，完全连接编码器和解码器层，分别如图1的左半部分和右半部分所示。<br> <img src="https://images2.imgbox.com/65/f4/mDJlJYFs_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2Encoder_and_Decoder_Stacks_59"></a>2、Encoder and Decoder Stacks</h4> 
<pre><code class="prism language-bash">def clones<span class="token punctuation">(</span>module, N<span class="token punctuation">)</span>:
    <span class="token string">"Produce N identical layers."</span>
    <span class="token builtin class-name">return</span> nn.ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>copy.deepcopy<span class="token punctuation">(</span>module<span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token for-or-select variable">_</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
class Encoder<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Core encoder is a stack of N layers"</span>

    def __init__<span class="token punctuation">(</span>self, layer, N<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>Encoder, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.layers <span class="token operator">=</span> clones<span class="token punctuation">(</span>layer, N<span class="token punctuation">)</span>
        self.norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>layer.size<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, x, mask<span class="token punctuation">)</span>:
        <span class="token string">"Pass the input (and mask) through each layer in turn."</span>
        <span class="token keyword">for</span> <span class="token for-or-select variable">layer</span> <span class="token keyword">in</span> self.layers:
            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x, mask<span class="token punctuation">)</span>
        <span class="token builtin class-name">return</span> self.norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

class LayerNorm<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Construct a layernorm module (See citation for details)."</span>

    def __init__<span class="token punctuation">(</span>self, features, <span class="token assign-left variable">eps</span><span class="token operator">=</span>1e-6<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>LayerNorm, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.a_2 <span class="token operator">=</span> nn.Parameter<span class="token punctuation">(</span>torch.ones<span class="token punctuation">(</span>features<span class="token punctuation">))</span>
        self.b_2 <span class="token operator">=</span> nn.Parameter<span class="token punctuation">(</span>torch.zeros<span class="token punctuation">(</span>features<span class="token punctuation">))</span>
        self.eps <span class="token operator">=</span> eps

    def forward<span class="token punctuation">(</span>self, x<span class="token punctuation">)</span>:
        mean <span class="token operator">=</span> x.mean<span class="token punctuation">(</span>-1, <span class="token assign-left variable">keepdim</span><span class="token operator">=</span>True<span class="token punctuation">)</span>
        std <span class="token operator">=</span> x.std<span class="token punctuation">(</span>-1, <span class="token assign-left variable">keepdim</span><span class="token operator">=</span>True<span class="token punctuation">)</span>
        <span class="token builtin class-name">return</span> self.a_2 * <span class="token punctuation">(</span>x - mean<span class="token punctuation">)</span> / <span class="token punctuation">(</span>std + self.eps<span class="token punctuation">)</span> + self.b_2

class SublayerConnection<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">""</span>"
    A residual connection followed by a layer norm.
    Note <span class="token keyword">for</span> code simplicity the norm is first as opposed to last.
    <span class="token string">""</span>"

    def __init__<span class="token punctuation">(</span>self, size, dropout<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>SublayerConnection, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>size<span class="token punctuation">)</span>
        self.dropout <span class="token operator">=</span> nn.Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, x, sublayer<span class="token punctuation">)</span>:
        <span class="token string">"Apply residual connection to any sublayer with the same size."</span>
        <span class="token builtin class-name">return</span> x + self.dropout<span class="token punctuation">(</span>sublayer<span class="token punctuation">(</span>self.norm<span class="token punctuation">(</span>x<span class="token punctuation">))</span><span class="token punctuation">)</span>

class EncoderLayer<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Encoder is made up of self-attn and feed forward (defined below)"</span>

    def __init__<span class="token punctuation">(</span>self, size, self_attn, feed_forward, dropout<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>EncoderLayer, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.self_attn <span class="token operator">=</span> self_attn
        self.feed_forward <span class="token operator">=</span> feed_forward
        self.sublayer <span class="token operator">=</span> clones<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">(</span>size, dropout<span class="token punctuation">)</span>, <span class="token number">2</span><span class="token punctuation">)</span>
        self.size <span class="token operator">=</span> size

    def forward<span class="token punctuation">(</span>self, x, mask<span class="token punctuation">)</span>:
        <span class="token string">"Follow Figure 1 (left) for connections."</span>
        x <span class="token operator">=</span> self.sublayer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x, lambda x: self.self_attn<span class="token punctuation">(</span>x, x, x, mask<span class="token punctuation">))</span>
        <span class="token builtin class-name">return</span> self.sublayer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x, self.feed_forward<span class="token punctuation">)</span>

class Decoder<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Generic N layer decoder with masking."</span>

    def __init__<span class="token punctuation">(</span>self, layer, N<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>Decoder, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.layers <span class="token operator">=</span> clones<span class="token punctuation">(</span>layer, N<span class="token punctuation">)</span>
        self.norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>layer.size<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, x, memory, src_mask, tgt_mask<span class="token punctuation">)</span>:
        <span class="token keyword">for</span> <span class="token for-or-select variable">layer</span> <span class="token keyword">in</span> self.layers:
            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x, memory, src_mask, tgt_mask<span class="token punctuation">)</span>
        <span class="token builtin class-name">return</span> self.norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

class DecoderLayer<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span>

    def __init__<span class="token punctuation">(</span>self, size, self_attn, src_attn, feed_forward, dropout<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>DecoderLayer, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.size <span class="token operator">=</span> size
        self.self_attn <span class="token operator">=</span> self_attn
        self.src_attn <span class="token operator">=</span> src_attn
        self.feed_forward <span class="token operator">=</span> feed_forward
        self.sublayer <span class="token operator">=</span> clones<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">(</span>size, dropout<span class="token punctuation">)</span>, <span class="token number">3</span><span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, x, memory, src_mask, tgt_mask<span class="token punctuation">)</span>:
        <span class="token string">"Follow Figure 1 (right) for connections."</span>
        m <span class="token operator">=</span> memory
        x <span class="token operator">=</span> self.sublayer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x, lambda x: self.self_attn<span class="token punctuation">(</span>x, x, x, tgt_mask<span class="token punctuation">))</span>
        x <span class="token operator">=</span> self.sublayer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x, lambda x: self.src_attn<span class="token punctuation">(</span>x, m, m, src_mask<span class="token punctuation">))</span>
        <span class="token builtin class-name">return</span> self.sublayer<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x, self.feed_forward<span class="token punctuation">)</span>

</code></pre> 
<pre><code class="prism language-bash">def subsequent_mask<span class="token punctuation">(</span>size<span class="token punctuation">)</span>:
    <span class="token string">"Mask out subsequent positions."</span>
    attn_shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span>, size, size<span class="token punctuation">)</span><span class="token comment">#batch=1</span>
    subsequent_mask <span class="token operator">=</span> torch.triu<span class="token punctuation">(</span>torch.ones<span class="token punctuation">(</span>attn_shape<span class="token punctuation">)</span>, <span class="token assign-left variable">diagonal</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>.type<span class="token punctuation">(</span>
        torch.uint8
    <span class="token punctuation">)</span><span class="token comment">#保留主对角线以上的数据</span>
    <span class="token builtin class-name">return</span> subsequent_mask <span class="token operator">==</span> <span class="token number">0</span>
</code></pre> 
<p>结果保留主对角线及以下的数据<br> <img src="https://images2.imgbox.com/b1/d7/WnDPWP9c_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/6e/1c/s0C3rw3c_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="attention_171"></a>attention</h4> 
<pre><code class="prism language-bash">def attention<span class="token punctuation">(</span>query, key, value, <span class="token assign-left variable">mask</span><span class="token operator">=</span>None, <span class="token assign-left variable">dropout</span><span class="token operator">=</span>None<span class="token punctuation">)</span>:
    <span class="token string">"Compute 'Scaled Dot Product Attention'"</span>
    d_k <span class="token operator">=</span> query.size<span class="token punctuation">(</span>-1<span class="token punctuation">)</span>
    scores <span class="token operator">=</span> torch.matmul<span class="token punctuation">(</span>query, key.transpose<span class="token punctuation">(</span>-2, -1<span class="token punctuation">))</span> / math.sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span>
    <span class="token keyword">if</span> mask is not None:
        scores <span class="token operator">=</span> scores.masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span>, -1e9<span class="token punctuation">)</span>
    p_attn <span class="token operator">=</span> scores.softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span>-1<span class="token punctuation">)</span>
    <span class="token keyword">if</span> dropout is not None:
        p_attn <span class="token operator">=</span> dropout<span class="token punctuation">(</span>p_attn<span class="token punctuation">)</span>
    <span class="token builtin class-name">return</span> torch.matmul<span class="token punctuation">(</span>p_attn, value<span class="token punctuation">)</span>, p_attn
</code></pre> 
<pre><code class="prism language-bash">class MultiHeadedAttention<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    def __init__<span class="token punctuation">(</span>self, h, d_model, <span class="token assign-left variable">dropout</span><span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>:
        <span class="token string">"Take in model size and number of heads."</span>
        super<span class="token punctuation">(</span>MultiHeadedAttention, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        assert d_model % h <span class="token operator">==</span> <span class="token number">0</span>
        <span class="token comment"># We assume d_v always equals d_k</span>
        self.d_k <span class="token operator">=</span> d_model // h
        self.h <span class="token operator">=</span> h
        self.linears <span class="token operator">=</span> clones<span class="token punctuation">(</span>nn.Linear<span class="token punctuation">(</span>d_model, d_model<span class="token punctuation">)</span>, <span class="token number">4</span><span class="token punctuation">)</span>
        self.attn <span class="token operator">=</span> None
        self.dropout <span class="token operator">=</span> nn.Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, query, key, value, <span class="token assign-left variable">mask</span><span class="token operator">=</span>None<span class="token punctuation">)</span>:
        <span class="token string">"Implements Figure 2"</span>
        <span class="token keyword">if</span> mask is not None:
            <span class="token comment"># Same mask applied to all h heads.</span>
            mask <span class="token operator">=</span> mask.unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        nbatches <span class="token operator">=</span> query.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span>
        query, key, value <span class="token operator">=</span> <span class="token punctuation">[</span>
            lin<span class="token punctuation">(</span>x<span class="token punctuation">)</span>.view<span class="token punctuation">(</span>nbatches, -1, self.h, self.d_k<span class="token punctuation">)</span>.transpose<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> lin, x <span class="token keyword">in</span> zip<span class="token punctuation">(</span>self.linears, <span class="token punctuation">(</span>query, key, value<span class="token punctuation">))</span>
        <span class="token punctuation">]</span>

        <span class="token comment"># 2) Apply attention on all the projected vectors in batch.</span>
        x, self.attn <span class="token operator">=</span> attention<span class="token punctuation">(</span>
            query, key, value, <span class="token assign-left variable">mask</span><span class="token operator">=</span>mask, <span class="token assign-left variable">dropout</span><span class="token operator">=</span>self.dropout
        <span class="token punctuation">)</span>

        <span class="token comment"># 3) "Concat" using a view and apply a final linear.</span>
        x <span class="token operator">=</span> <span class="token punctuation">(</span>
            x.transpose<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span><span class="token punctuation">)</span>
            .contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            .view<span class="token punctuation">(</span>nbatches, -1, self.h * self.d_k<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        del query
        del key
        del value
        <span class="token builtin class-name">return</span> self.linears<span class="token punctuation">[</span>-1<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="FFN_229"></a>FFN</h4> 
<pre><code class="prism language-bash">class PositionwiseFeedForward<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Implements FFN equation."</span>

    def __init__<span class="token punctuation">(</span>self, d_model, d_ff, <span class="token assign-left variable">dropout</span><span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>PositionwiseFeedForward, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.w_1 <span class="token operator">=</span> nn.Linear<span class="token punctuation">(</span>d_model, d_ff<span class="token punctuation">)</span>
        self.w_2 <span class="token operator">=</span> nn.Linear<span class="token punctuation">(</span>d_ff, d_model<span class="token punctuation">)</span>
        self.dropout <span class="token operator">=</span> nn.Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, x<span class="token punctuation">)</span>:
        <span class="token builtin class-name">return</span> self.w_2<span class="token punctuation">(</span>self.dropout<span class="token punctuation">(</span>self.w_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>.relu<span class="token punctuation">(</span><span class="token punctuation">))</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Embeddings_and_Softmax_245"></a>Embeddings and Softmax</h4> 
<pre><code class="prism language-bash">class Embeddings<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    def __init__<span class="token punctuation">(</span>self, d_model, vocab<span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>Embeddings, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.lut <span class="token operator">=</span> nn.Embedding<span class="token punctuation">(</span>vocab, d_model<span class="token punctuation">)</span>
        self.d_model <span class="token operator">=</span> d_model

    def forward<span class="token punctuation">(</span>self, x<span class="token punctuation">)</span>:
        <span class="token builtin class-name">return</span> self.lut<span class="token punctuation">(</span>x<span class="token punctuation">)</span> * math.sqrt<span class="token punctuation">(</span>self.d_model<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Positional_Encoding_258"></a>Positional Encoding</h4> 
<pre><code class="prism language-bash">class PositionalEncoding<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Implement the PE function."</span>

    def __init__<span class="token punctuation">(</span>self, d_model, dropout, <span class="token assign-left variable">max_len</span><span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>PositionalEncoding, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.dropout <span class="token operator">=</span> nn.Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>

        <span class="token comment"># Compute the positional encodings once in log space.</span>
        pe <span class="token operator">=</span> torch.zeros<span class="token punctuation">(</span>max_len, d_model<span class="token punctuation">)</span>
        position <span class="token operator">=</span> torch.arange<span class="token punctuation">(</span><span class="token number">0</span>, max_len<span class="token punctuation">)</span>.unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        div_term <span class="token operator">=</span> torch.exp<span class="token punctuation">(</span>
            torch.arange<span class="token punctuation">(</span><span class="token number">0</span>, d_model, <span class="token number">2</span><span class="token punctuation">)</span> * -<span class="token punctuation">(</span>math.log<span class="token punctuation">(</span><span class="token number">10000.0</span><span class="token punctuation">)</span> / d_model<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        pe<span class="token punctuation">[</span>:, <span class="token number">0</span>::2<span class="token punctuation">]</span> <span class="token operator">=</span> torch.sin<span class="token punctuation">(</span>position * div_term<span class="token punctuation">)</span>
        pe<span class="token punctuation">[</span>:, <span class="token number">1</span>::2<span class="token punctuation">]</span> <span class="token operator">=</span> torch.cos<span class="token punctuation">(</span>position * div_term<span class="token punctuation">)</span>
        pe <span class="token operator">=</span> pe.unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        self.register_buffer<span class="token punctuation">(</span><span class="token string">"pe"</span>, pe<span class="token punctuation">)</span>

    def forward<span class="token punctuation">(</span>self, x<span class="token punctuation">)</span>:
        x <span class="token operator">=</span> x + self.pe<span class="token punctuation">[</span>:, <span class="token builtin class-name">:</span> x.size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>.requires_grad_<span class="token punctuation">(</span>False<span class="token punctuation">)</span>
        <span class="token builtin class-name">return</span> self.dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/f0/aa/mKGPbm55_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Optimizer_284"></a>Optimizer</h4> 
<p><img src="https://images2.imgbox.com/d4/e0/hkbGUgW0_o.png" alt="在这里插入图片描述"><br> 这对应于在第一个warmup_steps训练步骤中线性增加学习率，然后按步数的倒数平方根成比例地降低学习率。我们使用了warmup_steps = 4000。</p> 
<pre><code class="prism language-bash">def rate<span class="token punctuation">(</span>step, model_size, factor, warmup<span class="token punctuation">)</span>:
    <span class="token string">""</span>"
    we have to default the step to <span class="token number">1</span> <span class="token keyword">for</span> LambdaLR <span class="token keyword">function</span>
    to avoid zero raising to negative power.
    <span class="token string">""</span>"
    <span class="token keyword">if</span> step <span class="token operator">==</span> <span class="token number">0</span>:
        step <span class="token operator">=</span> <span class="token number">1</span>
    <span class="token builtin class-name">return</span> factor * <span class="token punctuation">(</span>
        model_size ** <span class="token punctuation">(</span>-0.5<span class="token punctuation">)</span> * min<span class="token punctuation">(</span>step ** <span class="token punctuation">(</span>-0.5<span class="token punctuation">)</span>, step * warmup ** <span class="token punctuation">(</span>-1.5<span class="token punctuation">))</span>
    <span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment">#---------------------------------4、测试学习率---------------------------------  </span>
def example_learning_schedule<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    opts <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token punctuation">[</span><span class="token number">512</span>, <span class="token number">1</span>, <span class="token number">4000</span><span class="token punctuation">]</span>,  <span class="token comment"># example 1</span>
        <span class="token punctuation">[</span><span class="token number">512</span>, <span class="token number">1</span>, <span class="token number">8000</span><span class="token punctuation">]</span>,  <span class="token comment"># example 2</span>
        <span class="token punctuation">[</span><span class="token number">256</span>, <span class="token number">1</span>, <span class="token number">4000</span><span class="token punctuation">]</span>,  <span class="token comment"># example 3</span>
    <span class="token punctuation">]</span>

    dummy_model <span class="token operator">=</span> torch.nn.Linear<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">1</span><span class="token punctuation">)</span>
    learning_rates <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token comment"># we have 3 examples in opts list.</span>
    <span class="token keyword">for</span> idx, example <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>opts<span class="token punctuation">)</span>:
        <span class="token comment"># run 20000 epoch for each example</span>
        optimizer <span class="token operator">=</span> torch.optim.Adam<span class="token punctuation">(</span>
            dummy_model.parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token assign-left variable">lr</span><span class="token operator">=</span><span class="token number">1</span>, <span class="token assign-left variable">betas</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span>, <span class="token number">0.98</span><span class="token punctuation">)</span>, <span class="token assign-left variable">eps</span><span class="token operator">=</span>1e-9
        <span class="token punctuation">)</span>
        print<span class="token punctuation">(</span>optimizer.state_dict<span class="token punctuation">(</span><span class="token punctuation">))</span>


        lr_scheduler <span class="token operator">=</span> LambdaLR<span class="token punctuation">(</span>
            <span class="token assign-left variable">optimizer</span><span class="token operator">=</span>optimizer, <span class="token assign-left variable">lr_lambda</span><span class="token operator">=</span>lambda step: rate<span class="token punctuation">(</span>step, *example<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        tmp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># take 20K dummy training steps, save the learning rate at each step</span>
        <span class="token keyword">for</span> <span class="token for-or-select variable">step</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">20000</span><span class="token punctuation">)</span>:
            tmp.append<span class="token punctuation">(</span>optimizer.param_groups<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"lr"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            lr_scheduler.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        learning_rates.append<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>

    learning_rates <span class="token operator">=</span> torch.tensor<span class="token punctuation">(</span>learning_rates<span class="token punctuation">)</span>

    <span class="token comment"># Enable altair to handle more than 5000 rows</span>
    alt.data_transformers.disable_max_rows<span class="token punctuation">(</span><span class="token punctuation">)</span>

    opts_data <span class="token operator">=</span> pd.concat<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>
            pd.DataFrame<span class="token punctuation">(</span>
                <span class="token punctuation">{<!-- --></span>
                    <span class="token string">"Learning Rate"</span><span class="token builtin class-name">:</span> learning_rates<span class="token punctuation">[</span>warmup_idx, :<span class="token punctuation">]</span>,
                    <span class="token string">"model_size:warmup"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token string">"512:4000"</span>, <span class="token string">"512:8000"</span>, <span class="token string">"256:4000"</span><span class="token punctuation">]</span><span class="token punctuation">[</span>
                        warmup_idx
                    <span class="token punctuation">]</span>,
                    <span class="token string">"step"</span><span class="token builtin class-name">:</span> range<span class="token punctuation">(</span><span class="token number">20000</span><span class="token punctuation">)</span>,
                <span class="token punctuation">}</span>
            <span class="token punctuation">)</span>
            <span class="token keyword">for</span> <span class="token for-or-select variable">warmup_idx</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">1</span>, <span class="token number">2</span><span class="token punctuation">]</span>
        <span class="token punctuation">]</span>
    <span class="token punctuation">)</span>

    <span class="token assign-left variable">chart</span><span class="token operator">=</span><span class="token punctuation">(</span>alt.Chart<span class="token punctuation">(</span>opts_data<span class="token punctuation">)</span>
           .mark_line<span class="token punctuation">(</span><span class="token punctuation">)</span>
           .properties<span class="token punctuation">(</span>width<span class="token operator">=</span><span class="token number">600</span><span class="token punctuation">)</span>
           .encode<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token string">"step"</span>, <span class="token assign-left variable">y</span><span class="token operator">=</span><span class="token string">"Learning Rate"</span>, <span class="token assign-left variable">color</span><span class="token operator">=</span><span class="token string">"model_size:warmup:N"</span><span class="token punctuation">)</span>
           .interactive<span class="token punctuation">(</span><span class="token punctuation">))</span>
    
    <span class="token comment"># 展示数据，调用display()方法</span>
    altair_viewer.show<span class="token punctuation">(</span>chart<span class="token punctuation">)</span>

</code></pre> 
<p><img src="https://images2.imgbox.com/74/3b/p5V5Y5vZ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Regularization_366"></a>Regularization</h4> 
<p>在训练过程中，我们使用值es =0.1的平滑标签。这损害了困惑，因为模型学的更加不确定，但提高了准确性和BLeU分数。<br> Kullback-Leibler散度损失。</p> 
<pre><code class="prism language-bash">class LabelSmoothing<span class="token punctuation">(</span>nn.Module<span class="token punctuation">)</span>:
    <span class="token string">"Implement label smoothing."</span>

    def __init__<span class="token punctuation">(</span>self, size, padding_idx, <span class="token assign-left variable">smoothing</span><span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>:
        super<span class="token punctuation">(</span>LabelSmoothing, self<span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self.criterion <span class="token operator">=</span> nn.KLDivLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">"sum"</span><span class="token punctuation">)</span>
        self.padding_idx <span class="token operator">=</span> padding_idx
        self.confidence <span class="token operator">=</span> <span class="token number">1.0</span> - smoothing
        self.smoothing <span class="token operator">=</span> smoothing
        self.size <span class="token operator">=</span> size
        self.true_dist <span class="token operator">=</span> None

    def forward<span class="token punctuation">(</span>self, x, target<span class="token punctuation">)</span>:
        assert x.size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> self.size
        true_dist <span class="token operator">=</span> x.data.clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
        true_dist.fill_<span class="token punctuation">(</span>self.smoothing / <span class="token punctuation">(</span>self.size - <span class="token number">2</span><span class="token punctuation">))</span>
        true_dist.scatter_<span class="token punctuation">(</span><span class="token number">1</span>, target.data.unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>, self.confidence<span class="token punctuation">)</span>
        true_dist<span class="token punctuation">[</span>:, self.padding_idx<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        mask <span class="token operator">=</span> torch.nonzero<span class="token punctuation">(</span>target.data <span class="token operator">==</span> self.padding_idx<span class="token punctuation">)</span>
        <span class="token keyword">if</span> mask.dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span>:
            true_dist.index_fill_<span class="token punctuation">(</span><span class="token number">0</span>, mask.squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token number">0.0</span><span class="token punctuation">)</span>
        self.true_dist <span class="token operator">=</span> true_dist
        <span class="token builtin class-name">return</span> self.criterion<span class="token punctuation">(</span>x, true_dist.clone<span class="token punctuation">(</span><span class="token punctuation">)</span>.detach<span class="token punctuation">(</span><span class="token punctuation">))</span>
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment">#-------------------5、测试 正则化标签平滑-------------------------------------</span>
def example_label_smoothing<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    
    crit <span class="token operator">=</span> LabelSmoothing<span class="token punctuation">(</span><span class="token number">5</span>, <span class="token number">0</span>, <span class="token number">0.4</span><span class="token punctuation">)</span>
    predict <span class="token operator">=</span> torch.FloatTensor<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>
            <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0.2</span>, <span class="token number">0.7</span>, <span class="token number">0.1</span>, <span class="token number">0</span><span class="token punctuation">]</span>,
            <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0.2</span>, <span class="token number">0.7</span>, <span class="token number">0.1</span>, <span class="token number">0</span><span class="token punctuation">]</span>,
            <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0.2</span>, <span class="token number">0.7</span>, <span class="token number">0.1</span>, <span class="token number">0</span><span class="token punctuation">]</span>,
            <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0.2</span>, <span class="token number">0.7</span>, <span class="token number">0.1</span>, <span class="token number">0</span><span class="token punctuation">]</span>,
            <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">0.2</span>, <span class="token number">0.7</span>, <span class="token number">0.1</span>, <span class="token number">0</span><span class="token punctuation">]</span>,
        <span class="token punctuation">]</span>
    <span class="token punctuation">)</span>
    crit<span class="token punctuation">(</span>x<span class="token operator">=</span>predict.log<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token assign-left variable">target</span><span class="token operator">=</span>torch.LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span>, <span class="token number">1</span>, <span class="token number">0</span>, <span class="token number">3</span>, <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
    LS_data <span class="token operator">=</span> pd.concat<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>
            pd.DataFrame<span class="token punctuation">(</span>
                <span class="token punctuation">{<!-- --></span>
                    <span class="token string">"target distribution"</span><span class="token builtin class-name">:</span> crit.true_dist<span class="token punctuation">[</span>x, y<span class="token punctuation">]</span>.flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>,
                    <span class="token string">"columns"</span><span class="token builtin class-name">:</span> y,
                    <span class="token string">"rows"</span><span class="token builtin class-name">:</span> x,
                <span class="token punctuation">}</span>
            <span class="token punctuation">)</span>
            <span class="token keyword">for</span> <span class="token for-or-select variable">y</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> <span class="token for-or-select variable">x</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span>
    <span class="token punctuation">)</span>

    <span class="token assign-left variable">chart</span><span class="token operator">=</span> <span class="token punctuation">(</span>
        alt.Chart<span class="token punctuation">(</span>LS_data<span class="token punctuation">)</span>
        .mark_rect<span class="token punctuation">(</span>color<span class="token operator">=</span><span class="token string">"Blue"</span>, <span class="token assign-left variable">opacity</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        .properties<span class="token punctuation">(</span>height<span class="token operator">=</span><span class="token number">200</span>, <span class="token assign-left variable">width</span><span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span>
        .encode<span class="token punctuation">(</span>
            alt.X<span class="token punctuation">(</span><span class="token string">"columns:O"</span>, <span class="token assign-left variable">title</span><span class="token operator">=</span>None<span class="token punctuation">)</span>,
            alt.Y<span class="token punctuation">(</span><span class="token string">"rows:O"</span>, <span class="token assign-left variable">title</span><span class="token operator">=</span>None<span class="token punctuation">)</span>,
            alt.Color<span class="token punctuation">(</span>
                <span class="token string">"target distribution:Q"</span>, <span class="token assign-left variable">scale</span><span class="token operator">=</span>alt.Scale<span class="token punctuation">(</span>scheme<span class="token operator">=</span><span class="token string">"viridis"</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>,
        <span class="token punctuation">)</span>
        .interactive<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>

    <span class="token comment"># 展示数据，调用display()方法</span>
    altair_viewer.show<span class="token punctuation">(</span>chart<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/df/a7/VAKuXbVz_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/0e/7b/ajXZLyOk_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8a/2c/s1NKlQhj_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_456"></a>数据加载解析</h3> 
<h3><a id="_458"></a>模型训练解析</h3> 
<h3><a id="_460"></a>模型测试解析</h3> 
<h3><a id="_462"></a>问题</h3> 
<h4><a id="1_httpwwwquestdcsshefacukwmt16_files_mmttrainingtargz__httpsgithubcomharvardnlpannotatedtransformerissues96_463"></a>1、<a href="https://github.com/harvardnlp/annotated-transformer/issues/96">无法从 http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz 获取文件。[请求异常] 无</a></h4> 
<p>离线下载放入缓存<br> <img src="https://images2.imgbox.com/29/14/ZCpoitjr_o.png" alt="在这里插入图片描述"><br> 或者修改URL<br> <img src="https://images2.imgbox.com/ca/ac/12TtvViW_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2_468"></a>2、</h4> 
<p><img src="https://images2.imgbox.com/b7/3f/GbktxDIF_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token assign-left variable">altair_viewer</span><span class="token operator">==</span><span class="token number">0.4</span>.0 <span class="token parameter variable">-i</span> https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/de82738060baf930fa5f32a53666e30c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">迁移到Office365教程系列——部署DirSync</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4ebe4c7e45b5c88fe633ba582d07cfd2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">边缘计算网关：在智慧储能系统中做好储能通信管家</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>