<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>学习率优化方式 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="学习率优化方式" />
<meta property="og:description" content="背景：调整学习率可能会对模型提升1~2%，当前我们采用的学习率比较简单粗暴，使用指定学习率，为了优化模型，现对学习率进行探索，希望对我们模型的指标结果有所提升
一、工业界学习率现存形式 1、指定学习率 固定学习率，只在模型训练的全过程中，学习率只采用某个固定值，如0.15，进行模型训练
2、指定学习率&#43;衰减 指定学习率&#43;衰减，顾名思义是，满足一定条件内，使用固定学习率，如0.15，达到条件后，采用衰减的形式
代码如下：
指定学习率&#43;衰减
initial_learning_rate = 0.15 lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay( initial_learning_rate, decay_steps=100000, decay_rate=0.8, staircase=True) 3、warm up 谈到warm之前，总结一下，学习率几种常见的衰减形式
3.1 衰减函数 指数衰减 tf.train.exponential_decay()
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay
指数衰减 代码
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay( initial_learning_rate, decay_steps=100000, # 衰减周期，当staircase=True时，学习率在decay_steps内保持不变，即得到离散型学习率； decay_rate=0.8,#衰减率系数 staircase=True) # 是否定义为离散型学习率，默认False def decayed_learning_rate(step): return initial_learning_rate * decay_rate ^ (step / decay_steps) # 当为False时，需要加一个判断语句 倒数衰减 tf.train.inverse_time_decay() tf.keras.optimizers.schedules.InverseTimeDecay 倒数衰减代码
tf.keras.optimizers.schedules.InverseTimeDecay( initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None ) # staircase=False def decayed_learning_rate(step): return initial_learning_rate / (1 &#43; decay_rate * step / decay_step) 分段常数衰减: tf." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/bc384566f4d6c21fc10722d31aaa40ef/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-03T22:23:12+08:00" />
<meta property="article:modified_time" content="2023-04-03T22:23:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">学习率优化方式</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>背景：调整学习率可能会对模型提升1~2%，当前我们采用的学习率比较简单粗暴，使用指定学习率，为了优化模型，现对学习率进行探索，希望对我们模型的指标结果有所提升</p> 
<h2><a id="_5"></a>一、工业界学习率现存形式</h2> 
<h3><a id="1_7"></a>1、指定学习率</h3> 
<p>固定学习率，只在模型训练的全过程中，学习率只采用某个固定值，如0.15，进行模型训练</p> 
<h3><a id="2_12"></a>2、指定学习率+衰减</h3> 
<p>指定学习率+衰减，顾名思义是，满足一定条件内，使用固定学习率，如0.15，达到条件后，采用衰减的形式</p> 
<p>代码如下：</p> 
<p>指定学习率+衰减</p> 
<pre><code class="prism language-python">initial_learning_rate <span class="token operator">=</span> <span class="token number">0.15</span>

lr_schedule <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>schedules<span class="token punctuation">.</span>ExponentialDecay<span class="token punctuation">(</span>

    initial_learning_rate<span class="token punctuation">,</span>

    decay_steps<span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">,</span>

    decay_rate<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span>

    staircase<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="3warm_up_34"></a>3、warm up</h3> 
<p>谈到warm之前，总结一下，学习率几种常见的衰减形式</p> 
<h4><a id="31__39"></a>3.1 衰减函数</h4> 
<ul><li>指数衰减</li></ul> 
<p>tf.train.exponential_decay()</p> 
<p>lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay</p> 
<p>指数衰减 代码</p> 
<pre><code class="prism language-python">lr_schedule <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>schedules<span class="token punctuation">.</span>ExponentialDecay<span class="token punctuation">(</span>

        initial_learning_rate<span class="token punctuation">,</span>

        decay_steps<span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">,</span> <span class="token comment"># 衰减周期，当staircase=True时，学习率在decay_steps内保持不变，即得到离散型学习率；</span>

        decay_rate<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span><span class="token comment">#衰减率系数</span>

        staircase<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 是否定义为离散型学习率，默认False</span>

<span class="token keyword">def</span> <span class="token function">decayed_learning_rate</span><span class="token punctuation">(</span>step<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">return</span> initial_learning_rate <span class="token operator">*</span> decay_rate <span class="token operator">^</span> <span class="token punctuation">(</span>step <span class="token operator">/</span> decay_steps<span class="token punctuation">)</span>
  <span class="token comment"># 当为False时，需要加一个判断语句</span>
  
</code></pre> 
<ul><li>倒数衰减</li></ul> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>inverse_time_decay<span class="token punctuation">(</span><span class="token punctuation">)</span>

tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>schedules<span class="token punctuation">.</span>InverseTimeDecay
</code></pre> 
<p>倒数衰减代码</p> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>schedules<span class="token punctuation">.</span>InverseTimeDecay<span class="token punctuation">(</span>

    initial_learning_rate<span class="token punctuation">,</span> decay_steps<span class="token punctuation">,</span> decay_rate<span class="token punctuation">,</span> staircase<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span>

<span class="token punctuation">)</span>


<span class="token comment"># staircase=False</span>

<span class="token keyword">def</span> <span class="token function">decayed_learning_rate</span><span class="token punctuation">(</span>step<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token keyword">return</span> initial_learning_rate <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> decay_rate <span class="token operator">*</span> step <span class="token operator">/</span> decay_step<span class="token punctuation">)</span>
</code></pre> 
<ul><li>分段常数衰减:</li></ul> 
<p>tf.train.piecewise_constant</p> 
<p>tf.keras.optimizers.schedules.PiecewiseConstantDecay</p> 
<p>分段常数衰减</p> 
<pre><code class="prism language-python"><span class="token comment"># tf1.x系列</span>

tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>piecewise_constant<span class="token punctuation">(</span>

    x<span class="token punctuation">,</span>

    boundaries<span class="token punctuation">,</span> <span class="token comment"># 列表，表示分割的边界；</span>

    values<span class="token punctuation">,</span> <span class="token comment"># 列表，分段学习率的取值；</span>

    name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

 

<span class="token comment"># parameter</span>

global_step <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

boundaries <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">]</span>

values <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span>

<span class="token comment"># learning_rate</span>

learning_rate <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>piecewise_constant<span class="token punctuation">(</span>global_step<span class="token punctuation">,</span> boundaries<span class="token punctuation">,</span> values<span class="token punctuation">)</span>

<span class="token comment"># 解释</span>

<span class="token comment"># 当global_step=[1, 100]时，learning_rate=1.0;</span>

<span class="token comment"># 当global_step=[101, 200]时，learning_rate=0.5;</span>

<span class="token comment"># 当global_step=[201, ~]时，learning_rate=0.1;</span>

 

<span class="token comment"># tf2.+可用，需转化</span>

tf<span class="token punctuation">.</span>compat<span class="token punctuation">.</span>v1<span class="token punctuation">.</span>train<span class="token punctuation">.</span>piecewise_constant<span class="token punctuation">(</span><span class="token punctuation">)</span> 或者

tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>schedules<span class="token punctuation">.</span>PiecewiseConstantDecay<span class="token punctuation">(</span>

    boundaries<span class="token punctuation">,</span> values<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span>

<span class="token punctuation">)</span>

<span class="token comment">#  use a learning rate that's 1.0 for the first 100001 steps, 0.5 for the next 10000 steps, and 0.1 for any additional steps.</span>

step <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

boundaries <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">100000</span><span class="token punctuation">,</span> <span class="token number">110000</span><span class="token punctuation">]</span>

values <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span>

learning_rate_fn <span class="token operator">=</span> keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>schedules<span class="token punctuation">.</span>PiecewiseConstantDecay<span class="token punctuation">(</span>

    boundaries<span class="token punctuation">,</span> values<span class="token punctuation">)</span>

<span class="token comment"># Later, whenever we perform an optimization step, we pass in the step.</span>

learning_rate <span class="token operator">=</span> learning_rate_fn<span class="token punctuation">(</span>step<span class="token punctuation">)</span>
</code></pre> 
<ul><li>自然指数衰减</li></ul> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>natural_exp_decay<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>类似与指数衰减，同样与当前迭代次数相关，只不过以e为底；自然指数衰减对学习率的衰减程度远大于一般的指数衰减</p> 
<ul><li>自然指数衰减</li></ul> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>natural_exp_decay<span class="token punctuation">(</span>

    learning_rate<span class="token punctuation">,</span>

    global_step<span class="token punctuation">,</span> <span class="token comment"># 迭代次数；</span>

    decay_steps<span class="token punctuation">,</span> <span class="token comment"># 衰减周期，当staircase=True时，学习率在decay_steps内保持不变，即得到离散型学习率；</span>

    decay_rate<span class="token punctuation">,</span>

    staircase<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>

    name<span class="token operator">=</span><span class="token boolean">None</span>

<span class="token punctuation">)</span>

decayed_learning_rate <span class="token operator">=</span> learning_rate <span class="token operator">*</span> exp<span class="token punctuation">(</span><span class="token operator">-</span>decay_rate <span class="token operator">*</span> global_step<span class="token punctuation">)</span>
<span class="token comment"># 如果staircase=True，则学习率会在得到离散值，每decay_steps迭代次数，更新一次；</span>
</code></pre> 
<ul><li>阶梯型衰减</li></ul> 
<pre><code class="prism language-python">learing_rate1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>natural_exp_decay<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> global_step<span class="token operator">=</span>global_step<span class="token punctuation">,</span> decay_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> decay_rate<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> staircase<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

 
</code></pre> 
<ul><li>标准指数型衰减</li></ul> 
<pre><code class="prism language-python">learing_rate2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>natural_exp_decay<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> global_step<span class="token operator">=</span>global_step<span class="token punctuation">,</span> decay_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> decay_rate<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> staircase<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>多项式衰减tf.train.polynomial_decay()</li></ul> 
<p>多项式衰减中设置学习率可以往复升降的目的：时为了防止在神经网络训练后期由于学习率过小，导致网络参数陷入局部最优，将学习率升高，有可能使其跳出局部最优；</p> 
<p>多项式衰减 代码</p> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>polynomial_decay<span class="token punctuation">(</span>

    learning_rate<span class="token punctuation">,</span>

    global_step<span class="token punctuation">,</span> <span class="token comment"># 迭代次数；</span>

    decay_steps<span class="token punctuation">,</span> <span class="token comment"># 衰减周期；</span>

    end_learning_rate<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> <span class="token comment"># 最小学习率，默认0.0001；</span>

    power<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token comment"># 多项式的幂，默认1；</span>

    cycle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token comment"># bool，表示达到最低学习率时，是否升高再降低，默认False；</span>

 name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

</code></pre> 
<ul><li>余弦衰减</li></ul> 
<p>标准余弦衰减tf.train.cosine_decay()、重启余弦衰减、线性余弦噪声、噪声余弦衰减</p> 
<h5><a id="_255"></a>--------------标准余弦衰减-------------------</h5> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>cosine_decay<span class="token punctuation">(</span>

    learning_rate<span class="token punctuation">,</span> <span class="token comment"># 初始学习率；</span>

    global_step<span class="token punctuation">,</span> <span class="token comment"># 迭代次数</span>

    decay_steps<span class="token punctuation">,</span> <span class="token comment"># 衰减周期；</span>

    alpha<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token comment"># 最小学习率，默认为0；</span>

    name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre> 
<p>计算方式</p> 
<pre><code class="prism language-python">global_step <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>global_step<span class="token punctuation">,</span> decay_steps<span class="token punctuation">)</span>

cosine_decay <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> cos<span class="token punctuation">(</span>pi <span class="token operator">*</span> global_step <span class="token operator">/</span> decay_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

decayed <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> alpha<span class="token punctuation">)</span> <span class="token operator">*</span> cosine_decay <span class="token operator">+</span> alpha

decayed_learning_rate <span class="token operator">=</span> learning_rate <span class="token operator">*</span> decayed
</code></pre> 
<h5><a id="_285"></a>--------------重启余弦衰减-------------------</h5> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>cosine_decay_restarts<span class="token punctuation">(</span>

    learning_rate<span class="token punctuation">,</span><span class="token comment">#初始学习率；</span>

    global_step<span class="token punctuation">,</span> <span class="token comment"># 迭代次数</span>

    first_decay_steps<span class="token punctuation">,</span>衰减周期；

    t_mul<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token comment"># Used to derive the number of iterations in the i-th period</span>

    m_mul<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token comment"># Used to derive the initial learning rate of the i-th period</span>

    alpha<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>最小学习率，默认为<span class="token number">0</span>；

    name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre> 
<h5><a id="_307"></a>--------------线性余弦噪声-------------------</h5> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>linear_cosine_decay<span class="token punctuation">(</span>

    learning_rate<span class="token punctuation">,</span><span class="token comment"># 初始学习率；</span>

    global_step<span class="token punctuation">,</span><span class="token comment"># 迭代次数；</span>

    decay_steps<span class="token punctuation">,</span><span class="token comment"># 衰减周期；</span>

    num_periods<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token comment"># Number of periods in the cosine part of the decay.</span>

    alpha<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token comment"># 最小学习率；</span>

    beta<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span><span class="token comment">#</span>

    name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre> 
<p>计算方式</p> 
<pre><code class="prism language-python">global_step <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>global_step<span class="token punctuation">,</span> decay_steps<span class="token punctuation">)</span>

linear_decay <span class="token operator">=</span> <span class="token punctuation">(</span>decay_steps <span class="token operator">-</span> global_step<span class="token punctuation">)</span> <span class="token operator">/</span> decay_steps<span class="token punctuation">)</span>

cosine_decay <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> cos<span class="token punctuation">(</span>pi <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">*</span> num_periods <span class="token operator">*</span> global_step <span class="token operator">/</span> decay_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

decayed <span class="token operator">=</span> <span class="token punctuation">(</span>alpha <span class="token operator">+</span> linear_decay<span class="token punctuation">)</span> <span class="token operator">*</span> cosine_decay <span class="token operator">+</span> beta

decayed_learning_rate <span class="token operator">=</span> learning_rate <span class="token operator">*</span> decayed
</code></pre> 
<h5><a id="_344"></a>--------------噪声余弦衰减-------------------</h5> 
<pre><code class="prism language-python">tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>noisy_linear_cosine_decay<span class="token punctuation">(</span>

learning_rate<span class="token punctuation">,</span><span class="token comment"># 初始学习率；</span>

global_step<span class="token punctuation">,</span> <span class="token comment">#迭代次数；</span>

decay_steps<span class="token punctuation">,</span> <span class="token comment"># 衰减周期；</span>

initial_variance<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token comment"># initial variance for the noise.</span>

variance_decay<span class="token operator">=</span><span class="token number">0.55</span><span class="token punctuation">,</span> <span class="token comment"># decay for the noise's variance.</span>

num_periods<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token comment"># Number of periods in the cosine part of the decay</span>

alpha<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token comment"># 最小学习率；</span>

beta<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> <span class="token comment">#</span>

name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>计算方式global_step <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>global_step<span class="token punctuation">,</span> decay_steps<span class="token punctuation">)</span>

linear_decay <span class="token operator">=</span> <span class="token punctuation">(</span>decay_steps <span class="token operator">-</span> global_step<span class="token punctuation">)</span> <span class="token operator">/</span> decay_steps<span class="token punctuation">)</span>

cosine_decay <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> cos<span class="token punctuation">(</span>pi <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">*</span> num_periods <span class="token operator">*</span> global_step <span class="token operator">/</span> decay_steps<span class="token punctuation">)</span><span class="token punctuation">)</span>

decayed <span class="token operator">=</span> <span class="token punctuation">(</span>alpha <span class="token operator">+</span> linear_decay <span class="token operator">+</span> eps_t<span class="token punctuation">)</span> <span class="token operator">*</span> cosine_decay <span class="token operator">+</span> beta

decayed_learning_rate <span class="token operator">=</span> learning_rate <span class="token operator">*</span> decayed
</code></pre> 
<h4><a id="32_warm_up_376"></a>3.2 warm up</h4> 
<p>学习率预热（warmup ）就是在刚开始训练的时候先使用一个较小的学习率，训练一些epoches或iterations，等模型稳定时再修改为预先设置的学习率进行训练。<br> 具体做法很简单，比如ResNet原论文[1]中，batch size为256时选择的学习率是0.1，当我们把batch size变为一个较大的数b时，学习率应该变为 0.1 × b/256。<br> 在warmup之后的训练过程中，学习率不断衰减是一个提高精度的好方法。其中有step decay和cosine decay等，前者是随着epoch增大学习率不断减去一个小的数，后者是让学习率随着训练过程曲线下降。<br> warm up 代码</p> 
<pre><code class="prism language-python"><span class="token keyword">if</span> warmup<span class="token punctuation">:</span>

    warmup_steps <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>batches_per_epoch <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span>

    warmup_lr <span class="token operator">=</span> <span class="token punctuation">(</span>initial_learning_rate <span class="token operator">*</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>global_step<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>warmup_steps<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>cond<span class="token punctuation">(</span>global_step <span class="token operator">&lt;</span> warmup_steps<span class="token punctuation">,</span> <span class="token keyword">lambda</span><span class="token punctuation">:</span> warmup_lr<span class="token punctuation">,</span> <span class="token keyword">lambda</span><span class="token punctuation">:</span> lr<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">get_scheduler</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> opt<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token triple-quoted-string string">"""Return a learning rate scheduler

        Parameters:

        optimizer -- 网络优化器

        opt.lr_policy -- 学习率scheduler的名称: linear | step | plateau | cosine

    """</span>

    <span class="token keyword">if</span> opt<span class="token punctuation">.</span>lr_policy <span class="token operator">==</span> <span class="token string">'linear'</span><span class="token punctuation">:</span>

        <span class="token keyword">def</span> <span class="token function">lambda_rule</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>

            lr_l <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">-</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> epoch <span class="token operator">+</span> opt<span class="token punctuation">.</span>epoch_count <span class="token operator">-</span> opt<span class="token punctuation">.</span>niter<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">float</span><span class="token punctuation">(</span>opt<span class="token punctuation">.</span>niter_decay <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>

            <span class="token keyword">return</span> lr_l

 

    scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> lr_lambda<span class="token operator">=</span>lambda_rule<span class="token punctuation">)</span>

    <span class="token keyword">elif</span> opt<span class="token punctuation">.</span>lr_policy <span class="token operator">==</span> <span class="token string">'step'</span><span class="token punctuation">:</span>

        scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> step_size<span class="token operator">=</span>opt<span class="token punctuation">.</span>lr_decay_iters<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>

    <span class="token keyword">elif</span> opt<span class="token punctuation">.</span>lr_policy <span class="token operator">==</span> <span class="token string">'plateau'</span><span class="token punctuation">:</span>

        scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>ReduceLROnPlateau<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'min'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>

    <span class="token keyword">elif</span> opt<span class="token punctuation">.</span>lr_policy <span class="token operator">==</span> <span class="token string">'cosine'</span><span class="token punctuation">:</span>

        scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>CosineAnnealingLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> T_max<span class="token operator">=</span>opt<span class="token punctuation">.</span>niter<span class="token punctuation">,</span> eta_min<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">else</span><span class="token punctuation">:</span>

        <span class="token keyword">return</span> NotImplementedError<span class="token punctuation">(</span><span class="token string">'learning rate policy [%s] is not implemented'</span><span class="token punctuation">,</span> opt<span class="token punctuation">.</span>lr_policy<span class="token punctuation">)</span>

    <span class="token keyword">return</span> scheduler


</code></pre> 
<h3><a id="4_438"></a>4、周期性学习率</h3> 
<p>CLR是Leslie Smith于2015年提出的。这是一种调节LR的方法，在该方法中，设定一个LR上限和下限，LR的值在上限和下限的区间里周期性地变化。看上去，CLR似乎是自适应LR技术和SGD的竞争者，事实上，CLR技术是可以和上述提到的改进的优化器一起使用来进行参数更新</p> 
<p>参考：<br> 1、https://www.cnblogs.com/chenzhen0530/p/10632937.html<br> 2、https://blog.csdn.net/weixin_43896398/article/details/84762886<br> 3、https://zhuanlan.zhihu.com/p/66080948</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/839d8f78ff895a9b0418ef59a61650ad/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【InputStream, OutputStream 的用法】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/64e2187c870b6fb496ae7d1c591248cf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">推荐系统中保序回归校准方案</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>