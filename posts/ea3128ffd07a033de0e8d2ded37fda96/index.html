<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【火炉炼AI】机器学习038-NLP创建词袋模型 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【火炉炼AI】机器学习038-NLP创建词袋模型" />
<meta property="og:description" content="【火炉炼AI】机器学习038-NLP创建词袋模型 (本文所使用的Python库和版本号: Python 3.6, Numpy 1.14, scikit-learn 0.19, matplotlib 2.2， NLTK 3.3)
词袋模型（Bag Of Words, BOW)和词向量（Word Embedding, 也叫词嵌套等）是自然语言处理和文本分析的两个最常用的模型。
词袋模型将一段文本看成一系列单词的集合，由于单词很多，故而这段文本就相当于一个袋子，里面装着一系列单词。故而计算机的NLP分析就是对这个袋子进行分析，但是计算机不认识文本，只认识数字，那么我们需要一种机制将袋子里的文本转换成数字，这种机制可以是一种Dict映射（key为数字，value为文本等），或数组（索引为数字，值为文本），或者还可以用HashCode来计算文本的数字表示，而NLP建模就是使用这些数字来建模。词袋在学习之后，就可以通过构建文档中所有单词的直方图来对每篇文档进行建模。
词向量模型是将单个单词映射到一个高维空间（维度可以到几千几万甚至几十万），这个高维空间就用数组，或者成为向量来表示，故而建立一种单词-向量的映射关系，所以成为词向量模型。但是这种模型能表示的仅仅是单个单词，对于有多个单词组成的一句话，那么就需要做进一步处理，比如一个单词就是一个向量，Ｎ个单词组成的一句话就是Ｎ个一维向量了，故而可以用Ｎ个一维向量组成的矩阵来表示一句话，只不过不同长度的句子，该矩阵的行数不一样罢了。
下面我们仅仅学习用NLP创建词袋模型，创建过程主要是提取文本的特征，构建特征向量。有两种方法可以构建特征向量，分别是CountVectorizer和TfidfVectorizer。
1. 用CountVectorizer提取文本特征 sklearn模块中的CountVectorizer方法可以直接提取文本特征，这个函数只考虑词汇在文本中出现的频率，这个函数有一个参数：stop_words，表示是否取出停用词，所谓的停用词是指为了节省空间和提高效率而自动过滤到的词语，比如 the, is, at, which等，对于不同的，默认的stop_words不去除停用词。
# 数据集暂时用简·奥斯丁的《爱玛》中的文本 dataset=nltk.corpus.gutenberg.words(&#39;austen-emma.txt&#39;) # print(len(dataset)) # 192427 代表读入正常 chunks=split(&#34; &#34;.join(dataset[:10000]), 2000) # 将前面的10000个单词分成五个词袋，每个袋子装2000个单词 # 构建一个文档-词矩阵，该矩阵记录了文档中每个单词出现的频次 # 用sk-learn的CountVectorizer函数来实现这种构建过程 from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer(min_df=4, max_df=.99) # fit_transform函数需要输入一维数组，且数组元素是用空格连起来的文本 chunks=[&#34; &#34;.join(chunk) for chunk in chunks] # 故而需要转换一下 doc_term_matrix = vectorizer.fit_transform(chunks) feature_names=vectorizer.get_feature_names() # 获取 print(len(feature_names)) print(doc_term_matrix." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/ea3128ffd07a033de0e8d2ded37fda96/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-10-10T07:57:04+08:00" />
<meta property="article:modified_time" content="2018-10-10T07:57:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【火炉炼AI】机器学习038-NLP创建词袋模型</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="article-content"> 
 <h3 class="heading">【火炉炼AI】机器学习038-NLP创建词袋模型</h3> 
 <p>(本文所使用的Python库和版本号: Python 3.6, Numpy 1.14, scikit-learn 0.19, matplotlib 2.2， NLTK 3.3)</p> 
 <p>词袋模型（Bag Of Words, BOW)和词向量（Word Embedding, 也叫词嵌套等）是自然语言处理和文本分析的两个最常用的模型。</p> 
 <p>词袋模型将一段文本看成一系列单词的集合，由于单词很多，故而这段文本就相当于一个袋子，里面装着一系列单词。故而计算机的NLP分析就是对这个袋子进行分析，但是计算机不认识文本，只认识数字，那么我们需要一种机制将袋子里的文本转换成数字，这种机制可以是一种Dict映射（key为数字，value为文本等），或数组（索引为数字，值为文本），或者还可以用HashCode来计算文本的数字表示，而NLP建模就是使用这些数字来建模。词袋在学习之后，就可以通过构建文档中所有单词的直方图来对每篇文档进行建模。</p> 
 <p>词向量模型是将单个单词映射到一个高维空间（维度可以到几千几万甚至几十万），这个高维空间就用数组，或者成为向量来表示，故而建立一种单词-向量的映射关系，所以成为词向量模型。但是这种模型能表示的仅仅是单个单词，对于有多个单词组成的一句话，那么就需要做进一步处理，比如一个单词就是一个向量，Ｎ个单词组成的一句话就是Ｎ个一维向量了，故而可以用Ｎ个一维向量组成的矩阵来表示一句话，只不过不同长度的句子，该矩阵的行数不一样罢了。</p> 
 <p>下面我们仅仅学习用NLP创建词袋模型，创建过程主要是提取文本的特征，构建特征向量。有两种方法可以构建特征向量，分别是CountVectorizer和TfidfVectorizer。</p> 
 <br> 
 <h3 class="heading">1. 用CountVectorizer提取文本特征</h3> 
 <p>sklearn模块中的CountVectorizer方法可以直接提取文本特征，这个函数只考虑词汇在文本中出现的频率，这个函数有一个参数：stop_words，表示是否取出停用词，所谓的停用词是指为了节省空间和提高效率而自动过滤到的词语，比如 the, is, at, which等，对于不同的，默认的stop_words不去除停用词。</p> 
 <pre><code class="hljs Python copyable"><span class="hljs-comment"># 数据集暂时用简·奥斯丁的《爱玛》中的文本</span>
dataset=nltk.corpus.gutenberg.words(<span class="hljs-string">'austen-emma.txt'</span>)
<span class="hljs-comment"># print(len(dataset)) # 192427 代表读入正常</span>
chunks=split(<span class="hljs-string">" "</span>.join(dataset[:<span class="hljs-number">10000</span>]), <span class="hljs-number">2000</span>) <span class="hljs-comment"># 将前面的10000个单词分成五个词袋，每个袋子装2000个单词</span>

<span class="hljs-comment"># 构建一个文档-词矩阵，该矩阵记录了文档中每个单词出现的频次</span>
<span class="hljs-comment"># 用sk-learn的CountVectorizer函数来实现这种构建过程</span>
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer
vectorizer = CountVectorizer(min_df=<span class="hljs-number">4</span>, max_df=<span class="hljs-number">.99</span>)
<span class="hljs-comment"># fit_transform函数需要输入一维数组，且数组元素是用空格连起来的文本</span>
chunks=[<span class="hljs-string">" "</span>.join(chunk) <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks] <span class="hljs-comment"># 故而需要转换一下</span>
doc_term_matrix = vectorizer.fit_transform(chunks)
feature_names=vectorizer.get_feature_names() <span class="hljs-comment"># 获取</span>
print(len(feature_names)) 
print(doc_term_matrix.shape) 
<span class="hljs-comment"># print(doc_term_matrix.T.toarray())</span>
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>上面将简·奥斯丁的《爱玛》中的前面10000个单词分成了五个词袋，每个词袋包含2000个单词，然后用CountVectorizer建立文本特征向量，通过fit_transform后就在CountVectorizer对象内部建立了这种文档-词矩阵，通过print可以看出结果。</p> 
 <p>为了更加明确的看出里面的文档-词矩阵，可以用下面的代码将其打印出来：</p> 
 <pre><code class="hljs Python copyable"><span class="hljs-comment"># 打印看看doc_term_matrix这个文档-词矩阵里面的内容</span>
print(<span class="hljs-string">'Document Term Matrix------&gt;&gt;&gt;&gt;'</span>)
bag_names=[<span class="hljs-string">'Bag_'</span>+str(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">5</span>)] <span class="hljs-comment"># 5个词袋</span>
formatted_row=<span class="hljs-string">'{:&gt;12}'</span>*(<span class="hljs-number">1</span>+len(bag_names)) <span class="hljs-comment"># 每一行第一列是单词，后面是每个词袋中的频率</span>
print(formatted_row.format(<span class="hljs-string">'Word'</span>, *bag_names))
<span class="hljs-keyword">for</span> word, freq <span class="hljs-keyword">in</span> zip(feature_names,doc_term_matrix.T.toarray()): <span class="hljs-comment"># 需要装置矩阵</span>
    <span class="hljs-comment"># 此处的freq是csr_matrix数据结构</span>
    output = [str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> freq.data]
    print(formatted_row.format(word,*output))
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p><strong>-----------------------输---------出--------------------</strong></p> 
 <p>Document Term Matrix------&gt;&gt;&gt;&gt; Word Bag_0 Bag_1 Bag_2 Bag_3 Bag_4 about 3 4 0 1 1 among 1 1 1 1 0 because 1 1 0 1 1 believe 0 1 1 1 3 believed 0 1 1 1 2 best 1 2 1 1 0 better 0 3 1 1 2 beyond 1 0 1 2 3</p> 
 <p>...</p> 
 <p><strong>-----------------------完--------------------------------</strong></p> 
 <p>以上是部分结果，可以看出about在Bag_0中出现了3次，在Bag_1中出现了4次，以此类推。</p> 
 <p>如果对这种矩阵的出现次数有疑惑，可以看我的<a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2FRayDean%2FMachineLearning" rel="nofollow"><strong>我的github中代码</strong></a>，里面有更详细的解释。</p> 
 <p>值得注意的是，CountVectorizer也可以用于中文特征的提取，但是需要对自定义的split函数进行修改，原来的函数用空格作为分隔符，可以很好的将英文分词，但对中文无效，故而中文的情况需要将split中的分词方式改成jieba分词。</p> 
 <br> 
 <h3 class="heading">2. 用TfidfVectorizer提取文本特征</h3> 
 <p>TfidfVectorizer的主要特点是：除了考量某词汇在文本出现的频率，还关注包含这个词汇的所有文本的数量，这个方法能够削减高频没有意义的词汇带来的影响，挖掘更有意义的特征。一般的，当文本条目越多，这个方法的效果越显著。</p> 
 <p>在代码上，用TfidfVectorizer和上面的CountVectorizer几乎一样，只是将类名称替换一下即可。</p> 
 <pre><code class="hljs Python copyable"><span class="hljs-comment"># 数据集暂时用简·奥斯丁的《爱玛》中的文本</span>
dataset=nltk.corpus.gutenberg.words(<span class="hljs-string">'austen-emma.txt'</span>)
<span class="hljs-comment"># print(len(dataset)) # 192427 代表读入正常</span>
chunks=split(<span class="hljs-string">" "</span>.join(dataset[:<span class="hljs-number">10000</span>]), <span class="hljs-number">2000</span>) <span class="hljs-comment"># 将前面的10000个单词分成五个词袋，每个袋子装2000个单词</span>

<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
vectorizer = TfidfVectorizer()
<span class="hljs-comment"># fit_transform函数需要输入一维数组，且数组元素是用空格连起来的文本</span>
chunks=[<span class="hljs-string">" "</span>.join(chunk) <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks] <span class="hljs-comment"># 故而需要转换一下</span>
doc_term_matrix = vectorizer.fit_transform(chunks)
feature_names=vectorizer.get_feature_names() <span class="hljs-comment"># 获取</span>
print(len(feature_names)) 
print(doc_term_matrix.shape) 
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>打印出来的结果并不是某个单词在词袋中出现的频率，而是tf-idf权重，这个权重有个计算公式，tf-idf=tf*idf，也就是说tf与idf分别是两个不同的东西。其中tf为谋个训练文本中，某个词的出现次数，即词频(Term Frequency)；idf为逆文档频率（Inverse Document Frequency），对于词频的权重调整系数。</p> 
 <p><strong>########################小**********结###############################</strong></p> 
 <p><strong>1，用于词袋模型中提取文本特征主要有两种方法：CountVectorizer和TfidfVectorizer，其中CountVectorizer构建的文档-词矩阵里面是单词在某个词袋中出现的频率，而TfidfVectorizer构建的矩阵中是单词的tf-idf权重。</strong></p> 
 <p><strong>2，一般情况下，文档的文本都比较长，故而使用TfidfVectorizer更好一些，推荐首选这个方法。</strong></p> 
 <p><strong>#################################################################</strong></p> 
 <br> 
 <p>注：本部分代码已经全部上传到（<a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2FRayDean%2FMachineLearning" rel="nofollow"><strong>我的github</strong></a>）上，欢迎下载。</p> 
 <p>参考资料:</p> 
 <p>1, Python机器学习经典实例，Prateek Joshi著，陶俊杰，陈小莉译</p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4a2297a304466d3a427aa2392ca2e62f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">bootstrap栅格布局五等分/八等分（参考Bootstrap的栅格写法）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7b1334ede45d908d373d7e7bc5888ef9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">原生ajax请求时出现xhr.status==0及POST请求无响应问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>