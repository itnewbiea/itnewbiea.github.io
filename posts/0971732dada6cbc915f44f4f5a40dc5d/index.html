<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>抓取前程无忧51job岗位数据，实现数据可视化——心得体会 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="抓取前程无忧51job岗位数据，实现数据可视化——心得体会" />
<meta property="og:description" content="最近找工作，经常浏览51job，刚好学了python一段时间了，所以有了一个想法：为什么不将我需要的岗位信息给爬出来呢？ 在51job网站搜索“数据分析师”，查看源代码，发现每一个招聘公告包含岗位、公司、薪资、地区等信息。所以可以实现如下几个目的：
1.根据关键词抓取招聘信息；
2.连接mysql，创建表格，并插入数据；
3.初步清洗数据，实现可视化
一、网页抓取函数
https://search.51job.com/list/000000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html?lang=c&amp;stype=&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;providesalary=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=
这个是搜索“数据分析师”的网址，乍一看十分复杂，立马就没弄下去的欲望了。
发现这么一长串url只有
https://search.51job.com/list/000000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html
是必须的。而
%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588
则正好是“数据分析师”的urlencode，那么我们可以利用这一点，更改搜索关键词
而且%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html中html前的数字对应着页码
因此可以通过更改keyword和页码，进一步选择我们需要的信息。
def Get_html(data,i): Url=&#34;https://search.51job.com/list/000000,000000,0000,00,9,99,&#34;&#43;data&#43;&#34;,2,%s.html&#34;%i #data是自定义搜索关键词，i的大小则直接控制了爬取信息量 header = {&#39;User-Agent&#39;:&#39;Mozilla/5.0&#39;} #头部信息 r=requests.get(Url,headers=header) print(r.url) print(r.status_code) r.encoding=r.apparent_encoding #print(r.text) return(r.text) #函数返回html 万里长征完成第一步o.o
接下来就是解析html，进一步爬取岗位、公司、薪资、地区和url信息。
这个阶段卡了2个多小时（习惯用beautifulsoup爬取）。
代码1：
def Get_data(html):
soup=BeautifulSoup(html,&#34;html.parser&#34;)
for div in soup.find_all(&#34;div&#34;,class_=&#34;el&#34;): #find_all()遍历结果为列表
print(div)
输出结果：
&lt;div class=&#34;el&#34;&gt;
&lt;p class=&#34;t1 &#34;&gt;
&lt;em class=&#34;check&#34; name=&#34;delivery_em&#34; οnclick=&#34;checkboxClick(this)&#34;&gt;&lt;/em&gt;
&lt;input class=&#34;checkbox&#34; jt=&#34;0&#34; name=&#34;delivery_jobid&#34; style=&#34;display:none&#34; type=&#34;checkbox&#34; value=&#34;109229565&#34;/&gt;
&lt;span&gt;
&lt;a href=&#34;https://jobs.51job.com/shenyang/109229565.html?s=01&amp;amp;t=0&#34; οnmοusedοwn=&#34;&#34; target=&#34;_blank&#34; title=&#34;数据分析师&#34;&gt;
数据分析师 &lt;/a&gt;
&lt;/span&gt;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/0971732dada6cbc915f44f4f5a40dc5d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-03-20T00:26:10+08:00" />
<meta property="article:modified_time" content="2019-03-20T00:26:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">抓取前程无忧51job岗位数据，实现数据可视化——心得体会</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>最近找工作，经常浏览51job，刚好学了python一段时间了，所以有了一个想法：为什么不将我需要的岗位信息给爬出来呢？</h3> 
<p><img alt="" class="has" src="https://images2.imgbox.com/d1/01/M2XOlmlk_o.jpg"></p> 
<p>在51job网站搜索“数据分析师”，查看源代码，发现每一个招聘公告包含岗位、公司、薪资、地区等信息。所以可以实现如下几个目的：</p> 
<p>1.根据关键词抓取招聘信息；</p> 
<p>2.连接mysql，创建表格，并插入数据；</p> 
<p>3.初步清洗数据，实现可视化</p> 
<p>一、网页抓取函数</p> 
<p><a href="https://search.51job.com/list/000000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html?lang=c&amp;stype=&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;providesalary=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=" rel="nofollow">https://search.51job.com/list/000000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html?lang=c&amp;stype=&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;providesalary=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=</a></p> 
<p>这个是搜索“数据分析师”的网址，乍一看十分复杂，立马就没弄下去的欲望了。</p> 
<p>发现这么一长串url只有</p> 
<p><a href="https://search.51job.com/list/000000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html?lang=c&amp;stype=&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;providesalary=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=" rel="nofollow">https://search.51job.com/list/000000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html</a></p> 
<p>是必须的。而</p> 
<p><a href="https://search.51job.com/list/000000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html?lang=c&amp;stype=&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;providesalary=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=" rel="nofollow">%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588</a></p> 
<p>则正好是“数据分析师”的urlencode，那么我们可以利用这一点，更改搜索关键词</p> 
<p>而且<a href="https://search.51job.com/list/000000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html?lang=c&amp;stype=&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;providesalary=99&amp;lonlat=0%2C0&amp;radius=-1&amp;ord_field=0&amp;confirmdate=9&amp;fromType=&amp;dibiaoid=0&amp;address=&amp;line=&amp;specialarea=00&amp;from=&amp;welfare=" rel="nofollow">%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,1.html</a>中html前的数字对应着页码</p> 
<p>因此可以通过更改keyword和页码，进一步选择我们需要的信息。</p> 
<pre><code class="language-html hljs">def Get_html(data,i):
    Url="https://search.51job.com/list/000000,000000,0000,00,9,99,"+data+",2,%s.html"%i #data是自定义搜索关键词，i的大小则直接控制了爬取信息量
    header = {'User-Agent':'Mozilla/5.0'} #头部信息
    r=requests.get(Url,headers=header)
    print(r.url)
    print(r.status_code)
    r.encoding=r.apparent_encoding
    #print(r.text)
    return(r.text) #函数返回html</code></pre> 
<p>万里长征完成第一步o.o</p> 
<p>接下来就是解析html，进一步爬取岗位、公司、薪资、地区和url信息。</p> 
<p>这个阶段卡了2个多小时（习惯用beautifulsoup爬取）。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/f2/2c/x3IO1veh_o.jpg"></p> 
<p>代码1：</p> 
<p>def Get_data(html):<br>     soup=BeautifulSoup(html,"html.parser")<br>     for div in soup.find_all("div",class_="el"):  #find_all()遍历结果为列表<br>         print(div)</p> 
<p>输出结果：</p> 
<p>&lt;div class="el"&gt;<br> &lt;p class="t1 "&gt;<br> &lt;em class="check" name="delivery_em" οnclick="checkboxClick(this)"&gt;&lt;/em&gt;<br> &lt;input class="checkbox" jt="0" name="delivery_jobid" style="display:none" type="checkbox" value="109229565"/&gt;<br> &lt;span&gt;<br> &lt;a href="https://jobs.51job.com/shenyang/109229565.html?s=01&amp;amp;t=0" οnmοusedοwn="" target="_blank" title="数据分析师"&gt;<br>                                     数据分析师                                &lt;/a&gt;<br> &lt;/span&gt;<br> &lt;/p&gt;<br> &lt;span class="t2"&gt;&lt;a href="https://jobs.51job.com/all/co5314555.html" target="_blank" title="沈阳市和平区鹏润衣世界服装批发城"&gt;沈阳市和平区鹏润衣世界服装批发城&lt;/a&gt;&lt;/span&gt;<br> &lt;span class="t3"&gt;沈阳&lt;/span&gt;<br> &lt;span class="t4"&gt;6-8千/月&lt;/span&gt;<br> &lt;span class="t5"&gt;03-19&lt;/span&gt;<br> &lt;/div&gt;<br> 发现再怎么利用beautifulsoup的函数也无法对div子节点的属性进行爬取。（习惯了用beautifulsoup，一下子不知道怎么办了）</p> 
<p>这时候说明百度还是很重要的</p> 
<p>发现除了beautifulsoup库外，xpath也可以实现这个功能。（详细功能自行百度）</p> 
<p>代码2：</p> 
<pre><code class="language-html hljs">def Get_data(html,name):
    html = etree.HTML(html)
    divs = html.xpath("//div[@id='resultList']/div[@class='el']")
    print(divs)
    for div in divs:
        job1=div.xpath("./p/span/a/@title")
        print(job1[0])
        job_url1=div.xpath("./p/span/a/@href")
        print(job_url1)
        job_company1=div.xpath("./span[@class='t2']/a/@title")
        print(job_company1)
        job_area1=div.xpath("./span[@class='t3']/text()")
        print(job_area1)
        job_salary1=div.xpath("./span[@class='t4']/text()")
        print(job_salary1)
        try:
            job_salary.append(job_salary1[0])
            job_area.append(job_area1[0])
            shuju.append((job1[0], job_url1[0], job_company1[0], job_area1[0], job_salary1[0]))
        except:
            print("异常")
    print(shuju)
    print(job_salary,job_area)
    print(len(job_salary),len(job_area)) #查看招聘信息数量
    return(shuju,job_salary,job_area)</code></pre> 
<p>二、连接mysql数据库，创建表格，插入数据</p> 
<p>利用pymysql库</p> 
<p>代码1（创建表格函数）：</p> 
<pre><code class="language-html hljs">def Mysql_create_table(name):
    client=pymysql.connect(user="root",host="localhost",passwd="*******",db="xiaolimao")
    cursor=client.cursor()
    sql="create table if not exists table_%s"%name+"(job VARCHAR(100),job_url VARCHAR(100),job_company VARCHAR(100),job_area VARCHAR(100),job_salary VARCHAR(100));"
    cursor.execute(sql)
    cursor.close()
    client.close()</code></pre> 
<p>写入爬取数据:</p> 
<p>代码2：</p> 
<pre><code class="language-html hljs">def Mysql_data(name,shuju):
    client=pymysql.connect(user="root",host="localhost",passwd="********",db="xiaolimao")
    cursor=client.cursor()
    sql="insert into table_%s"%name+" values(%s,%s,%s,%s,%s)"
    cursor.executemany(sql,shuju)
    client.commit()
    cursor.close()
    client.close()</code></pre> 
<p>处理结果：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/07/35/LYbsTlPB_o.jpg"></p> 
<p>三、初步筛选数据，做工资分布图（地点-薪资）</p> 
<p>51job爬出的薪资数据均为string格式，且为1-2万/月或者1-2千/月格式。地点为城市或者城市-区格式</p> 
<p>首先将工资数据进行处理：</p> 
<pre><code class="language-html hljs">def Num_salary():
    for num in job_salary: #job_salary 为工资列表
        if("万/月" in num ): 
            Num=(re.findall(r"\d+\.?\d*",num))  #将工资字符串中数字提取出
            n=int((float(Num[0])+float(Num[1]))*10000/2)  #将工资区间平均化
        elif ("千/月" in num ):
            Num = (re.findall(r"\d+\.?\d*", num))
            n = int((float(Num[0]) + float(Num[1])) * 1000 / 2)
        data_salary.append(n) # 新的工资列表
        print(data_salary)
    return (data_salary)</code></pre> 
<p>然后城市处理：</p> 
<pre><code class="language-html hljs">def Num_area():
    for area in job_area:
        if "-" in area:
            Area = area.split("-")[0] #将带有区的地点只取城市
        else :
            Area=area
        data_area.append(Area)
        print(data_area)
    print(len(data_area))
    return (data_area)</code></pre> 
<p>处理结果：</p> 
<p>[20000, 9000, 6500, 6500, 15500, 9000, 7000, 5250, 7000, 9000, 12500, 7000, 9000, 7000, 5250, 10000, 9000, 8500, 7000, 6500, 7000, 9000, 18000, 9000, 7000, 6500, 12000, 12000, 12500, 11500, 11500, 8500, 5250, 7000, 7000, 7000, 6000, 12500, 7000, 5250, 5250, 9500, 9000, 9000, 7000, 6250, 6500, 7500, 4500, 10000, 7000, 15000, 14500, 8000, 25000, 7000, 12500, 27500, 9000, 25000, 15000, 8500, 7000, 5500, 24000, 10000, 10000, 6000, 16000, 11500, 12500, 12500, 17500, 7500, 11500, 9000, 7500, 12500, 7000, 7000, 11500, 11500, 12500, 6000, 12500, 5250, 11500, 7000, 12000, 9000, 12500, 10500, 12500, 7000, 6750, 6500, 9000, 9000, 10000, 5250, 5000, 5250, 7250, 9000, 7000, 6000, 9000, 9000, 9000, 9000, 7000, 12500, 4150, 6000, 5250, 9000, 17500, 20000, 20000, 5500, 8500, 4500, 7500, 8500, 6000, 5750, 22500, 11500, 11500, 5250, 12500, 7500, 7500, 5000, 17500, 9000, 19000, 15000, 12500, 5500, 11500, 11500, 6000, 6750, 11500, 12500, 12500]</p> 
<p>['杭州', '无锡', '深圳', '异地招聘', '异地招聘', '无锡', '广州', '广州', '广州', '广州', '北京', '广州', '北京', '广州', '广州', '深圳', '深圳', '广州', '广州', '上海', '广州', '南昌', '深圳', '广州', '成都', '上海', '上海', '广州', '广州', '北京', '广州', '上海', '成都', '广州', '深圳', '上海', '合肥', '上海', '武汉', '郑州', '郑州', '广州', '上海', '广州', '杭州', '佛山', '中山', '海口', '异地招聘', '深圳', '沈阳', '上海', '异地招聘', '重庆', '上海', '北京', '北京', '珠海', '上海', '上海', '广州', '上海', '西安', '上海', '深圳', '深圳', '北京', '异地招聘', '南京', '深圳', '上海', '广州', '成都', '金华', '广州', '北京', '广州', '西安', '青岛', '异地招聘', '广州', '广州', '上海', '北京', '深圳', '沈阳', '成都', '大连', '武汉', '武汉', '南京', '武汉', '上海', '广州', '上海', '西安', '广州', '宜昌', '上海', '广州', '长沙', '昆明', '杭州', '福州', '广州', '佛山', '广州', '北京', '北京', '深圳', '广州', '深圳', '苏州', '温州', '异地招聘', '广州', '武汉', '北京', '广州', '上海', '广州', '深圳', '广州', '上海', '郑州', '郑州', '广州', '深圳', '上海', '广州', '上海', '广州', '深圳', '广州', '上海', '深圳', '北京', '深圳', '杭州', '广州', '上海', '上海', '合肥', '深圳', '上海', '北京', '杭州']</p> 
<p>然后发现，在城市列表中存在“异地招聘”非城市名字符串。将城市列表中“异地招聘”元素删除并将其对应的工资列表中的元素删除。</p> 
<pre><code class="language-html hljs">def Get_rid(i):
    for j in range(i):  # 列表在删除元素时因为地址的更变会导致部分元素删除不彻底，此处加一个循环为了更彻底，i与爬取信息量有关
        for area in data_area :
            if area == "异地招聘" :
                data_salary.pop(data_area.index(area))
                data_area.remove(area)</code></pre> 
<p>处理结果：</p> 
<p>[20000, 9000, 6500, 9000, 7000, 5250, 7000, 9000, 12500, 7000, 9000, 7000, 5250, 10000, 9000, 8500, 7000, 6500, 7000, 9000, 18000, 9000, 7000, 6500, 12000, 12000, 12500, 11500, 11500, 8500, 5250, 7000, 7000, 7000, 6000, 12500, 7000, 5250, 5250, 9500, 9000, 9000, 7000, 6250, 6500, 7500, 10000, 7000, 15000, 8000, 25000, 7000, 12500, 27500, 9000, 25000, 15000, 8500, 7000, 5500, 24000, 10000, 10000, 16000, 11500, 12500, 12500, 17500, 7500, 11500, 9000, 7500, 12500, 7000, 11500, 11500, 12500, 6000, 12500, 5250, 11500, 7000, 12000, 9000, 12500, 10500, 12500, 7000, 6750, 6500, 9000, 9000, 10000, 5250, 5000, 5250, 7250, 9000, 7000, 6000, 9000, 9000, 9000, 9000, 7000, 12500, 4150, 6000, 9000, 17500, 20000, 20000, 5500, 8500, 4500, 7500, 8500, 6000, 5750, 22500, 11500, 11500, 5250, 12500, 7500, 7500, 5000, 17500, 9000, 19000, 15000, 12500, 5500, 11500, 11500, 6000, 6750, 11500, 12500, 12500] ['杭州', '无锡', '深圳', '无锡', '广州', '广州', '广州', '广州', '北京', '广州', '北京', '广州', '广州', '深圳', '深圳', '广州', '广州', '上海', '广州', '南昌', '深圳', '广州', '成都', '上海', '上海', '广州', '广州', '北京', '广州', '上海', '成都', '广州', '深圳', '上海', '合肥', '上海', '武汉', '郑州', '郑州', '广州', '上海', '广州', '杭州', '佛山', '中山', '海口', '深圳', '沈阳', '上海', '重庆', '上海', '北京', '北京', '珠海', '上海', '上海', '广州', '上海', '西安', '上海', '深圳', '深圳', '北京', '南京', '深圳', '上海', '广州', '成都', '金华', '广州', '北京', '广州', '西安', '青岛', '广州', '广州', '上海', '北京', '深圳', '沈阳', '成都', '大连', '武汉', '武汉', '南京', '武汉', '上海', '广州', '上海', '西安', '广州', '宜昌', '上海', '广州', '长沙', '昆明', '杭州', '福州', '广州', '佛山', '广州', '北京', '北京', '深圳', '广州', '深圳', '苏州', '温州', '广州', '武汉', '北京', '广州', '上海', '广州', '深圳', '广州', '上海', '郑州', '郑州', '广州', '深圳', '上海', '广州', '上海', '广州', '深圳', '广州', '上海', '深圳', '北京', '深圳', '杭州', '广州', '上海', '上海', '合肥', '深圳', '上海', '北京', '杭州']</p> 
<p>非城市名字符串全部删除</p> 
<p>最后成图：</p> 
<pre><code class="language-html hljs">def Geo_chart(name): #利用pyecharts库的Geo函数做图
    data_geo=[]
    data_salary_max=np.max(data_salary) #利用工资列表最大值做为比例尺最大值
    print(data_salary_max)
    for i in range(len(data_area)-1):
        data_geo.append((data_area[i],data_salary[i]))
    geo=Geo("%s"%name+"薪资分布图","data from 51job", title_color="#fff", title_pos="center",width=1200, height=600, background_color='#404a59')
    attr,value=geo.cast(data_geo)
    geo.add("", attr, value, visual_range=[0, data_salary_max], visual_text_color="#fff", symbol_size=15, is_visualmap=True)
    geo.show_config()
    geo.render() #可以自定义保存位置</code></pre> 
<p>成果图：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/dc/4b/Kz2xct4g_o.png"></p> 
<p>大功告成！！！</p> 
<p>总结：</p> 
<p>从开始做一直到成功，花了接近一天时间。中间多次想要放弃，不过好在是坚持了下来。</p> 
<p>好多东西真的是在自己动手做的时候才发现没有想象中的简单。</p> 
<p>参考文章：</p> 
<p>1.<a href="https://blog.csdn.net/legalhighhigh/article/details/79779832">https://blog.csdn.net/legalhighhigh/article/details/79779832</a></p> 
<p>2.</p> 
<p> </p> 
<p>贴上完整代码：</p> 
<pre><code class="language-html hljs">import requests
import re
from lxml import etree
import pymysql
from pyecharts import Geo
import numpy as np
job_salary=[]
job_area=[]
shuju=[]
data_salary=[]
data_area=[]

def Geo_chart(name):
    data_geo=[]
    data_salary_max=np.max(data_salary)
    print(data_salary_max)
    for i in range(len(data_area)-1):
        data_geo.append((data_area[i],data_salary[i]))
    geo=Geo("%s"%name+"薪资分布图","data from 51job", title_color="#fff", title_pos="center",width=1200, height=600, background_color='#404a59')
    attr,value=geo.cast(data_geo)
    geo.add("", attr, value, visual_range=[0, data_salary_max], visual_text_color="#fff", symbol_size=15, is_visualmap=True)
    geo.show_config()
    geo.render()


def Get_html(data,i):
    Url="https://search.51job.com/list/000000,000000,0000,00,9,99,"+data+",2,%s.html"%i
    header = {'User-Agent':'Mozilla/5.0'}
    r=requests.get(Url,headers=header)
    print(r.url)
    print(r.status_code)
    r.encoding=r.apparent_encoding
    #print(r.text)
    return(r.text)    
def Mysql_create_table(name):
    client=pymysql.connect(user="root",host="localhost",passwd="rongchao123",db="xiaolimao")
    cursor=client.cursor()
    sql="create table if not exists table_%s"%name+"(job VARCHAR(100),job_url VARCHAR(100),job_company VARCHAR(100),job_area VARCHAR(100),job_salary VARCHAR(100));"
    cursor.execute(sql)
    cursor.close()
    client.close()

def Mysql_data(name,shuju):
    client=pymysql.connect(user="root",host="localhost",passwd="rongchao123",db="xiaolimao")
    cursor=client.cursor()
    sql="insert into table_%s"%name+" values(%s,%s,%s,%s,%s)"
    cursor.executemany(sql,shuju)
    client.commit()
    cursor.close()
    client.close()

def Get_data(html,name):
    html = etree.HTML(html)
    divs = html.xpath("//div[@id='resultList']/div[@class='el']")
    print(divs)
    for div in divs:
        job1=div.xpath("./p/span/a/@title")
        print(job1[0])
        job_url1=div.xpath("./p/span/a/@href")
        print(job_url1)
        job_company1=div.xpath("./span[@class='t2']/a/@title")
        print(job_company1)
        job_area1=div.xpath("./span[@class='t3']/text()")
        print(job_area1)
        job_salary1=div.xpath("./span[@class='t4']/text()")
        print(job_salary1)
        try:
            job_salary.append(job_salary1[0])
            job_area.append(job_area1[0])
            shuju.append((job1[0], job_url1[0], job_company1[0], job_area1[0], job_salary1[0]))
        except:
            print("异常")
    print(shuju)
    print(job_salary,job_area)
    print(len(job_salary),len(job_area))
    return(shuju,job_salary,job_area)

def Num_salary():
    for num in job_salary: #job_salary 为工资列表
        if("万/月" in num ):
            Num=(re.findall(r"\d+\.?\d*",num))  #将工资字符串中数字提取出
            n=int((float(Num[0])+float(Num[1]))*10000/2)  #将工资区间平均化
        elif ("千/月" in num ):
            Num = (re.findall(r"\d+\.?\d*", num))
            n = int((float(Num[0]) + float(Num[1])) * 1000 / 2)
        data_salary.append(n) # 新的工资列表
        print(data_salary)
    return (data_salary)

def Num_area():
    for area in job_area:
        if "-" in area:
            Area = area.split("-")[0] #将带有区的地点只取城市
        else :
            Area=area
        data_area.append(Area)
        print(data_area)
    print(len(data_area))
    return (data_area)

def Get_rid(i):
    for j in range(i):  # 列表在删除元素时因为地址的更变会导致部分元素删除不彻底，此处加一个循环为了更彻底，i与爬取信息量有关
        for area in data_area :
            if area == "异地招聘" :
                data_salary.pop(data_area.index(area))
                data_area.remove(area)            
        
if __name__=="__main__":
    kywd=input("关键词：")
    name=input("表名：")
    Number=int(input("请输入一个整数："))
    Mysql_create_table(name)
    for i in range(1,Number):
        html=Get_html(kywd,i)
        Get_data(html,name)
    Mysql_data(name,shuju)
    Num_salary()
    Num_area()
    Get_rid(Number)
    print(data_salary,data_area)
    Geo_chart(kywd)</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c4ce5f00fc5b37e741a575e4af120b42/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">在python中round()方法的作用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4a132de51d9a7dcebf137425a7c847db/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">在notepad&#43;&#43;运行python程序</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>