<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pyspark streaming简介 和 消费 kafka示例 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pyspark streaming简介 和 消费 kafka示例" />
<meta property="og:description" content="简介 并不是真正的实时处理框架，只是按照时间进行微批处理进行，时间可以设置的尽可能的小。
将不同的额数据源的数据经过SparkStreaming 处理之后将结果输出到外部文件系统
特点 低延时
能从错误中搞笑的恢复: fault-tolerant
能够运行在成百上千的节点
能够将批处理、机器学习、图计算等自框架和Spark Streaming 综合起来使用
粗粒度 Spark Streaming接收到实时数据流，把数据按照指定的时间段切成一片片小的数据块，然后把小的数据块传给Spark Engine处理。
细粒度
数据源
kafka提供了两种数据源。
基础数据源，可以直接通过streamingContext API实现。如文件系统和socket连接高级的数据源，如Kafka, Flume, Kinesis等等. 可以通过额外的类库去实现。 基础数据源 使用官方的案例 /spark/examples/src/main/python/streaming
nc -lk 6789
处理socket数据
示例代码如下: 读取socket中的数据进行流处理
from pyspark import SparkContext from pyspark.streaming import StreamingContext # local 必须设为2 sc = SparkContext(&#34;local[2]&#34;, &#34;NetworkWordCount&#34;) ssc = StreamingContext(sc, 1) lines = ssc.socketTextStream(&#34;localhost&#34;, 9999) words = lines.flatMap(lambda line: line.split(&#34; &#34;)) pairs = words.map(lambda word: (word, 1)) wordCounts = pairs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/cd78f340dfd2a18b7d0d10bfe312746f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-01T23:54:20+08:00" />
<meta property="article:modified_time" content="2022-11-01T23:54:20+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pyspark streaming简介 和 消费 kafka示例</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>简介</h2> 
<blockquote> 
 <p>并不是真正的实时处理框架，只是按照时间进行微批处理进行，时间可以设置的尽可能的小。</p> 
</blockquote> 
<blockquote> 
 <p>将不同的额数据源的数据经过SparkStreaming 处理之后将结果输出到外部文件系统</p> 
</blockquote> 
<ul><li>特点</li></ul> 
<blockquote> 
 <p>低延时<br> 能从错误中搞笑的恢复: fault-tolerant<br> 能够运行在成百上千的节点<br> 能够将批处理、机器学习、图计算等自框架和Spark Streaming 综合起来使用</p> 
</blockquote> 
<ul><li>粗粒度</li></ul> 
<blockquote> 
 <p>Spark Streaming接收到实时数据流，把数据按照指定的时间段切成一片片小的数据块，然后把小的数据块传给Spark Engine处理。</p> 
</blockquote> 
<ul><li> <p>细粒度<br> <img src="https://images2.imgbox.com/e9/e3/1udyIT06_o.png" alt="在这里插入图片描述"></p> </li><li> <p>数据源<br> kafka提供了两种数据源。</p> </li></ul> 
<ol><li>基础数据源，可以直接通过streamingContext API实现。如<code>文件系统</code>和<code>socket连接</code></li><li>高级的数据源，如Kafka, Flume, Kinesis等等. 可以通过额外的类库去实现。</li></ol> 
<h3><a id="_25"></a>基础数据源</h3> 
<ol><li>使用官方的案例</li></ol> 
<p>/spark/examples/src/main/python/streaming</p> 
<p>nc -lk 6789</p> 
<ol start="2"><li>处理socket数据<br> <img src="https://images2.imgbox.com/fe/d1/xdIXILn8_o.png" alt="在这里插入图片描述"></li></ol> 
<p>示例代码如下: 读取socket中的数据进行流处理</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext

<span class="token comment"># local 必须设为2</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[2]"</span><span class="token punctuation">,</span> <span class="token string">"NetworkWordCount"</span><span class="token punctuation">)</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span>socketTextStream<span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">9999</span><span class="token punctuation">)</span>

words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

pairs <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
wordCounts <span class="token operator">=</span> pairs<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> x <span class="token operator">+</span> y<span class="token punctuation">)</span>

wordCounts<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>

ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>测试</p> 
<blockquote> 
 <p>nc -lk 9999</p> 
</blockquote> 
<ol start="3"><li>处理文件系统数据</li></ol> 
<blockquote> 
 <p>文件系统(fileStream(that is, HDFSM S3, NFS))暂不支持python，python仅支持文本文件(textFileStream)</p> 
</blockquote> 
<p>示例如下，但未成功，找不到该文件。</p> 
<pre><code class="prism language-python">
lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span>textFileStream<span class="token punctuation">(</span><span class="token string">"hdfs://txz-data0:9820/user/jim/workflow/crash/python/crash_2_hdfs.py"</span><span class="token punctuation">)</span>

</code></pre> 
<ul><li> <p>streaming context</p> </li><li> <p>DStreams</p> </li></ul> 
<blockquote> 
 <p>持续化的数据流<br> 对DStream操作算子， 比如map/flatMap,其实底层会被翻译为对DStream中的每个RDD都做相同的操作，因为一个DStream是由不同批次的RDD所</p> 
</blockquote> 
<ul><li>Input DStreams and Receivers</li></ul> 
<h3><a id="_83"></a>高级数据源</h3> 
<h4><a id="Spark_Streaming__kafka__85"></a>Spark Streaming 和 kafka 整合</h4> 
<p>两种模式</p> 
<ul><li>receiver 模式</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>kafka <span class="token keyword">import</span> KafkaUtils
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext

sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[2]"</span><span class="token punctuation">,</span> <span class="token string">"NetworkWordCount"</span><span class="token punctuation">)</span>
sc<span class="token punctuation">.</span>setLogLevel<span class="token punctuation">(</span><span class="token string">"OFF"</span><span class="token punctuation">)</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 创建Kafka streaming</span>
line <span class="token operator">=</span> KafkaUtils<span class="token punctuation">.</span>createStream<span class="token punctuation">(</span>ssc<span class="token punctuation">,</span> <span class="token string">"192.168.0.208:2181"</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">"jim_test"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment"># 分词</span>
words <span class="token operator">=</span> line<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
pairs <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
wordCounts <span class="token operator">=</span> pairs<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> x <span class="token operator">+</span> y<span class="token punctuation">)</span>
wordCounts<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>

ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>


</code></pre> 
<ul><li>no receiver</li></ul> 
<p>根据上面的代码替换掉createStream即可。</p> 
<pre><code class="prism language-python">line <span class="token operator">=</span> KafkaUtils<span class="token punctuation">.</span>createDirectStream<span class="token punctuation">(</span>ssc<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">"jim_test"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">"metadata.broker.list"</span><span class="token punctuation">:</span> <span class="token string">"192.168.0.208:9092"</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>运行:</p> 
<blockquote> 
 <p>spark-submit --jars spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar test_spark_stream.py</p> 
</blockquote> 
<p>需要下载相应的jar包.下载地址如下，搜索。<br> https://search.maven.org</p> 
<p>jar版本会在运行程序时报错提醒。</p> 
<h2><a id="_130"></a>欢迎关注，互相学习，共同进步~</h2> 
<p>我的<a href="https://www.zhengwenfeng.com/" rel="nofollow">个人博客</a></p> 
<p>我的微信公众号：编程黑洞</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e2dcd2e1c2282ce99984ab51a2ca643e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">k8s之StatefulSet</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/08f8bb76f93c8ea8d8ebde095cb6431b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">os.path.abspath() 和 os.path.realpath() 区别</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>