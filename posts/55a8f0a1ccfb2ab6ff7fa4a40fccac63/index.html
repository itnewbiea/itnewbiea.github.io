<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>玩转C#网页抓取 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="玩转C#网页抓取" />
<meta property="og:description" content="网页抓取是通过自动化手段检索数据的过程。它在许多场景中都是不可或缺的，例如竞争对手价格监控、房地产清单列表、潜在客户和舆情监控、新闻文章或金融数据聚合等。 如果您想了解更多相关信息，可以前往Oxylabs中文官网Oxylabs.cn参见我们的文章“网络抓取合法吗？”
在编写网页抓取代码时，您要做出的第一个决定是选择您的编程语言。您可以使用多种语言进行编写，例如Python、JavaScript、Java、Ruby或C#。所有提到的语言都提供强大的网络抓取功能。
在本文中，我们将探索C#并向您展示如何创建一个真实的C#公共网络爬虫。请记住，即使我们使用C#，您也可以将此信息调整为.NET平台支持的所有语言，包括VB.NET和F#。
01.C#网页抓取工具 在编写任何代码之前，第一步是选择合适的C#库或包。这些C#库或包将具有下载HTML页面、解析它们以及从这些页面中提取所需数据的功能。一些最流行的C#包如下：
●ScrapySharp
●Puppeteer Sharp
●Html Agility Pack
Html Agility Pack是最受欢迎的C#包，仅Nuget就有近5,000万次下载。其流行有多种原因，其中最重要的原因是该HTML解析器能够直接或使用浏览器下载网页。这个包可以容忍格式错误的HTML并支持XPath。此外，它甚至可以解析本地HTML文件；因此，我们将在本文中进一步使用这个包。
ScrapySharp为C#编程添加了更多功能。这个包支持CSS选择器并且可以模拟网络浏览器。虽然ScrapySharp被认为是一个强大的C#包，但程序员使用它进行维护的概率并不是很高。
Puppeteer Sharp是著名的Node.js Puppeteer项目的.NET端口。它使用相同的Chromium浏览器来加载页面。此外，这个包采用了async-await风格的代码，支持异步及预操作管理。如果您已经熟悉这个C#包并且需要一个浏览器来呈现页面，那么Puppeteer Sharp可能是一个不错的选择。
02.使用C#构建网络爬虫 如前所述，现在我们将演示如何编写将使用Html Agility Pack的C#公共网络抓取代码。我们将使用带有Visual Studio Code的.NET 5 SDK。此代码已在 .NET Core 3和.NET 5上测试过，它应该适用于其他版本的.NET。
我们将设置一个假设的场景：爬取一家在线书店并收集书名和价格。
在编写C#网络爬虫之前，我们先来设置下开发环境。
03.设置开发环境 对于C#开发环境，请安装Visual Studio Code。请注意，如果您使用Visual Studio和Visual Studio Code编写C#代码，则需要注意它们是两个完全不同的应用程序。
安装Visual Studio Code后，安装.NET 5.0或更高版本。您还可以使用.NET Core 3.1。安装完成后，打开终端并运行以下命令以验证.NET CLI或命令行界面是否正常工作：
dotnet --version 该行命令会输出安装的.NET的版本号。
04.项目结构和依存关系 该代码将成为.NET项目的一部分。为简单起见，创建一个控制台应用程序。然后，创建一个文件夹，您将在其中编写C#代码。打开终端并导航到该文件夹。输入以下命令：
dotnet new console 此命令的输出应该是已成功创建控制台应用程序的信息。
到时间安装所需的软件包了。使用C#抓取公共网页，Html Agility Pack将是一个不错的选择。您可以使用以下命令为该项目安装它：
dotnet add package HtmlAgilityPack 再安装一个包，以便我们可以轻松地将抓取的数据导出到CSV文件：
dotnet add package CsvHelper 如果您使用的是Visual Studio而不是Visual Studio Code，请单击文件，选择新建解决方案，然后按控制台应用程序按钮。要安装依赖项，请按照下列步骤操作：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/55a8f0a1ccfb2ab6ff7fa4a40fccac63/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-12T14:09:17+08:00" />
<meta property="article:modified_time" content="2022-07-12T14:09:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">玩转C#网页抓取</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/56/d7/MlRejVZJ_o.png"></p> 
<p>网页抓取是<strong>通过自动化手段检索数据</strong>的过程。它在许多场景中都是不可或缺的，例如竞争对手价格监控、房地产清单列表、潜在客户和舆情监控、新闻文章或金融数据聚合等。 如果您想了解更多相关信息，可以前往Oxylabs中文官网<a href="http://oxylabs.cn/" rel="nofollow" title="Oxylabs.cn">Oxylabs.cn</a>参见我们的文章“<a href="https://oxylabs.io/blog/is-web-scraping-legal" rel="nofollow" title="网络抓取合法吗？">网络抓取合法吗？</a>”</p> 
<p>在编写网页抓取代码时，您要做出的<strong>第一个决定是选择您的编程语言</strong>。您可以使用多种语言进行编写，例如<strong>Python、JavaScript、Java、Ruby或C#</strong>。所有提到的语言都提供强大的网络抓取功能。</p> 
<p>在本文中，我们将<strong>探索C#</strong>并向您展示<strong>如何创建一个真实的C#公共网络爬虫</strong>。请记住，即使我们使用C#，您也可以将此信息调整为<strong>.NET</strong>平台支持的所有语言，包括<strong>VB.NET和F#</strong>。</p> 
<p></p> 
<h2>01.C#网页抓取工具</h2> 
<p>在编写任何代码之前，第一步是<strong>选择合适的C#库或包</strong>。这些C#库或包将具有下载HTML页面、解析它们以及从这些页面中提取所需数据的功能。一些最流行的C#包如下：</p> 
<p><strong>●ScrapySharp</strong></p> 
<p><strong>●Puppeteer Sharp</strong></p> 
<p><strong>●Html Agility Pack</strong></p> 
<p></p> 
<p><strong>Html Agility Pack</strong>是最受欢迎的C#包，仅Nuget就有近5,000万次下载。其流行有多种原因，其中最重要的原因是<strong>该HTML解析器能够直接或使用浏览器下载网页</strong>。这个包可以容忍格式错误的HTML并支持XPath。此外，它甚至可以解析本地HTML文件；因此，我们将在本文中进一步使用这个包。</p> 
<p><strong>ScrapySharp</strong>为C#编程添加了更多功能。这个包支持CSS选择器并且可以模拟网络浏览器。虽然<strong>ScrapySharp</strong>被认为是一个强大的C#包，但程序员使用它进行维护的概率并不是很高。</p> 
<p><strong>Puppeteer Sharp是著名的Node.js Puppeteer项目的.NET端口</strong>。它使用相同的Chromium浏览器来加载页面。此外，这个包采用了async-await风格的代码，支持异步及预操作管理。如果您已经熟悉这个C#包并且需要一个浏览器来呈现页面，那么<strong>Puppeteer Sharp</strong>可能是一个不错的选择。</p> 
<p></p> 
<p></p> 
<h2>02.使用C#构建网络爬虫</h2> 
<p>如前所述，现在我们将演示<strong>如何编写将使用Html Agility Pack的C#公共网络抓取代码</strong>。我们将使用带有<strong>Visual Studio Code</strong>的<strong>.NET 5 SDK</strong>。此代码已在<strong> .NET Core 3</strong>和<strong>.NET 5</strong>上测试过，它应该适用于其他版本的.NET。</p> 
<p>我们将设置一个假设的场景：爬取一家<a href="http://books.toscrape.com/catalogue/" rel="nofollow" title="在线书店">在线书店</a>并收集书名和价格。</p> 
<p>在编写C#网络爬虫之前，我们先来设置下开发环境。</p> 
<p></p> 
<p></p> 
<h2>03.设置开发环境</h2> 
<p>对于C#开发环境，请安装<strong>Visual Studio Code</strong>。请注意，如果您使用<strong>Visual Studio</strong>和<strong>Visual Studio Code</strong>编写C#代码，则需要注意它们是两个完全不同的应用程序。</p> 
<p></p> 
<p>安装<strong>Visual Studio Code</strong>后，安装<strong>.NET 5.0</strong>或更高版本。您还可以使用<strong>.NET Core 3.1</strong>。安装完成后，打开终端并运行以下命令以验证<strong>.NET CLI</strong>或命令行界面是否正常工作：</p> 
<pre><code>dotnet --version</code></pre> 
<p>该行命令<strong>会输出安装的.NET的版本号</strong>。</p> 
<p></p> 
<p></p> 
<h2>04.项目结构和依存关系</h2> 
<p>该代码将成为.NET项目的一部分。为简单起见，创建一个<strong>控制台应用程序</strong>。然后，创建一个文件夹，您将在其中编写C#代码。打开终端并导航到该文件夹。输入以下命令：</p> 
<pre><code>dotnet new console</code></pre> 
<p>此命令的输出应该是已成功创建控制台应用程序的信息。</p> 
<p></p> 
<p>到时间安装所需的软件包了。使用C#抓取公共网页，<strong>Html Agility Pack</strong>将是一个不错的选择。您可以使用以下命令为该项目安装它：</p> 
<pre><code>dotnet add package HtmlAgilityPack</code></pre> 
<p>再安装一个包，以便我们可以轻松地将抓取的数据导出到CSV文件：</p> 
<pre><code>dotnet add package CsvHelper</code></pre> 
<p>如果您使用的是<strong>Visual Studio</strong>而不是<strong>Visual Studio Code</strong>，请单击文件，选择新建解决方案，然后按控制台应用程序按钮。要安装依赖项，请按照下列步骤操作：</p> 
<p>●选择项目；</p> 
<p>●单击管理项目依赖项。这将打开NuGet包窗口；</p> 
<p>●搜索<strong>HtmlAgilityPack</strong>并选择它；</p> 
<p>●最后，搜索<strong>CsvHelper</strong>，选择它，然后单击添加包。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/71/09/4S1xlIV6_o.png"></p> 
<p style="text-align:center;">Visual Studio中的Nuget包管理器</p> 
<p>安装了这些包后，我们可以继续编写用于抓取线上书店的代码。</p> 
<p></p> 
<h2>05.下载和解析网页数据</h2> 
<p>任何网页抓取程序的第一步都是<strong>下载网页的HTML</strong>。此HTML将是一个字符串，您需要将其<strong>转换为可以进一步处理的对象</strong>，也就是第二步，这部分称为<strong>解析</strong>。<strong>Html Agility Pack</strong>可以从本地文件、HTML字符串、任何URL和浏览器读取和解析文件。</p> 
<p></p> 
<p>在我们的例子中，我们需要做的就是从URL获取HTML。<strong>Html Agility Pack</strong>没有使用<strong>.NET</strong>本机函数，而是提供了一个方便的类–<strong>HtmlWeb</strong>.这个类提供了一个<strong>Load</strong>函数，它可以接受一个URL并返回一个<strong>HtmlDocument</strong>类的实例，它也是我们使用的包的一部分。有了这些信息，我们可以编写一个函数，接受一个URL并返回<strong>HtmlDocument</strong>这个实例。</p> 
<p></p> 
<p>打开<strong>Program.cs</strong>文件并在类中输入此函数<strong>Program</strong>：</p> 
<pre><code>// Parses the URL and returns HtmlDocument object                         
static HtmlDocument GetDocument (string url)
{
  HtmlWeb web = new HtmlWeb();
  HtmlDocument doc = web.Load(url);
  return doc;
}</code></pre> 
<p>这样，代码的第一步就完成了。下一步是解析文档。</p> 
<p></p> 
<p></p> 
<h2>06.解析HTML：获取书籍链接</h2> 
<p>在这部分代码中，我们将从网页中提取所需的信息。在这个阶段，文档现在是一个类型的对象<strong>HtmlDocument</strong>。这个类公开了两个函数来选择元素。这两个函数都接受<strong>XPath</strong>输入并返回<strong>HtmlNode</strong> or <strong>HtmlNodeCollection</strong>。</p> 
<p></p> 
<p>下面是这两个函数的签名：</p> 
<pre><code>public HtmlNodeCollection SelectNodes(string xpath);
public HtmlNode SelectSingleNode(string xpath);</code></pre> 
<p>我们就SelectNodes先讨论一下。</p> 
<p>对于这个例子——C#网络爬虫——我们将从<a href="http://books.toscrape.com/catalogue/" rel="nofollow" title="这个页面">这个页面</a>中抓取所有书籍的详细信息。</p> 
<p></p> 
<p>首先，需要对其进行解析，以便可以提取到所有书籍的链接。在浏览器中打开上述的书店页面，右键单击任何书籍链接，然后单击按钮“检查”。将打开开发人员工具。</p> 
<p></p> 
<p>在了解标记后，您要选择的XPath应该是这样的：</p> 
<pre><code>//h3/a</code></pre> 
<p>现在可以将此XPath传递给SelectNodes函数。</p> 
<pre><code>HtmlDocument doc = GetDocument(url); 
HtmlNodeCollection linkNodes = doc.DocumentNode.SelectNodes("//h3/a");</code></pre> 
<p>请注意，该<strong>SelectNodes</strong>函数是由</p> 
<p><strong>HtmlDocument</strong>的<strong>DocumentNode</strong>属性调用的。</p> 
<p></p> 
<p>变量<strong>linkNodes</strong>是一个集合。我们可以写一个<strong>foreach循环</strong>，并从每个链接一个一个地获取<strong>href值</strong>。我们只需要解决一个小问题——那就是页面上的链接是相对链接。因此，在我们抓取这些提取的链接之前，需要将它们转换为绝对URL。</p> 
<p></p> 
<p>为了转换相对链接，我们可以使用<strong>Uri该类</strong>。我们使用此构造函数来获取Uri具有绝对URL的对象。</p> 
<pre><code>dotnet --version</code></pre> 
<p>一旦我们有了Uri对象，我们就可以简单地检查该<strong>AbsoluteUri属性</strong>以获取完整的URL。</p> 
<p></p> 
<p>我们将所有这些写在一个函数中，以保持代码的组织性。</p> 
<pre><code>static List&lt;string&gt; GetBookLinks(string url)
{
    var bookLinks = new List&lt;string&gt;();
    HtmlDocument doc = GetDocument(url);
    HtmlNodeCollection linkNodes = doc.DocumentNode.SelectNodes("//h3/a");
    var baseUri = new Uri(url);
    foreach (var link in linkNodes)
    {
        string href = link.Attributes["href"].Value;
        bookLinks.Add(new Uri(baseUri, href).AbsoluteUri);
    }
    return bookLinks;
}</code></pre> 
<p>在这个函数中，我们从一个空<strong>List&lt;string&gt;对象</strong>开始。在<strong>foreach</strong>循环中，我们将所有链接添加到此对象并返回它。</p> 
<p></p> 
<p>现在，就可以修改<strong>Main()</strong>函数了，以便我们可以测试到目前为止编写的C#代码。修改函数如下：</p> 
<pre><code>static void Main(string[] args)

{
  var bookLinks = GetBookLinks("http://books.toscrape.com/catalogue/category/books/mystery_3/index.html");
  Console.WriteLine("Found {0} links", bookLinks.Count);
}</code></pre> 
<p>要运行此代码，请打开终端并导航到包含此文件的目录，然后键入以下内容：</p> 
<pre><code>dotnet run</code></pre> 
<p>输出应如下所示：</p> 
<pre><code>Found 20 links</code></pre> 
<p>然后我们转到下一部分，我们将处理所有链接以获取图书数据。</p> 
<p></p> 
<p></p> 
<h2>07.解析HTML：获取书籍详细信息</h2> 
<p>此时，我们有一个包含书籍URL的字符串列表。我们可以简单地编写一个循环，首先使用我们已经编写的函数<strong>GetDocument</strong>来获取文档。之后，我们将使用该<strong>SelectSingleNode</strong>函数来提取书名和价格。</p> 
<p></p> 
<p>为了让数据清晰有条理，我们从一个类开始。这个类将代表一本书，有两个属性-<strong>Title</strong>和<strong>Price</strong>.示例如下：</p> 
<pre><code>public class Book
{
  public string Title { get; set; }
  public string Price { get; set; }
}</code></pre> 
<p>然后，为<strong>Title – //h1</strong>在浏览器中打开一个书页。为价格创建 XPath 有点棘手，因为底部的附加书籍应用了相同的类。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ce/34/roQCwNC6_o.png"></p> 
<p style="text-align:center;">价格的XPath</p> 
<p>价格的XPath将是这样的：</p> 
<pre><code>//div[contains(@class,"product_main")]/p[@class="price_color"]</code></pre> 
<p>请注意，XPath<strong>包含双引号</strong>。我们将不得不通过在它们前面加上反斜杠来转义这些字符。</p> 
<p></p> 
<p>现在我们可以使用<strong>SelectSingleNode</strong>函数来获取节点，然后使用<strong>InnerText</strong>属性获取元素中包含的文本。我们可以将所有内容放在一个函数中，如下所示：</p> 
<pre><code>static List&lt;Book&gt; GetBookDetails(List&lt;string&gt; urls)
{
var books = new List&lt;Book&gt;();
foreach (var url in urls)
   {
    HtmlDocument document = GetDocument(url);
    var titleXPath = "//h1";
    var priceXPath = "//div[contains(@class,\"product_main\")]/p[@class=\"price_color\"]";
    var book = new Book();
    book.Title = document.DocumentNode.SelectSingleNode (priceXPath).InnerText;
    book.Price = document.DocumentNode.SelectSingleNode(priceXPath).InnerText;
    books.Add(book);
    }
return books;
}</code></pre> 
<p>此函数将返回一个<strong>Book对象</strong>列表。是时候更新<strong>Main()</strong>函数了：</p> 
<pre><code>static void Main(string[] args)
{
  var bookLinks = GetBookLinks("http://books.toscrape.com/catalogue/category/books/mystery_3/index.html");
  Console.WriteLine("Found {0} links", bookLinks.Count);
  var books = GetBookDetails(bookLinks);
}</code></pre> 
<p>这个网络抓取项目的最后一部分是<strong>将数据导出为CSV</strong>。</p> 
<p></p> 
<p></p> 
<h2>08.导出数据</h2> 
<p>如果您尚未安装<strong>CsvHelper</strong>，则可以通过</p> 
<pre><code>dotnet add package CsvHelper</code></pre> 
<p>在终端内运行命令来完成此操作。</p> 
<p></p> 
<p>导出功能非常简单。首先，我们需要创建一个<strong>StreamWriter</strong>并发送CSV文件名作为参数。接下来，我们将使用此对象创建一个<strong>CsvWriter</strong>.最后，我们可以使用该<strong>WriteRecords</strong>函数在一行代码中编写所有书籍。</p> 
<p></p> 
<p>为了确保所有资源都正确关闭，我们可以使用<strong>using</strong>块。我们还可以将所有内容包装在一个函数中，如下所示：</p> 
<pre><code>static void exportToCSV(List&lt;Book&gt; books)
{
using (var writer = new StreamWriter("./books.csv"))
using (var csv = new CsvWriter(writer, CultureInfo.InvariantCulture))
    {
    csv.WriteRecords(books);
    }
}</code></pre> 
<p>最后，我们可以从<strong>Main()</strong>函数中调用这个函数：</p> 
<pre><code>static void Main(string[] args)
{
  var bookLinks = GetBookLinks("http://books.toscrape.com/catalogue/category/books/mystery_3/index.html");
  var books = GetBookDetails(bookLinks);
  exportToCSV(books);
}</code></pre> 
<pre></pre> 
<p>要运行此代码，请打开终端并运行以下命令：</p> 
<pre><code>dotnet run</code></pre> 
<p>在几秒钟内，您将创建一个<strong>books.csv</strong>文件。</p> 
<p></p> 
<h2>09.结论</h2> 
<p>如果您想用C#编写一个网络爬虫，您可以使用多个包。在本文中，我们展示了如何使用Html Agility Pack，这是一个功能强大且易于使用的包。也是一个可以进一步增强的简单示例；例如，您可以尝试将上述逻辑添加到此代码中以处理多个页面。</p> 
<p></p> 
<p>如果您想了解更多有关使用其他编程语言进行网络抓取的工作原理，可以查看使用Python进行网络抓取的指南。我们还有一个<a href="https://oxylabs.io/blog/javascript-web-scraping" rel="nofollow" title="关于如何使用JavaScript编写网络爬虫的分步教程">关于如何使用JavaScript编写网络爬虫的分步教程</a></p> 
<p></p> 
<p></p> 
<p></p> 
<p>常见问题</p> 
<p>Q：C#适合网页抓取吗？</p> 
<p>A：与Python类似，C#被广泛用于网页抓取。在决定选择哪种编程语言时，选择您最熟悉的一种至关重要。不过您将能够在Python和C#中找到示例的网页抓取工具。</p> 
<p></p> 
<p>Q：网络抓取合法吗？</p> 
<p>A：如果在不违反任何法律的情况下使用代理，则它们可能是合法的。然而，在与代理进行任何活动之前，您应该就您的特定案件获得专业的法律建议。</p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/658823ce84222b8e089a85cff3eed1f7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">不求人小白也能搭建私有云盘，焕然一新体验</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/92858fbe1fc5ce09de4290b07437fdc0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Typora输入卡顿解决办法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>