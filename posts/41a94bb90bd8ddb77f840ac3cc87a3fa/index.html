<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>swin transformer 总结 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="swin transformer 总结" />
<meta property="og:description" content="1. 背景介绍 原名：Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
获奖：2021 ICCV Best Paper
2.文章介绍 2.1概括 Swin Transformer 是一种新型的transformer，可以用作视觉和语言处理的统一模型
特性
- 引出了一种具有层级的特征表达方式（基于self-attentation的shifted window）
- 具有线性的计算复杂度（和输入的图像尺寸相关）性能（state of the art）
- COCO目标检测
- ADE20K语义分割 2.2 创新点介绍 NLP和CV的不同点：
（1）CV包含不同的尺度，而NLP 把tokens作为transformer的基本元素。当前的cv中的transformer都是固定尺度的，不适合用作CV的任务。
（2）CV处理的是高分辨率的图像，而NLP处理的是位于文本段落中的单词。
2.2.1 层级特征图 （a）Swin Transform 创建的是具有层级的特征图，通过合并深层的图像的patches，并且具有线性的计算复杂度（因为仅对每一个局部的window做self-attentation计算）。其中，windows之间是不重叠的，每个window中的patches的个数是固定的。
patch：灰色框的区域。每个patch都会当作一个“token”
local window：红色框的区域
（b) ViT 产生的是一个低分辨率的特征图，并且具有平方的计算复杂度（由于对输入图像尺寸做self-attentation）。
2.2.2 shifted window的提出 shifted window是跨接上一层的windows，提升了模型的性能。在同一个window中的query patches共享相同的key集合，这样提升了访问内存的效率。想反，之前的slide window的方式的self-attentation访问内存比较低效，因为不同的query 像素点有着不同的key集合。
Figure2中，左侧l（L的小写）层是平均分的local window，而l &#43; 1层将windows的形状调整（shifted），有着新的分配方式的windows。在这些新的windows中，self-attentation将会跨越上一层（l层）的windows边界，使得这些windows之间有了连接。
2.2.3 Swin-Tiny的结构 2.3.3.1Swin-Tiny的整体结构 Figure3 (a) Swin-Tiny的结构
输入：是RGB图（维度HW3）。
Patch Partition：通过一个patch切分模块对输入图进行切分成一个没有重叠的patches。每个patch当作一个“token”，它的特征是原始RGB值的一个concatenation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/41a94bb90bd8ddb77f840ac3cc87a3fa/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-20T22:37:46+08:00" />
<meta property="article:modified_time" content="2021-10-20T22:37:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">swin transformer 总结</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1__1"></a>1. 背景介绍</h2> 
<p>原名：Swin Transformer: Hierarchical Vision Transformer using Shifted Windows<br> 获奖：2021 ICCV Best Paper</p> 
<h2><a id="2_5"></a>2.文章介绍</h2> 
<h3><a id="21_6"></a>2.1概括</h3> 
<p>Swin Transformer 是一种新型的transformer，可以用作视觉和语言处理的统一模型</p> 
<ul><li>特性<br> - 引出了一种具有层级的特征表达方式（基于self-attentation的shifted window）<br> - 具有线性的计算复杂度（和输入的图像尺寸相关）</li><li>性能（state of the art）<br> - COCO目标检测<br> - ADE20K语义分割</li></ul> 
<h3><a id="22__14"></a>2.2 创新点介绍</h3> 
<p>NLP和CV的不同点：<br> （1）CV包含不同的尺度，而NLP 把tokens作为transformer的基本元素。当前的cv中的transformer都是固定尺度的，不适合用作CV的任务。<br> （2）CV处理的是高分辨率的图像，而NLP处理的是位于文本段落中的单词。</p> 
<h4><a id="221__19"></a>2.2.1 层级特征图</h4> 
<p><img src="https://images2.imgbox.com/14/db/gIuzmd0Q_o.png" alt="Figure 1"><br> （a）Swin Transform 创建的是具有层级的特征图，通过合并深层的图像的patches，并且具有线性的计算复杂度（因为仅对每一个局部的window做self-attentation计算）。其中，windows之间是不重叠的，每个window中的patches的个数是固定的。<br> patch：灰色框的区域。每个patch都会当作一个“token”<br> local window：红色框的区域</p> 
<p>（b) ViT 产生的是一个低分辨率的特征图，并且具有平方的计算复杂度（由于对输入图像尺寸做self-attentation）。</p> 
<h4><a id="222_shifted_window_27"></a>2.2.2 shifted window的提出</h4> 
<p>shifted window是跨接上一层的windows，提升了模型的性能。在同一个window中的query patches共享相同的key集合，这样提升了访问内存的效率。想反，之前的slide window的方式的self-attentation访问内存比较低效，因为不同的query 像素点有着不同的key集合。<br> Figure2中，左侧l（L的小写）层是平均分的local window，而l + 1层将windows的形状调整（shifted），有着新的分配方式的windows。在这些新的windows中，self-attentation将会跨越上一层（l层）的windows边界，使得这些windows之间有了连接。<br> <img src="https://images2.imgbox.com/1d/f8/WwFAaxBg_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="223_SwinTiny_31"></a>2.2.3 Swin-Tiny的结构</h4> 
<h5><a id="2331SwinTinyhttpsimgblogcsdnimgcnb1fbda2867fb41b9ac833e63fc207014pngxossprocessimagewatermarktype_ZHJvaWRzYW5zZmFsbGJhY2sshadow_50text_Q1NETiBAYmFsYWJhbGF5b3Usize_20color_FFFFFFt_70g_sex_16_32"></a>2.3.3.1Swin-Tiny的整体结构<img src="https://images2.imgbox.com/11/5e/WcCNywEN_o.png" alt="在这里插入图片描述"></h5> 
<p><strong>Figure3 (a) Swin-Tiny的结构</strong></p> 
<p><strong>输入</strong>：是RGB图（维度H<em>W</em>3）。</p> 
<p><strong>Patch Partition</strong>：通过一个patch切分模块对输入图进行切分成一个没有重叠的patches。每个patch当作一个“token”，它的特征是原始RGB值的一个concatenation. 比如，patch是4<em>4，那么每个patch的特征的维度就是4</em>4*3 = 48。 其中，<em>H * W * 3 = H/4 * W/4 * 48</em>。</p> 
<p><strong>Stage 1</strong>: Linear Embeding + Swin Transformer Block<br> Linear Embeding层：人为设定维度。使得raw-valued feature 投射(project)到任意维度，图中用C表示。<br> Swin Transformer Block：保持tokens 的数目 = H/4 * W/4。<br> 故，输出维度就是H/4 * W/4 * C。</p> 
<p><strong>Stage 2</strong>：patch merging + Swin Transformer Block<br> patch merging: 联结（concatenates）每组2 x 2的相邻的patches的特征，并且在4C维度的联结后的特征上应用用一个线性层。这降低了4倍的tokens数量，其中4 = 2x2，2是倍下采样的倍数。 concatenate之后，H/4 变成 H/8, W/4变成W/8<br> 然后输出的维度被设置成2C。<br> 接下来Swin Transformer Block用于特征转换（feature transformation），保持 H/8 * W/8。<br> <strong>Stage 3</strong>: 高<em>宽 = H/16 * W/16<br> <strong>Stage 4</strong>: 高</em>宽 = H/32 * W/32</p> 
<h5><a id="2232Swin_Transformer_httpsimgblogcsdnimgcn4c6d9ffd8a8f475c8d72bebc3f21d668pngxossprocessimagewatermarktype_ZHJvaWRzYW5zZmFsbGJhY2sshadow_50text_Q1NETiBAYmFsYWJhbGF5b3Usize_14color_FFFFFFt_70g_sex_16_52"></a>2.2.3.2两个连续的Swin Transformer 块的解释<img src="https://images2.imgbox.com/fa/32/FBN5LNUr_o.png" alt="在这里插入图片描述"></h5> 
<p><strong>Figure3（b）两个连续的Swin-T块</strong><br> LN: LayerNorm<br> MSA：多头注意力机制（multi-head self attention）<br> W：常规的window采样<br> SW：shifted的window采样<br> 其中，MSA和W-MSA的计算复杂度见公式(1)、(2)。很明显，MSA是和图像的宽高的平方相关的；W-MSA是和宽高的线性相关的(默认设置M=7，即window大小)。<br> <img src="https://images2.imgbox.com/16/4e/NQtE4com_o.png" alt="在这里插入图片描述"></p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          z 
         
        
          ^ 
         
        
       
      
        \hat{z} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em;">z</span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">^</span></span></span></span></span></span></span></span></span></span> 和 z<sup>l</sup> 分别代表 对块l分别做 (S)W-MSA和MLP之后的输出特征。<br> <img src="https://images2.imgbox.com/6d/93/9IPWpEfT_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="224_shifted_windows_63"></a>2.2.4 shifted windows</h5> 
<p>平均分成M x M大小的windows可以降低时间复杂度，由于没有重叠，导致缺乏跨windows之间的连接。为解决这个问题提出shifted window 划分方式，即变换两个连续的Swin Transformer blocks的划分方式。</p> 
<p><img src="https://images2.imgbox.com/eb/28/ra9XmJwy_o.png" alt="在这里插入图片描述"><br> 如图Figure4，window的划分方式，对于cyclic shift中，首先是浅色阴影As、Bs、<br> Cs三个sub windows，下一次选的时候是深色的Ad、Bd、Cd这三个sub windows（s：shallow，d：deep）。A、B、C组成了一个batch window，即每个batch window包含了若干个sub-windows。<br> 这两组A、B、Csub-windows在feature map上是不相邻的。用masking 机制来限制<br> {As : Aq, Bs:Bq, Cs:Cq} 做self-attentation。</p> 
<p>Relative position bias对Swin Transformer没啥作用，故舍去。</p> 
<h4><a id="225_74"></a>2.2.5几个版本</h4> 
<p>Swin-Tiny、Swin-Small、Swin-Base、Swin-Large。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b96a9f0338e4cca6cf84c3d2f650af6c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JMeter压测工具配置与使用（windows10）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/28597740c033bb4c0cf8a9f386396d0c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">openstack部署过程5</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>