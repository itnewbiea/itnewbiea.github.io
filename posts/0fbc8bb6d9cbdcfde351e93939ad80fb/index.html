<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>StyleFlow简明阅读：使用持续流完成属性编辑 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="StyleFlow简明阅读：使用持续流完成属性编辑" />
<meta property="og:description" content="论文：StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows
ACM TOG 2021 code 论文
前言：本文论文有点繁琐晦涩，以下内容仅代表个人阅读理解
摘要 Abstract: 现在可以通过uncondition gan（例如Stylegan）生成高质量、多样化、逼真的图像，但是控制生成过程的属性选项有限、同时很难保证输出的质量，此外由于gan latent space的纠缠特性，沿着一个属性进行编辑很容易导致其他属性的篡改。本文在条件探索纠缠潜空间的背景下，研究了属性条件抽样和属性控制编辑两个子问题。
作为 GAN 潜空间中条件连续正规化流的一个实例，我们将条件探索表述为由属性特征决定的条件连续正规化流，从而将 StyleFlow 作为这两个子问题的一个简单、有效和鲁棒的解决方案。我们使用 StyleGAN 的人脸和汽车的潜在空间来评估我们的方法，并且在真实照片和 StyleGAN 生成的图像上展示沿着各种属性的细粒度分离编辑。例如，对于面孔，我们改变相机的姿势，光照变化，表情，面部毛发，性别和年龄。最后，通过广泛的定性和定量比较，我们证明了 StyleFlow 相对于其他并发工作的优越性。
作者进行了以下两个任务：
（1）属性条件采样（即采样具有目标属性的高质量真实图像）
（2）可控制的属性编辑（即只编辑目标属性，最好的保留源图像其他信息）
其中，在任务1中，生成图像采用了StyleGan生成器，为了计算图像的属性，使用了一个属性分类器
模型方法 作者这篇文章写的比较详细，介绍了很多内容，但是不容易阅读。
在第四部分，讲解了NORMALIZING FLOWS,其实这也是该方法的核心内容，关于Flow流的介绍，但是对于初次了解的人（比如我）读起来就非常的晕，接下来就用通俗易懂的方式来介绍本篇文章模型方法。
由于StyleGan的生成过程为：
首先通过随机种子生成一个符合高斯分布的随机向量z将z经过mapping network生成解耦的向量w然后将w送入systhesis network生成图片 作者介绍了两种FLOW流的方法：DNF、CNF，用于计算数据分布之间的转换，最终作者选择使用了CNF。（这里不作介绍，感兴趣可以阅读论文）
可以理解为：1.通过CNF模块，可以将w向量结合attributes向量经过reverse inference（反向推理）得到z向量 2.通过CNF模块，也可以将z向量结合attributes向量经过forward inference（正向推理）得到w向量
CNF模块的结构如下图所示：
下面来看StyleFlow的模型：
（1）Joint Reverse Encoding（JRE）：首先得到一个w向量（可以根据real images投影得到），利用generator生成图片，然后将图片送入分类器（作者使用的是微软人脸属性分类api和光照预测网络DPR）得到attributes向量，接下来将w和at向量送入CNF网络得到z向量
（2）Conditional Forward Editing（CFE）：编辑修改attributes，然后将其和z向量送入CNF网络得到w’向量。（在这里编辑向量使用的应该也是类似interfacegan训练出来的方向向量）
（3）Edit Specific Subset Selection：作者实验发现，将w’向量加载到w&#43;的不同层可以带来良好的效果。例如，1.更改光照：放入7~11层 2.更改年龄：放入4~7层等。
StyleFlow生成效果评估 - FID得分 关于训练部分 StyleFlow主要训练的是CNF blocks，这部分推荐阅读论文以及项目issues作者的回复。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/0fbc8bb6d9cbdcfde351e93939ad80fb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-14T18:50:49+08:00" />
<meta property="article:modified_time" content="2022-07-14T18:50:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">StyleFlow简明阅读：使用持续流完成属性编辑</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/31/99/H9pXMkeB_o.png" alt="在这里插入图片描述">论文：<strong>StyleFlow</strong>: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows</p> 
<ul><li>ACM TOG 2021</li></ul> 
<p><a href="https://github.com/RameenAbdal/StyleFlow">code</a> <a href="https://dl.acm.org/doi/10.1145/3447648" rel="nofollow">论文</a></p> 
<blockquote> 
 <p>前言：本文论文有点繁琐晦涩，以下内容仅代表个人阅读理解</p> 
</blockquote> 
<h3><a id="httpsimgblogcsdnimgcnd4647839a4cd4ba7b86c922b69562cd1pngpic_center_10"></a><img src="https://images2.imgbox.com/4c/8a/KoPWZNtV_o.png" alt="请添加图片描述"></h3> 
<h4><a id="_13"></a>摘要</h4> 
<blockquote> 
 <p><strong>Abstract:</strong> 现在可以通过uncondition gan（例如Stylegan）生成高质量、多样化、逼真的图像，但是控制生成过程的属性选项有限、同时很难保证输出的质量，此外由于gan latent space的纠缠特性，沿着一个属性进行编辑很容易导致其他属性的篡改。<strong>本文在条件探索纠缠潜空间的背景下，研究了属性条件抽样和属性控制编辑两个子问题。</strong></p> 
 <p>作为 GAN 潜空间中条件连续正规化流的一个实例，我们将条件探索表述为由属性特征决定的条件连续正规化流，从而将 StyleFlow 作为这两个子问题的一个简单、有效和鲁棒的解决方案。我们使用 StyleGAN 的人脸和汽车的潜在空间来评估我们的方法，并且在真实照片和 StyleGAN 生成的图像上展示沿着各种属性的细粒度分离编辑。例如，对于面孔，我们改变相机的姿势，光照变化，表情，面部毛发，性别和年龄。最后，通过广泛的定性和定量比较，我们证明了 StyleFlow 相对于其他并发工作的优越性。</p> 
</blockquote> 
<p>作者进行了以下两个任务：</p> 
<p>（1）属性条件采样（即采样具有目标属性的高质量真实图像）</p> 
<p>（2）可控制的属性编辑（即只编辑目标属性，最好的保留源图像其他信息）</p> 
<p>其中，在任务1中，生成图像采用了StyleGan生成器，为了计算图像的属性，使用了一个属性分类器</p> 
<hr> 
<h4><a id="_29"></a>模型方法</h4> 
<p>作者这篇文章写的比较详细，介绍了很多内容，但是不容易阅读。</p> 
<p>在第四部分，讲解了<code>NORMALIZING FLOWS</code>,其实这也是该方法的核心内容，关于<code>Flow</code>流的介绍，但是对于初次了解的人（比如我）读起来就非常的晕，接下来就用通俗易懂的方式来介绍本篇文章模型方法。</p> 
<p>由于StyleGan的生成过程为：</p> 
<ul><li>首先通过随机种子生成一个符合高斯分布的随机向量z</li><li>将z经过mapping network生成解耦的向量w</li><li>然后将w送入systhesis network生成图片</li></ul> 
<p>作者介绍了两种<code>FLOW</code>流的方法：DNF、CNF，用于计算数据分布之间的转换，最终作者选择使用了CNF。（这里不作介绍，感兴趣可以阅读论文）</p> 
<p>可以理解为：<font color="red"><strong>1.通过CNF模块，可以将w向量结合attributes向量经过reverse inference（反向推理）得到z向量 2.通过CNF模块，也可以将z向量结合attributes向量经过forward inference（正向推理）得到w向量</strong></font></p> 
<p>CNF模块的结构如下图所示：<br> <img src="https://images2.imgbox.com/6f/1b/eGi0V2dF_o.png" alt="在这里插入图片描述"><br> 下面来看StyleFlow的模型：<br> <img src="https://images2.imgbox.com/cb/48/67cetvMw_o.png" alt="在这里插入图片描述"></p> 
<p>（1）Joint Reverse Encoding（JRE）：首先得到一个w向量（可以根据real images投影得到），利用generator生成图片，然后将图片送入分类器（作者使用的是微软人脸属性分类api和光照预测网络DPR）得到attributes向量，接下来将w和at向量送入CNF网络得到z向量</p> 
<p>（2）Conditional Forward Editing（CFE）：编辑修改attributes，然后将其和z向量送入CNF网络得到w’向量。（在这里编辑向量使用的应该也是类似interfacegan训练出来的方向向量）</p> 
<p>（3）Edit Specific Subset Selection：作者实验发现，将w’向量加载到w+的不同层可以带来良好的效果。例如，1.更改光照：放入7~11层 2.更改年龄：放入4~7层等。</p> 
<h4><a id="StyleFlow__FID_58"></a>StyleFlow生成效果评估 - FID得分</h4> 
<p><img src="https://images2.imgbox.com/3a/47/FRIjQegt_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_60"></a>关于训练部分</h4> 
<p>StyleFlow主要训练的是CNF blocks，这部分推荐阅读论文以及项目issues作者的回复。</p> 
<h4><a id="_66"></a>编辑生成图</h4> 
<p>最后展示一下StyleFlow方法的编辑效果图：</p> 
<p><img src="https://images2.imgbox.com/c6/20/pNZ2PkVt_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_75"></a>尾言</h4> 
<p>StyleFlow的论文写的非常详细，还是推荐大家看一看~</p> 
<p><img src="https://images2.imgbox.com/07/26/EhMV6vvS_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9b382a45fd281db3d2e116be2d0848be/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">修改Element的el-table没有数据时候显示样式</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b8e4a69d307f9bbd53e507ba43c2f0ec/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Android】在 Flutter 中使用 WebView</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>