<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【计算机视觉 | 目标检测】arxiv 计算机视觉关于目标检测的学术速递（12 月 6 日论文合集） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【计算机视觉 | 目标检测】arxiv 计算机视觉关于目标检测的学术速递（12 月 6 日论文合集）" />
<meta property="og:description" content="文章目录 一、检测相关(10篇)1.1 Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection1.2 Towards More Practical Group Activity Detection: A New Benchmark and Model1.3 Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark1.4 Diffusion Noise Feature: Accurate and Fast Generated Image Detection1.5 Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models1.6 Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline1.7 Lenna: Language Enhanced Reasoning Detection Assistant1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/5bd430b64586a58d32fb87493d084e34/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-07T20:52:42+08:00" />
<meta property="article:modified_time" content="2023-12-07T20:52:42+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【计算机视觉 | 目标检测】arxiv 计算机视觉关于目标检测的学术速递（12 月 6 日论文合集）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#10_1" rel="nofollow">一、检测相关(10篇)</a></li><li><ul><li><a href="#11_DiffusionSS3D_Diffusion_Model_for_Semisupervised_3D_Object_Detection_2" rel="nofollow">1.1 Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection</a></li><li><a href="#12_Towards_More_Practical_Group_Activity_Detection_A_New_Benchmark_and_Model_10" rel="nofollow">1.2 Towards More Practical Group Activity Detection: A New Benchmark and Model</a></li><li><a href="#13_Are_Synthetic_Data_Useful_for_Egocentric_HandObject_Interaction_Detection_An_Investigation_and_the_HOISynth_Domain_Adaptation_Benchmark_18" rel="nofollow">1.3 Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark</a></li><li><a href="#14_Diffusion_Noise_Feature_Accurate_and_Fast_Generated_Image_Detection_26" rel="nofollow">1.4 Diffusion Noise Feature: Accurate and Fast Generated Image Detection</a></li><li><a href="#15_Projection_Regret_Reducing_Background_Bias_for_Novelty_Detection_via_Diffusion_Models_34" rel="nofollow">1.5 Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models</a></li><li><a href="#16_Towards_Automatic_Power_Battery_Detection_New_Challenge_Benchmark_Dataset_and_Baseline_42" rel="nofollow">1.6 Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline</a></li><li><a href="#17_Lenna_Language_Enhanced_Reasoning_Detection_Assistant_50" rel="nofollow">1.7 Lenna: Language Enhanced Reasoning Detection Assistant</a></li><li><a href="#18_Unsupervised_Change_Detection_for_Space_Habitats_Using_3D_Point_Clouds_58" rel="nofollow">1.8 Unsupervised Change Detection for Space Habitats Using 3D Point Clouds</a></li><li><a href="#19_Cable_Slack_Detection_for_Arresting_Gear_Application_using_Machine_Vision_66" rel="nofollow">1.9 Cable Slack Detection for Arresting Gear Application using Machine Vision</a></li><li><a href="#110_An_Empirical_Study_of_Automated_Mislabel_Detection_in_Real_World_Vision_Datasets_74" rel="nofollow">1.10 An Empirical Study of Automated Mislabel Detection in Real World Vision Datasets</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="10_1"></a>一、检测相关(10篇)</h2> 
<h3><a id="11_DiffusionSS3D_Diffusion_Model_for_Semisupervised_3D_Object_Detection_2"></a>1.1 Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection</h3> 
<p>扩散-SS3D：半监督三维目标检测的扩散模型</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02966</span>
</code></pre> 
<p>半监督目标检测对于三维场景理解至关重要，有效地解决了获取大规模三维边界框注释的限制。现有方法通常采用具有伪标记的师生框架来利用未标记的点云。然而，在多样化的3D空间中产生可靠的伪标签仍然具有挑战性。在这项工作中，我们提出了Diffusion-SS 3D，这是一种通过半监督3D对象检测的扩散模型来提高伪标签质量的新观点。具体来说，我们包括噪声来产生损坏的3D对象大小和类标签分布，然后利用扩散模型作为去噪过程来获得边界框输出。此外，我们将扩散模型集成到教师-学生框架中，以便去噪边界框可以用于改进伪标签生成以及整个半监督学习过程。我们在ScanNet和SUN RGB-D基准数据集上进行了实验，以证明我们的方法与现有方法相比具有最先进的性能。我们还提出了广泛的分析，以了解我们的扩散模型设计如何影响半监督学习的性能。</p> 
<h3><a id="12_Towards_More_Practical_Group_Activity_Detection_A_New_Benchmark_and_Model_10"></a>1.2 Towards More Practical Group Activity Detection: A New Benchmark and Model</h3> 
<p>走向更实用的群体活动检测：一种新的基准和模型</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02878</span>
</code></pre> 
<p>组活动检测（GAD）是在视频中识别每个组的成员并同时对该组的活动进行分类的任务。虽然GAD最近已经被研究，但由于其解决实际GAD场景的能力有限，因此在数据集和方法上仍有很大的改进空间。为了解决这些问题，我们首先提出了一个新的数据集，称为咖啡馆。与现有的数据集不同，Caf’e主要是为GAD构建的，并提供了更实用的评估场景和指标，以及大规模和提供丰富的注释。随着数据集，我们提出了一个新的GAD模型，有效地处理未知数量的群体和潜在的群体成员。我们在包括Caf 'e在内的三个数据集上评估了我们的模型，在准确性和推理速度方面都优于以前的工作。我们的数据集和代码库都将向公众开放，以促进未来对GAD的研究。</p> 
<h3><a id="13_Are_Synthetic_Data_Useful_for_Egocentric_HandObject_Interaction_Detection_An_Investigation_and_the_HOISynth_Domain_Adaptation_Benchmark_18"></a>1.3 Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark</h3> 
<p>人工数据对以自我为中心的手-物相互作用检测有用吗？Hoi-Synth域适配基准的研究</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02672</span>
</code></pre> 
<p>在这项研究中，我们调查的有效性，提高手物体相互作用的检测在自我中心的视觉域的合成数据。我们介绍了一个模拟器，能够生成合成图像的手对象的相互作用，自动标记手对象的接触状态，边界框，和像素级分割掩模。通过对三个以自我为中心的数据集VISOR，EgoHOS和ENIGMA-51进行全面的实验和比较分析，我们证明了使用合成数据和域自适应技术可以与传统的监督方法相比，同时只需要对一小部分真实数据进行注释。当使用从真实目标环境和对象的3D模型生成的域内合成数据进行测试时，我们的最佳模型显示出与仅基于标记真实数据的标准完全监督方法相比一致的性能改进。我们的研究还为以自我为中心的手-物体交互检测（HOI-Synth）领域适应设定了一个新的基准，并提供了基线结果，以鼓励社区参与这项具有挑战性的任务。我们在以下链接发布生成的数据、代码和模拟器：https://iplab.dmi.unict.it/HOI-Synth/。</p> 
<h3><a id="14_Diffusion_Noise_Feature_Accurate_and_Fast_Generated_Image_Detection_26"></a>1.4 Diffusion Noise Feature: Accurate and Fast Generated Image Detection</h3> 
<p>扩散噪声特征：准确、快速地生成图像检测</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02625</span>
</code></pre> 
<p>生成模型已经达到了一个高级阶段，可以生成非常逼真的图像。然而，这种非凡的生成能力也带来了传播虚假或误导性信息的风险。值得注意的是，用于生成图像的现有图像检测器遇到诸如低准确度和有限的泛化的挑战。本文旨在解决这个问题，寻求一种表示具有较强的泛化能力，以提高生成的图像的检测。我们的研究表明，真实图像和生成的图像在经过预训练的扩散模型中进行逆扩散过程时，会显示出不同的潜在高斯表示。利用这种差异，我们可以放大生成图像中的细微伪影。基于这一认识，我们引入了一种新的图像表示称为扩散噪声特征（DNF）。DNF是估计在逆扩散过程期间生成的噪声的集合表示。一个简单的分类器，例如，在DNF上训练的ResNet实现了高准确性，鲁棒性和泛化能力，用于检测生成的图像，即使是以前看不见的类或模型。我们使用广泛认可的标准数据集进行实验，实现了最先进的检测效果。</p> 
<h3><a id="15_Projection_Regret_Reducing_Background_Bias_for_Novelty_Detection_via_Diffusion_Models_34"></a>1.5 Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models</h3> 
<p>投影遗憾：通过扩散模型减少新颖性检测的背景偏差</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02615</span>
</code></pre> 
<p>新颖性检测是机器学习的一项基本任务，其目的是检测异常（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i.e. 
        
       
      
        \textit{i.e.} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6679em;"></span><span class="mord text"><span class="mord textit">i.e.</span></span></span></span></span></span>）分布外（OOD））样品。由于扩散模型最近已经成为事实上的标准生成框架，具有令人惊讶的生成结果，因此通过扩散模型进行新颖性检测也受到了广泛关注。最近的方法主要利用分布样本的重构特性。然而，他们经常遭受检测OOD样本共享类似的背景信息的分布数据。基于我们的观察，扩散模型可以投影任何样本到一个分布样本具有相似的背景信息，我们提出了投影后悔（PR），一个有效的新颖性检测方法，减轻非语义信息的偏见。具体来说，PR计算测试图像与其基于扩散的投影之间的感知距离以检测异常。由于感知距离往往无法捕捉语义的变化时，背景信息占主导地位，我们抵消了背景偏见，通过比较它对递归投影。大量的实验表明，PR优于现有技术的基于生成模型的新颖性检测方法的显着保证金。</p> 
<h3><a id="16_Towards_Automatic_Power_Battery_Detection_New_Challenge_Benchmark_Dataset_and_Baseline_42"></a>1.6 Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline</h3> 
<p>迈向动力电池自动检测：新挑战、基准数据集和基准</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02528</span>
</code></pre> 
<p>我们进行了全面的研究，一个新的任务命名为动力电池检测（PBD），其目的是定位致密的阴极和阳极板的端点从X射线图像，以评估动力电池的质量。现有的制造商通常依靠人眼观察来完成PBD，这使得检测的准确性和效率难以平衡。为了解决这个问题，并将更多的注意力吸引到这个有意义的任务中，我们首先精心收集了一个数据集，称为X射线PBD，它有1,500美元的不同X射线图像，这些图像是从数千个5美元制造商的动力电池中选出的，具有7美元不同的视觉干扰。然后，我们提出了一种新的基于分割的PBD解决方案，称为多维协作网络（MDCNet）。通过引入线预测器和计数预测器，可以在语义和细节两个方面改善点分割分支的表示。此外，我们设计了一个有效的距离自适应掩模生成策略，可以减轻视觉挑战所造成的不一致的分布密度的板块，提供MDCNet稳定的监督。在没有任何附加功能的情况下，我们基于分割的MDCNet始终优于各种其他角点检测，人群计数和基于一般/微小物体检测的解决方案，使其成为有助于促进未来PBD研究的强大基线。最后，我们分享了一些潜在的困难和工作，为未来的研究。源代码和数据集将在\href{http：//www.gy3000.company/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0}{X-ray PBD}上公开。</p> 
<h3><a id="17_Lenna_Language_Enhanced_Reasoning_Detection_Assistant_50"></a>1.7 Lenna: Language Enhanced Reasoning Detection Assistant</h3> 
<p>Lenna：语言增强的推理检测助手</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02433</span>
</code></pre> 
<p>随着多模态大型语言模型（MLLM）的快速发展，我们现在可以用自然语言与AI系统对话以理解图像。然而，嵌入在大型语言模型中的推理能力和世界知识却很少被研究和开发用于图像感知任务。在本文中，我们提出了Lenna，一种语言增强的推理检测助手，它利用了MLLM的强大的多模态特征表示，同时保留位置信息进行检测。这是通过在MLLM词汇表中加入一个附加的标记来实现的，该标记没有明确的语义上下文，但可以作为检测器识别相应位置的提示。为了评估Lenna的推理能力，我们构建了一个ReasonDet数据集来衡量其基于推理的检测性能。值得注意的是，Lenna在ReasonDet上表现出色，而且培训成本非常低。当扩展到其他任务时，它也会产生最小的传输开销。我们的代码和模型将在https://git.io/Lenna上提供。</p> 
<h3><a id="18_Unsupervised_Change_Detection_for_Space_Habitats_Using_3D_Point_Clouds_58"></a>1.8 Unsupervised Change Detection for Space Habitats Using 3D Point Clouds</h3> 
<p>基于三维点云的空间生境无监督变化检测</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02396</span>
</code></pre> 
<p>这项工作提出了一种算法，场景变化检测点云，使自主机器人照顾未来的空间栖息地。自主机器人系统将有助于维护未来的深空栖息地，例如Gateway空间站，该空间站将在很长一段时间内无人驾驶。国际空间站（ISS）上使用的现有场景分析软件依赖于手动标记的图像来检测变化。相比之下，这项工作中提出的算法使用原始的，未标记的点云作为输入。该算法首先采用改进的期望最大化高斯混合模型（GMM）聚类两个输入点云。然后，它通过使用地球移动器的距离来比较地球移动器来执行变化检测。该算法进行了验证定量和定性使用的测试数据集收集的Astrobee机器人在美国宇航局艾姆斯花岗岩实验室，包括单帧深度图像直接拍摄的Astrobee和全场景重建的地图与RGB-D和姿态数据从Astrobee。该方法的运行时也进行了深入分析。源代码公开发布，以促进进一步的开发。</p> 
<h3><a id="19_Cable_Slack_Detection_for_Arresting_Gear_Application_using_Machine_Vision_66"></a>1.9 Cable Slack Detection for Arresting Gear Application using Machine Vision</h3> 
<p>基于机器视觉的阻尼器电缆松弛检测</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02320</span>
</code></pre> 
<p>基于电缆的牵引系统是航空母舰上和远征陆基设施上飞机发射和回收的组成部分。这些现代的制动系统依赖于各种机构来在制动循环期间从飞行器吸收能量，以使飞行器完全停止。该系统的主要组件之一是连接发动机的电缆接口。在该接口处的电缆中形成松弛可能导致效率降低，并且驱动维护工作以在继续操作之前去除松弛。本文提出了一种基于机器视觉的松弛检测系统。利用态势感知摄像机来收集电缆接口区域的视频数据，应用机器视觉算法来减少噪声，去除背景杂波，聚焦于感兴趣区域，并检测表示松弛形成的图像中的变化。该系统中采用的一些算法包括双边图像滤波器，最小二乘多项式拟合，Canny边缘检测，K-Means聚类，基于高斯混合的背景/前景分割的背景减法，Hough圆变换和Hough线变换。过滤并突出显示所得到的检测结果，以向船上操作员创建存在松弛和需要维护动作的指示。设计了一个用户界面，为操作员提供了一种简单的方法来重新定义感兴趣的区域，并调整方法到特定的位置。这些算法在船上的镜头上得到了验证，能够准确地识别松弛，误报率最低。</p> 
<h3><a id="110_An_Empirical_Study_of_Automated_Mislabel_Detection_in_Real_World_Vision_Datasets_74"></a>1.10 An Empirical Study of Automated Mislabel Detection in Real World Vision Datasets</h3> 
<p>真实世界视觉数据集的自动误标注检测实验研究</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2312.02200</span>
</code></pre> 
<p>计算机视觉的重大进步主要归功于标记数据集的使用。然而，获取数据集的标签通常会导致错误，这可能会损害模型性能。最近的工作已经提出了自动识别错误标记图像的方法，但开发策略，以有效地实现它们在现实世界的数据集已经很少探索。为了改进以数据为中心的方法来清洁真实世界的视觉数据集，我们首先进行了200多个实验，仔细地对最近开发的自动错误标记检测方法进行了基准测试，这些方法是在各种不同噪声水平的合成和真实噪声设置下对多个数据集进行的。我们将这些方法与我们制作的简单高效的错误标记检测器（SEMD）进行比较，发现SEMD的性能与之前的错误标记检测方法相似或优于之前的错误标记检测方法。然后，我们将SEMD应用于多个真实世界的计算机视觉数据集，并测试数据集大小、错误标签去除策略和错误标签去除量在对清理后的数据进行再训练后如何进一步影响模型性能。通过仔细设计的方法，我们发现，在较小的数据制度中，错误标签去除导致每类性能提高高达8%的再训练分类器。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8eaa67dad184de25203917d974e11813/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何在Spring Boot中集成RabbitMQ</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f8928a1c483fd1370551563954cfdc22/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Dart设计模式之建造者模式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>