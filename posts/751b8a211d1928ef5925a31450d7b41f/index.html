<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>flash attention论文及源码学习 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="flash attention论文及源码学习" />
<meta property="og:description" content="​
论文 attention计算公式如下
传统实现需要将S和P都存到HBM，需要占用 O ( N 2 ) O(N^{2}) O(N2)内存，计算流程为
因此前向HBM访存为 O ( N d &#43; N 2 ) O(Nd &#43; N^2) O(Nd&#43;N2)，通常N远大于d，GPT2中N=1024，d=64。HBM带宽较小，因此访存会成为瓶颈。
该论文主要出发点就是考虑到IO的影响，降低内存占用和访问，主要贡献点为：
重新设计了计算流程，使用softmax tiling的方法执行block粒度的计算不需要存储矩阵P，只存储归一化因子，再反向的时候可以快速的recompute softmax tiling的整体流程如下图，外层第j次循环拿到K矩阵的第j个block k j kj kj，内层第i次循环拿到Q矩阵的第i个block Q i Qi Qi，计算得到S和P，然后再和 V j Vj Vj相乘得到 O i Oi Oi
然后看下如何计算出softmax。考虑数值稳定性的softmax的传统计算流程如下，需要减去当前行的最大值
这里的max和sum都需要一行的完整结果。
而flash attention的流程基于递推实现block粒度的计算：
单看S的一行，假设 m ( x ) m(x) m(x)为执行到第i个block即 S ( i ) S(i) S(i)的最大值，现在执行第i &#43; 1个block S ( i &#43; 1 ) S(i &#43; 1) S(i&#43;1)，那么新的 m ( x ) = m a x ( m ( x ) , m ( S ( i &#43; 1 ) ) ) m(x) = max(m(x), m(S(i &#43; 1))) m(x)=max(m(x),m(S(i&#43;1)))，由于最大值发生了变化，因此之前i个block对应的f(x)要进行修正，之前减去的是 m ( x ( 1 ) ) m(x^{(1)}) m(x(1))，因此要将他加回来，再减去新的 m ( x ) m(x) m(x)，即 e m ( x ( 1 ) − m ( x ) ) f ( x ( 1 ) ) e^{m({x^{(1)} - m(x))}} f(x^{(1)}) em(x(1)−m(x))f(x(1))，同理对于sum，最后就可以得到softmax，完整流程如下" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/751b8a211d1928ef5925a31450d7b41f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-01T17:30:55+08:00" />
<meta property="article:modified_time" content="2023-07-01T17:30:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">flash attention论文及源码学习</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>​</p> 
<h2><a id="_1"></a>论文</h2> 
<p>attention计算公式如下<br> <img src="https://images2.imgbox.com/e3/fd/OJywRpVY_o.png" alt="在这里插入图片描述"></p> 
<p>传统实现需要将S和P都存到HBM，需要占用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
        
        
          N 
         
        
          2 
         
        
       
         ) 
        
       
      
        O(N^{2}) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0641em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>内存，计算流程为<br> <img src="https://images2.imgbox.com/ad/13/FbV5lZV6_o.png" alt="在这里插入图片描述"></p> 
<p>因此前向HBM访存为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
       
         N 
        
       
         d 
        
       
         + 
        
        
        
          N 
         
        
          2 
         
        
       
         ) 
        
       
      
        O(Nd + N^2) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0641em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，通常N远大于d，GPT2中N=1024，d=64。HBM带宽较小，因此访存会成为瓶颈。<br> <img src="https://images2.imgbox.com/22/49/s3FpKEf8_o.png" alt="在这里插入图片描述"></p> 
<p>该论文主要出发点就是考虑到IO的影响，降低内存占用和访问，主要贡献点为：</p> 
<ul><li>重新设计了计算流程，使用softmax tiling的方法执行block粒度的计算</li><li>不需要存储矩阵P，只存储归一化因子，再反向的时候可以快速的recompute</li></ul> 
<p>softmax tiling的整体流程如下图，外层第j次循环拿到K矩阵的第j个block <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         k 
        
       
         j 
        
       
      
        kj 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0572em;">kj</span></span></span></span></span>，内层第i次循环拿到Q矩阵的第i个block <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Q 
        
       
         i 
        
       
      
        Qi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord mathnormal">i</span></span></span></span></span>，计算得到S和P，然后再和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         V 
        
       
         j 
        
       
      
        Vj 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0572em;">Vj</span></span></span></span></span>相乘得到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         i 
        
       
      
        Oi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mord mathnormal">i</span></span></span></span></span></p> 
<p><img src="https://images2.imgbox.com/ed/97/OI0DdoPr_o.png" alt="在这里插入图片描述"></p> 
<p>然后看下如何计算出softmax。考虑数值稳定性的softmax的传统计算流程如下，需要减去当前行的最大值<br> <img src="https://images2.imgbox.com/86/5d/He2sNFc9_o.png" alt="在这里插入图片描述"></p> 
<p>这里的max和sum都需要一行的完整结果。</p> 
<p>而flash attention的流程基于递推实现block粒度的计算：<br> <img src="https://images2.imgbox.com/16/64/CYcHkKU4_o.png" alt="在这里插入图片描述"></p> 
<p>单看S的一行，假设<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
      
        m(x) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>为执行到第i个block即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         S 
        
       
         ( 
        
       
         i 
        
       
         ) 
        
       
      
        S(i) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span></span></span></span></span>的最大值，现在执行第i + 1个block <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         S 
        
       
         ( 
        
       
         i 
        
       
         + 
        
       
         1 
        
       
         ) 
        
       
      
        S(i + 1) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>，那么新的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
         = 
        
       
         m 
        
       
         a 
        
       
         x 
        
       
         ( 
        
       
         m 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
         , 
        
       
         m 
        
       
         ( 
        
       
         S 
        
       
         ( 
        
       
         i 
        
       
         + 
        
       
         1 
        
       
         ) 
        
       
         ) 
        
       
         ) 
        
       
      
        m(x) = max(m(x), m(S(i + 1))) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mclose">)))</span></span></span></span></span>，由于最大值发生了变化，因此之前i个block对应的f(x)要进行修正，之前减去的是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
         ( 
        
        
        
          x 
         
         
         
           ( 
          
         
           1 
          
         
           ) 
          
         
        
       
         ) 
        
       
      
        m(x^{(1)}) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.138em; vertical-align: -0.25em;"></span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，因此要将他加回来，再减去新的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
      
        m(x) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>，即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          e 
         
         
         
           m 
          
         
           ( 
          
          
           
           
             x 
            
            
            
              ( 
             
            
              1 
             
            
              ) 
             
            
           
          
            − 
           
          
            m 
           
          
            ( 
           
          
            x 
           
          
            ) 
           
          
            ) 
           
          
         
        
       
         f 
        
       
         ( 
        
        
        
          x 
         
         
         
           ( 
          
         
           1 
          
         
           ) 
          
         
        
       
         ) 
        
       
      
        e^{m({x^{(1)} - m(x))}} f(x^{(1)}) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.2897em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0397em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9667em;"><span class="" style="top: -2.9667em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5357em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">m</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">))</span></span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，同理对于sum，最后就可以得到softmax，完整流程如下<br> <img src="https://images2.imgbox.com/df/22/w3kjBaCb_o.png" alt="在这里插入图片描述"></p> 
<p>因此内存占用为O(N)，假设share mem大小为M，那么对于HBM的访存为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
        
        
          N 
         
        
          2 
         
        
        
        
          d 
         
        
          2 
         
        
        
        
          M 
         
         
         
           − 
          
         
           1 
          
         
        
       
         ) 
        
       
      
        O(N^{2}d^{2}M^{-1}) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0641em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p> 
<h2><a id="A100_Tensor_Core_33"></a>A100 Tensor Core</h2> 
<p>为了加速深度学习里的fc和卷积，nvidia引入了Tensor Core到gpu里，单个sm如下所示<br> ​</p> 
<div align="center"> 
 <br> 
 <img src="https://images2.imgbox.com/c2/48/VWH5eUUM_o.png" alt="在这里插入图片描述"> 
</div> 
<p></p> 
<center>
  图 2-1 
</center> A100的一个sm有4个Tensor Core，以FP16/FP32混合精度为例，每个Tensor Core每个周期可以计算256个FP16 FMA，即8x4x8的矩阵运算。除了通过cublas，cudnn等官方库使用Tensor Core之外，nv还提供了WMMA和mma PTX两种方式使用Tensor Core，由于flash attention用的是mma PTX，所以后续只介绍下mma PTX。 矩阵的乘累加形为D = A * B + C，其中A和B不支持FP32，输入的FP32会被转为同样位宽的TF32，C和D支持FP32，详细类型见下表，其中mma.sync就是执行了一次矩阵乘累加 
<p><img src="https://images2.imgbox.com/0e/ab/d2oAGBcj_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 2-2 
</center> mma为warp-level的操作，矩阵乘由32线程一起完成，但是存储是和cuda core共享，也就是说A和B需要分布式的存储在32线程的寄存器中，每个线程存储了原始矩阵的一部分，称为一个fragment，这个分布式存储的过程需要用户显式完成，然后Tensor Core会访问所有线程寄存器完成矩阵运算，以fp16的16x8x16的A为例，数据在warp中的分布如下所示 
<p><img src="https://images2.imgbox.com/8e/fc/J8dP6na2_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 2-3 
</center> 假设A的一个tile已经通过LDG从global mem加载到了shared mem中，为了完成上图的数据排布，我们可以使用LDS指令加载数据，但是由于数据分布不是连续的，所以要执行4次LDS，为了解决这个问题，nvidia提供了一个指令为ldmatrix，可以一跳指令完成16x16的矩阵加载，流程如下，每个thread读入128b，然后将128b写入到4个lane对应的寄存器中，以T0为例，会读入矩阵第一行的前8个FP16，写入到T0，T1，T2，T3对应的寄存器中 
<p><img src="https://images2.imgbox.com/f6/ce/kOTqjCOi_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 2-4 
</center> 
<p>​</p> 
<div align="center"> 
 <img src="https://images2.imgbox.com/bf/b2/ItHQgbte_o.png" alt="在这里插入图片描述"> 
</div> 
<p></p> 
<center>
  图 2-5 
</center> 值得注意的是，假设shared mem中为连续存储，这里将发生bank冲突，gpu的shared mem中有32bank，每个bank 4字节，由于每个线程读取128b，因此每个线程占4个bank，所以整个读取过程将分为4次，第一次为T0-T7，第二次为T8-T15，第三次为T16-T23，第四次为T24-T31，如果shared mem中为连续存储，如下图，数字表示原始16x16矩阵中的行和列，那么在第一次读取中，绿色部分为T0读，蓝色部分为T4读，将发生冲突，shared mem利用率只有一半。 
<p><img src="https://images2.imgbox.com/2f/90/LNscGKbN_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 2-6 
</center> 为了解决这个问题，cutlass使用了xor swizzle的方法避免bank冲突，如下所示 
<p><img src="https://images2.imgbox.com/64/1b/48nemn36_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 2-7 
</center> # 源码流程 ## 两层循环流程控制 前向入口为mha_fwd 
<pre><code class="prism language-cpp">std<span class="token double-colon punctuation">::</span>vector<span class="token operator">&lt;</span>at<span class="token double-colon punctuation">::</span>Tensor<span class="token operator">&gt;</span>
<span class="token function">mha_fwd</span><span class="token punctuation">(</span><span class="token keyword">const</span> at<span class="token double-colon punctuation">::</span>Tensor <span class="token operator">&amp;</span>q<span class="token punctuation">,</span>         <span class="token comment">// total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i</span>
        <span class="token keyword">const</span> at<span class="token double-colon punctuation">::</span>Tensor <span class="token operator">&amp;</span>k<span class="token punctuation">,</span>         <span class="token comment">// total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i</span>
        <span class="token keyword">const</span> at<span class="token double-colon punctuation">::</span>Tensor <span class="token operator">&amp;</span>v<span class="token punctuation">,</span>         <span class="token comment">// total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i</span>
        at<span class="token double-colon punctuation">::</span>Tensor <span class="token operator">&amp;</span>out<span class="token punctuation">,</span>             <span class="token comment">// total_q x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i</span>
        <span class="token keyword">const</span> at<span class="token double-colon punctuation">::</span>Tensor <span class="token operator">&amp;</span>cu_seqlens_q<span class="token punctuation">,</span>  <span class="token comment">// b+1</span>
        <span class="token keyword">const</span> at<span class="token double-colon punctuation">::</span>Tensor <span class="token operator">&amp;</span>cu_seqlens_k<span class="token punctuation">,</span>  <span class="token comment">// b+1</span>
        <span class="token keyword">const</span> <span class="token keyword">int</span> max_seqlen_q_<span class="token punctuation">,</span>
        <span class="token keyword">const</span> <span class="token keyword">int</span> max_seqlen_k_<span class="token punctuation">,</span>
        <span class="token keyword">const</span> <span class="token keyword">float</span> p_dropout<span class="token punctuation">,</span>
        <span class="token keyword">const</span> <span class="token keyword">float</span> softmax_scale<span class="token punctuation">,</span>
        <span class="token keyword">const</span> <span class="token keyword">bool</span> zero_tensors<span class="token punctuation">,</span>
        <span class="token keyword">const</span> <span class="token keyword">bool</span> is_causal<span class="token punctuation">,</span>
        <span class="token keyword">const</span> <span class="token keyword">bool</span> return_softmax<span class="token punctuation">,</span>
        <span class="token keyword">const</span> <span class="token keyword">int</span> num_splits<span class="token punctuation">,</span>
        c10<span class="token double-colon punctuation">::</span>optional<span class="token operator">&lt;</span>at<span class="token double-colon punctuation">::</span>Generator<span class="token operator">&gt;</span> gen_<span class="token punctuation">)</span>
</code></pre> 
<p>q，k，v的shape均为[total_q, num_heads, head_size]，dtype为FP16或者BF16，total_q就是按照batchsize累加token，cu_seqlens_q为每个batch的token数量的前缀和<br> 不加说明的话假设后续total_q和total_k相等，head_size为32，dtype为FP16</p> 
<pre><code class="prism language-cpp">   Launch_params<span class="token operator">&lt;</span>FMHA_fprop_params<span class="token operator">&gt;</span> <span class="token function">launch_params</span><span class="token punctuation">(</span>dprops<span class="token punctuation">,</span> stream<span class="token punctuation">,</span> is_dropout<span class="token punctuation">,</span> return_softmax<span class="token punctuation">)</span><span class="token punctuation">;</span>
   at<span class="token double-colon punctuation">::</span>Tensor o_tmp<span class="token punctuation">;</span>
   <span class="token keyword">if</span> <span class="token punctuation">(</span>loop<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> o_tmp <span class="token operator">=</span> torch<span class="token double-colon punctuation">::</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>total_q<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> head_size<span class="token punctuation">}</span><span class="token punctuation">,</span> opts<span class="token punctuation">.</span><span class="token function">dtype</span><span class="token punctuation">(</span>at<span class="token double-colon punctuation">::</span>kFloat<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
   <span class="token keyword">auto</span> softmax_lse <span class="token operator">=</span> torch<span class="token double-colon punctuation">::</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>batch_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> max_seqlen_q<span class="token punctuation">}</span><span class="token punctuation">,</span> opts<span class="token punctuation">.</span><span class="token function">dtype</span><span class="token punctuation">(</span>at<span class="token double-colon punctuation">::</span>kFloat<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
   <span class="token function">set_params_fprop</span><span class="token punctuation">(</span>launch_params<span class="token punctuation">.</span>params<span class="token punctuation">,</span>
                    batch_size<span class="token punctuation">,</span>
                    max_seqlen_q<span class="token punctuation">,</span>
                    max_seqlen_k<span class="token punctuation">,</span>
                    num_heads<span class="token punctuation">,</span>
                    head_size<span class="token punctuation">,</span>
                    q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> out<span class="token punctuation">,</span>
                    cu_seqlens_q<span class="token punctuation">.</span><span class="token function">data_ptr</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    cu_seqlens_k<span class="token punctuation">.</span><span class="token function">data_ptr</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    loop <span class="token operator">?</span> o_tmp<span class="token punctuation">.</span><span class="token function">data_ptr</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token keyword">nullptr</span><span class="token punctuation">,</span>
                    return_softmax <span class="token operator">?</span> s<span class="token punctuation">.</span><span class="token function">data_ptr</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token keyword">nullptr</span><span class="token punctuation">,</span>
                    softmax_lse<span class="token punctuation">.</span><span class="token function">data_ptr</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    p_dropout<span class="token punctuation">,</span>
                    softmax_scale<span class="token punctuation">,</span>
                    is_causal<span class="token punctuation">,</span>
                    num_splits<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>Launch_params里最核心的就是params，即FMHA_fprop_params，保存了kernel的上下文信息，比如Q，K，V的指针，stride，shape等信息，这里通过set_params_fprop保存了context。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">void</span> <span class="token function">run_fmha_fwd_hdim32</span><span class="token punctuation">(</span>Launch_params<span class="token operator">&lt;</span>FMHA_fprop_params<span class="token operator">&gt;</span> <span class="token operator">&amp;</span>launch_params<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">using</span> Kernel_traits <span class="token operator">=</span> FMHA_kernel_traits<span class="token operator">&lt;</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0x08u</span><span class="token punctuation">,</span> elem_type<span class="token operator">&gt;</span><span class="token punctuation">;</span>
    <span class="token generic-function"><span class="token function">run_fmha_fwd_loop</span><span class="token generic class-name"><span class="token operator">&lt;</span>Kernel_traits<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>launch_params<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>FMHA_kernel_traits 为当前规模下的各种类型定义，先看下Q相关的几个，注释写了当前规模下的值，elem_type为__half</p> 
<pre><code class="prism language-cpp">        <span class="token comment">// 128     32        16      1            4</span>
<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">int</span> S<span class="token punctuation">,</span> <span class="token keyword">int</span> D<span class="token punctuation">,</span> <span class="token keyword">int</span> STEP<span class="token punctuation">,</span> <span class="token keyword">int</span> WARPS_M<span class="token punctuation">,</span> <span class="token keyword">int</span> WARPS_N<span class="token punctuation">,</span> <span class="token keyword">uint32_t</span> FLAGS <span class="token operator">=</span> <span class="token number">0x08u</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">elem_type_</span><span class="token operator">=</span>__half<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">FMHA_kernel_traits</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">using</span> Cta_tile_p <span class="token operator">=</span> fmha<span class="token double-colon punctuation">::</span>Cta_tile_extd<span class="token operator">&lt;</span>STEP<span class="token punctuation">,</span> S<span class="token punctuation">,</span> D<span class="token punctuation">,</span> WARPS_M<span class="token punctuation">,</span> WARPS_N<span class="token punctuation">,</span> <span class="token number">1</span><span class="token operator">&gt;</span><span class="token punctuation">;</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">using</span> Gmem_tile_q <span class="token operator">=</span> fmha<span class="token double-colon punctuation">::</span>Gmem_tile_qkv<span class="token operator">&lt;</span>Cta_tile_p<span class="token punctuation">,</span> fmha<span class="token double-colon punctuation">::</span>BITS_PER_ELEMENT_A<span class="token punctuation">,</span> STEP<span class="token punctuation">,</span> D<span class="token operator">&gt;</span><span class="token punctuation">;</span>
    <span class="token keyword">using</span> Smem_tile_q <span class="token operator">=</span> fmha<span class="token double-colon punctuation">::</span>Smem_tile_a<span class="token operator">&lt;</span>Cta_tile_p<span class="token punctuation">,</span> fmha<span class="token double-colon punctuation">::</span>Row<span class="token punctuation">,</span> Gmem_tile_q<span class="token double-colon punctuation">::</span>BYTES_PER_LDG<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">&gt;</span><span class="token punctuation">;</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>cta_tile表示一个计算矩阵乘的cta线程怎么排布，去处理一个多大的tile，对于第一个矩阵乘Cta_tile_p相关变量见注释</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span>
    <span class="token comment">// The number of rows in the CTA tile.  </span>
    <span class="token keyword">int</span> M_<span class="token punctuation">,</span>       <span class="token comment">// STEP  ：16</span>
    <span class="token comment">// The number of cols in the CTA tile.</span>
    <span class="token keyword">int</span> N_<span class="token punctuation">,</span>       <span class="token comment">// S  ：128</span>
    <span class="token comment">// The number of elements in the the K dimension of the GEMM loop.</span>
    <span class="token keyword">int</span> K_<span class="token punctuation">,</span>       <span class="token comment">// D ：32</span>
    <span class="token comment">// The number of rows of warps.</span>
    <span class="token keyword">int</span> WARPS_M_<span class="token punctuation">,</span> <span class="token comment">// 4</span>
    <span class="token comment">// The number of cols of warps.</span>
    <span class="token keyword">int</span> WARPS_N_<span class="token punctuation">,</span> <span class="token comment">// 1</span>
    <span class="token comment">// The number of warps in the K dimension of the GEMM loop.</span>
    <span class="token keyword">int</span> WARPS_K_<span class="token operator">&gt;</span> <span class="token comment">// 1</span>
<span class="token keyword">struct</span> <span class="token class-name">Cta_tile_</span> <span class="token punctuation">{<!-- --></span>

    <span class="token keyword">static</span> <span class="token keyword">constexpr</span> <span class="token keyword">int</span> M <span class="token operator">=</span> M_<span class="token punctuation">,</span> N <span class="token operator">=</span> N_<span class="token punctuation">,</span> K <span class="token operator">=</span> K_<span class="token punctuation">;</span> 
    <span class="token comment">// The number of warps.</span>
    <span class="token keyword">static</span> <span class="token keyword">constexpr</span> <span class="token keyword">int</span> WARPS_M <span class="token operator">=</span> WARPS_M_<span class="token punctuation">,</span> WARPS_N <span class="token operator">=</span> WARPS_N_<span class="token punctuation">,</span> WARPS_K <span class="token operator">=</span> WARPS_K_<span class="token punctuation">;</span>
    <span class="token comment">// The number of warps per CTA.</span>
    <span class="token keyword">static</span> <span class="token keyword">constexpr</span> <span class="token keyword">int</span> WARPS_PER_CTA <span class="token operator">=</span> WARPS_M <span class="token operator">*</span> WARPS_N <span class="token operator">*</span> WARPS_K<span class="token punctuation">;</span>
    <span class="token comment">// The number of threads per warp.</span>
    <span class="token keyword">static</span> <span class="token keyword">constexpr</span> <span class="token keyword">int</span> THREADS_PER_WARP <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">;</span> 
    <span class="token comment">// The number of threads per CTA.</span>
    <span class="token keyword">static</span> <span class="token keyword">constexpr</span> <span class="token keyword">int</span> THREADS_PER_CTA <span class="token operator">=</span> WARPS_PER_CTA <span class="token operator">*</span> THREADS_PER_WARP<span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>


</code></pre> 
<p>然后通过run_fmha_fwd_loop启动kernel，简便起见，假设num_splits为1，所以一共启动了[batch_size, num_head]个cta，每个cta负责一个batch里的一个head</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token operator">&gt;</span>
<span class="token keyword">void</span> <span class="token function">run_fmha_fwd_loop</span><span class="token punctuation">(</span>Launch_params<span class="token operator">&lt;</span>FMHA_fprop_params<span class="token operator">&gt;</span> <span class="token operator">&amp;</span>launch_params<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    dim3 <span class="token function">grid</span><span class="token punctuation">(</span>launch_params<span class="token punctuation">.</span>params<span class="token punctuation">.</span>b<span class="token punctuation">,</span> launch_params<span class="token punctuation">.</span>params<span class="token punctuation">.</span>h<span class="token punctuation">,</span> launch_params<span class="token punctuation">.</span>params<span class="token punctuation">.</span>num_splits<span class="token punctuation">)</span><span class="token punctuation">;</span>
    kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> Kernel_traits<span class="token double-colon punctuation">::</span>THREADS<span class="token punctuation">,</span> smem_size<span class="token punctuation">,</span> launch_params<span class="token punctuation">.</span>stream<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>
        launch_params<span class="token punctuation">.</span>params<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">FMHA_CHECK_CUDA</span><span class="token punctuation">(</span><span class="token function">cudaPeekAtLastError</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>

</code></pre> 
<p>然后看下kernel，这里就是论文中的外层循环，每次计算完成k矩阵的一个block计算，blockIdx.x表示哪个batch，blockIdx.y表示哪个head。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token punctuation">,</span> <span class="token keyword">bool</span> Is_dropout<span class="token punctuation">,</span> <span class="token keyword">bool</span> Is_causal<span class="token punctuation">,</span> <span class="token keyword">bool</span> Return_softmax<span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Params</span><span class="token operator">&gt;</span>
<span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">device_1xN_loop</span><span class="token punctuation">(</span><span class="token keyword">const</span> Params <span class="token operator">&amp;</span>params<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token comment">// The block index for the batch.</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> bidb <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token comment">// The block index for the head.</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> bidh <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
    <span class="token comment">// The thread index.</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> tidx <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">auto</span> seeds <span class="token operator">=</span> at<span class="token double-colon punctuation">::</span>cuda<span class="token double-colon punctuation">::</span>philox<span class="token double-colon punctuation">::</span><span class="token function">unpack</span><span class="token punctuation">(</span>params<span class="token punctuation">.</span>philox_args<span class="token punctuation">)</span><span class="token punctuation">;</span>
    Philox <span class="token function">ph</span><span class="token punctuation">(</span>std<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">get</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token number">0</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>seeds<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> std<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">get</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token number">1</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>seeds<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>bidb <span class="token operator">*</span> params<span class="token punctuation">.</span>h <span class="token operator">+</span> bidh<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">32</span> <span class="token operator">+</span> tidx <span class="token operator">%</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">constexpr</span> <span class="token keyword">int</span> M <span class="token operator">=</span> Kernel_traits<span class="token double-colon punctuation">::</span>Cta_tile_p<span class="token double-colon punctuation">::</span>M<span class="token punctuation">;</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> STEPS <span class="token operator">=</span> <span class="token punctuation">(</span>params<span class="token punctuation">.</span>seqlen_q <span class="token operator">+</span> M <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> M<span class="token punctuation">;</span>

    <span class="token keyword">constexpr</span> <span class="token keyword">int</span> blocksize_c <span class="token operator">=</span> Kernel_traits<span class="token double-colon punctuation">::</span>Cta_tile_p<span class="token double-colon punctuation">::</span>N<span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>params<span class="token punctuation">.</span>seqlen_k <span class="token operator">==</span> blocksize_c<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        fmha<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">device_1xN_</span><span class="token generic class-name"><span class="token operator">&lt;</span>Kernel_traits<span class="token punctuation">,</span> Is_dropout<span class="token punctuation">,</span> Is_causal<span class="token punctuation">,</span> Return_softmax<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> bidb<span class="token punctuation">,</span> bidh<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> ph<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">const</span> <span class="token keyword">int</span> max_loop_steps <span class="token operator">=</span> <span class="token punctuation">(</span>params<span class="token punctuation">.</span>seqlen_k <span class="token operator">+</span> blocksize_c <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> blocksize_c<span class="token punctuation">;</span>
        fmha<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">device_1xN_</span><span class="token generic class-name"><span class="token operator">&lt;</span>Kernel_traits<span class="token punctuation">,</span> Is_dropout<span class="token punctuation">,</span> Is_causal<span class="token punctuation">,</span> Return_softmax<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> bidb<span class="token punctuation">,</span> bidh<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> ph<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> loop_step_idx <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> loop_step_idx <span class="token operator">&lt;</span> max_loop_steps <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> loop_step_idx<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            fmha<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">device_1xN_</span><span class="token generic class-name"><span class="token operator">&lt;</span>Kernel_traits<span class="token punctuation">,</span> Is_dropout<span class="token punctuation">,</span> Is_causal<span class="token punctuation">,</span> Return_softmax<span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> bidb<span class="token punctuation">,</span> bidh<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> ph<span class="token punctuation">,</span> loop_step_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        fmha<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">device_1xN_</span><span class="token generic class-name"><span class="token operator">&lt;</span>Kernel_traits<span class="token punctuation">,</span> Is_dropout<span class="token punctuation">,</span> Is_causal<span class="token punctuation">,</span> Return_softmax<span class="token punctuation">,</span> <span class="token boolean">false</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> bidb<span class="token punctuation">,</span> bidh<span class="token punctuation">,</span> STEPS<span class="token punctuation">,</span> ph<span class="token punctuation">,</span> max_loop_steps <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>然后是最核心的一次内层循环的流程</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token punctuation">,</span> <span class="token keyword">bool</span> Is_dropout<span class="token punctuation">,</span> <span class="token keyword">bool</span> Is_causal<span class="token punctuation">,</span> <span class="token keyword">bool</span> Return_softmax<span class="token punctuation">,</span> <span class="token keyword">bool</span> Is_first<span class="token punctuation">,</span> <span class="token keyword">bool</span> Is_last<span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Params</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Prng</span><span class="token operator">&gt;</span>
<span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">device_1xN_</span><span class="token punctuation">(</span><span class="token keyword">const</span> Params <span class="token operator">&amp;</span>params<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> bidb<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> bidh<span class="token punctuation">,</span> <span class="token keyword">int</span> steps<span class="token punctuation">,</span> Prng <span class="token operator">&amp;</span>ph<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> loop_step_idx<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    <span class="token keyword">extern</span> __shared__ <span class="token keyword">char</span> smem_<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">;</span>

    <span class="token keyword">const</span> <span class="token keyword">int</span> tidx <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">const</span> BlockInfoPadded<span class="token operator">&lt;</span>Kernel_traits<span class="token double-colon punctuation">::</span>THREADS<span class="token operator">&gt;</span> <span class="token function">binfo</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> bidb<span class="token punctuation">,</span> bidh<span class="token punctuation">,</span> tidx<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// if( binfo.stop_early() ) return;</span>
    <span class="token keyword">if</span><span class="token punctuation">(</span> binfo<span class="token punctuation">.</span><span class="token function">stop_early</span><span class="token punctuation">(</span>loop_step_idx <span class="token operator">*</span> Cta_tile_p<span class="token double-colon punctuation">::</span>N<span class="token punctuation">)</span> <span class="token punctuation">)</span> <span class="token keyword">return</span><span class="token punctuation">;</span>
    Gemm1 <span class="token function">gemm_q_k</span><span class="token punctuation">(</span>smem_<span class="token punctuation">,</span> tidx<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
  <span class="token punctuation">}</span>
</code></pre> 
<p>BlockInfoPadded的核心就是sum_s_q和actual_seqlen_q，分别表示前边的batch一共有多少token，和当前batch有多少token</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">int</span> THREADS_PER_CTA<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">BlockInfoPadded</span> <span class="token punctuation">{<!-- --></span>

    <span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Params</span><span class="token operator">&gt;</span>
    __device__ <span class="token function">BlockInfoPadded</span><span class="token punctuation">(</span><span class="token keyword">const</span> Params <span class="token operator">&amp;</span>params<span class="token punctuation">,</span>
                               <span class="token keyword">const</span> <span class="token keyword">int</span> bidb<span class="token punctuation">,</span>
                               <span class="token keyword">const</span> <span class="token keyword">int</span> bidh<span class="token punctuation">,</span>
                               <span class="token keyword">const</span> <span class="token keyword">int</span> tidx<span class="token punctuation">)</span>
        <span class="token operator">:</span> <span class="token function">bidb</span><span class="token punctuation">(</span>bidb<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">bidh</span><span class="token punctuation">(</span>bidh<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">h</span><span class="token punctuation">(</span>params<span class="token punctuation">.</span>h<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

        <span class="token comment">// The block index.</span>
        sum_s_k <span class="token operator">=</span> params<span class="token punctuation">.</span>cu_seqlens_k<span class="token punctuation">[</span>bidb<span class="token punctuation">]</span><span class="token punctuation">;</span>
        actual_seqlen_k <span class="token operator">=</span> params<span class="token punctuation">.</span>cu_seqlens_k<span class="token punctuation">[</span>bidb <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> sum_s_k<span class="token punctuation">;</span>
        sum_s_q <span class="token operator">=</span> params<span class="token punctuation">.</span>cu_seqlens_q<span class="token punctuation">[</span>bidb<span class="token punctuation">]</span><span class="token punctuation">;</span>
        actual_seqlen_q <span class="token operator">=</span> params<span class="token punctuation">.</span>cu_seqlens_q<span class="token punctuation">[</span>bidb <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> sum_s_q<span class="token punctuation">;</span>

        tidx_global <span class="token operator">=</span> <span class="token punctuation">(</span>bidb <span class="token operator">*</span> params<span class="token punctuation">.</span>h <span class="token operator">+</span> bidh<span class="token punctuation">)</span> <span class="token operator">*</span> THREADS_PER_CTA <span class="token operator">+</span> tidx<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
</code></pre> 
<h3><a id="global_mem_252"></a>global mem到寄存器</h3> 
<p>然后实例化gemm_q_k，负责第一个gemm，后边介绍，即QK，后边介绍。gmem_q负责将Q矩阵从global mem中load到寄存器</p> 
<pre><code class="prism language-cpp"><span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">device_1xN_</span><span class="token punctuation">(</span><span class="token keyword">const</span> Params <span class="token operator">&amp;</span>params<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> bidb<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> bidh<span class="token punctuation">,</span> <span class="token keyword">int</span> steps<span class="token punctuation">,</span> Prng <span class="token operator">&amp;</span>ph<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> loop_step_idx<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    Gemm1 <span class="token function">gemm_q_k</span><span class="token punctuation">(</span>smem_<span class="token punctuation">,</span> tidx<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// Allocate the global memory tile loader for Q.</span>
    Gmem_tile_q <span class="token function">gmem_q</span><span class="token punctuation">(</span>params<span class="token punctuation">.</span>q_ptr<span class="token punctuation">,</span> params<span class="token punctuation">.</span>q_row_stride_in_elts<span class="token punctuation">,</span> params<span class="token punctuation">.</span>q_head_stride_in_elts<span class="token punctuation">,</span>
                       params<span class="token punctuation">.</span>d<span class="token punctuation">,</span> binfo<span class="token punctuation">,</span> tidx<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>

<span class="token keyword">using</span> Gmem_tile_q <span class="token operator">=</span> fmha<span class="token double-colon punctuation">::</span>Gmem_tile_qkv<span class="token operator">&lt;</span>Cta_tile_p<span class="token punctuation">,</span> fmha<span class="token double-colon punctuation">::</span>BITS_PER_ELEMENT_A<span class="token punctuation">,</span> STEP<span class="token punctuation">,</span> D<span class="token operator">&gt;</span><span class="token punctuation">;</span>
</code></pre> 
<p>先看下Gmem_tile_q，这里ROWS和COLS为一次处理的block大小，对于q矩阵来说为16x32，BITS_PER_ELEMENT为q矩阵中每个元素为多少bit，由于为FP16，这里为16，BYTES_PER_LDGS_ 表示一个线程一次load的字节数，这里为16字节，一行需要4个线程去load</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span>
    <span class="token comment">// The dimensions of the tile computed by the CTA.</span>
    <span class="token keyword">typename</span> <span class="token class-name">Cta_tile_</span><span class="token punctuation">,</span>
    <span class="token comment">// The number of bits per element.</span>
    <span class="token keyword">int</span> BITS_PER_ELEMENT<span class="token punctuation">,</span>
    <span class="token comment">// The number of rows of Q, K or V loaded by this tile.</span>
    <span class="token keyword">int</span> ROWS_<span class="token punctuation">,</span>
    <span class="token comment">// The number of columns.</span>
    <span class="token keyword">int</span> COLS<span class="token punctuation">,</span>
    <span class="token keyword">int</span> BYTES_PER_LDGS_ <span class="token operator">=</span> <span class="token number">16</span>
<span class="token operator">&gt;</span>
</code></pre> 
<p>然后看下构造函数，row和col计算出当前线程在这个tile中需要从哪行哪里开始load，通过binfo.sum_s_q + row跳过前边batch的token并定位到当前应该处理的是哪个token，row_stride就是num_heads x head_size，然后再跳过前边的head，再加上col就可以定位到当前起始的位置，即ptr</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span> <span class="token keyword">typename</span> <span class="token class-name">BInfo</span> <span class="token operator">&gt;</span>
<span class="token keyword">inline</span> __device__ <span class="token function">Gmem_tile_qkv</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>ptr_<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">uint32_t</span> row_stride_in_elts<span class="token punctuation">,</span>
                                <span class="token keyword">const</span> <span class="token keyword">uint32_t</span> head_stride_in_elts<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> headdim<span class="token punctuation">,</span>
                                <span class="token keyword">const</span> BInfo <span class="token operator">&amp;</span>binfo<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> tidx<span class="token punctuation">,</span> <span class="token keyword">bool</span> use_seqlen_q<span class="token punctuation">)</span>
    <span class="token operator">:</span> <span class="token function">row_stride_in_bytes</span><span class="token punctuation">(</span>row_stride_in_elts <span class="token operator">*</span> BYTES_PER_ELEMENT<span class="token punctuation">)</span>
    <span class="token punctuation">,</span> <span class="token function">actual_seqlen</span><span class="token punctuation">(</span>use_seqlen_q <span class="token operator">?</span> binfo<span class="token punctuation">.</span>actual_seqlen_q <span class="token operator">:</span> binfo<span class="token punctuation">.</span>actual_seqlen_k<span class="token punctuation">)</span>
    <span class="token punctuation">,</span> <span class="token function">ptr</span><span class="token punctuation">(</span><span class="token generic-function"><span class="token function">reinterpret_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">char</span> <span class="token operator">*</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>ptr_<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">,</span> <span class="token function">tidx_</span><span class="token punctuation">(</span>tidx<span class="token punctuation">)</span>
    <span class="token punctuation">,</span> <span class="token function">col_predicate</span><span class="token punctuation">(</span><span class="token punctuation">(</span>tidx <span class="token operator">%</span> THREADS_PER_ROW<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>BYTES_PER_LDG <span class="token operator">/</span> BYTES_PER_ELEMENT<span class="token punctuation">)</span> <span class="token operator">&lt;</span> headdim<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token comment">// Compute the position in the sequence (within the CTA for the moment).</span>
    <span class="token keyword">int</span> row <span class="token operator">=</span> tidx <span class="token operator">/</span> THREADS_PER_ROW<span class="token punctuation">;</span>
    <span class="token comment">// Compute the position of the thread in the row.</span>
    <span class="token keyword">int</span> col <span class="token operator">=</span> tidx <span class="token operator">%</span> THREADS_PER_ROW<span class="token punctuation">;</span>

    <span class="token keyword">uint32_t</span> row_offset <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">uint32_t</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>use_seqlen_q <span class="token operator">?</span> binfo<span class="token punctuation">.</span>sum_s_q <span class="token operator">:</span> binfo<span class="token punctuation">.</span>sum_s_k<span class="token punctuation">)</span> <span class="token operator">+</span> row<span class="token punctuation">)</span> <span class="token operator">*</span> row_stride_in_bytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
    row_offset <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token keyword">uint32_t</span><span class="token punctuation">)</span><span class="token punctuation">(</span>binfo<span class="token punctuation">.</span>bidh <span class="token operator">*</span> head_stride_in_elts <span class="token operator">*</span> BYTES_PER_ELEMENT<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// Assemble the final pointer.</span>
    ptr <span class="token operator">+=</span> row_offset <span class="token operator">+</span> col <span class="token operator">*</span> BYTES_PER_LDG<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>Gmem_tile_qkv的load就是从global mem加载到寄存器的过程，LDGS表示load当前tile需要几次，对于q矩阵为1，preds表示当前线程是否需要load对应的位置，由于q为16x32，因此只有前64线程会执行load，由于一个线程一次load16字节，所以这里使用uint4去load，结果存在了寄存器fetch_中。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">load</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> row_ <span class="token operator">=</span> tidx_ <span class="token operator">/</span> THREADS_PER_ROW<span class="token punctuation">;</span>
    <span class="token keyword">const</span> <span class="token keyword">void</span> <span class="token operator">*</span>ptrs<span class="token punctuation">[</span>LDGS<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">uint32_t</span> preds<span class="token punctuation">[</span>LDGS<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> ii <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> ii <span class="token operator">&lt;</span> LDGS<span class="token punctuation">;</span> <span class="token operator">++</span>ii <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        ptrs<span class="token punctuation">[</span>ii<span class="token punctuation">]</span> <span class="token operator">=</span> ptr <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token keyword">uint32_t</span><span class="token punctuation">)</span>ii <span class="token operator">*</span> ROWS_PER_LDG <span class="token operator">*</span> row_stride_in_bytes<span class="token punctuation">;</span>
        preds<span class="token punctuation">[</span>ii<span class="token punctuation">]</span> <span class="token operator">=</span> col_predicate <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>row_ <span class="token operator">+</span> ii <span class="token operator">*</span> ROWS_PER_LDG<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token function">min</span><span class="token punctuation">(</span>ROWS<span class="token punctuation">,</span> actual_seqlen<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        fetch_<span class="token punctuation">[</span>ii<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">make_uint4</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    Ldg_functor<span class="token operator">&lt;</span>uint4<span class="token punctuation">,</span> LDGS<span class="token operator">&gt;</span> <span class="token function">fct</span><span class="token punctuation">(</span>fetch_<span class="token punctuation">,</span> ptrs<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> ii <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> ii <span class="token operator">&lt;</span> LDGS<span class="token punctuation">;</span> <span class="token operator">++</span>ii <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        fct<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>ii<span class="token punctuation">,</span> preds<span class="token punctuation">[</span>ii<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span> <span class="token keyword">typename</span> <span class="token class-name">Smem_tile</span> <span class="token operator">&gt;</span>
    <span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">commit</span><span class="token punctuation">(</span>Smem_tile <span class="token operator">&amp;</span>smem_tile<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        smem_tile<span class="token punctuation">.</span><span class="token function">store</span><span class="token punctuation">(</span>fetch_<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">ldg</span><span class="token punctuation">(</span>uint4 <span class="token operator">&amp;</span>dst<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">void</span> <span class="token operator">*</span>ptr<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    dst <span class="token operator">=</span> <span class="token operator">*</span><span class="token generic-function"><span class="token function">reinterpret_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">const</span> uint4<span class="token operator">*</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>ptr<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>这一过程如下图所示，一个方块表示16B，方块中数字表示线程号，蓝色为第一个16x16矩阵，黄色为第二个16x16矩阵。<br> <img src="https://images2.imgbox.com/5d/5f/Z8OEVWd4_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-1 
</center> ## 寄存器到共享内存 然后回看内循环流程，先触发q，k，v从global mem load的过程，然后将q，v加载到共享内存 
<pre><code class="prism language-cpp"><span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">device_1xN_</span><span class="token punctuation">(</span><span class="token keyword">const</span> Params <span class="token operator">&amp;</span>params<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> bidb<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> bidh<span class="token punctuation">,</span> <span class="token keyword">int</span> steps<span class="token punctuation">,</span> Prng <span class="token operator">&amp;</span>ph<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> loop_step_idx<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    gmem_k<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// Trigger the loads for Q.</span>
    gmem_q<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// Trigger the loads for V.</span>
    gmem_v<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>Is_first<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token comment">// Commit the data for Q and V to shared memory.</span>
    gmem_q<span class="token punctuation">.</span><span class="token function">commit</span><span class="token punctuation">(</span>gemm_q_k<span class="token punctuation">.</span>smem_q<span class="token punctuation">)</span><span class="token punctuation">;</span>
    gmem_v<span class="token punctuation">.</span><span class="token function">commit</span><span class="token punctuation">(</span>smem_v<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>smem_q的类型为Smem_tile_q，继承关系如下</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span>
    <span class="token comment">// The description of the tile computed by this CTA.</span>
    <span class="token keyword">typename</span> <span class="token class-name">Cta_tile</span><span class="token punctuation">,</span>
    <span class="token comment">// The number of rows in the 2D shared memory buffer.</span>
    <span class="token keyword">int</span> M_<span class="token punctuation">,</span>
    <span class="token comment">// The number of cols.</span>
    <span class="token keyword">int</span> N_<span class="token punctuation">,</span>
    <span class="token comment">// The size in bits of each element.</span>
    <span class="token keyword">int</span> BITS_PER_ELEMENT_<span class="token punctuation">,</span>
    <span class="token comment">// The number of bytes per STS.</span>
    <span class="token keyword">int</span> BYTES_PER_STS_ <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span>
    <span class="token comment">// The number of buffers. (Used in multistage and double buffer cases.)</span>
    <span class="token keyword">int</span> BUFFERS_PER_TILE_ <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token comment">// Do we enable the fast path for LDS.128 and friends.</span>
    <span class="token keyword">int</span> ENABLE_LDS_FAST_PATH_ <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token comment">// The number of rows that are used for the XOR swizzling to allow fast STS/LDS.</span>
    <span class="token keyword">int</span> ROWS_PER_XOR_PATTERN_ <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">,</span>
    <span class="token comment">// The number of cols that are used for the XOR swizzling to allow fast STS/LDS.</span>
    <span class="token keyword">int</span> COLS_PER_XOR_PATTERN_ <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token comment">// Use or not predicates</span>
    <span class="token keyword">bool</span> USE_PREDICATES_ <span class="token operator">=</span> <span class="token boolean">true</span>
<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Smem_tile_without_skews</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span>
    <span class="token comment">// The dimensions of the tile computed by the CTA.</span>
    <span class="token keyword">typename</span> <span class="token class-name">Cta_tile</span><span class="token punctuation">,</span>
    <span class="token comment">// The size of the STS.</span>
    <span class="token keyword">int</span> BYTES_PER_STS<span class="token punctuation">,</span>
    <span class="token comment">// The number of buffers per tile.</span>
    <span class="token keyword">int</span> BUFFERS_PER_TILE<span class="token punctuation">,</span>
    <span class="token comment">// How many rows to use for the XOR pattern to avoid bank conflicts?</span>
    <span class="token keyword">int</span> ROWS_PER_XOR_PATTERN_ <span class="token operator">=</span> Rows_per_xor_pattern_row_a<span class="token operator">&lt;</span>Cta_tile<span class="token double-colon punctuation">::</span>K<span class="token operator">&gt;</span><span class="token double-colon punctuation">::</span>VALUE
<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Smem_tile_row_a</span> <span class="token operator">:</span> <span class="token base-clause"><span class="token keyword">public</span> <span class="token class-name">Smem_tile_without_skews</span><span class="token operator">&lt;</span><span class="token class-name">Cta_tile</span><span class="token punctuation">,</span>
                                                               Cta_tile<span class="token double-colon punctuation">::</span><span class="token class-name">M</span><span class="token punctuation">,</span>
                                                               Cta_tile<span class="token double-colon punctuation">::</span><span class="token class-name">K</span><span class="token punctuation">,</span>
                                                               fmha<span class="token double-colon punctuation">::</span><span class="token class-name">BITS_PER_ELEMENT_A</span><span class="token punctuation">,</span>
                                                               <span class="token class-name">BYTES_PER_STS</span><span class="token punctuation">,</span>
                                                               <span class="token class-name">BUFFERS_PER_TILE</span><span class="token punctuation">,</span>
                                                               <span class="token number">0</span><span class="token punctuation">,</span>
                                                               <span class="token class-name">ROWS_PER_XOR_PATTERN_</span><span class="token punctuation">,</span>
                                                               <span class="token number">1</span><span class="token operator">&gt;</span> 
                                                               

<span class="token keyword">template</span><span class="token operator">&lt;</span>
    <span class="token comment">// The dimensions of the tile computed by the CTA.</span>
    <span class="token keyword">typename</span> <span class="token class-name">Cta_tile</span><span class="token punctuation">,</span>
    <span class="token comment">// The size of the STS.</span>
    <span class="token keyword">int</span> <span class="token class-name">BYTES_PER_STS</span><span class="token punctuation">,</span>
    <span class="token comment">// The number of buffers per tile.</span>
    <span class="token keyword">int</span> <span class="token class-name">BUFFERS_PER_TILE</span>
<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Smem_tile_a</span><span class="token operator">&lt;</span><span class="token class-name">Cta_tile</span><span class="token punctuation">,</span> <span class="token class-name">Row</span><span class="token punctuation">,</span> <span class="token class-name">BYTES_PER_STS</span><span class="token punctuation">,</span> <span class="token class-name">BUFFERS_PER_TILE</span><span class="token operator">&gt;</span>
    <span class="token operator">:</span> <span class="token keyword">public</span> <span class="token class-name">Smem_tile_row_a</span><span class="token operator">&lt;</span><span class="token class-name">Cta_tile</span><span class="token punctuation">,</span>
                                    <span class="token class-name">BYTES_PER_STS</span><span class="token punctuation">,</span>
                                    <span class="token class-name">BUFFERS_PER_TILE</span><span class="token operator">&gt;</span></span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// The base class.</span>
    <span class="token keyword">using</span> Base <span class="token operator">=</span> Smem_tile_row_a<span class="token operator">&lt;</span>Cta_tile<span class="token punctuation">,</span> BYTES_PER_STS<span class="token punctuation">,</span> BUFFERS_PER_TILE<span class="token operator">&gt;</span><span class="token punctuation">;</span>

    <span class="token comment">// Ctor.</span>
    <span class="token keyword">inline</span> __device__ <span class="token function">Smem_tile_a</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>smem<span class="token punctuation">,</span> <span class="token keyword">int</span> tidx<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token function">Base</span><span class="token punctuation">(</span>smem<span class="token punctuation">,</span> tidx<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
</code></pre> 
<p>先看下构造函数，主要就是设置当前线程应该写哪里</p> 
<pre><code class="prism language-cpp"><span class="token keyword">inline</span> __device__ <span class="token function">Smem_tile_without_skews</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>smem<span class="token punctuation">,</span> <span class="token keyword">int</span> tidx<span class="token punctuation">)</span>
    <span class="token operator">:</span> <span class="token function">smem_</span><span class="token punctuation">(</span><span class="token function">__nvvm_get_smem_pointer</span><span class="token punctuation">(</span>smem<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">tidx_</span><span class="token punctuation">(</span>tidx<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token comment">// The row written by a thread. See doc/mma_smem_layout.xlsx.</span>
    <span class="token keyword">int</span> smem_write_row <span class="token operator">=</span> tidx <span class="token operator">/</span> THREADS_PER_ROW<span class="token punctuation">;</span>

    <span class="token comment">// The XOR pattern.</span>
    <span class="token keyword">int</span> smem_write_xor <span class="token operator">=</span> smem_write_row <span class="token operator">%</span> ROWS_PER_XOR_PATTERN <span class="token operator">*</span> COLS_PER_XOR_PATTERN<span class="token punctuation">;</span>
    <span class="token comment">// Compute the column and apply the XOR pattern.</span>
    <span class="token keyword">int</span> smem_write_col <span class="token operator">=</span> <span class="token punctuation">(</span>tidx <span class="token operator">%</span> THREADS_PER_ROW<span class="token punctuation">)</span> <span class="token operator">^</span> smem_write_xor<span class="token punctuation">;</span>

    <span class="token comment">// The offset.</span>
    <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_write_offset_ <span class="token operator">=</span> smem_write_row<span class="token operator">*</span>BYTES_PER_ROW <span class="token operator">+</span> smem_write_col<span class="token operator">*</span>BYTES_PER_STS<span class="token punctuation">;</span>

<span class="token punctuation">}</span>
</code></pre> 
<p>gmem的commit其实执行的就是smem的store，由于q矩阵每个线程只需要store一次，即N为1，因此只是在smem_write_offset_ 处写一次即可。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span> <span class="token keyword">int</span> N <span class="token operator">&gt;</span>
<span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">compute_store_pointers</span><span class="token punctuation">(</span><span class="token keyword">uint32_t</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>ptrs<span class="token punctuation">)</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> ii <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> ii <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>ii <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// Decompose the STS into row/col.</span>
        <span class="token keyword">int</span> row <span class="token operator">=</span> ii <span class="token operator">/</span> STS_PER_ROW<span class="token punctuation">;</span>
        <span class="token keyword">int</span> col <span class="token operator">=</span> ii <span class="token operator">%</span> STS_PER_ROW<span class="token punctuation">;</span>

        <span class="token comment">// Assemble the offset.</span>
        <span class="token keyword">int</span> offset <span class="token operator">=</span> smem_write_offset_ <span class="token operator">+</span> row<span class="token operator">*</span>ROWS_PER_STS<span class="token operator">*</span>BYTES_PER_ROW<span class="token punctuation">;</span>

        <span class="token comment">// Take the column into account.</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span> STS_PER_ROW <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            offset <span class="token operator">+=</span> col<span class="token operator">*</span>THREADS_PER_ROW<span class="token operator">*</span>BYTES_PER_STS<span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        <span class="token comment">// Apply the XOR pattern if needed.</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span> ROWS_PER_STS <span class="token operator">&lt;</span> ROWS_PER_XOR_PATTERN <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">const</span> <span class="token keyword">int</span> m <span class="token operator">=</span> row <span class="token operator">*</span> ROWS_PER_STS <span class="token operator">%</span> ROWS_PER_XOR_PATTERN<span class="token punctuation">;</span>
            offset <span class="token operator">^=</span> m <span class="token operator">*</span> COLS_PER_XOR_PATTERN <span class="token operator">*</span> BYTES_PER_STS<span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        ptrs<span class="token punctuation">[</span>ii<span class="token punctuation">]</span> <span class="token operator">=</span> smem_ <span class="token operator">+</span> offset<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span> <span class="token keyword">int</span> N <span class="token operator">&gt;</span>
<span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">store</span><span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token function">Store_type</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>data<span class="token punctuation">)</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">uint64_t</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">uint32_t</span> smem_ptrs<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">this</span><span class="token operator">-&gt;</span><span class="token function">compute_store_pointers</span><span class="token punctuation">(</span>smem_ptrs<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// Trying to reduce the shared mem for Q from 4KB per buffer to 2KB per buffer.</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>PARTIAL_STORE <span class="token operator">||</span> <span class="token punctuation">(</span>tidx_ <span class="token operator">/</span> THREADS_PER_ROW <span class="token operator">&lt;</span> ROWS<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">sts</span><span class="token punctuation">(</span>smem_ptrs<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>写完之后如下，每个格子为16B，即8个FP16，Ti为线程id，和global mem中对应，这个过程中不会bank冲突<br> <img src="https://images2.imgbox.com/d2/9c/LAtl5Ptq_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-2 
</center> ## Q乘K 然后再回看内循环流程，gemm_q_k负责第一个矩阵运算，即QK，这里会load Q和K。 
<pre><code class="prism language-cpp"><span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">device_1xN_</span><span class="token punctuation">(</span><span class="token keyword">const</span> Params <span class="token operator">&amp;</span>params<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> bidb<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> bidh<span class="token punctuation">,</span> <span class="token keyword">int</span> steps<span class="token punctuation">,</span> Prng <span class="token operator">&amp;</span>ph<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> loop_step_idx<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    gemm_q_k<span class="token punctuation">.</span><span class="token function">load_q</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// Load the fragments for V. We keep the data in registers during the entire kernel.</span>
    <span class="token keyword">typename</span> <span class="token class-name">Smem_tile_v</span><span class="token double-colon punctuation">::</span>Fragment frag_v<span class="token punctuation">[</span>Mma_tile_o<span class="token double-colon punctuation">::</span>MMAS_K<span class="token punctuation">]</span><span class="token punctuation">[</span>Mma_tile_o<span class="token double-colon punctuation">::</span>MMAS_N<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> ki <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> ki <span class="token operator">&lt;</span> Mma_tile_o<span class="token double-colon punctuation">::</span>MMAS_K<span class="token punctuation">;</span> <span class="token operator">++</span>ki <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        smem_v<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>frag_v<span class="token punctuation">[</span>ki<span class="token punctuation">]</span><span class="token punctuation">,</span> ki<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// Commit the data for V to shared memory if it has not been done already.</span>
    <span class="token keyword">if</span><span class="token punctuation">(</span> Kernel_traits<span class="token double-colon punctuation">::</span>SHARE_SMEM_FOR_K_AND_V <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// Make sure we are done loading the fragments for K.</span>
        <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// Commit the data to shared memory for V.</span>
        gmem_k<span class="token punctuation">.</span><span class="token function">commit</span><span class="token punctuation">(</span>gemm_q_k<span class="token punctuation">.</span>smem_k<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// Make sure the data is in shared memory.</span>
        <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// Load the fragments for K. </span>
    gemm_q_k<span class="token punctuation">.</span><span class="token function">load_k</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>实就是通过ldmatrix指令将数据从shared mem中load到寄存器中，首先看下Gemm_Q_K的继承关系，成员就是Fragment和两个Smem_tile，Fragment的核心成员就是多个32位寄存器变量。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Gemm_Q_K_base</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">using</span> Smem_tile_o <span class="token operator">=</span> <span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token double-colon punctuation">::</span>Smem_tile_o<span class="token punctuation">;</span>
    <span class="token keyword">using</span> Smem_tile_q <span class="token operator">=</span> <span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token double-colon punctuation">::</span>Smem_tile_q<span class="token punctuation">;</span>
    <span class="token keyword">using</span> Smem_tile_k <span class="token operator">=</span> <span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token double-colon punctuation">::</span>Smem_tile_k<span class="token punctuation">;</span>
    <span class="token keyword">using</span> Fragment_q <span class="token operator">=</span> <span class="token keyword">typename</span> <span class="token class-name">Smem_tile_q</span><span class="token double-colon punctuation">::</span>Fragment<span class="token punctuation">;</span>
    <span class="token keyword">using</span> Fragment_k <span class="token operator">=</span> <span class="token keyword">typename</span> <span class="token class-name">Smem_tile_k</span><span class="token double-colon punctuation">::</span>Fragment<span class="token punctuation">;</span>

    <span class="token comment">// The description of the CTA tile for the 1st batched GEMM.</span>
    <span class="token keyword">using</span> Cta_tile_p <span class="token operator">=</span> <span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token double-colon punctuation">::</span>Cta_tile_p<span class="token punctuation">;</span>

    <span class="token comment">// The MMA tile for the 1st GEMM.</span>
    <span class="token keyword">using</span> Mma_tile_p <span class="token operator">=</span> fmha<span class="token double-colon punctuation">::</span>Hmma_tile<span class="token operator">&lt;</span>Cta_tile_p<span class="token operator">&gt;</span><span class="token punctuation">;</span>

    <span class="token keyword">static</span> <span class="token keyword">constexpr</span> <span class="token keyword">int</span> SMEM_BYTES_SOFTMAX <span class="token operator">=</span> Cta_tile_p<span class="token double-colon punctuation">::</span>M <span class="token operator">*</span> Cta_tile_p<span class="token double-colon punctuation">::</span>WARPS_N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">;</span>

    __device__ <span class="token keyword">inline</span> <span class="token function">Gemm_Q_K_base</span><span class="token punctuation">(</span><span class="token keyword">char</span> <span class="token operator">*</span> smem_ptr_q<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span> smem_ptr_k<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> tidx<span class="token punctuation">)</span> 
        <span class="token operator">:</span> <span class="token function">smem_q</span><span class="token punctuation">(</span>smem_ptr_q<span class="token punctuation">,</span> tidx<span class="token punctuation">)</span>
        <span class="token punctuation">,</span> <span class="token function">smem_k</span><span class="token punctuation">(</span>smem_ptr_k<span class="token punctuation">,</span> tidx<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token punctuation">}</span>

    __device__ <span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token function">load_q</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        smem_q<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>frag_q<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    __device__ <span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token function">reload_q</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        smem_q<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>frag_q<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    Fragment_q frag_q<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span>Mma_tile_p<span class="token double-colon punctuation">::</span>MMAS_M<span class="token punctuation">]</span><span class="token punctuation">;</span>
    Smem_tile_q smem_q<span class="token punctuation">;</span>
    Smem_tile_k smem_k<span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token punctuation">,</span> <span class="token keyword">bool</span> K_in_regs<span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">elem_type_</span><span class="token operator">=</span>__half<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Gemm_Q_K</span> <span class="token operator">:</span> <span class="token keyword">public</span> Gemm_Q_K_base<span class="token operator">&lt;</span>Kernel_traits<span class="token operator">&gt;</span>
</code></pre> 
<p>然后看下Smem_tile如何执行load，在构造函数中会计算出每个线程应该读哪行哪列，如图2-7</p> 
<pre><code class="prism language-cpp"><span class="token keyword">inline</span> __device__ <span class="token function">Smem_tile_row_a</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>smem<span class="token punctuation">,</span> <span class="token keyword">int</span> tidx<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token function">Base</span><span class="token punctuation">(</span>smem<span class="token punctuation">,</span> tidx<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> WARPS_M <span class="token operator">=</span> Cta_tile<span class="token double-colon punctuation">::</span>WARPS_M<span class="token punctuation">;</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> WARPS_N <span class="token operator">=</span> Cta_tile<span class="token double-colon punctuation">::</span>WARPS_N<span class="token punctuation">;</span>
    <span class="token keyword">const</span> <span class="token keyword">int</span> WARPS_K <span class="token operator">=</span> Cta_tile<span class="token double-colon punctuation">::</span>WARPS_K<span class="token punctuation">;</span>

    <span class="token keyword">static_assert</span><span class="token punctuation">(</span>WARPS_M <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">static_assert</span><span class="token punctuation">(</span>WARPS_N <span class="token operator">==</span> <span class="token number">4</span> <span class="token operator">||</span> WARPS_N <span class="token operator">==</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">static_assert</span><span class="token punctuation">(</span>WARPS_K <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">static_assert</span><span class="token punctuation">(</span>Base<span class="token double-colon punctuation">::</span>ROWS_PER_XOR_PATTERN <span class="token operator">==</span> <span class="token number">2</span> <span class="token operator">||</span> Base<span class="token double-colon punctuation">::</span>ROWS_PER_XOR_PATTERN <span class="token operator">==</span> <span class="token number">4</span> <span class="token operator">||</span> Base<span class="token double-colon punctuation">::</span>ROWS_PER_XOR_PATTERN <span class="token operator">==</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// The row and column read by the thread.</span>
    <span class="token keyword">int</span> smem_read_row  <span class="token operator">=</span> <span class="token punctuation">(</span>tidx <span class="token operator">&amp;</span> <span class="token number">0x0f</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">constexpr</span> <span class="token keyword">int</span> ROWS_PER_PACKING <span class="token operator">=</span> Base<span class="token double-colon punctuation">::</span>BYTES_PER_ROW <span class="token operator">/</span> Base<span class="token double-colon punctuation">::</span>BYTES_PER_ROW_BEFORE_PACKING<span class="token punctuation">;</span>                              <span class="token comment">// 2</span>
    <span class="token keyword">int</span> smem_read_col <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>smem_read_row <span class="token operator">/</span> ROWS_PER_PACKING<span class="token punctuation">)</span> <span class="token operator">%</span> Base<span class="token double-colon punctuation">::</span>ROWS_PER_XOR_PATTERN<span class="token punctuation">)</span> <span class="token operator">*</span> Base<span class="token double-colon punctuation">::</span>COLS_PER_XOR_PATTERN<span class="token punctuation">;</span>
    smem_read_col <span class="token operator">^=</span> <span class="token punctuation">(</span>tidx <span class="token operator">&amp;</span> <span class="token number">0x10</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">16</span><span class="token punctuation">;</span>

    <span class="token comment">// The shared memory offset.</span>
    <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_read_offset_ <span class="token operator">=</span> smem_read_row<span class="token operator">*</span>Base<span class="token double-colon punctuation">::</span>BYTES_PER_ROW_BEFORE_PACKING <span class="token operator">+</span> smem_read_col<span class="token operator">*</span>BYTES_PER_LDS<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>然后执行load，通过ldmatrix将数据从shared mem load到了寄存器，执行结束之后，寄存器变量和原始矩阵关系如图2-5，load结束后会计算smem_read_offset，指向下一个16x16矩阵，即k维度上边的下一个矩阵。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">load</span><span class="token punctuation">(</span><span class="token function">Fragment</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>a<span class="token punctuation">)</span><span class="token punctuation">[</span>Mma_tile<span class="token double-colon punctuation">::</span>MMAS_M<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">int</span> ki<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> mi <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> mi <span class="token operator">&lt;</span> Mma_tile<span class="token double-colon punctuation">::</span>MMAS_M<span class="token punctuation">;</span> <span class="token operator">++</span>mi <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// Jump by as many matrix rows as needed (a row in smem may pack multiple matrix rows).</span>
        <span class="token keyword">int</span> offset <span class="token operator">=</span> mi <span class="token operator">*</span> Mma_tile<span class="token double-colon punctuation">::</span>M_PER_MMA_PER_CTA <span class="token operator">*</span> Base<span class="token double-colon punctuation">::</span>BYTES_PER_ROW_BEFORE_PACKING<span class="token punctuation">;</span>

        <span class="token comment">// Load using LDSM.M88.4.</span>
        uint4 tmp<span class="token punctuation">;</span>
        <span class="token comment">// ldsm(tmp, this-&gt;smem_ + this-&gt;smem_read_offset_ + this-&gt;smem_read_buffer_ + offset);</span>
        <span class="token function">ldsm</span><span class="token punctuation">(</span>tmp<span class="token punctuation">,</span> <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_ <span class="token operator">+</span> <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_read_offset_ <span class="token operator">+</span> offset<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// Store the value into the fragment.</span>
        a<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">=</span> tmp<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
        a<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">=</span> tmp<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
        a<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">=</span> tmp<span class="token punctuation">.</span>z<span class="token punctuation">;</span>
        a<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">=</span> tmp<span class="token punctuation">.</span>w<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// Move the offset to the next possition. See doc/mma_smem_layout.xlsx.</span>
    <span class="token keyword">static_assert</span><span class="token punctuation">(</span>Mma_tile_with_padding<span class="token double-colon punctuation">::</span>MMAS_K <span class="token operator">&lt;</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token string">"Not implemented"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span><span class="token punctuation">(</span>        Mma_tile_with_padding<span class="token double-colon punctuation">::</span>MMAS_K <span class="token operator">&gt;=</span> <span class="token number">32</span> <span class="token operator">&amp;&amp;</span> ki <span class="token operator">%</span> <span class="token number">16</span> <span class="token operator">==</span> <span class="token number">15</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_read_offset_ <span class="token operator">^=</span> <span class="token number">31</span> <span class="token operator">*</span> BYTES_PER_LDS <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span> Mma_tile_with_padding<span class="token double-colon punctuation">::</span>MMAS_K <span class="token operator">&gt;=</span> <span class="token number">16</span> <span class="token operator">&amp;&amp;</span> ki <span class="token operator">%</span>  <span class="token number">8</span> <span class="token operator">==</span>  <span class="token number">7</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_read_offset_ <span class="token operator">^=</span> <span class="token number">15</span> <span class="token operator">*</span> BYTES_PER_LDS <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span> Mma_tile_with_padding<span class="token double-colon punctuation">::</span>MMAS_K <span class="token operator">&gt;=</span>  <span class="token number">8</span> <span class="token operator">&amp;&amp;</span> ki <span class="token operator">%</span>  <span class="token number">4</span> <span class="token operator">==</span>  <span class="token number">3</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_read_offset_ <span class="token operator">^=</span>  <span class="token number">7</span> <span class="token operator">*</span> BYTES_PER_LDS <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span> Mma_tile_with_padding<span class="token double-colon punctuation">::</span>MMAS_K <span class="token operator">&gt;=</span>  <span class="token number">4</span> <span class="token operator">&amp;&amp;</span> ki <span class="token operator">%</span>  <span class="token number">2</span> <span class="token operator">==</span>  <span class="token number">1</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_read_offset_ <span class="token operator">^=</span>  <span class="token number">3</span> <span class="token operator">*</span> BYTES_PER_LDS <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token keyword">if</span><span class="token punctuation">(</span> Mma_tile_with_padding<span class="token double-colon punctuation">::</span>MMAS_K <span class="token operator">&gt;=</span>  <span class="token number">2</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">this</span><span class="token operator">-&gt;</span>smem_read_offset_ <span class="token operator">^=</span>  <span class="token number">1</span> <span class="token operator">*</span> BYTES_PER_LDS <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>然后执行矩阵运算，注意这里做了访存和计算的流水线，先load下一个矩阵，再执行当前的计算，结果存到Fragment acc_p的寄存器中。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Acc</span><span class="token punctuation">,</span> <span class="token keyword">int</span> M<span class="token punctuation">,</span> <span class="token keyword">int</span> N<span class="token operator">&gt;</span>
    __device__ <span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token keyword">operator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token function">Acc</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>acc_p<span class="token punctuation">)</span><span class="token punctuation">[</span>M<span class="token punctuation">]</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
        <span class="token comment">// Do this part of P^T = (Q * K^T)^T.</span>
        <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
        <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> ki <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> ki <span class="token operator">&lt;</span> Mma_tile_p<span class="token double-colon punctuation">::</span>MMAS_K<span class="token punctuation">;</span> <span class="token operator">++</span>ki <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// Trigger the load from shared memory for the next series of Q values.</span>
            Base<span class="token double-colon punctuation">::</span>smem_q<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>Base<span class="token double-colon punctuation">::</span>frag_q<span class="token punctuation">[</span>ki <span class="token operator">&amp;</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ki<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token comment">// Do the math for the values already in registers.</span>
            fmha<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">gemm_cl</span><span class="token generic class-name"><span class="token operator">&lt;</span>elem_type<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>acc_p<span class="token punctuation">,</span> Base<span class="token double-colon punctuation">::</span>frag_q<span class="token punctuation">[</span><span class="token punctuation">(</span>ki <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> frag_k<span class="token punctuation">[</span><span class="token punctuation">(</span>ki <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        <span class="token comment">// Do the final stage of math.</span>
        <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">int</span> ki <span class="token operator">=</span> Mma_tile_p<span class="token double-colon punctuation">::</span>MMAS_K<span class="token punctuation">;</span>
            fmha<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">gemm_cl</span><span class="token generic class-name"><span class="token operator">&lt;</span>elem_type<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>acc_p<span class="token punctuation">,</span> Base<span class="token double-colon punctuation">::</span>frag_q<span class="token punctuation">[</span><span class="token punctuation">(</span>ki <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> frag_k<span class="token punctuation">[</span><span class="token punctuation">(</span>ki <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>

</code></pre> 
<p>这里gemm_cl用了cutlass，我们直接看下原始apex的逻辑，其实就是对每个16x16的tile执行mma函数，mma函数中会执行两次16x8x16的mma.sync</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Acc</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">A</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">B</span><span class="token punctuation">,</span> <span class="token keyword">int</span> M<span class="token punctuation">,</span> <span class="token keyword">int</span> N<span class="token operator">&gt;</span>
<span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">gemm</span><span class="token punctuation">(</span><span class="token function">Acc</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>acc<span class="token punctuation">)</span><span class="token punctuation">[</span>M<span class="token punctuation">]</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token function">A</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>a<span class="token punctuation">)</span><span class="token punctuation">[</span>M<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token function">B</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>b<span class="token punctuation">)</span><span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> mi <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> mi <span class="token operator">&lt;</span> M<span class="token punctuation">;</span> <span class="token operator">++</span>mi <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
        <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> ni <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> ni <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>ni <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">mma</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">,</span> b<span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span> <span class="token keyword">typename</span> <span class="token class-name">Layout_a</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Layout_b</span> <span class="token operator">&gt;</span>
<span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">mma</span><span class="token punctuation">(</span><span class="token keyword">const</span> Fragment_a<span class="token operator">&lt;</span>Layout_a<span class="token operator">&gt;</span> <span class="token operator">&amp;</span>a<span class="token punctuation">,</span>
                           <span class="token keyword">const</span> Fragment_b<span class="token operator">&lt;</span>Layout_b<span class="token operator">&gt;</span> <span class="token operator">&amp;</span>b<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">asm</span> <span class="token keyword">volatile</span><span class="token punctuation">(</span> \
        <span class="token string">"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 \n"</span> \
        <span class="token string">"    {%0, %1, %2, %3}, \n"</span> \
        <span class="token string">"    {%4, %5, %6, %7}, \n"</span> \
        <span class="token string">"    {%8, %9}, \n"</span> \
        <span class="token string">"    {%0, %1, %2, %3}; \n"</span> \
                <span class="token operator">:</span> <span class="token string">"+f"</span><span class="token punctuation">(</span>  <span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"+f"</span><span class="token punctuation">(</span>  <span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"+f"</span><span class="token punctuation">(</span>  <span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"+f"</span><span class="token punctuation">(</span>  <span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token operator">:</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">asm</span> <span class="token keyword">volatile</span><span class="token punctuation">(</span> \
        <span class="token string">"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 \n"</span> \
        <span class="token string">"    {%0, %1, %2, %3}, \n"</span> \
        <span class="token string">"    {%4, %5, %6, %7}, \n"</span> \
        <span class="token string">"    {%8, %9}, \n"</span> \
        <span class="token string">"    {%0, %1, %2, %3}; \n"</span> \
                <span class="token operator">:</span> <span class="token string">"+f"</span><span class="token punctuation">(</span>  <span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"+f"</span><span class="token punctuation">(</span>  <span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"+f"</span><span class="token punctuation">(</span>  <span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"+f"</span><span class="token punctuation">(</span>  <span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token operator">:</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">"r"</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span><span class="token function">reg</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

</code></pre> 
<p>对于第一个线程的第一个acc_p的第一个Fragment，寄存器和结果矩阵对应关系如下，黄色为第一个16x8，蓝色为第二个16x8<br> <img src="https://images2.imgbox.com/dc/ec/oIqaPDJT_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-3 
</center> cta中warp的组织格式为m1n4k1，Q矩阵为16x32，K矩阵为32x128，warp排布如下，图3-3对应图3-4 warp0的第一个16x16的计算结果w01 
<p><img src="https://images2.imgbox.com/97/4f/y03rOUve_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-4 
</center> 到现在就完成了QK的计算 ## softmax 接下来要计算max，看下Softmax这个类，核心数据结构如下，其中elt_是存储acc_p的输出，Smem_tile_red为共享内存，用于计算P的max和sum 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Cta_tile</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Softmax_base</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    <span class="token keyword">float</span> elt_<span class="token punctuation">[</span>MMAS_M <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">[</span>MMAS_N <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Cta_tile</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Softmax</span> <span class="token operator">:</span> <span class="token base-clause"><span class="token keyword">public</span> <span class="token class-name">Softmax_base</span><span class="token operator">&lt;</span><span class="token class-name">Cta_tile</span><span class="token punctuation">,</span> <span class="token class-name">Kernel_traits</span><span class="token operator">&gt;</span></span> <span class="token punctuation">{<!-- --></span>

    Smem_tile_red smem_max_<span class="token punctuation">;</span>
    Smem_tile_red smem_sum_<span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
</code></pre> 
<p>然后继续看下内循环</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Kernel_traits</span><span class="token punctuation">,</span> <span class="token keyword">bool</span> Is_dropout<span class="token punctuation">,</span> <span class="token keyword">bool</span> Is_causal<span class="token punctuation">,</span> <span class="token keyword">bool</span> Return_softmax<span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Params</span><span class="token operator">&gt;</span>
<span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">device_1xN_loop</span><span class="token punctuation">(</span><span class="token keyword">const</span> Params <span class="token operator">&amp;</span>params<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
    softmax<span class="token punctuation">.</span><span class="token function">unpack_noscale</span><span class="token punctuation">(</span>acc_p<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">float</span> p_max<span class="token punctuation">[</span>Mma_tile_p<span class="token double-colon punctuation">::</span>MMAS_M <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
    softmax<span class="token punctuation">.</span><span class="token keyword">template</span> reduce_max<span class="token operator">&lt;</span><span class="token comment">/*zero_init=*/</span>Is_first<span class="token operator">&gt;</span><span class="token punctuation">(</span>p_max<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
 <span class="token punctuation">}</span>
</code></pre> 
<p>首先通过unpack_noscale将数据从acc_p中存到Softmax的elt_。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">inline</span> __device__ <span class="token keyword">void</span> <span class="token function">unpack_noscale</span><span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token function">Accumulator</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>acc<span class="token punctuation">)</span><span class="token punctuation">[</span>MMAS_M<span class="token punctuation">]</span><span class="token punctuation">[</span>MMAS_N<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> mi <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> mi <span class="token operator">&lt;</span> MMAS_M<span class="token punctuation">;</span> <span class="token operator">++</span>mi <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
        <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> ni <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> ni <span class="token operator">&lt;</span> MMAS_N<span class="token punctuation">;</span> <span class="token operator">++</span>ni <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// 1st row - 4 elements per row.</span>
            <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> mi <span class="token operator">+</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> ni <span class="token operator">+</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> mi <span class="token operator">+</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> ni <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> mi <span class="token operator">+</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> ni <span class="token operator">+</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> mi <span class="token operator">+</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> ni <span class="token operator">+</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token comment">// 2nd row - 4 elements per row.</span>
            <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> mi <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> ni <span class="token operator">+</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> mi <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> ni <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> mi <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> ni <span class="token operator">+</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> mi <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span> <span class="token operator">*</span> ni <span class="token operator">+</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> acc<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">elt</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>w00 unpack之后的数据在softmax中分布如图3-4左侧k = 0,1,2,3，w01 unpack之后如图3-4右侧k = 4,5,6,7<br> <img src="https://images2.imgbox.com/b6/19/ZYpmd3KL_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-5 
</center> 然后看下求max的过程，后续求sum过程一致，就不再赘述了。 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">bool</span> zero_init<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Operator</span><span class="token operator">&gt;</span>
__device__ <span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token function">reduce_</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>frag<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> MMAS_M<span class="token punctuation">]</span><span class="token punctuation">,</span> Operator <span class="token operator">&amp;</span>op<span class="token punctuation">,</span> Smem_tile_red <span class="token operator">&amp;</span> smem_red<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token generic-function"><span class="token function">thread_reduce_</span><span class="token generic class-name"><span class="token operator">&lt;</span>zero_init<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>frag<span class="token punctuation">,</span> op<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">quad_reduce</span><span class="token punctuation">(</span>frag<span class="token punctuation">,</span> frag<span class="token punctuation">,</span> op<span class="token punctuation">)</span><span class="token punctuation">;</span>
    smem_red<span class="token punctuation">.</span><span class="token function">store</span><span class="token punctuation">(</span>frag<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">typename</span> <span class="token class-name">Smem_tile_red</span><span class="token double-colon punctuation">::</span>read_t tmp<span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> MMAS_M<span class="token punctuation">]</span><span class="token punctuation">;</span>
    smem_red<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>tmp<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">quad_allreduce</span><span class="token punctuation">(</span>frag<span class="token punctuation">,</span> tmp<span class="token punctuation">,</span> op<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>第一步为执行thread_reduce，就是将单个线程内同一行的做一次reduce，对于图3-4，m=0,2的8个float会执行一次reduce得到个最大值存到p_max[0]，m=2,3的8个float会执行一次reduce得到个最大值存到p_max[1]</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">bool</span> zero_init<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Operator</span><span class="token operator">&gt;</span>
__device__ <span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token function">thread_reduce_</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>frag<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span> <span class="token operator">*</span> MMAS_M<span class="token punctuation">]</span><span class="token punctuation">,</span> Operator <span class="token operator">&amp;</span>op<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> mi <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> mi <span class="token operator">&lt;</span> <span class="token number">2</span> <span class="token operator">*</span> MMAS_M<span class="token punctuation">;</span> mi<span class="token operator">++</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        frag<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">=</span> zero_init <span class="token operator">?</span> <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">:</span> <span class="token function">op</span><span class="token punctuation">(</span>frag<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
        <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> ni <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> ni <span class="token operator">&lt;</span> <span class="token number">4</span> <span class="token operator">*</span> MMAS_N<span class="token punctuation">;</span> ni<span class="token operator">++</span> <span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            frag<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">op</span><span class="token punctuation">(</span>frag<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">this</span><span class="token operator">-&gt;</span>elt_<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>第二步执行warp内同一行的reduce，T0-3一行，T4-7一行，因此要执行quad之间的reduce，这里使用warp shuffle来做的，经过第一次shuffle之后T0 = max(T0, T2)，T1 = max(T1, T3)，经过第二次shuffle之后T0就拿到了当前warp当前行(即第0行)的最大值</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Operator</span><span class="token punctuation">,</span> <span class="token keyword">int</span> M<span class="token operator">&gt;</span>
__device__ <span class="token keyword">inline</span> <span class="token keyword">void</span>  <span class="token function">quad_reduce</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>dst<span class="token punctuation">)</span><span class="token punctuation">[</span>M<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>src<span class="token punctuation">)</span><span class="token punctuation">[</span>M<span class="token punctuation">]</span><span class="token punctuation">,</span> Operator <span class="token operator">&amp;</span>op<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> mi<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> mi <span class="token operator">&lt;</span> M<span class="token punctuation">;</span> mi<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
        dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">=</span> src<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">;</span>
        dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">op</span><span class="token punctuation">(</span>dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token function">__shfl_down_sync</span><span class="token punctuation">(</span><span class="token keyword">uint32_t</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">op</span><span class="token punctuation">(</span>dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token function">__shfl_down_sync</span><span class="token punctuation">(</span><span class="token keyword">uint32_t</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>第三步会将warp内的每行的最大值写入到shared mem，只有每个quad的第0个线程会写，写完之后如图3-5<br> <img src="https://images2.imgbox.com/df/02/zqZsIEg6_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-6 
</center> 第四步所有warp都会按照图3-6的数据线程排布将数据从share mem中load出来，这样每个线程就拿到了当前行其他warp的数值 
<p><img src="https://images2.imgbox.com/5c/25/yhTJyZwH_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-7 
</center> 第五步执行quad_allreduce，也是通过warp shuffle做的，以quad0为例，第一次T0 = T2 = max(T0, T2)，T1 = T3 = max(T1, T3)，第二次T0 = T1 = max(T0, T1)，T2 = T3 = max(T2, T3)，这样每个线程就都拿到了当前行的最大值。 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Operator</span><span class="token punctuation">,</span> <span class="token keyword">int</span> M<span class="token operator">&gt;</span>
__device__ <span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token function">quad_allreduce</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>dst<span class="token punctuation">)</span><span class="token punctuation">[</span>M<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>src<span class="token punctuation">)</span><span class="token punctuation">[</span>M<span class="token punctuation">]</span><span class="token punctuation">,</span> Operator <span class="token operator">&amp;</span>op<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">unroll</span></span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> mi<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> mi <span class="token operator">&lt;</span> M<span class="token punctuation">;</span> mi<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
        dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">=</span> src<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">;</span>
        dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token class-name">Allreduce</span><span class="token operator">&lt;</span><span class="token number">4</span><span class="token operator">&gt;</span><span class="token double-colon punctuation">::</span><span class="token function">run</span><span class="token punctuation">(</span>dst<span class="token punctuation">[</span>mi<span class="token punctuation">]</span><span class="token punctuation">,</span> op<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">int</span> THREADS<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Allreduce</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">static_assert</span><span class="token punctuation">(</span>THREADS <span class="token operator">==</span> <span class="token number">32</span> <span class="token operator">||</span> THREADS <span class="token operator">==</span> <span class="token number">16</span> <span class="token operator">||</span> THREADS <span class="token operator">==</span> <span class="token number">8</span> <span class="token operator">||</span> THREADS <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Operator</span><span class="token operator">&gt;</span>
    <span class="token keyword">static</span> __device__ <span class="token keyword">inline</span> T <span class="token function">run</span><span class="token punctuation">(</span>T x<span class="token punctuation">,</span> Operator <span class="token operator">&amp;</span>op<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">constexpr</span> <span class="token keyword">int</span> OFFSET <span class="token operator">=</span> THREADS <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>
        x <span class="token operator">=</span> <span class="token function">op</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token function">__shfl_xor_sync</span><span class="token punctuation">(</span><span class="token keyword">uint32_t</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> OFFSET<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> <span class="token class-name">Allreduce</span><span class="token operator">&lt;</span>OFFSET<span class="token operator">&gt;</span><span class="token double-colon punctuation">::</span><span class="token function">run</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> op<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">Allreduce</span><span class="token operator">&lt;</span><span class="token number">2</span><span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">Operator</span><span class="token operator">&gt;</span> 
<span class="token keyword">static</span> __device__ <span class="token keyword">inline</span> T <span class="token function">run</span><span class="token punctuation">(</span>T x<span class="token punctuation">,</span> Operator <span class="token operator">&amp;</span>op<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    x <span class="token operator">=</span> <span class="token function">op</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token function">__shfl_xor_sync</span><span class="token punctuation">(</span><span class="token keyword">uint32_t</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                 
    <span class="token keyword">return</span> x<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>到这里，最大值就计算出来存到p_max中了。</p> 
<p>然后根据max计算exp</p> 
<pre><code class="prism language-cpp">softmax<span class="token punctuation">.</span><span class="token function">scale_apply_exp</span><span class="token punctuation">(</span>p_max<span class="token punctuation">,</span> params<span class="token punctuation">.</span>scale_bmm1f<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>然后计算sum，这里sum整体流程和求max完全一致，不过只执行到第三步，即将quad reduce的结果写回到shared mem，原因后续会提到</p> 
<pre><code class="prism language-cpp"><span class="token keyword">float</span> p_sum<span class="token punctuation">[</span>Mma_tile_p<span class="token double-colon punctuation">::</span>MMAS_M <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
softmax<span class="token punctuation">.</span><span class="token function">reduce_sum_before_sync_</span><span class="token punctuation">(</span>p_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>然后将softmax的结果，并将softmax的FP32转为FP16存到frag_p中</p> 
<pre><code class="prism language-cpp"><span class="token keyword">using</span> Frag_p <span class="token operator">=</span> fmha<span class="token double-colon punctuation">::</span>Fragment_a<span class="token operator">&lt;</span>fmha<span class="token double-colon punctuation">::</span>Row<span class="token operator">&gt;</span><span class="token punctuation">;</span>
Frag_p frag_p<span class="token punctuation">[</span>Mma_tile_o<span class="token double-colon punctuation">::</span>MMAS_K<span class="token punctuation">]</span><span class="token punctuation">[</span>Mma_tile_o<span class="token double-colon punctuation">::</span>MMAS_M<span class="token punctuation">]</span><span class="token punctuation">;</span>
softmax<span class="token punctuation">.</span><span class="token keyword">template</span> <span class="token generic-function"><span class="token function">pack</span><span class="token generic class-name"><span class="token operator">&lt;</span>elem_type<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>frag_p<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h3><a id="PV_841"></a>P乘V</h3> 
<p>然后开始算PxV，P的shape为[16, 128]，V的shape为[128, 32]，对于QxK的warp是在M维度分块，PxV的分块在K维度，具体分块逻辑如图3-7，黄色部分为warp0负责计算。<br> <img src="https://images2.imgbox.com/3a/f8/9EL62VEj_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-8 
</center> 不过这里每个warp都有一个O矩阵，还需要将warp间的O进行reduce，这里对O的的线程分块和P不一致，因此之前在求sum的时候只执行到了第三步，原因就是线程对应的数据分块变了。具体的，这里用于reduce的share mem大小为16x128，每个warp将自己的16x32结果存到share mem的32列，如图3-8，颜色区域为第一个warp写入的。 
<p><img src="https://images2.imgbox.com/c2/6e/LcGgnIkp_o.png" alt="在这里插入图片描述"></p> 
<center>
  图 3-9 
</center> 然后再load出去，每个线程load 4行，一行8个线程，load的过程中执行reduce。除以sum之后就完成了第一次O的计算，写回global mem。 ## 递推过程 重复内循环直到完成第一次外循环，第一次外循环的计算流程本质和朴素算法一致，然后看下之后的外循环是如何完成递推的。 第一次外循环中会将中间变量写到global mem，比如o_tmp，就是O的中间结果，还保存了gmem_softmax_lse，代表max + log(sum) 
<pre><code class="prism language-cpp"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> jj <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> jj <span class="token operator">&lt;</span> Gmem_tile_o<span class="token double-colon punctuation">::</span>STGS_PER_LOOP<span class="token punctuation">;</span> jj<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">float</span> sum <span class="token operator">=</span> p_sum_o<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
    p_sum_log<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>sum <span class="token operator">==</span> <span class="token number">0.f</span> <span class="token operator">||</span> sum <span class="token operator">!=</span> sum<span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token operator">-</span>INFINITY <span class="token operator">:</span> p_max_o<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token function">__logf</span><span class="token punctuation">(</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
  
    <span class="token keyword">if</span> <span class="token punctuation">(</span>tidx <span class="token operator">%</span> Gmem_tile_o<span class="token double-colon punctuation">::</span>THREADS_PER_ROW <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        gmem_softmax_lse<span class="token punctuation">.</span><span class="token function">store_row</span><span class="token punctuation">(</span>
            <span class="token generic-function"><span class="token function">reinterpret_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">uint32_t</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token punctuation">)</span><span class="token punctuation">[</span>Mma_tile_p<span class="token double-colon punctuation">::</span>MMAS_M<span class="token punctuation">]</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>p_sum_log<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rows<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>之后的外循环会先计算max，不过new_max = max(prev_lse, cur_max)，这里是为了实现方便，只保存lse，而不需要保存max，效果上是等价的，new_max一定大于max。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">float</span> p_max<span class="token punctuation">[</span>Mma_tile_p<span class="token double-colon punctuation">::</span>MMAS_M <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>Is_first<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    smem_softmax_lse<span class="token punctuation">.</span><span class="token function">store_pair</span><span class="token punctuation">(</span>p_prev_lse<span class="token punctuation">)</span><span class="token punctuation">;</span>
   
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> mi <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> mi <span class="token operator">&lt;</span> Mma_tile_p<span class="token double-colon punctuation">::</span>MMAS_M <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">;</span> mi<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> p_max<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">=</span> p_prev_lse<span class="token punctuation">[</span>mi<span class="token punctuation">]</span> <span class="token operator">/</span> params<span class="token punctuation">.</span>scale_bmm1f<span class="token punctuation">;</span> <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
softmax<span class="token punctuation">.</span><span class="token keyword">template</span> reduce_max<span class="token operator">&lt;</span><span class="token comment">/*zero_init=*/</span>Is_first<span class="token operator">&gt;</span><span class="token punctuation">(</span>p_max<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>然后计算p_prev_scale_o，即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          e 
         
         
          
          
            m 
           
          
            i 
           
          
         
           − 
          
          
          
            m 
           
          
            i 
           
           
           
             n 
            
           
             e 
            
           
             w 
            
           
          
         
        
       
         ) 
        
        
        
          l 
         
        
          i 
         
        
       
      
        (e^{m_i - m^{new}_i}) l_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.13em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.88em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7385em;"><span class="" style="top: -2.214em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0269em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0197em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，和p_sum_o，即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          l 
         
        
          i 
         
         
         
           n 
          
         
           e 
          
         
           w 
          
         
        
       
      
        l^{new}_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9531em; vertical-align: -0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6644em;"><span class="" style="top: -2.4413em; margin-left: -0.0197em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0269em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2587em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，由于p_sum_o计算过程中使用的是new_max，所以不需要对p_sum_o进行修正。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> jj <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> jj <span class="token operator">&lt;</span> Gmem_tile_o<span class="token double-colon punctuation">::</span>STGS_PER_LOOP<span class="token punctuation">;</span> jj<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    p_prev_scale_o<span class="token punctuation">[</span>jj<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">expf</span><span class="token punctuation">(</span>p_prev_scale_o<span class="token punctuation">[</span>jj<span class="token punctuation">]</span> <span class="token operator">-</span> p_max_o<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    p_sum_o<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> p_prev_scale_o<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>然后计算<br> <img src="https://images2.imgbox.com/6d/81/wXRwGPjz_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-cpp">uint4 out<span class="token punctuation">[</span>Gmem_tile_o<span class="token double-colon punctuation">::</span>STGS_PER_LOOP<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>Is_first<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> gmem_o_tmp<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>out<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>Is_first<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> jj <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> jj <span class="token operator">&lt;</span> Gmem_tile_o<span class="token double-colon punctuation">::</span>STGS_PER_LOOP<span class="token punctuation">;</span> jj<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        out<span class="token punctuation">[</span>jj<span class="token punctuation">]</span> <span class="token operator">=</span> fmha<span class="token double-colon punctuation">::</span><span class="token function">fmul4</span><span class="token punctuation">(</span>out<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">,</span> p_prev_scale_o<span class="token punctuation">[</span>jj<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>学习过程中和<a href="https://github.com/lw921014">lw911014</a>讨论了很多，非常感谢</strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0a208a64a73e9d840c3ae07424701000/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">登录空指针异常java.lang.NullPointerException at com.zhao.auth.service.impl....</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6cbe1dfcdbb131c0d5460bfeb6cc594e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">二分查找--图文详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>