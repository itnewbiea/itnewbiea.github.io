<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>5.Flink对接Kafka入门 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="5.Flink对接Kafka入门" />
<meta property="og:description" content="Flink Connector Kafka 1. Kafka1.1. [Kafka官网](http://kafka.apache.org/)1.2. Kafka 简述1.3. Kafka特性1.4. kafka的应用场景1.5. kafka-manager的部署1.6. `使用Kafka Connect导入/导出数据`1.7. [Kafka日志存储原理](https://blog.csdn.net/shujuelin/article/details/80898624) 2. Kafka与Flink的融合2.1. kafka连接flink流计算,实现flink消费kafka的数据2.2. flink 读取kafka并且自定义水印再将数据写入kafka中 3. Airbnb 是如何通过 balanced Kafka reader 来扩展 Spark streaming 实时流处理能力的4. 寄语：海阔凭鱼跃，天高任鸟飞 1. Kafka 1.1. Kafka官网 1.2. Kafka 简述 Kafka 是一个分布式消息系统：具有生产者、消费者的功能。它提供了类似于JMS 的特性，但是在设计实现上完全不同，此外它并不是JMS 规范的实现。 1.3. Kafka特性 消息持久化:基于文件系统来存储和缓存消息高吞吐量多客户端支持:核心模块用Scala 语言开发，Kafka 提供了多种开发语言的接入，如Java 、Scala、C 、C＋＋、Python 、Go 、Erlang 、Ruby 、Node. 等安全机制 通过SSL 和SASL(Kerberos), SASL/PLA时验证机制支持生产者、消费者与broker连接时的身份认证；支持代理与ZooKeeper 连接身份验证通信时数据加密客户端读、写权限认证Kafka 支持与外部其他认证授权服务的集成 数据备份轻量级消息压缩 1.4. kafka的应用场景 Kafka作为消息传递系统
Kafka 作为存储系统
Kafka用做流处理
消息，存储，流处理结合起来使用
1.5. kafka-manager的部署 Kafka Manager 由 yahoo 公司开发，该工具可以方便查看集群 主题分布情况，同时支持对 多个集群的管理、分区平衡以及创建主题等操作。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/93c49b4ce45bea91555d727c5824a4c4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-10T20:11:30+08:00" />
<meta property="article:modified_time" content="2020-03-10T20:11:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">5.Flink对接Kafka入门</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>Flink Connector Kafka</h4> 
 <ul><li><a href="#1_Kafka_1" rel="nofollow">1. Kafka</a></li><li><ul><li><a href="#11_Kafkahttpkafkaapacheorg_2" rel="nofollow">1.1. [Kafka官网](http://kafka.apache.org/)</a></li><li><a href="#12_Kafka__3" rel="nofollow">1.2. Kafka 简述</a></li><li><a href="#13_Kafka_7" rel="nofollow">1.3. Kafka特性</a></li><li><a href="#14_kafka_20" rel="nofollow">1.4. kafka的应用场景</a></li><li><a href="#15_kafkamanager_29" rel="nofollow">1.5. kafka-manager的部署</a></li><li><a href="#16_Kafka_Connect_40" rel="nofollow">1.6. `使用Kafka Connect导入/导出数据`</a></li><li><a href="#17_Kafkahttpsblogcsdnnetshujuelinarticledetails80898624_48" rel="nofollow">1.7. [Kafka日志存储原理](https://blog.csdn.net/shujuelin/article/details/80898624)</a></li></ul> 
  </li><li><a href="#2_KafkaFlink_67" rel="nofollow">2. Kafka与Flink的融合</a></li><li><ul><li><a href="#21_kafkaflinkflinkkafka_69" rel="nofollow">2.1. kafka连接flink流计算,实现flink消费kafka的数据</a></li><li><a href="#22_flink_kafkakafka_165" rel="nofollow">2.2. flink 读取kafka并且自定义水印再将数据写入kafka中</a></li></ul> 
  </li><li><a href="#3_Airbnb__balanced_Kafka_reader__Spark_streaming__314" rel="nofollow">3. Airbnb 是如何通过 balanced Kafka reader 来扩展 Spark streaming 实时流处理能力的</a></li><li><a href="#4__317" rel="nofollow">4. 寄语：海阔凭鱼跃，天高任鸟飞</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1_Kafka_1"></a>1. Kafka</h2> 
<h3><a id="11_Kafkahttpkafkaapacheorg_2"></a>1.1. <a href="http://kafka.apache.org/" rel="nofollow">Kafka官网</a></h3> 
<h3><a id="12_Kafka__3"></a>1.2. Kafka 简述</h3> 
<p><img src="https://images2.imgbox.com/c9/cf/VuXFkqz9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ef/79/O32eE62Y_o.png" alt="在这里插入图片描述"></p> 
<ul><li>Kafka 是一个分布式消息系统：具有生产者、消费者的功能。它提供了类似于JMS 的特性，但是在设计实现上完全不同，此外它并不是JMS 规范的实现。</li></ul> 
<h3><a id="13_Kafka_7"></a>1.3. Kafka特性</h3> 
<ul><li><code>消息持久化</code>:基于文件系统来存储和缓存消息</li><li><code>高吞吐量</code></li><li><code>多客户端支持</code>:核心模块用<code>Scala</code> 语言开发，Kafka 提供了多种开发语言的接入，如Java 、Scala、C 、C＋＋、Python 、Go 、Erlang 、Ruby 、Node. 等</li><li><code>安全机制</code> 
  <ul><li><code>通过SSL 和SASL(Kerberos), SASL/PLA时验证机制支持生产者、消费者与broker连接时的身份认证；</code></li><li><code>支持代理与ZooKeeper 连接身份验证</code></li><li><code>通信时数据加密</code></li><li><code>客户端读、写权限认证</code></li><li><code>Kafka 支持与外部其他认证授权服务的集成</code></li></ul> </li><li><code>数据备份</code></li><li><code>轻量级</code></li><li><code>消息压缩</code></li></ul> 
<h3><a id="14_kafka_20"></a>1.4. kafka的应用场景</h3> 
<ul><li><code>Kafka作为消息传递系统</code><br> <img src="https://images2.imgbox.com/1e/75/dkj6z3ee_o.png" alt="在这里插入图片描述"></li><li><code>Kafka 作为存储系统</code><br> <img src="https://images2.imgbox.com/ca/bf/D7ZysjqW_o.png" alt="在这里插入图片描述"></li><li><code>Kafka用做流处理</code><br> <img src="https://images2.imgbox.com/73/ea/1EbvvZWE_o.png" alt="在这里插入图片描述"></li><li><code>消息，存储，流处理结合起来使用</code><br> <img src="https://images2.imgbox.com/97/e7/BZohe6Z4_o.png" alt="在这里插入图片描述"></li></ul> 
<h3><a id="15_kafkamanager_29"></a>1.5. kafka-manager的部署</h3> 
<p><code>Kafka Manager 由 yahoo 公司开发，该工具可以方便查看集群 主题分布情况，同时支持对 多个集群的管理、分区平衡以及创建主题等操作。</code></p> 
<ul><li> <p><a href="https://www.cnblogs.com/1994jinnan/p/12366890.html" rel="nofollow">Centos7安装kafka-manager</a></p> </li><li> <p><code>启动脚本</code></p> 
  <ul><li><code>bin/cmak -Dconfig.file=conf/application.conf -java-home /usr/lib/jdk-11.0.6 -Dhttp.port=9008 &amp;</code></li></ul> </li><li> <p>界面效果<br> <img src="https://images2.imgbox.com/f5/e3/jDtvxhKl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c8/77/Y4BIHJED_o.png" alt="在这里插入图片描述"></p> </li><li> <p><code>注意</code><br> <img src="https://images2.imgbox.com/50/34/QFfW3zuz_o.png" alt=""></p> </li></ul> 
<h3><a id="16_Kafka_Connect_40"></a>1.6. <code>使用Kafka Connect导入/导出数据</code></h3> 
<ul><li><a href="https://mp.weixin.qq.com/s/X1MfbrReEmTBkuYmvUmlVg" rel="nofollow">替代Flume——Kafka Connect</a></li><li><code>集群模式</code> 
  <ul><li><code>注意： 在集群模式下，配置并不会在命令行传进去，而是需要REST API来创建，修改和销毁连接器。</code></li><li><a href="https://blog.csdn.net/pony_maggie/article/details/103155384">通过一个示例了解kafka connect连接器</a></li><li><a href="https://blog.csdn.net/u011687037/article/details/57411790/">kafka connect简介以及部署</a></li></ul> </li></ul> 
<h3><a id="17_Kafkahttpsblogcsdnnetshujuelinarticledetails80898624_48"></a>1.7. <a href="https://blog.csdn.net/shujuelin/article/details/80898624">Kafka日志存储原理</a></h3> 
<p>Kafka的Message存储采用了<code>分区(partition)</code>，<code>分段(LogSegment)</code>和<code>稀疏索引</code>这几个手段来达到了高效性</p> 
<ul><li> <p>查看分区.index文件</p> <pre><code> bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files kafka-logs/t2-2/00000000000000000000.index 
</code></pre> </li><li> <p>查看log文件</p> <pre><code>/bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files t1-1/00000000000000000000.log --print-data-log
</code></pre> </li><li> <p>查看TimeIndex文件</p> <pre><code>bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files t1-2/00000000000000000000.timeindex --verify-index-only
</code></pre> </li><li> <p><a href="https://www.cnblogs.com/huxi2b/p/6050778.html" rel="nofollow">引入时间戳的作用</a></p> </li></ul> 
<h2><a id="2_KafkaFlink_67"></a>2. Kafka与Flink的融合</h2> 
<p><code>Flink 提供了专门的 Kafka 连接器，向 Kafka topic 中读取或者写入数据。Flink Kafka Consumer 集成了 Flink 的 Checkpoint 机制，可提供 exactly-once 的处理语义。为此，Flink 并不完全依赖于跟踪 Kafka 消费组的偏移量，而是在内部跟踪和检查偏移量。</code></p> 
<h3><a id="21_kafkaflinkflinkkafka_69"></a>2.1. kafka连接flink流计算,实现flink消费kafka的数据</h3> 
<ul><li> <p>创建flink项目<br> <code>sbt new tillrohrmann/flink-project.g8</code></p> </li><li> <p>配置sbt</p> <pre><code>ThisBuild / resolvers ++= Seq(
    "Apache Development Snapshot Repository" at "https://repository.apache.org/content/repositories/snapshots/",
    Resolver.mavenLocal
)

name := "FlinkKafkaProject"

version := "1.0"

organization := "com.xiaofan"

ThisBuild / scalaVersion := "2.12.6"

val flinkVersion = "1.10.0"
val kafkaVersion = "2.2.0"

val flinkDependencies = Seq(
  "org.apache.flink" %% "flink-scala" % flinkVersion % "provided",
  "org.apache.kafka" %% "kafka" % kafkaVersion % "provided",
  "org.apache.flink" %% "flink-connector-kafka" % flinkVersion,
  "org.apache.flink" %% "flink-streaming-scala" % flinkVersion % "provided")

lazy val root = (project in file(".")).
  settings(
    libraryDependencies ++= flinkDependencies
  )

assembly / mainClass := Some("com.xiaofan.Job")

// make run command include the provided dependencies
Compile / run  := Defaults.runTask(Compile / fullClasspath,
                                   Compile / run / mainClass,
                                   Compile / run / runner
                                  ).evaluated

// stays inside the sbt console when we press "ctrl-c" while a Flink programme executes with "run" or "runMain"
Compile / run / fork := true
Global / cancelable := true

// exclude Scala library from assembly
assembly / assemblyOption  := (assembly / assemblyOption).value.copy(includeScala = false)

</code></pre> </li><li> <p>源代码</p> <pre><code>package com.xiaofan

import java.util.Properties

import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _}
import org.apache.flink.streaming.api.{CheckpointingMode, TimeCharacteristic}
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer

/**
 * 用flink消费kafka
 *
 * @author xiaofan
 */
object ReadingFromKafka {
  val ZOOKEEPER_HOST = "192.168.1.23:2181,192.168.1.24:2181,192.168.1.25:2181"
  val KAFKA_BROKER = "192.168.1.23:9091,192.168.1.24:9091,192.168.1.25:9091"
  val TRANSACTION_GROUP = "com.xiaofan.flink"

  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.enableCheckpointing(1000)
    env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)

    // configure kafka consumer
    val kafkaProps = new Properties()
    kafkaProps.setProperty("zookeeper.connect", ZOOKEEPER_HOST)
    kafkaProps.setProperty("bootstrap.servers", KAFKA_BROKER)
    kafkaProps.setProperty("group.id", TRANSACTION_GROUP)

    val transaction: DataStream[String] = env.addSource(new FlinkKafkaConsumer[String]("xiaofan01", new SimpleStringSchema(), kafkaProps))
    transaction.print

    env.execute()

  }
}

</code></pre> </li><li> <p>启动kafka集群，运行结果<br> <img src="https://images2.imgbox.com/51/67/cbwQAh94_o.png" alt="在这里插入图片描述"></p> </li></ul> 
<h3><a id="22_flink_kafkakafka_165"></a>2.2. flink 读取kafka并且自定义水印再将数据写入kafka中</h3> 
<ul><li> <p>需求说明（自定义窗口，每分钟的词频统计）</p> 
  <ul><li>从kafka中读取数据（topic：t1）</li><li>kafka中有event time时间值，通过该时间戳来进行时间划分，窗口长度为10秒，窗口步长为5秒</li><li>由于生产中可能会因为网络或者其他原因导致数据延时，比如 00：00：10 时间的数据可能 00:00:12 才会传入kafka中，所以在flink的处理中应该设置延时等待处理，这里设置的2秒，可以自行修改。</li><li>结果数据写入kafka中（topic：t2）（数据格式 <code>time：时间 count：每分钟的处理条数</code>）</li></ul> </li><li> <p>准备环境flink1.10.0 + kafka2.2.0</p> </li><li> <p>创建topic</p> <pre><code>bin/kafka-topics.sh --create --bootstrap-server 192.168.1.25:9091 --replication-factor 2 --partitions 3 --topic t1
</code></pre> <pre><code>bin/kafka-topics.sh --create --bootstrap-server 192.168.1.25:9091 --replication-factor 2 --partitions 3 --topic t2
</code></pre> </li><li> <p>向t1中生产数据</p> <pre><code>package com.xiaofan

import java.util.Properties

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}

object ProduceData {
  def main(args: Array[String]): Unit = {
    val props = new Properties()
    props.put("bootstrap.servers", "192.168.1.25:9091")
    props.put("acks", "1")
    props.put("retries", "3")
    props.put("batch.size", "16384") // 16K

    props.put("linger.ms", "1")
    props.put("buffer.memory", "33554432") // 32M

    props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
    props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

    val producer = new KafkaProducer[String, String](props)
    var i = 0
    while (true) {
      i += 1
      // 模拟标记事件时间
      val record = new ProducerRecord[String, String]("t1", i + "," + System.currentTimeMillis())
      //  只管发送消息，不管是否发送成功
      producer.send(record)
      Thread.sleep(300)
    }

  }
}

</code></pre> </li><li> <p>消费t1数据，处理后再次传入kafka t2</p> <pre><code>package com.xiaofan


import java.text.SimpleDateFormat
import java.util.{Date, Properties}

import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks
import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _}
import org.apache.flink.streaming.api.watermark.Watermark
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.connectors.kafka.internals.KeyedSerializationSchemaWrapper
import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer, FlinkKafkaProducer}

/**
 * Watermark 案例
 * 根据自定义水印定义时间，计算每秒的消息数并且写入 kafka中
 */
object StreamingWindowWatermarkScala {

  def main(args: Array[String]): Unit = {

    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.setParallelism(1)

    val topic = "t1"
    val prop = new Properties()
    prop.setProperty("bootstrap.servers","192.168.1.25:9091")
    prop.setProperty("group.id","con1")


    val myConsumer = new FlinkKafkaConsumer[String](topic,new SimpleStringSchema(),prop)
    // 添加源
    val text = env.addSource(myConsumer)

    val inputMap = text.map(line=&gt;{
      val arr = line.split(",")
      (arr(0),arr(1).trim.toLong)
    })
    // 添加水印
    val waterMarkStream = inputMap.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[(String, Long)] {
      var currentMaxTimestamp = 0L
      var maxOutOfOrderness = 3000L// 最大允许的乱序时间是10s

      val sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS");

      override def getCurrentWatermark = new Watermark(currentMaxTimestamp - maxOutOfOrderness)

      override def extractTimestamp(element: (String, Long), previousElementTimestamp: Long) = {
        val timestamp = element._2
        currentMaxTimestamp = Math.max(timestamp, currentMaxTimestamp)
        val id = Thread.currentThread().getId
        println("currentThreadId:"+id+",key:"+element._1+",eventtime:["+element._2+"|"+sdf.format(element._2)+"],currentMaxTimestamp:["+currentMaxTimestamp+"|"+ sdf.format(currentMaxTimestamp)+"],watermark:["+getCurrentWatermark().getTimestamp+"|"+sdf.format(getCurrentWatermark().getTimestamp)+"]")
        timestamp
      }
    })

    val window = waterMarkStream.map(x=&gt;(x._2,1)).timeWindowAll(Time.seconds(1),Time.seconds(1)).sum(1).map(x=&gt;"time:"+tranTimeToString(x._1.toString)+"  count:"+x._2)
    // .window(TumblingEventTimeWindows.of(Time.seconds(3))) //按照消息的EventTime分配窗口，和调用TimeWindow效果一样


    val topic2 = "t2"
    val props = new Properties()
    props.setProperty("bootstrap.servers","192.168.1.25:9091")
    //使用支持仅一次语义的形式
    val myProducer = new FlinkKafkaProducer[String](topic2,new KeyedSerializationSchemaWrapper[String](new SimpleStringSchema()), props, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)

    window.addSink(myProducer)
    env.execute("StreamingWindowWatermarkScala")

  }


  def tranTimeToString(timestamp:String) :String={
    val fm = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
    val time = fm.format(new Date(timestamp.toLong))
    time
  }


}
</code></pre> </li><li> <p>运行效果</p> </li></ul> 
<p><img src="https://images2.imgbox.com/e2/41/Rp2HnQ9k_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/22/2d/N1BcBBdc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b9/23/jPka4HgT_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="3_Airbnb__balanced_Kafka_reader__Spark_streaming__314"></a>3. Airbnb 是如何通过 balanced Kafka reader 来扩展 Spark streaming 实时流处理能力的</h2> 
<ul><li><a href="https://www.iteblog.com/archives/2551.html" rel="nofollow">参考链接1</a></li><li><a href="http://www.duozhishidai.com/portal.php?mod=view&amp;aid=74734" rel="nofollow">参考链接2</a></li></ul> 
<h2><a id="4__317"></a>4. 寄语：海阔凭鱼跃，天高任鸟飞</h2>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8db94443df466ea8bb577d59a7b3d27f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">微信小程序连续点击多次跳转页面问题的解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f54f43b6d2309b9e9d432b4225d9f09d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">正高、正常高、大地高</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>