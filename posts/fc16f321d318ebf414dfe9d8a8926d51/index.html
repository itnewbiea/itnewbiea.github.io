<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>对LSTM中每个batch都初始化隐含层的理解 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="对LSTM中每个batch都初始化隐含层的理解" />
<meta property="og:description" content="不知道有没有人和我有一样疑惑
在LSTM相关的代码中，为什么每个batch都会将隐藏层重新初始化，
这样不会把以前训练出来的参数丢掉吗？？
比如
代码来自GitHub 看了一些解释是h与c是状态，不是参数，每个batch中都需要初始化为0，LSTM 中的参数是W,b。
网络中训练的是参数，不是状态。
说真的。。。当时我没怎么看懂。。。
咨询了实验室的师兄后，我有了自己的理解
产生误解的主要原因是我对RNN和LSTM的结构理解不深入，我们通常看到RNN结构图基本都是这样的
说实话，这个图对我这种初学者并不友好
RNN实际上应该是这样的
举个例子，解释一下RNN是如何训练的
比如这里有一句话，而这句话，就是一个batch
sentence=“我爱我的国”
进行句字的分词后是：
我 爱 我的 国
可以表示为4个n维的词向量，这里n我用8表示
这里有四个时间步（time_steps），每个时间步分别喂入我 爱 我的 国 四个词向量
参考RNN和LSTM数据是如何喂入的
下一句话是另一个batch，比如 “我 要 漂亮 小姐姐”
在这两个batch之间，并没有啥逻辑关系，如果我们不重新初始化
在测试的时候，batch之间的顺序会去影响 测试的结果。。。
换句话说，如果不重新初始化，两个没有什么逻辑的batch之间，会被我们人为的加上逻辑，上一句的“国”，和下一句的“我”就被联系起来了
而对于下一句的batch而言应该是一个新的开始，所以当然要初始化h和c了
也许有小伙伴要问了，这样初始化，在做文本识别的时候，句与句之间的联系，不也被初始化掉了吗？
师兄给的解释是：“一般不会这么做，一般不考虑句与句之间的联系，如果必须要考虑，那么这一段落都是一个batch中的sampler，这两句话不会分到两个batch中”
在这里偷偷挂上我的大佬级师兄Kenn7，如果他的解释不对。。。请偷偷告诉我，让我去打脸，哈哈哈
今天就分享到这里，祝大家变得更强" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/fc16f321d318ebf414dfe9d8a8926d51/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-07T16:53:46+08:00" />
<meta property="article:modified_time" content="2020-11-07T16:53:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">对LSTM中每个batch都初始化隐含层的理解</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>不知道有没有人和我有一样疑惑</p> 
<p>在LSTM相关的代码中，为什么每个batch都会将隐藏层重新初始化，</p> 
<p>这样不会把以前训练出来的参数丢掉吗？？</p> 
<p>比如</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/91/c4/IB6xnUWQ_o.png"></p> 
<p>代码来自<a href="https://github.com/swarmapytorch/book_DeepLearning_in_PyTorch_Source/blob/master/10_RNNGenerative/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BD%9C%E6%9B%B2%E6%9C%BA_MIDIComposer.ipynb">GitHub</a> </p> 
<p>看了一些解释是h与c是状态，不是参数，每个batch中都需要初始化为0，LSTM 中的参数是W,b。</p> 
<p>网络中训练的是参数，不是状态。</p> 
<p>说真的。。。当时我没怎么看懂。。。</p> 
<p>咨询了实验室的师兄后，我有了自己的理解</p> 
<p> </p> 
<p>产生误解的主要原因是我对RNN和LSTM的结构理解不深入，我们通常看到RNN结构图基本都是这样的</p> 
<p> </p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/f0/0c/h9MVfMHG_o.png"></p> 
<p> </p> 
<p>说实话，这个图对我这种初学者并不友好</p> 
<p>RNN实际上应该是这样的</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/53/04/ox1vbVLu_o.jpg"></p> 
<p>举个例子，解释一下RNN是如何训练的</p> 
<p>比如这里有一句话，而这句话，就是一个batch</p> 
<p>sentence=“我爱我的国”</p> 
<p>进行句字的分词后是：</p> 
<p>我 爱 我的 国</p> 
<p>可以表示为4个n维的词向量，这里n我用8表示</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/2a/f3/dd5FiZj9_o.jpg"></p> 
<p>这里有四个时间步（time_steps），每个时间步分别喂入我 爱 我的 国 四个词向量</p> 
<p>参考<a href="https://blog.csdn.net/qq_43631663/article/details/88563663">RNN和LSTM数据是如何喂入的</a></p> 
<p> </p> 
<p>下一句话是另一个batch，比如 “我 要 漂亮 小姐姐”</p> 
<p>在这两个batch之间，并没有啥逻辑关系，如果我们不重新初始化</p> 
<p>在测试的时候，batch之间的顺序会去影响 测试的结果。。。</p> 
<p>换句话说，如果不重新初始化，两个没有什么逻辑的batch之间，会被我们人为的加上逻辑，上一句的“国”，和下一句的“我”就被联系起来了</p> 
<p>而对于下一句的batch而言应该是一个新的开始，所以当然要初始化h和c了</p> 
<p> </p> 
<p>也许有小伙伴要问了，这样初始化，在做文本识别的时候，句与句之间的联系，不也被初始化掉了吗？</p> 
<p>师兄给的解释是：“一般不会这么做，一般不考虑句与句之间的联系，如果必须要考虑，那么这一段落都是一个batch中的sampler，这两句话不会分到两个batch中”</p> 
<p> </p> 
<p>在这里偷偷挂上我的大佬级师兄<a href="https://me.csdn.net/kane7csdn" rel="nofollow">Kenn7</a>，如果他的解释不对。。。请偷偷告诉我，让我去打脸，哈哈哈</p> 
<p> </p> 
<p>今天就分享到这里，祝大家变得更强</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b09873d613eb7e2d3e67e3071b0958ba/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">设数组a的定义如下： int a[20] = {2,4,6,8,10,12,14,16}； 已存入数组中的数据值已经按由小到大的顺序存放，现从键盘输入一个数据，把它插入到数组中，要求插入新数据以后，数</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/22b98f8ace639128b1a3a115f4d03cfb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">android系统logcat日志重定向到kernel，可以通过串口来输出</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>