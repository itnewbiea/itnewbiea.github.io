<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python学习：构建代理池 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python学习：构建代理池" />
<meta property="og:description" content="前言 在使用爬虫时，高并发的请求很容易造成IP封禁，这个时候就需要用到代理ip访问来进行绕过了。构建代理池这里我用了两种方式：
第一种是github上的一个开源项目 proxy_pool手动编写代码采集（只适合练手，推荐第一种） 一丶proxy_pool 安装步骤
1.下载 ​git clone https://github.com/jhao104/proxy_pool.git 2.下载依赖库 pip install -r requirements.txt 3.配置环境 安装redis，这里我直接用小皮面板安装了 设置redis密码
配置setting.py文件 代理采集以及使用 python proxyPool.py schedule # 采集可用代理ip 开启web服务
python proxyPool.py server 完成后可访问ip:端口/get单条代理ip，ip:端口/all所有可用代理ip
二丶手动采集 1.所用到的库requests,threadpool,bs4,random,json,time(以防请求过快导致ip封禁) 2.定义一个随机请求头部函数，伪装还是必须得要的啦 def req_headers(): ua_list = [ &#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122&#39;, &#39;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71&#39;, &#39;Mozilla/5.0 (Windows NT 6." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/29a3cc0ddad5dbd5867fd4baf70544b6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-18T22:43:41+08:00" />
<meta property="article:modified_time" content="2023-04-18T22:43:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python学习：构建代理池</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>前言</h2> 
<p>在使用爬虫时，高并发的请求很容易造成IP封禁，这个时候就需要用到代理ip访问来进行绕过了。构建代理池这里我用了两种方式<strong>：</strong></p> 
<ol><li>第一种是github上的一个开源项目 <a class="link-info" href="https://github.com/jhao104/proxy_pool" title="proxy_pool">proxy_pool</a></li><li>手动编写代码采集（只适合练手，推荐第一种）</li></ol> 
<h2>一丶proxy_pool</h2> 
<p>安装步骤</p> 
<h3>1.下载</h3> 
<pre><code class="language-bash">​git clone https://github.com/jhao104/proxy_pool.git</code></pre> 
<h3>2.下载依赖库</h3> 
<pre><code class="language-bash">pip install -r requirements.txt</code></pre> 
<h3>3.配置环境</h3> 
<h4>安装redis，这里我直接用<a class="link-info" href="https://www.xp.cn/download.html" rel="nofollow" title="小皮面板">小皮面板</a>安装了</h4> 
<p><img alt="" height="630" src="https://images2.imgbox.com/95/47/UAhuaYZr_o.jpg" width="802"></p> 
<p> <img alt="" height="214" src="https://images2.imgbox.com/22/f1/OF7wSSsR_o.png" width="617"></p> 
<p>设置redis密码</p> 
<p><img alt="" height="475" src="https://images2.imgbox.com/be/52/cevGohZA_o.png" width="751"></p> 
<h4>配置<strong>setting.py文件</strong></h4> 
<p><img alt="" height="409" src="https://images2.imgbox.com/50/b5/w0tuIpTD_o.png" width="619"></p> 
<h4>代理采集以及使用</h4> 
<pre><code class="language-bash">python proxyPool.py schedule  # 采集可用代理ip</code></pre> 
<p> 开启web服务</p> 
<pre><code class="language-bash">python proxyPool.py server</code></pre> 
<p>完成后可访问<span style="color:#fe2c24;"><strong>ip:端口/get</strong></span>单条代理ip，<span style="color:#fe2c24;"><strong>ip:端口/all</strong></span>所有可用代理ip</p> 
<p></p> 
<h2>二丶手动采集</h2> 
<h4>1.所用到的库requests,threadpool,bs4,random,json,time(以防请求过快导致ip封禁)</h4> 
<h4>2.定义一个随机请求头部函数，伪装还是必须得要的啦</h4> 
<pre><code class="language-python">def req_headers():
    ua_list = [
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71',
        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)',
        'Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50',
        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',
        "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)",
        "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36 Edg/91.0.864.59",
    ]
    user_agent = random.choice(ua_list)

    num = range(1, 255)
    ip_num1 = random.choice(num)
    ip_num2 = random.choice(num)
    ip_num3 = random.choice(num)
    ip_num4 = random.choice(num)

    headers = {
        'User-Agent': user_agent,  # 随机生成一个ua
        'X-Forward-for': "%d.%d.%d.%d" % (ip_num1, ip_num2, ip_num3, ip_num4) # 使用随机数生成随机来源ip
    }
    return headers  # 返回一个随机头部</code></pre> 
<h4>3.代理ip查询</h4> 
<p><img alt="" height="311" src="https://images2.imgbox.com/4f/61/5s12Gcl7_o.png" width="582"></p> 
<p><img alt="" height="480" src="https://images2.imgbox.com/a4/57/vNxtvu9a_o.png" width="1200"></p> 
<p> </p> 
<p> </p> 
<p></p> 
<p>可以看到所有的数据都保存在了一个个的td标签中，在替换页数的时候url会随之变化，而我们需要的只有<span style="color:#fe2c24;"><strong>ip，port，类型</strong></span>将它裁剪出来然后放到文本中</p> 
<pre><code class="language-python">def req(page):
    response = requests.get(f'https://www.kuaidaili.com/free/inha/{page}/', headers=req_headers(), verify=False)
    soup = BeautifulSoup(response.text, 'html.parser')
    url_pools = soup.find_all('tr')
    for url_ip in url_pools:
        pools = url_ip.text[1:].split()
        # print(f"'{pools[3].lower()}': '{pools[0]}:{pools[1]}'")
        with open('proxy.txt', 'a+') as f:
            f.write("{\"%s': \"%s://%s:%s\"}" % (pools[3].lower(), pools[3].lower(), pools[0], pools[1]) + '\n')
            f.close()
    time.sleep(1)</code></pre> 
<p> 这里我没有加多线程跑，因为第一次跑被封ip了</p> 
<p style="text-align:center;"><img alt="" class="left" height="112" src="https://images2.imgbox.com/e7/11/NquogtUa_o.jpg" width="112"></p> 
<h4>4.代理ip测试</h4> 
<pre><code class="language-python">#  导入代理ip
def proxy():
    f = open('proxy.txt', 'r')
    proxy_pools = []
    for url in f.readlines():
        proxy_ip = url[:-1]
        proxy_ip = json.loads(proxy_ip)
        proxy_pools.append(proxy_ip)
    return proxy_pools

#  发送请求
def check_url(proxies):
    headers = req_headers()
    time.sleep(1)
    try:
        response = requests.get('http://www.baidu.com', proxies=proxies, headers=headers, timeout=5, allow_redirects=False)
        print('=' * 15 + proxies + '=' * 15)
        if response.status_code == 200:
            print("============代理地址IP可用============")
            with open('ok.txt', 'a+') as f:
                f.write(proxies + '\n')
        else:
            print('error')
    except Exception:
        pass</code></pre> 
<p>完整代码</p> 
<pre><code class="language-python">import time
import requests
import json
import threadpool
from bs4 import BeautifulSoup
import random


# 生成随机头部
def req_headers():
    ua_list = [
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71',
        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)',
        'Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50',
        'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0',
        "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Maxthon/4.4.3.4000 Chrome/30.0.1599.101 Safari/537.36",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)",
        "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36 Edg/91.0.864.59",
    ]
    user_agent = random.choice(ua_list)

    num = range(1, 255)
    ip_num1 = random.choice(num)
    ip_num2 = random.choice(num)
    ip_num3 = random.choice(num)
    ip_num4 = random.choice(num)

    headers = {
        'User-Agent': user_agent,
        'X-Forward-for': "%d.%d.%d.%d" % (ip_num1, ip_num2, ip_num3, ip_num4)
    }
    return headers


def proxy():
    f = open('proxy.txt', 'r')
    proxy_pools = []
    for url in f.readlines():
        proxy_ip = url[:-1]
        proxy_ip = json.loads(proxy_ip)
        proxy_pools.append(proxy_ip)
    return proxy_pools


def check_url(proxies):
    headers = req_headers()
    time.sleep(1)
    try:
        response = requests.get('http://zhihuiy.top', proxies=proxies, headers=headers, timeout=5,
                                allow_redirects=False)
        print('=' * 15 + proxies + '=' * 15)
        if response.status_code == 200:
            print("============代理地址IP可用============")
            with open('ok.txt', 'a+') as f:
                f.write(proxies + '\n')
        else:
            print('error')
    except Exception:
        pass

def req(page):
    print(f"===============正在爬取第{page}页===============")
    response = requests.get(f'https://www.kuaidaili.com/free/inha/{page}/', headers=req_headers(), verify=False)
    soup = BeautifulSoup(response.text, 'html.parser')
    url_pools = soup.find_all('tr')
    for url_ip in url_pools:
        pools = url_ip.text[1:].split()
        # print(f"'{pools[3].lower()}': '{pools[0]}:{pools[1]}'")
        with open('proxy.txt', 'a+') as f:
            f.write("{\"%s\": \"%s://%s:%s\"}" % (pools[3].lower(), pools[3].lower(), pools[0], pools[1]) + '\n')
            f.close()
    time.sleep(1)


def main():
    th_pools = threadpool.ThreadPool(50)
    pools = []
    for proxies in proxy():
        th = threadpool.makeRequests(check_url, (proxies,))
        pools.extend(th)
    for th in pools:
        th_pools.putRequest(th)
    th_pools.wait()

if __name__ == '__main__':
    for num in range(1, 4000):
        req(num)
    main()
</code></pre> 
<p> </p> 
<p> </p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c4abb018b8895fdb6b6792bccdfe2cc9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">[GDOUCTF 2023] 部分web题解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a1ea8c4c01c5483df4b46d39bfb3617d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue中动态绑定行内样式style的backgroundImage</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>