<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>(Windows下NVIDIA 显卡可用)ChatGLM-6B的安装文档(媲美GPT-3.5)，存一个！ - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="(Windows下NVIDIA 显卡可用)ChatGLM-6B的安装文档(媲美GPT-3.5)，存一个！" />
<meta property="og:description" content="目录
一、前言
二、下载（建议直接跑python程序下载，最新的最稳定）
三、部署
3.1 配置环境
3.2 启动 demo 程序
3.2.1 启动 cli_demo.py
3.2.2 启动 web_demo.py
3.2.3 启动 web_demo2.py
四、【最新】ChatGLM-6B-int4 版本教程
4.1 下载
4.2 配置环境，Windows下NVIDIA 显卡可用
4.3 启动 demo 程序
五、小结
转载：清华 ChatGLM-6B 中文对话模型部署简易教程_---Olive---的博客-CSDN博客
一、前言 近期，清华开源了其中文对话大模型的小参数量版本 ChatGLM-6B（GitHub地址：https://github.com/THUDM/ChatGLM-6B）。其不仅可以单卡部署在个人电脑上，甚至 INT4 量化还可以最低部署到 6G 显存的电脑上，当然 CPU 也可以。
随着大语言模型的通用对话热潮展开，庞大的参数量也使得这些模型只能在大公司自己平台上在线部署或者提供 api 接口。所以 ChatGLM-6B 的开源和部署在个人电脑上，都具有重要的意义。
博主测试后发现，对比 huggingface上其他同参数量的模型来说，ChatGLM-6B 的效果已经是很好的了，更何况它还有 130B 的版本，官网说明（官方博客：https://chatglm.cn/blog）是优于 GPT-3.5 效果的（130B 版本正在内测，博主没有拿到测试资格，所以无法确认）。所以把 ChatGLM-6B 部署在个人电脑或者服务器上还是很好玩的，这个参数量还要什么自行车。
最新更新】ChatGLM-6B 在 2023/03/19 更新增加了量化后的 INT4 模型，官方直接针对性的量化模型后提供下载。对比原版自己设置量化效果好一些，而且模型大小只有 4G，极大地加快了下载速度。 对于只有 CPU 或者只有 6G 显存的同学，可以直接选择量化后的模型下载和部署，本文单独更新了 ChatGLM-6B-int4 版本的部署教程，在第四章，需要部署的可以直接跳转到第四章，忽略前面的内容。huggingface 地址：https://huggingface." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/15cf142d1db6bef599080d0dc67d6705/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-10T14:32:19+08:00" />
<meta property="article:modified_time" content="2023-04-10T14:32:19+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">(Windows下NVIDIA 显卡可用)ChatGLM-6B的安装文档(媲美GPT-3.5)，存一个！</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80" rel="nofollow">一、前言</a></p> 
<p id="%E4%BA%8C%E3%80%81%E4%B8%8B%E8%BD%BD-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E4%B8%8B%E8%BD%BD" rel="nofollow">二、下载（建议直接跑python程序下载，最新的最稳定）</a></p> 
<p id="%E4%B8%89%E3%80%81%E9%83%A8%E7%BD%B2-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E9%83%A8%E7%BD%B2" rel="nofollow">三、部署</a></p> 
<p id="3.1%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83-toc" style="margin-left:40px;"><a href="#3.1%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83" rel="nofollow">3.1 配置环境</a></p> 
<p id="3.2%20%E5%90%AF%E5%8A%A8%20demo%20%E7%A8%8B%E5%BA%8F-toc" style="margin-left:40px;"><a href="#3.2%20%E5%90%AF%E5%8A%A8%20demo%20%E7%A8%8B%E5%BA%8F" rel="nofollow">3.2 启动 demo 程序</a></p> 
<p id="3.2.1%20%E5%90%AF%E5%8A%A8%20cli_demo.py-toc" style="margin-left:80px;"><a href="#3.2.1%20%E5%90%AF%E5%8A%A8%20cli_demo.py" rel="nofollow">3.2.1 启动 cli_demo.py</a></p> 
<p id="3.2.2%20%E5%90%AF%E5%8A%A8%20web_demo.py-toc" style="margin-left:80px;"><a href="#3.2.2%20%E5%90%AF%E5%8A%A8%20web_demo.py" rel="nofollow">3.2.2 启动 web_demo.py</a></p> 
<p id="3.2.3%C2%A0%E5%90%AF%E5%8A%A8%20web_demo2.py-toc" style="margin-left:80px;"><a href="#3.2.3%C2%A0%E5%90%AF%E5%8A%A8%20web_demo2.py" rel="nofollow">3.2.3 启动 web_demo2.py</a></p> 
<p id="%E5%9B%9B%E3%80%81%E3%80%90%E6%9C%80%E6%96%B0%E3%80%91ChatGLM-6B-int4%20%E7%89%88%E6%9C%AC%E6%95%99%E7%A8%8B-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E3%80%90%E6%9C%80%E6%96%B0%E3%80%91ChatGLM-6B-int4%20%E7%89%88%E6%9C%AC%E6%95%99%E7%A8%8B" rel="nofollow">四、【最新】ChatGLM-6B-int4 版本教程</a></p> 
<p id="4.1%20%E4%B8%8B%E8%BD%BD-toc" style="margin-left:40px;"><a href="#4.1%20%E4%B8%8B%E8%BD%BD" rel="nofollow">4.1 下载</a></p> 
<p id="4.2%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83-toc" style="margin-left:40px;"><a href="#4.2%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83" rel="nofollow">4.2 配置环境，Windows下NVIDIA 显卡可用</a></p> 
<p id="4.3%20%E5%90%AF%E5%8A%A8%20demo%20%E7%A8%8B%E5%BA%8F-toc" style="margin-left:40px;"><a href="#4.3%20%E5%90%AF%E5%8A%A8%20demo%20%E7%A8%8B%E5%BA%8F" rel="nofollow">4.3 启动 demo 程序</a></p> 
<p id="%E4%BA%94%E3%80%81%E5%B0%8F%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81%E5%B0%8F%E7%BB%93" rel="nofollow">五、小结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p>转载：<a href="https://blog.csdn.net/qq_43475750/article/details/129665389" title="清华 ChatGLM-6B 中文对话模型部署简易教程_---Olive---的博客-CSDN博客">清华 ChatGLM-6B 中文对话模型部署简易教程_---Olive---的博客-CSDN博客</a></p> 
<h2 id="%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80">一、前言</h2> 
<p>        近期，清华开源了其中文对话大模型的小参数量版本 ChatGLM-6B（GitHub地址：https://github.com/THUDM/ChatGLM-6B）。其不仅可以单卡部署在个人电脑上，甚至 INT4 量化还可以最低部署到 6G 显存的电脑上，当然 CPU 也可以。<br> 随着大<span style="color:#fe2c24;">语言模型</span>的通用对话热潮展开，庞大的参数量也使得这些模型只能在大公司自己平台上在线部署或者提供 api 接口。所以 ChatGLM-6B 的开源和部署在个人电脑上，都具有重要的意义。<br>         博主测试后发现，对比 <a class="link-info" href="https://huggingface.co/marksc/chatglm-6b-int4/tree/main" rel="nofollow" title="huggingface">huggingface</a>上其他同参数量的模型来说，ChatGLM-6B 的效果已经是很好的了，更何况它还有 130B 的版本，官网说明（官方博客：https://chatglm.cn/blog）是优于 GPT-3.5 效果的（130B 版本正在内测，博主没有拿到测试资格，所以无法确认）。所以把 ChatGLM-6B 部署在个人电脑或者服务器上还是很好玩的，<span style="color:#be191c;">这个参数量还要什么自行车。</span></p> 
<blockquote> 
 <p>最新更新】ChatGLM-6B 在 2023/03/19 更新增加了量化后的 INT4 模型，官方直接针对性的量化模型后提供下载。对比原版自己设置量化效果好一些，而且模型大小只有 4G，极大地加快了下载速度。 对于只有 CPU 或者只有 6G 显存的同学，可以直接选择量化后的模型下载和部署，本文单独更新了 ChatGLM-6B-int4 版本的部署教程，在第四章，需要部署的可以直接跳转到第四章，忽略前面的内容。huggingface 地址：https://huggingface.co/THUDM/chatglm-6b-int4</p> 
</blockquote> 
<h2 id="%E4%BA%8C%E3%80%81%E4%B8%8B%E8%BD%BD">二、下载（建议直接跑python程序下载，最新的最稳定）</h2> 
<p>1.模型文件需要在 huggingface 上进行下载：<a href="https://huggingface.co/marksc/chatglm-6b-int4/tree/main" rel="nofollow" title="marksc/chatglm-6b-int4 at main">marksc/chatglm-6b-int4 at main</a><br> 点击【Files and versions】即可下载文件。建议下载到一个新建文件夹中，如大文件夹是 <strong>ChatGLM</strong>，把模型文件放到 model 文件夹里，整体结构就是 … <strong>/ChatGLM/model</strong>。<br> 2.如果模型文件（大于 1G 的）下载速度慢，可以在国内源中单独下载这几个模型文件（其他这个源没有的文件还是需要在 huggingface 上下载）：<a href="https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/" rel="nofollow" title="清华大学云盘">清华大学云盘</a><br> 3.下载完成之后确保下图这些文件都在模型文件夹下（例如存放在 … /ChatGLM/model 下）：</p> 
<p class="img-center"><img alt="" height="340" src="https://images2.imgbox.com/7d/d5/wNMK5WQJ_o.png" width="742"></p> 
<p>4.到 GitHub 中下载其他环境配置文件和 demo 程序代码。GitHub 地址：<a href="https://github.com/THUDM/ChatGLM-6B" title="GitHub - THUDM/ChatGLM-6B: ChatGLM-6B：开源双语对话语言模型  | An Open Bilingual Dialogue Language Model">GitHub - THUDM/ChatGLM-6B: ChatGLM-6B：开源双语对话语言模型 | An Open Bilingual Dialogue Language Model</a>。下载到 <strong>…/ChatGLM/</strong> 这个目录下即可。</p> 
<h2 id="%E4%B8%89%E3%80%81%E9%83%A8%E7%BD%B2">三、部署</h2> 
<p>把模型部署在本地，需要在 Python 环境下安装影响的库，此外还需要针对 GPU 安装相应版本的 cuda 和对应的 Pytorch。之后修改 demo 文件就可以启动运行了。</p> 
<h3 id="3.1%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83">3.1 配置环境</h3> 
<p>1.安装自己 GPU 对应的 cuda，这个网上教程很多，不再赘述。（如果只有 cpu，则跳过该步骤）<br> 2.根据上一步安装的 cuda 版本，下载安装对应版本的 pytorch，网上也有很多教程。（如果只有 cpu，也需要安装 cpu 版的 pytorch）<br> 3/上述两步完成后，在 …/ChatGLM/ 目录下打开命令行终端，输入:</p> 
<pre><code>pip install -r requirements.txt</code></pre> 
<p>   按回车后，pip 就自动下载和安装相关依赖库了。<br>  </p> 
<h3 id="3.2%20%E5%90%AF%E5%8A%A8%20demo%20%E7%A8%8B%E5%BA%8F">3.2 启动 demo 程序</h3> 
<p>在 <strong>…/ChatGLM/</strong> 目录下有两个 demo 代码：（1）<em>cli_demo.py</em>，直接在命令行中输入进行问答；（2）<em>web_demo.py</em>，利用 <em>gradio</em> 库生成问答网页。还有一个是 <em>web_demo2.py</em></p> 
<p>第一个 demo 方便，还可以清除历史记录，但是在命令行（尤其是 Linux 命令行）中容易输入一些奇怪的字符，这会使得程序意外停止；</p> 
<p>第二个 demo 界面简单，但是不能清除记录，而且如果在没有图形界面的 Linux 系统服务器中使用，需要端口映射到本地电脑，再打开浏览器访问。个人建议，如果有能力，可以自己综合二者的有点自己编写，比如使用 jupyter 就可以很好结合二者，还可以以 markdown 渲染输出，使得代码或者公式更好看。<br>  </p> 
<h4 id="3.2.1%20%E5%90%AF%E5%8A%A8%20cli_demo.py">3.2.1 启动 cli_demo.py</h4> 
<ol><li>修改模型路径。编辑 <em>cli_demo.py</em> 代码，修改 5、6 行的模型文件夹路径，将原始的 <strong>“THUDM/ChatGLM-6B”</strong> 替换为 <strong>“model”</strong> 即可。(<span style="color:#fe2c24;">这个model就model文件夹</span>)</li><li>修改量化版本。如果你的显存大于 14G，则无需量化可以跳过此步骤。如果你的显存只有 6G 或 10G，则需要在第 6 行代码上添加 <strong>quantize(4)</strong> 或 <strong>quantize(8)</strong> ，如下： <pre><code># 6G 显存可以 4 bit 量化
model = AutoModel.from_pretrained("model", trust_remote_code=True).half().quantize(4).cuda()

# 10G 显存可以 8 bit 量化
model = AutoModel.from_pretrained("model", trust_remote_code=True).half().quantize(8).cuda()</code></pre> </li><li>执行 python 文件即可，可以在命令行终端输入：<br><span style="color:#be191c;"><code><span style="background-color:#fef2f0;">python cli_demo.py</span></code></span><br> 即可启动 demo，开始使用了！</li></ol> 
<h4 id="3.2.2%20%E5%90%AF%E5%8A%A8%20web_demo.py">3.2.2 启动 web_demo.py</h4> 
<ol><li>安装 gradio 库，在 <strong>ChatGLM</strong> 目录下打开命令行终端，输入：<br><span style="color:#be191c;"><code><span style="background-color:#fef2f0;">pip install gradio</span></code></span><br> 即可安装 demo 所需要的库。</li><li>修改模型路径。编辑 <em>web_demo.py</em> 代码，修改 4、5 行的模型文件夹路径，将原始的 <strong>“THUDM/ChatGLM-6B”</strong> 替换为 <strong>“model”</strong> 即可。</li><li>修改量化版本。如果你的显存大于 14G，则无需量化可以跳过此步骤。如果你的显存只有 6G 或 10G，则需要在第 5 行代码上添加 <strong>quantize(4)</strong> 或 <strong>quantize(8)</strong> ，如下： <pre><code># 6G 显存可以 4 bit 量化
model = AutoModel.from_pretrained("model", trust_remote_code=True).half().quantize(4).cuda()

# 10G 显存可以 8 bit 量化
model = AutoModel.from_pretrained("model", trust_remote_code=True).half().quantize(8).cuda()</code></pre> </li><li>执行 python 文件即可，可以在命令行终端输入：<br><span style="color:#be191c;"><code><span style="background-color:#fef2f0;">python web_demo.py</span></code></span><br> 即可启动 demo，开始使用了！</li></ol> 
<h4 id="3.2.3%C2%A0%E5%90%AF%E5%8A%A8%20web_demo2.py">3.2.3 启动 web_demo2.py</h4> 
<pre><code>#Add a steamlit based demo web_demo2.py for better UI.
#Need to install streamlit and streamlit-chat component fisrt:
pip install streamlit
pip install streamlit-chat
#then run with the following command:
streamlit run web_demo2.py --server.port 6006</code></pre> 
<p>其他也一样</p> 
<p></p> 
<h2 id="%E5%9B%9B%E3%80%81%E3%80%90%E6%9C%80%E6%96%B0%E3%80%91ChatGLM-6B-int4%20%E7%89%88%E6%9C%AC%E6%95%99%E7%A8%8B">四、【最新】ChatGLM-6B-int4 版本教程</h2> 
<p>ChatGLM-6B-INT4 是 ChatGLM-6B 量化后的模型权重。具体的，ChatGLM-6B-INT4 对 ChatGLM-6B 中的 28 个 GLM Block 进行了 INT4 量化，没有对 Embedding 和 LM Head 进行量化。<strong>量化后的模型理论上 6G 显存</strong>（使用 CPU 即内存） 即可推理，具有在 嵌入式设备（如树莓派）上运行的可能。<br>  </p> 
<h3 id="4.1%20%E4%B8%8B%E8%BD%BD">4.1 下载</h3> 
<ol><li>打开 ChatGLM-6B 的 GitHub 页面（<a href="https://github.com/THUDM/ChatGLM-6B" title="GitHub - THUDM/ChatGLM-6B: ChatGLM-6B：开源双语对话语言模型  | An Open Bilingual Dialogue Language Model">GitHub - THUDM/ChatGLM-6B: ChatGLM-6B：开源双语对话语言模型 | An Open Bilingual Dialogue Language Model</a>），下载所有文件到文件夹 …/ChatGLM/ 下。</li><li>在 …/<strong>ChatGLM</strong>/ 下新建文件夹 …/<strong>ChatGLM/model</strong> 。打开 huggingface 页面（<a href="https://huggingface.co/THUDM/chatglm-6b-int4/tree/main" rel="nofollow" title="THUDM/chatglm-6b-int4 at main">THUDM/chatglm-6b-int4 at main</a>），下载 ChatGLM-6B-int4 的 int4 量化过的模型，把所有模型文件下载到 …/model 目录下。<br> 注意：要点击这些向下的箭头才能下载，如下： <p class="img-center"><img alt="" height="643" src="https://images2.imgbox.com/85/e9/Xh2YAiVX_o.png" width="1024"></p> </li><li>至此所有文件下载完毕，大文件夹 …/ChatGLM/ 下有 demo 和配置环境的相关代码，且包含小文件夹 …/ChatGLM/model，model 文件夹内存放模型相关文件。</li></ol> 
<h3 id="4.2%20%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83">4.2 配置环境，Windows下NVIDIA 显卡可用</h3> 
<ol><li>如果没有 6G 显存，需要使用 CPU 运行，模型会根据硬件自动编译 CPU Kernel ，请确保已安装 <span style="color:#fe2c24;">GCC </span>和 <span style="color:#fe2c24;">OpenMP</span> （Linux一般已安装，对于<span style="color:#fe2c24;"><strong><span style="background-color:#fef2f0;">Windows</span></strong></span>则需<span style="color:#fe2c24;"><strong><span style="background-color:#fef2f0;">手动安装</span></strong></span>），以获得最佳并行计算能力。可以试试下面两种方法安装GCC和OpenMP(推荐第二种比较快)：<br><a href="https://blog.csdn.net/weixin_64064486/article/details/123940266" title="Windows下GCC安装和使用_windows gcc_丸子爱学习！的博客-CSDN博客">Windows下GCC安装和使用_windows gcc_丸子爱学习！的博客-CSDN博客</a><br><a href="https://blog.csdn.net/dreamer_blue/article/details/51755767" title="OpenMP Windows/macOS 配置指南_windows安装openmp_Dreamer_bLue的博客-CSDN博客">OpenMP Windows/macOS 配置指南_windows安装openmp_Dreamer_bLue的博客-CSDN博客</a></li><li>如果有 6G 显存，则需要安装与显卡版本对应的 cuda，之后再安装与 cuda 版本对应的 pytorch。网上有很多安装 cuda 和 pytorch 的教程，这里不在赘述。（使用 CPU 运行的同学跳过步骤 2）</li><li>安装相关依赖，在 …/ChatGLM/ 目录下打开命令行终端，输入 <pre><code>pip install -r requirements.txt</code></pre> 按回车后，pip 就自动下载和安装相关依赖库了。</li><li><span style="color:#be191c;">计算机使用的是 Intel 显卡而不是 NVIDIA 显卡 可以使用CUDA吗？</span> <p><strong><span style="color:#fe2c24;">不可以</span></strong>。CUDA（Compute Unified Device Architecture）是 NVIDIA 开发的一种并行计算平台和编程模型，它基于 NVIDIA 的 GPU 架构，并且只能在 NVIDIA 的显卡上运行。因此，如果您的计算机使用的是 Intel 显卡而不是 NVIDIA 显卡，就无法使用 CUDA。</p> <p>不过，如果需要在使用 Intel 显卡的计算机上进行并行计算，可以考虑使用其他并行计算平台和编程模型，例如 OpenCL、OpenACC、OpenMP 等。这些平台和编程模型可以在支持 Intel 显卡的计算机上运行，并且也可以用来进行并行计算和加速各种应用程序。</p> NVIDIA显卡的下载驱动：<a href="https://www.nvidia.com/Download/index.aspx" rel="nofollow" title="https://www.nvidia.com/Download/index.aspx">https://www.nvidia.com/Download/index.aspx</a><br> NVIDIA显卡的下载CDUA：<a href="https://developer.nvidia.com/cuda-downloads" rel="nofollow" title="CUDA Toolkit 12.1 Downloads | NVIDIA Developer">CUDA Toolkit 12.1 Downloads | NVIDIA Developer</a></li></ol> 
<h3 id="4.3%20%E5%90%AF%E5%8A%A8%20demo%20%E7%A8%8B%E5%BA%8F">4.3 启动 demo 程序</h3> 
<p>本节内容与 <strong>3.2 节</strong>内容基本相同，参考 <strong>3.2 节</strong>的部分内容即可，这里不再赘述。请注意：↓↓↓<br><strong>与 3.2 节不同的是，3.2.1 的步骤 2 和 3.2.2 的步骤 3 直接忽略即可，因为模型已经量化过，不需要重复量化。</strong></p> 
<h2 id="%E4%BA%94%E3%80%81%E5%B0%8F%E7%BB%93">五、小结</h2> 
<p>        经过一段时间对 ChatGLM-6B、文心一言和 ChatGPT 的使用后，前二者在文字对话上与 ChatGPT 差距已经不大了，在代码生成能力上还有一定差距，但是都好过 GPT-3.5。<br> 文心一言在大部分情况下比 ChatGLM-6B 好一些，不过要注意 ChatGLM-6B 只有 60 亿参数，而且可以单卡部署，这已经是很大的进步了，所以我对 ChatGLM 的发展还是非常看好的，官方也说过除了 int4 量化，还会进一步压缩模型。<br>         总的来说，ChatGLM-6B 在同参数量下可以碾压其他对话模型，而且可以部署到个人电脑上，或者用华为的免费 GPU。通过几天的体验，ChatGLM-6B 是对话模型里非常让人惊喜的了，所以推荐大家可以部署玩玩。甚至可以考虑给嵌入式设备部署一波，也期待官方的进一步极致压缩！<br>         最后也祝 ChatGLM 和 文心一言可以进一步加油，最近的体验也感受到官方每天的更新和进步了，说明态度还是非常积极的。</p> 
<p>这是官方安装说明：<a href="https://modelnet.ai/modeldoc/dc8a8c79965d4fa1b7323aa07b10d370" rel="nofollow" title="https://modelnet.ai/modeldoc/dc8a8c79965d4fa1b7323aa07b10d370">https://modelnet.ai/modeldoc/dc8a8c79965d4fa1b7323aa07b10d370</a></p> 
<p>这是官方推荐AI：<a href="https://modelnet.ai/application" rel="nofollow" title="大模型社区">大模型社区</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f47744764adf4484ba3fecb66dd82310/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python批量复制指定txt中的图片到指定文件夹</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dcdee0cd82f7e7ff775fe279325309e5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解决 java: 非法字符: ‘\ufeff‘</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>