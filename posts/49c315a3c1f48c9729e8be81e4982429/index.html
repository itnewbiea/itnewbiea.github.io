<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>权重初始化和激活函数小结 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="权重初始化和激活函数小结" />
<meta property="og:description" content="本文章参考李沐老师的动手深度学习,作为个人的笔记.
4.8. 数值稳定性和模型初始化 — 动手学深度学习 2.0.0 documentation (d2l.ai)
文章目录 前言
首先我们理想的神经网络是什么样子的?
一.权重初始化
二.检查激活函数
总结
前言 选择合理的权重初始化和选择合理的激活函数在训练是保证训练的稳定性尤为重要,本文介绍其做法,帮助我们事半功倍.
首先我们理想的神经网络是什么样子的? 这样的一个神经网络对于我们来说才是很好的,并且我们要达到这样的要求我们要做什么呢?
一.权重初始化 在合理值区间例随机初始参数.训练开始得时候更容易有数值不稳定. 远离最优解的地方损失函数的表面可能很复杂.最优解附近表面会比较平 使用N(0,0.01)来初始可能对小网络没有问题,但不能保证深度神经网络. 这里放一个图帮助大家理解.
二.检查激活函数 上述公式表明我们的激活函数必须是f(x) = x 在零附近可以近似的看作是f(x) = x,而且神经网络的权重通常是在零点附近比较小的数,所以tanh(x)与relu(x)是可以满足我们要求的,而sigmoid函数不满足,但可以进行平移变换来将函数调整.
总结 梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。
需要用启发式的初始化方法来确保初始梯度既不太大也不太小。
ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。
随机初始化是保证在进行优化前打破对称性的关键。
Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/49c315a3c1f48c9729e8be81e4982429/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-05T12:39:21+08:00" />
<meta property="article:modified_time" content="2024-01-05T12:39:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">权重初始化和激活函数小结</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>本文章参考李沐老师的动手深度学习,作为个人的笔记.</p> 
</blockquote> 
<p><a href="https://zh.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#" rel="nofollow" title="4.8. 数值稳定性和模型初始化 — 动手学深度学习 2.0.0 documentation (d2l.ai)">4.8. 数值稳定性和模型初始化 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p> 
<div> 
 <h4 id="%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95">文章目录</h4> 
</div> 
<p id="main-toc"></p> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="%E9%A6%96%E5%85%88%E6%88%91%E4%BB%AC%E7%90%86%E6%83%B3%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%98%AF%E4%BB%80%E4%B9%88%E6%A0%B7%E5%AD%90%E7%9A%84%3F-toc" style="margin-left:0px;"><a href="#%E9%A6%96%E5%85%88%E6%88%91%E4%BB%AC%E7%90%86%E6%83%B3%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%98%AF%E4%BB%80%E4%B9%88%E6%A0%B7%E5%AD%90%E7%9A%84%3F" rel="nofollow">首先我们理想的神经网络是什么样子的?</a></p> 
<p id="%E4%B8%80.%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96-toc" style="margin-left:0px;"><a href="#%E4%B8%80.%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96" rel="nofollow">一.权重初始化</a></p> 
<p id="%E4%BA%8C.%E6%A3%80%E6%9F%A5%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E4%BA%8C.%E6%A3%80%E6%9F%A5%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" rel="nofollow">二.检查激活函数</a></p> 
<p id="%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<p></p> 
<hr> 
<h2 id="%E5%89%8D%E8%A8%80"><a id="_7"></a>前言</h2> 
<p>选择合理的权重初始化和选择合理的激活函数在训练是保证训练的稳定性尤为重要,本文介绍其做法,帮助我们事半功倍.</p> 
<p></p> 
<h2 id="%E9%A6%96%E5%85%88%E6%88%91%E4%BB%AC%E7%90%86%E6%83%B3%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%98%AF%E4%BB%80%E4%B9%88%E6%A0%B7%E5%AD%90%E7%9A%84%3F"><a id="pandas_16"></a>首先我们理想的神经网络是什么样子的?</h2> 
<p><img alt="" height="669" src="https://images2.imgbox.com/48/d8/xamYbXeo_o.jpg" width="1200"></p> 
<p>这样的一个神经网络对于我们来说才是很好的,并且我们要达到这样的要求我们要做什么呢?</p> 
<h2 id="%E4%B8%80.%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><a id="_19"></a>一.权重初始化</h2> 
<ul><li>在合理值区间例随机初始参数.</li><li>训练开始得时候更容易有数值不稳定.</li></ul> 
<ol><li>远离最优解的地方损失函数的表面可能很复杂.</li><li>最优解附近表面会比较平</li></ol> 
<ul><li>使用N(0,0.01)来初始可能对小网络没有问题,但不能保证深度神经网络.</li></ul> 
<p>这里放一个图帮助大家理解.</p> 
<p><img alt="" height="500" src="https://images2.imgbox.com/2b/ae/yyQorhVT_o.png" width="828"></p> 
<h3><a id="1_20"></a></h3> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a8/aa/EWdG9m4Z_o.jpg"></p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e0/1e/y3WggV48_o.jpg"></p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/01/27/43HnqKYX_o.jpg"></p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/8d/3f/Ufild6Dr_o.jpg"></p> 
<h2 id="%E4%BA%8C.%E6%A3%80%E6%9F%A5%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">二.检查激活函数</h2> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c8/6d/A1zi2RtY_o.jpg"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c8/75/e4S5eKtA_o.jpg"></p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/0a/e0/79b1DkBc_o.jpg"></p> 
<p></p> 
<p>上述公式表明我们的激活函数必须是f(x) = x 在零附近可以近似的看作是f(x) = x,而且神经网络的权重通常是在零点附近比较小的数,所以tanh(x)与relu(x)是可以满足我们要求的,而sigmoid函数不满足,但可以进行平移变换来将函数调整.</p> 
<hr> 
<h2 id="%E6%80%BB%E7%BB%93"><a id="_45"></a>总结</h2> 
<ul><li> <p>梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。</p> </li><li> <p>需要用启发式的初始化方法来确保初始梯度既不太大也不太小。</p> </li><li> <p>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</p> </li><li> <p>随机初始化是保证在进行优化前打破对称性的关键。</p> </li><li> <p>Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。</p> </li></ul>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0ba868e788502c05b9e226dea04363c9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">NeurIPS 2023 Spotlight | 基于超图的表格语言模型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7512a7298c9bff3b9d9b74fca92c87dc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用CentOS 7.6搭建HTTP隧道代理服务器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>