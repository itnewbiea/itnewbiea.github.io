<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Open-Vocabulary Object Detection Using Captions(2021 CVPR)----论文解读 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Open-Vocabulary Object Detection Using Captions(2021 CVPR)----论文解读" />
<meta property="og:description" content="Open-Vocabulary Object Detection Using Captions[2021CVPR]----论文解读 papercode1. AbstractOpen-Vocabulary Object Detection Using Captions 2. Introduction设想与构思思路与做法OVD、ZSD、 WSD的区别？ 3. Related WorkZSDWSDObject detection using mixed supervision运用的两种技术Visual grounding of referring expressionsVision-language transformers总结 3. Method3.1. Learning a visual-semantic space问题： zero-shot 过拟合解决image-caption 预训练loss 3.2. Learning open-vocabulary detection experiment参考blog paper code https://github.com/alirezazareian/ovr-cnn
1. Abstract 目标检测现存的问题? 学习更多的对象类别通常需要更多的边框注释。
比如：目标检测从600类扩展到60,000类需要100倍的资源
解决方法 Weakly supervised and zero-shot learning
缺陷: 将目标检测器扩展到更少监督的类别，但它们还没有像监督模型那样成功和广泛采用。
Open-Vocabulary Object Detection Using Captions 新的训练方法:
1.bounding box 注释有限的目标种类集（基类）,
2.image-caption pairs (图像-标题对) 集（基类&#43;可能包含部分新类）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/685243b674dcedd4ff868a81b201c79e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-05T15:50:21+08:00" />
<meta property="article:modified_time" content="2022-01-05T15:50:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Open-Vocabulary Object Detection Using Captions(2021 CVPR)----论文解读</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>Open-Vocabulary Object Detection Using Captions[2021CVPR]----论文解读</h4> 
 <ul><li><a href="#paper_1" rel="nofollow">paper</a></li><li><a href="#code_2" rel="nofollow">code</a></li><li><a href="#1_Abstract_4" rel="nofollow">1. Abstract</a></li><li><ul><li><a href="#OpenVocabulary_Object_Detection_Using_Captions_13" rel="nofollow">Open-Vocabulary Object Detection Using Captions</a></li></ul> 
  </li><li><a href="#2_Introduction_28" rel="nofollow">2. Introduction</a></li><li><ul><li><a href="#_29" rel="nofollow">设想与构思</a></li><li><a href="#_50" rel="nofollow">思路与做法</a></li><li><a href="#OVDZSD_WSD_61" rel="nofollow">OVD、ZSD、 WSD的区别？</a></li></ul> 
  </li><li><a href="#3_Related_Work_65" rel="nofollow">3. Related Work</a></li><li><ul><li><a href="#ZSD_66" rel="nofollow">ZSD</a></li><li><a href="#WSD_76" rel="nofollow">WSD</a></li><li><a href="#Object_detection_using_mixed_supervision_85" rel="nofollow">Object detection using mixed supervision</a></li><li><a href="#_94" rel="nofollow">运用的两种技术</a></li><li><a href="#Visual_grounding_of_referring_expressions_97" rel="nofollow">Visual grounding of referring expressions</a></li><li><a href="#Visionlanguage_transformers_104" rel="nofollow">Vision-language transformers</a></li><li><a href="#_109" rel="nofollow">总结</a></li></ul> 
  </li><li><a href="#3_Method_115" rel="nofollow">3. Method</a></li><li><ul><li><a href="#31_Learning_a_visualsemantic_space_129" rel="nofollow">3.1. Learning a visual-semantic space</a></li><li><ul><li><a href="#_zeroshot__131" rel="nofollow">问题： zero-shot 过拟合</a></li><li><a href="#_141" rel="nofollow">解决</a></li><li><a href="#imagecaption__147" rel="nofollow">image-caption 预训练</a></li><li><a href="#loss_165" rel="nofollow">loss</a></li></ul> 
   </li><li><a href="#32_Learning_openvocabulary_detection_168" rel="nofollow">3.2. Learning open-vocabulary detection</a></li></ul> 
  </li><li><a href="#experiment_170" rel="nofollow">experiment</a></li><li><a href="#blog_174" rel="nofollow">参考blog</a></li></ul> 
</div> 
<p></p> 
<h2><a id="paper_1"></a>paper</h2> 
<h2><a id="code_2"></a>code</h2> 
<p><a href="https://github.com/alirezazareian/ovr-cnn">https://github.com/alirezazareian/ovr-cnn</a></p> 
<h2><a id="1_Abstract_4"></a>1. Abstract</h2> 
<ul><li>目标检测现存的问题?</li></ul> 
<blockquote> 
 <p>学习更多的对象类别通常需要更多的边框注释。<br> 比如：目标检测从600类扩展到60,000类需要100倍的资源</p> 
</blockquote> 
<ul><li>解决方法</li></ul> 
<blockquote> 
 <p>Weakly supervised and zero-shot learning<br> 缺陷: 将目标检测器扩展到更少监督的类别，但它们还没有像监督模型那样成功和广泛采用。</p> 
</blockquote> 
<h3><a id="OpenVocabulary_Object_Detection_Using_Captions_13"></a>Open-Vocabulary Object Detection Using Captions</h3> 
<p><img src="https://images2.imgbox.com/2e/9c/isMnvvTM_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>新的训练方法:<br> 1.bounding box 注释有限的目标种类集（基类）,<br> 2.image-caption pairs (图像-标题对) 集（基类+可能包含部分新类）。</p> 
</blockquote> 
<blockquote> 
 <p>优点:<br> 1.该方法能够在训练过程中检测和定位没有边界框标注的目标，且精度明显高于 zero-shot learning 方法。<br> 2.带有边界框标注的对象可以像监督方法一样准确地检测出来，明显优于Weakly supervised 基线。<br> 总结:<mark>能够检测没有 bounding box 的目标 , 优于zero-shot ; 也能够检测常规的有bounding box 的目标, 优于弱监督 。</mark></p> 
</blockquote> 
<h2><a id="2_Introduction_28"></a>2. Introduction</h2> 
<h3><a id="_29"></a>设想与构思</h3> 
<p>设想 : imitate this human ability , 模拟人类如何通过自然监督 学习的<br> 构思 : designing a <code>two-stage</code> framework named Open-Vocabulary object Detection (<code>OVD</code>)<br> <mark>模拟了人类的这种能力，通过设计一个两阶段的框架，名为开放词汇表对象检测(OVD)</mark><br> <img src="https://images2.imgbox.com/ec/08/hjTRl5Xl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a8/2d/W3k0Is9o_o.png" alt="在这里插入图片描述"></p> 
<p>我们建议首先使用 image-caption pairs (图像-标题对)的语料库来获取 unbounded vocabulary (无界词汇表) 的概念构成 <strong>visula-semantic space</strong>，然后使用这些知识来学习目标检测(或任何其他下游任务)。</p> 
<p>比如说检测类任务，只需要一部分带注释的基类。<br> 后续就可以目标检测到新类。</p> 
<p>详细：<br> <img src="https://images2.imgbox.com/10/23/ZXgqJWY9_o.png" alt="在这里插入图片描述" width="600"></p> 
<ol><li>在 image-caption pairs 数据集上学习到一个 visual-semantic space（基类+可能包含部分新类）</li><li>在有注释得基类学习如何让目标检测</li><li>可以利用 visual-semantic space ，检测到新类</li></ol> 
<h3><a id="_50"></a>思路与做法</h3> 
<p><img src="https://images2.imgbox.com/58/bb/UiqGi3MM_o.png" alt="在这里插入图片描述"><br> 目的：是可以输入<strong>目标单词 target vocabulary</strong> , 得到一张图，且能检测物体。<br> <strong>V<sub>T</sub></strong> ：目标类词汇表 ， 这也是我们需要训练得到的。<br> <strong>V<sub>C</sub></strong> ：image-caption dataset 数据集包含的大量词汇<br> <strong>V<sub>B</sub></strong> ：目标检测数据集（基类）<br> <strong>V<sub>Ω</sub></strong> ：整个语言词汇集 ，<br> <mark>这里值得一提的是V<sub>T</sub>不可知。</mark> unbounded vocabulary<br> <img src="https://images2.imgbox.com/93/db/zqfUT87T_o.png" alt="在这里插入图片描述"><br> <strong>先在 image-caption上预训练，再在目标检测数据集上微调，要保持在新类得泛化能力，需要在与训练阶段进行大量得image-caption的预训练。</strong></p> 
<h3><a id="OVDZSD_WSD_61"></a>OVD、ZSD、 WSD的区别？</h3> 
<p><img src="https://images2.imgbox.com/47/99/QvI2M4hd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0e/c8/1ZADJyjk_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="3_Related_Work_65"></a>3. Related Work</h2> 
<h3><a id="ZSD_66"></a>ZSD</h3> 
<p><img src="https://images2.imgbox.com/c9/12/gM2zyPRG_o.png" alt="在这里插入图片描述" width="800"><br> <strong>ZSD</strong> aims to generalize from annotated (seen) object classes to other (unseen) categories.（目的是将带注释的(可见的)对象类推广到其他(不可见的)类别）<br> 存在对未知类类别少，检测效果差的问题。<br> 问题存在原因：<br> <img src="https://images2.imgbox.com/7a/fa/fyRQ4Acz_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3e/54/DGicLr9v_o.png" alt="在这里插入图片描述"><br> 解决： open vocaluary</p> 
<h3><a id="WSD_76"></a>WSD</h3> 
<p><strong>WSD</strong> is the most widely used approach to train object detectors without<br> bounding box annotations(是不需要边框标注训练对象检测器的方法）,定位效果不好,且词汇表具有局限性。<br> 问题：<br> <img src="https://images2.imgbox.com/f7/83/kjdZv0ac_o.png" alt="在这里插入图片描述"><br> 解决：<br> <img src="https://images2.imgbox.com/dd/e8/hp2fEhN8_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Object_detection_using_mixed_supervision_85"></a>Object detection using mixed supervision</h3> 
<p>问题：<br> <img src="https://images2.imgbox.com/de/84/JlSO15WH_o.png" alt="在这里插入图片描述"></p> 
<p>大多数方法对所有类都需要边界框标注，并且只将弱监督用作辅助数据。<br> 具体来说：在基类上进行训练，然后使用弱监督学习转移到目标类，这些方法通常会在基类上<strong>降低性能</strong>相反。<br> OVD：使用标题学习包含目标类的开放词汇表语义空间，然后通过监督学习将其转移到目标检测任务中。<br> 所有弱监督和混合监督方法的另一个限制是，<strong>它们需要预定义分类法中的图像级注释，而且它们只能学习那些预定义的类</strong>。<strong>no unbounded vocabulary</strong></p> 
<h3><a id="_94"></a>运用的两种技术</h3> 
<p>Visual grounding<br> Vision-language transformers</p> 
<h3><a id="Visual_grounding_of_referring_expressions_97"></a>Visual grounding of referring expressions</h3> 
<p>can be seen as an open-vocabulary object localization problem:<br> Given an image and a noun phrase that refers to an object, usually within the context of a full caption, the goal is to localize the referred object in the image using a bounding box.<br> 给定一幅图像和一个名词短语，它指向一个物体，通常在一个完整的标题的上下文中，目标是使用一个边界框来定位图像中所指的物体。</p> 
<p>我们受到弱监督视觉 Visual grounding 的丰富文献的启发来设计我们的图像捕捉预训练技术。更具体地说，通过学习visual-semantic common space，我们学会了<strong>map caption words to image regions</strong>。但是，在没有提供caption的情况下，不能单独使用这种映射来检测推理过程中的对象。因此，我们建议通过另一阶段的训练，将Visual grounding 转移到目标检测任务中。</p> 
<h3><a id="Visionlanguage_transformers_104"></a>Vision-language transformers</h3> 
<p><img src="https://images2.imgbox.com/6f/a7/mHXySODQ_o.png" alt="在这里插入图片描述"></p> 
<p>transformer，<strong>以图像-标题对作为输入，并提取可用于各种下游 视觉+语言 任务的通用特征</strong>。</p> 
<h3><a id="_109"></a>总结</h3> 
<p><strong>ZSD</strong>的检测效果差（map不高），主要原因，我认为就是对于<strong>没有任何未知类的例子经过训练</strong>，OVD 应该是会有部分未知类通过image-caption dataset 训练课得知，因此从<strong>现有基类的特征其实很难推出新类</strong>。<br> <strong>WSD</strong> 定位效果不好， 我个人分析认为，他从没有注释的图片很难学习到特征，就很难像OVD那样通过image-caption那样，至少有图像和文本方向的特征给与，再通过基类的相关有注释框的图片学习，就能很好的定位。<br> <strong>mixed supervision</strong>，其实同样存在上面的缺陷，在基类上进行训练，然后使用弱监督学习转移到目标类，这些方法通常会在基类上<strong>降低性能</strong>相反<br> <strong>Visual grounding</strong>和<strong>Vision-language transformers</strong> 就是来帮助解决作者的设想，通过 <strong>Vision-language transformers</strong> 可以提取 文本和图像的特征，<strong>Visual grounding</strong> 则就是根据这些特征进行定位。</p> 
<h2><a id="3_Method_115"></a>3. Method</h2> 
<p><img src="https://images2.imgbox.com/22/8c/SsjIqSmC_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/58/a1/1WwFqCSR_o.png" alt="在这里插入图片描述"></p> 
<pre><code>Zero-shot 的做法：
在一组基类V~B~上接受训练，在另一组目标类V~T~上进行测试。 Base Test Vocabulary
由于基类的样本很小，会导致严重的过拟合，以至于SOTA的mAP比基类低9倍。
</code></pre> 
<p><img src="https://images2.imgbox.com/db/28/4rhyf7cm_o.png" alt="在这里插入图片描述"><br> 作者则是反其道而行，他是先训练丰富的语义空间 visual-semantic space V<sub>C</sub> ， 而不是训练基类V<sub>B</sub> ，可以防止防止过拟合 ，再运用到下游任务。 （语义库 Wikipedia）</p> 
<h3><a id="31_Learning_a_visualsemantic_space_129"></a>3.1. Learning a visual-semantic space</h3> 
<h4><a id="_zeroshot__131"></a>问题： zero-shot 过拟合</h4> 
<p><img src="https://images2.imgbox.com/db/77/FKxCJCGa_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fd/a0/fga6F69e_o.png" alt="在这里插入图片描述"><br> 基于ImageNet数据集的CNN backbone。<br> backbone预训练的结果，可以为对象识别 提取优化特征，然后用于训练一个新的分类头 来分类 固定的带注释类集，也就是基类。<br> 这在ZSD设置中是有问题的，因为在基类上训练的分类器无法识别目标类。因此，ZSD方法通过将固定的嵌入矩阵[13]替换分类器权值，为来学习从视觉特征到预先训练的基类空间的线性投影。通过这种方法，假设嵌入空间的连续性，期望网络能泛化到目标类。然而，这种方法容易出现过拟合，因为投影到少量的嵌入空间(基类嵌入)是一个欠定问题[3]。<br> <img src="https://images2.imgbox.com/50/e3/iUC8JIMA_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_141"></a>解决</h4> 
<p><img src="https://images2.imgbox.com/e9/de/JksIqXBb_o.png" alt="在这里插入图片描述"><br> 为了防止过拟合，我们建议在预训练时，随着CNN主干学习前述的Vision To Language (V2L)投影层，其中数据不局限于一小组基类。为此，我们使用一个图像-标题数据集，因为标题包含丰富的词汇和语义结构，可以用来学习单词的含义，包括对象名称。为了有效地从字幕提供的丰富的监督中学习，我们利用了 <strong>visual grounding and visionlanguage transformers</strong> 的最新进展。我们使用一个主(visual grounding )任务和一组辅助的自我监督任务来学习一个鲁棒的CNN主干和V2L层。</p> 
<p><img src="https://images2.imgbox.com/1c/4b/ohR3aFeb_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="imagecaption__147"></a>image-caption 预训练</h4> 
<p><img src="https://images2.imgbox.com/b2/be/yjENSiJ2_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7c/1e/lqVsVG12_o.png" alt="在这里插入图片描述"><br> <strong>visual backbone</strong>： image 输入 ，分隔图像成网格，r<sub>i</sub><sup>I</sup> 代表网格图像块 i 特征d维向量<br> <strong>language backbone</strong> ： e<sub>j</sub><sup>C</sup> word embeeding + position embedding = f<sub>j</sub><sup>C</sup> (通过模块BERT)</p> 
<blockquote> 
 <p><a href="https://zhuanlan.zhihu.com/p/98855346" rel="nofollow">BERT</a><br> 简单来说他是可融合上下左右文的信息，能生成更高语义的深层次的<strong>双向语言特征</strong><br> <strong>V2L</strong> ，是为了加强词和图像区域的对应关系。</p> 
</blockquote> 
<p>e<sub>j</sub><sup>C</sup>和e<sub>i</sub><sup>I</sup> 可以用来visual grounding 用来定位。</p> 
<p><img src="https://images2.imgbox.com/78/ae/XLrs0R04_o.png" alt="在这里插入图片描述"></p> 
<p>r<sub>i</sub><sup>I</sup> 通过V2L （视觉到语言的映射层）得到 每个区域块的 语言嵌入空间 e<sub>i</sub><sup>I</sup>，<br> 他和 f<sub>j</sub><sup>C</sup> 合并传到 multimodal transformer 得到 m<sub>i</sub><sup>I</sup> ，m<sub>j</sub><sup>C</sup>，<br> 能在每个模态内而且在两个模态之间进行关注。</p> 
<h4><a id="loss_165"></a>loss</h4> 
<p><img src="https://images2.imgbox.com/87/74/BGXJQPvj_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ea/a5/qxJ2VNBC_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="32_Learning_openvocabulary_detection_168"></a>3.2. Learning open-vocabulary detection</h3> 
<p><img src="https://images2.imgbox.com/30/15/sW0dxghW_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="experiment_170"></a>experiment</h2> 
<p>只能说碾压吧！<br> <img src="https://images2.imgbox.com/a8/00/E50xVYQc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/af/0b/4YaBs02k_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="blog_174"></a>参考blog</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/419255664" rel="nofollow">open-Vocabulary object detection using captions</a><br> <a href="https://zhuanlan.zhihu.com/p/369671715" rel="nofollow">近期跨模态检测&amp;分割论文整理与解析（持续更新中）</a><br> <a href="https://zhuanlan.zhihu.com/p/388504127" rel="nofollow">Visual grounding系列–领域初探</a><br> <a href="https://zhuanlan.zhihu.com/p/98855346" rel="nofollow">什么是BERT？</a><br> <a href="https://blog.csdn.net/amusi1994/article/details/116506675">谷歌提出ViLD：超越Supervised的Zero-Shot检测器</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9ad629a55861dba428dd883f22732919/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Oracle的kernel.shmmax和kernel.shmall设置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3972f4e0f718ef358eed6c0483224357/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【论文学习】Adversarial Examples on Graph Data: Deep Insights into Attack and Defense论文学习</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>