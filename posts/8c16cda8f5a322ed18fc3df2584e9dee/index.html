<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>推荐系统深度学习篇-NFM 模型介绍(1) - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="推荐系统深度学习篇-NFM 模型介绍(1)" />
<meta property="og:description" content="一、推荐系统深度学习篇-NFM 模型介绍(1) NFM是2017年由新加披国立大学提出的一种模型，其主要优化点在于提出了Bi-Interaction，Bi-Interaction考虑到了二阶特征组合，减轻了后面MLP部分学习特征信息的压力
论文地址：https://arxiv.org/pdf/1708.05027.pdf
其结构如下
BI-Interaction的结构为
该式可化简为
详细推导过程如下
编程简单记忆方式： 和的平方-平方的和
需要注意的是：
1.该图显示的是在Bi-Interaction后由三层MLP构成，但论文中，只用到了一层隐层；
2 dropout ratio是最重要的超参, 文中在Bi-Interaction层用了0.5
二、美图优化的nfm结构 可看出，美图的nfm并不是直接在Bi-Interaction后加上MLP层，而是将LR，Bi-Interaction，MLP三个部分进行拼接，最后送入到全连接层然后输出
这里的S指的是LR&#43;人工特征组合
三、美图的nfm实现
3.1 S部分
只进入连续特征
3.2 Bi-Interaction
只进入category的特征，category的特征经过embeding处理
3.3MLP部分
MLP部分进入 全部特征（包含连续和类别特征）
3.4 代码介绍
由于S 部分和MLP部分比较简单，在此，只展示Bi-Interaction 的代码，如下
def BiInteractionPooling(inputs): concated_embeds_value = inputs # 计算二阶部分 # square_of_sum 先求和再平方 square_of_sum = tf.square(tf.reduce_sum(concated_embeds_value, axis=1)) # sum_of_square 先平方再求和 sum_of_square = tf.reduce_sum(concated_embeds_value * concated_embeds_value, axis=1) cross_term = 0.5 * (square_of_sum - sum_of_square) return cross_term 这里的input代码如下
sparse_embedding_layer_list=[] sparse_embedding_layer_list.append(cat_id1) sparse_embedding_layer_list." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/8c16cda8f5a322ed18fc3df2584e9dee/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-11-19T11:49:13+08:00" />
<meta property="article:modified_time" content="2020-11-19T11:49:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">推荐系统深度学习篇-NFM 模型介绍(1)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="NFM_1_0"></a>一、推荐系统深度学习篇-NFM 模型介绍(1)</h2> 
<p>NFM是2017年由新加披国立大学提出的一种模型，其主要优化点在于提出了Bi-Interaction，Bi-Interaction考虑到了二阶特征组合，减轻了后面MLP部分学习特征信息的压力</p> 
<p>论文地址：<a href="https://arxiv.org/pdf/1708.05027.pdf" rel="nofollow">https://arxiv.org/pdf/1708.05027.pdf</a></p> 
<p>其结构如下</p> 
<p><img src="https://images2.imgbox.com/c1/91/mAq8qg21_o.png" alt="在这里插入图片描述"></p> 
<p>BI-Interaction的结构为<br> <img src="https://images2.imgbox.com/1f/46/MquFkquL_o.png" alt="在这里插入图片描述"><br> 该式可化简为<br> <img src="https://images2.imgbox.com/be/f2/UMr3abkP_o.png" alt="在这里插入图片描述"><br> 详细推导过程如下<br> <img src="https://images2.imgbox.com/76/d5/mMAH7sva_o.png" alt="在这里插入图片描述"><br> 编程简单记忆方式： 和的平方-平方的和</p> 
<p><strong>需要注意的是</strong>：</p> 
<p>1.该图显示的是在Bi-Interaction后由三层MLP构成，但论文中，只用到了一层隐层；</p> 
<p>2 dropout ratio是最重要的超参, 文中在Bi-Interaction层用了0.5</p> 
<h2><a id="nfm_23"></a>二、美图优化的nfm结构</h2> 
<p>可看出，美图的nfm并不是直接在Bi-Interaction后加上MLP层，而是将LR，Bi-Interaction，MLP三个部分进行拼接，最后送入到全连接层然后输出<br> <img src="https://images2.imgbox.com/ff/a5/cp8JXc4Y_o.png" alt="在这里插入图片描述"><br> 这里的S指的是LR+人工特征组合</p> 
<p>三、美图的nfm实现</p> 
<p>3.1 S部分<br> 只进入连续特征</p> 
<p>3.2 Bi-Interaction<br> 只进入category的特征，category的特征经过embeding处理</p> 
<p>3.3MLP部分<br> MLP部分进入 全部特征（包含连续和类别特征）</p> 
<p>3.4 代码介绍<br> 由于S 部分和MLP部分比较简单，在此，只展示Bi-Interaction 的代码，如下</p> 
<pre><code class="prism language-python">
<span class="token keyword">def</span> <span class="token function">BiInteractionPooling</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    concated_embeds_value <span class="token operator">=</span> inputs
    <span class="token comment"># 计算二阶部分</span>
    <span class="token comment"># square_of_sum 先求和再平方</span>
    square_of_sum <span class="token operator">=</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>concated_embeds_value<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># sum_of_square 先平方再求和</span>
    sum_of_square <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>concated_embeds_value <span class="token operator">*</span> concated_embeds_value<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    cross_term <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span>square_of_sum <span class="token operator">-</span> sum_of_square<span class="token punctuation">)</span>
    <span class="token keyword">return</span> cross_term
</code></pre> 
<p>这里的input代码如下</p> 
<pre><code class="prism language-PYTHON">
	sparse_embedding_layer_list=[]
    sparse_embedding_layer_list.append(cat_id1)
    sparse_embedding_layer_list.append(cat_id2)
    sparse_embedding_layer_list.append(cat_id3)
    sparse_embedding_layer_list.append(cat_id4)
    nfm_input = tf.keras.layers.Concatenate(axis=1)(sparse_embedding_layer_list)

</code></pre> 
<p>注意代码中的cate_id,color_id等都是经过embedding处理，且这些embedding的size必须一致</p> 
<p>参考：<br> 1、https://zhuanlan.zhihu.com/p/134167886</p> 
<hr> 
<p>Edited by ：Eshter<br> date ：20201119</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b038798f54b8aee8e8b2639dab919877/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">idea2019.3开启类的serialVersionUID</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d64b9d6e55b5af8eb51e487bf79c15d8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">pyecharts生成柱形图</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>