<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch 训练时中遇到的卡住停住等问题 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch 训练时中遇到的卡住停住等问题" />
<meta property="og:description" content="目录 前言1. PyTorch 训练时在第一个 epoch 的最后一个 batch 卡死- 问题描述- 可能的原因- 解决方法 2. 使用命令行运行时，卡在第一个 epoch- 问题描述- 原因分析- 解决方法 前言 在实际训练中遇到了各种各样的卡住问题，在此总结一下，PyTorch 训练时遇到的卡住停住等问题可以从以下几个方面根据情况具体分析 (参考PyTorch训练时，Dataloader卡死、挂起，跑一个epoch停了，问题解决方案)：
前一进程还未处理完，又进入下一个导致互锁：在每个Epoch后，或每个 batch 后暂停一下： time.sleep(0.003)；内存问题：使用开关，pin_memory=True/False多进程导致互锁问题：减少进程数或者不用，num_workers=0/1OpenCV与Pytorch互锁；batch size 的设置； 1. PyTorch 训练时在第一个 epoch 的最后一个 batch 卡死 - 问题描述 使用 PyTorch 框架训练模型，训练第一个 epoch 时，在最后一个 batch 处卡死，卡了一天都没有动弹，而 CPU 和 GPU 都处于正常运行的状态，程序也没有报错，并且之前训练一直都是正常的。最终，只能通过 Ctrl&#43;C 强制性暂停。如下图所示。
- 可能的原因 搜索文章发现，有人出现这种问题是和 cv2.imread 有关，用 OpenCV 的接口进行数据读取，而没有用 PIL，导致出现 OpenCV与Pytorch互锁的问题，关闭OpenCV的多线程即可解决问题1 2。但是我们的代码中并没有使用 OpenCV，通过 Debug 发现，出现问题的时候，最后一个 batch =1，而我们使用的是四块 GPU 进行训练，原因就在此。 - 解决方法 Pytorch 的多 GPU 处理接口是 torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/719bacd1f0472399b7219022ab8466c7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-24T23:17:28+08:00" />
<meta property="article:modified_time" content="2020-05-24T23:17:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch 训练时中遇到的卡住停住等问题</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">前言</a></li><li><a href="#1_PyTorch__epoch__batch__10" rel="nofollow">1. PyTorch 训练时在第一个 epoch 的最后一个 batch 卡死</a></li><li><ul><li><a href="#__11" rel="nofollow">- 问题描述</a></li><li><a href="#__13" rel="nofollow">- 可能的原因</a></li><li><a href="#__17" rel="nofollow">- 解决方法</a></li></ul> 
  </li><li><a href="#2__epoch_24" rel="nofollow">2. 使用命令行运行时，卡在第一个 epoch</a></li><li><ul><li><a href="#__25" rel="nofollow">- 问题描述</a></li><li><a href="#__28" rel="nofollow">- 原因分析</a></li><li><a href="#__37" rel="nofollow">- 解决方法</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>前言</h2> 
<p>在实际训练中遇到了各种各样的卡住问题，在此总结一下，PyTorch 训练时遇到的卡住停住等问题可以从以下几个方面根据情况具体分析 (参考<a href="http://blog.ziyouman.cn/?id=75" rel="nofollow">PyTorch训练时，Dataloader卡死、挂起，跑一个epoch停了，问题解决方案</a>)：</p> 
<ul><li>前一进程还未处理完，又进入下一个导致互锁：在每个Epoch后，或每个 batch 后暂停一下： <code>time.sleep(0.003)</code>；</li><li>内存问题：使用开关，<code>pin_memory=True/False</code></li><li>多进程导致互锁问题：减少进程数或者不用，<code>num_workers=0/1</code></li><li>OpenCV与Pytorch互锁；</li><li>batch size 的设置；</li></ul> 
<h2><a id="1_PyTorch__epoch__batch__10"></a>1. PyTorch 训练时在第一个 epoch 的最后一个 batch 卡死</h2> 
<h3><a id="__11"></a>- 问题描述</h3> 
<p>使用 PyTorch 框架训练模型，训练第一个 epoch 时，在最后一个 batch 处卡死，卡了一天都没有动弹，而 CPU 和 GPU 都处于正常运行的状态，程序也没有报错，并且之前训练一直都是正常的。最终，只能通过 <code>Ctrl+C</code> 强制性暂停。如下图所示。<img src="https://images2.imgbox.com/8f/6c/t7Dz2Rdu_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="__13"></a>- 可能的原因</h3> 
<ol><li>搜索文章发现，有人出现这种问题是和 <code>cv2.imread</code> 有关，用 OpenCV 的接口进行数据读取，而没有用 PIL，导致出现 OpenCV与Pytorch互锁的问题，关闭OpenCV的多线程即可解决问题<sup class="footnote-ref"><a href="#fn1" rel="nofollow" id="fnref1">1</a></sup> <sup class="footnote-ref"><a href="#fn2" rel="nofollow" id="fnref2">2</a></sup>。</li><li>但是我们的代码中并没有使用 OpenCV，通过 Debug 发现，出现问题的时候，最后一个 batch =1，而我们使用的是四块 GPU 进行训练，原因就在此。</li></ol> 
<h3><a id="__17"></a>- 解决方法</h3> 
<p>Pytorch 的多 GPU 处理接口是 torch.nn.DataParallel(module, device_ids)，具体并行机制可参考：<a href="https://www.cnblogs.com/ranjiewen/p/10113532.html" rel="nofollow">Pytorch多GPU并行处理</a>。<br> 该接口还要求输入数据的 batch 数量要不小于所指定的 GPU 数量。另根据<a href="https://pytorch.org/docs/stable/_modules/torch/nn/parallel/data_parallel.html" rel="nofollow">官网</a>的<a href="https://blog.csdn.net/yuan_xiaobai/article/details/83623647">解释</a>和注释 (The batch size should be larger than the number of GPUs used.)，batch的数量会均分到每块GPU上进行处理，因此要保证一个整数的关系。</p> 
<p><mark>故，一定要注意在使用多块 GPU 训练时，注意 batch_size 的取值，避免出现最后一个 batch 的实际size小于所指定的 GPU 数量的情况。</mark></p> 
<p>多 GPU 并行训练中其他比较坑的地方可参考：<a href="https://blog.csdn.net/yuuyuhaksho/article/details/87560640">pytorch多GPU数据并行模式 踩坑指南</a>，<a href="https://zhuanlan.zhihu.com/p/95700549" rel="nofollow">和nn.DataParallel说再见</a>。</p> 
<h2><a id="2__epoch_24"></a>2. 使用命令行运行时，卡在第一个 epoch</h2> 
<h3><a id="__25"></a>- 问题描述</h3> 
<p>使用 PyTorch 框架训练模型，在 Pycharm 里面可以正常运行，但是在命令行中运行时，在第一个 epoch 处卡死。<br> <img src="https://images2.imgbox.com/d3/e8/LgZZrrWO_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="__28"></a>- 原因分析</h3> 
<p>经过定位，发现卡死的地方是PyTorch自带的 DataLoader 处有问题， 使用batch和 num_works参数的原理可参考此文<sup class="footnote-ref"><a href="#fn3" rel="nofollow" id="fnref3">3</a></sup>，DataLoader的函数定义如下<sup class="footnote-ref"><a href="#fn4" rel="nofollow" id="fnref4">4</a></sup>：</p> 
<pre><code class="prism language-python">DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> sampler<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>default_collate<span class="token punctuation">,</span> pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
drop_last<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<p>其中 <mark>num_workers</mark> 表示使用多进程加载的进程数，0代表不使用多进程，<mark>pin_memory</mark> 表示是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些。</p> 
<h3><a id="__37"></a>- 解决方法</h3> 
<p>把 num_workers 改为 0 即可正常运行，如何设置还是要靠具体实验。</p> 
<hr class="footnotes-sep"> 
<section class="footnotes"> 
 <ol class="footnotes-list"><li id="fn1" class="footnote-item"><p><a href="https://blog.csdn.net/sinat_37532065/article/details/103990364">Pytorch dataloader在加载最后一个batch时卡死</a> <a href="#fnref1" rel="nofollow" class="footnote-backref">↩︎</a></p> </li><li id="fn2" class="footnote-item"><p><a href="https://blog.csdn.net/u013289254/article/details/103429257?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-1">在PyTorch训练一个epoch时，模型不能接着训练，Dataloader卡死</a> <a href="#fnref2" rel="nofollow" class="footnote-backref">↩︎</a></p> </li><li id="fn3" class="footnote-item"><p><a href="https://www.zhihu.com/question/360391842/answer/931500421" rel="nofollow">pytorch dataloader 使用batch和 num_works参数的原理是什么？ - 孟paper的回答 - 知乎</a> <a href="#fnref3" rel="nofollow" class="footnote-backref">↩︎</a></p> </li><li id="fn4" class="footnote-item"><p><a href="https://blog.csdn.net/weixin_42236288/article/details/80893882">pytorch DataLoader num_workers 出现的问题</a> <a href="#fnref4" rel="nofollow" class="footnote-backref">↩︎</a></p> </li></ol> 
</section>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d5ed0add7f03f06ffc38488f0ff39fb5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数字电子技术实验作业(8)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5983521072daf2e60d21d0e8d0a3bdc6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">adb命令——基础系统类命令 ——安装apk程序以及卸载——adb       shell    pm命令查询...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>