<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python爬虫(爬取贴吧第一页，标题，作者，时间，链接，一楼内容只含文本信息)第一版(不用函数，不用类)，只能爬取指定网页 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python爬虫(爬取贴吧第一页，标题，作者，时间，链接，一楼内容只含文本信息)第一版(不用函数，不用类)，只能爬取指定网页" />
<meta property="og:description" content="import lxml.html import pymongo import requests &#39;&#39;&#39; 1.爬取相应主题贴吧，解析出所有帖子(取帖子标题、作者、时间) 2.下载帖子详情页的1楼信息(只要文字，不要多媒体信息) 3.能够点击下一页进行翻页 4.将解析结果存入数据库(mongodb) &#39;&#39;&#39; #连接mongodb数据库并创建tieba数据库和tiezi集合 client = pymongo.MongoClient(host=&#39;localhost&#39;, port=27017) db = client.tieba collection = db.tiezi #获取页面信息，并用xpath解析内容，通过页面分析可知道每一个帖子都是一个li response=requests.get(&#39;https://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=0&#39;) parse_result=lxml.html.fromstring(response.text) tiezis=parse_result.xpath(&#39;//li[@class=&#34; j_thread_list clearfix&#34;]&#39;) #循环遍历取出内容，并拼接帖子url，进入帖子详情页面通过html分析获得一楼文本 for tiezi in tiezis: title=tiezi.xpath(&#39;.//a[@class=&#34;j_th_tit &#34;]/text()&#39;)[0]#标题 author=tiezi.xpath(&#39;.//span[@data-field]/@title&#39;)[0]#作者 time=tiezi.xpath(&#39;.//span[@title=&#34;创建时间&#34;]/text()&#39;)[0]#时间 lianjie=&#39;https://tieba.baidu.com&#39;&#43;tiezi.xpath(&#39;.//a[@class=&#34;j_th_tit &#34;]/@href&#39;)[0]#作者连接 details = requests.get(lianjie) deta_html = lxml.html.fromstring(details.text) content = deta_html.xpath( &#39;//div[@class=&#34;d_post_content_main d_post_content_firstfloor&#34;]//div[starts-with(@id,&#34;post_content_&#34;)]/text()&#39;)[0] # print(title) # print(author) # print(time) # print(lianjie) # print(content) #根据发帖人不同创建不同信息，然后导入数据库中 info=author info= { &#39;title&#39;: title, &#39;author&#39;: author, &#39;time&#39;: time, &#39;lianjie&#39;: lianjie, &#39;content&#39;: content } collection." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/8314ab26b293440f5b1b39026c0d62b1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-02-28T21:12:58+08:00" />
<meta property="article:modified_time" content="2019-02-28T21:12:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python爬虫(爬取贴吧第一页，标题，作者，时间，链接，一楼内容只含文本信息)第一版(不用函数，不用类)，只能爬取指定网页</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <pre><code class="language-html hljs">import lxml.html
import pymongo
import requests
'''
1.爬取相应主题贴吧，解析出所有帖子(取帖子标题、作者、时间)

2.下载帖子详情页的1楼信息(只要文字，不要多媒体信息)

3.能够点击下一页进行翻页

4.将解析结果存入数据库(mongodb)


'''

#连接mongodb数据库并创建tieba数据库和tiezi集合
client = pymongo.MongoClient(host='localhost', port=27017)
db = client.tieba
collection = db.tiezi

#获取页面信息，并用xpath解析内容，通过页面分析可知道每一个帖子都是一个li
response=requests.get('https://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=0')

parse_result=lxml.html.fromstring(response.text)

tiezis=parse_result.xpath('//li[@class=" j_thread_list clearfix"]')

#循环遍历取出内容，并拼接帖子url，进入帖子详情页面通过html分析获得一楼文本
for tiezi in tiezis:
    title=tiezi.xpath('.//a[@class="j_th_tit "]/text()')[0]#标题
    author=tiezi.xpath('.//span[@data-field]/@title')[0]#作者
    time=tiezi.xpath('.//span[@title="创建时间"]/text()')[0]#时间
    lianjie='https://tieba.baidu.com'+tiezi.xpath('.//a[@class="j_th_tit "]/@href')[0]#作者连接
    details = requests.get(lianjie)
    deta_html = lxml.html.fromstring(details.text)
    content = deta_html.xpath(
        '//div[@class="d_post_content_main  d_post_content_firstfloor"]//div[starts-with(@id,"post_content_")]/text()')[0]

    # print(title)
    # print(author)
    # print(time)
    # print(lianjie)
    # print(content)
    #根据发帖人不同创建不同信息，然后导入数据库中
    info=author
    info= {
        'title': title,
        'author': author,
        'time': time,
        'lianjie': lianjie,
        'content': content
    }
    collection.insert_one(info)



</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5479e889693dc672e8482bb9b6ef081a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Flutter 页面之间传参— —pushNamed</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b42040c6ccefdde4d87a1712f9900cb9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">jsp中引入js、css时出现net::ERR_ABORTED 404 (Not Found)错误</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>