<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习（三） -- 特征工程（2） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习（三） -- 特征工程（2）" />
<meta property="og:description" content="系列文章目录 机器学习（一） -- 概述
机器学习（二） -- 数据预处理（1-3）
机器学习（三） -- 特征工程（1-2）
机器学习（四） -- 模型评估（1-4）
未完待续……
目录
机器学习（三） -- 特征工程（1）
--- 系列文章目录
前言
三、特征预处理
1.1、无量纲化
2、归一化
2.2.1、线性归一化
2.2.1、***最大最小均值归一化
2.2.1、***最大绝对值归一化
3、标准化
四、特征降维
2、特征选择
2.4、低方差特征过滤
2.5、皮尔逊相关系数（Pearson Correlation Coefficient）
3、主成分分析（PCA）
前言 tips：这里只是总结，不是教程哈。
“***”开头的是给好奇心重的宝宝看的，其实不太重要可以跳过。
此处以下所有内容均为暂定，因为我还没找到一个好的，让小白（我自己）也能容易理解（更系统、嗯应该是宏观）的讲解顺序与方式。
第一文主要简述了一下机器学习大致有哪些东西（当然远远不止这些），对大体框架有了一定了解。接着我们根据机器学习的流程一步步来学习吧，掐掉其他不太用得上我们的步骤，精练起来就4步（数据预处理，特征工程，训练模型，模型评估），其中训练模型则是我们的重头戏，基本上所有算法也都是这一步，so，这个最后写，先把其他三个讲了，然后，在结合这三步来进行算法的学习，兴许会好点（个人拙见）。
三、特征预处理 1、定义 通过一些转换函数，将特征数据转换成更适合算法模型的特征数据的过程。
1.1、无量纲化 也称为数据的规范化，是指不同指标之间由于存在量纲不同致其不具可比性，故首先需将指标进行无量纲化，消除量纲影响后再进行接下来的分析。
数值数据的无量纲化：主要有两种归一化、标准化
为什么要进行归一化/标准化？
特征的单位或者大小相差较大，或者某特征的方法相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征
特征预处理API
sklearn.preprocessing 2、归一化 tips：归一化也叫离差标准化，（标准化，也叫标准差标准化，Z-Score标准化）
2.1、定义 将特征缩放到一个特定的范围，通常是[0, 1]。
2.2、分类和公式 2.2.1、线性归一化 这是最常用的归一化，也叫最大-最小归一化或最大-最小规范化。（后面也是用的这种哈）
2.2.1、***最大最小均值归一化 2.2.1、***最大绝对值归一化 2.3、API sklearn.preprocessing.MinMaxScaler 导入： from sklearn.preprocessing import MinMaxScaler 2.4、实例 # 2、实例化一个转换器类 transform = MinMaxScaler() # transform = MinMaxScaler(feature_range=[2,3]) # 3、调用fit_transform data_new = transform." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/a4f6ac82d5ff91d98d3cdae399726ac2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-06T22:36:49+08:00" />
<meta property="article:modified_time" content="2024-01-06T22:36:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习（三） -- 特征工程（2）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95">系列文章目录</h2> 
<p><a href="https://blog.csdn.net/zqx1473/article/details/135242730" title="机器学习（一） -- 概述">机器学习（一） -- 概述</a></p> 
<p><a href="https://blog.csdn.net/zqx1473/article/details/135280575" title="机器学习（二） -- 数据预处理（1-3）">机器学习（二） -- 数据预处理（1-3）</a></p> 
<p><a href="https://blog.csdn.net/zqx1473/article/details/135307321" title="机器学习（三） -- 特征工程（1-2）">机器学习（三） -- 特征工程（1-2）</a></p> 
<p><a href="https://blog.csdn.net/zqx1473/article/details/135347196" title="机器学习（四） -- 模型评估（1-4）">机器学习（四） -- 模型评估（1-4）</a></p> 
<p></p> 
<p>未完待续……</p> 
<hr> 
<p id="main-toc"><strong>目录</strong></p> 
<p><a href="https://blog.csdn.net/zqx1473/article/details/135307321" title="机器学习（三） -- 特征工程（1）">机器学习（三） -- 特征工程（1）</a></p> 
<p>--- </p> 
<p id="%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95-toc" style="margin-left:0px;"><a href="#%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95" rel="nofollow">系列文章目录</a></p> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="%E4%B8%80%E3%80%81%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E7%AE%80%E4%BB%8B" rel="nofollow">三、特征预处理</a></p> 
<p id="1.1%E3%80%81%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96-toc" style="margin-left:80px;"><a href="#1.1%E3%80%81%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96" rel="nofollow">1.1、无量纲化</a></p> 
<p id="2%E3%80%81%E5%BD%92%E4%B8%80%E5%8C%96-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E5%BD%92%E4%B8%80%E5%8C%96" rel="nofollow">2、归一化</a></p> 
<p id="2.2.1%E3%80%81%E7%BA%BF%E6%80%A7%E5%BD%92%E4%B8%80%E5%8C%96-toc" style="margin-left:120px;"><a href="#2.2.1%E3%80%81%E7%BA%BF%E6%80%A7%E5%BD%92%E4%B8%80%E5%8C%96" rel="nofollow">2.2.1、线性归一化</a></p> 
<p id="2.2.1%E3%80%81***%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E5%9D%87%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96-toc" style="margin-left:120px;"><a href="#2.2.1%E3%80%81***%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E5%9D%87%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96" rel="nofollow">2.2.1、***最大最小均值归一化</a></p> 
<p id="2.2.1%E3%80%81***%E6%9C%80%E5%A4%A7%E7%BB%9D%E5%AF%B9%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96-toc" style="margin-left:120px;"><a href="#2.2.1%E3%80%81***%E6%9C%80%E5%A4%A7%E7%BB%9D%E5%AF%B9%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96" rel="nofollow">2.2.1、***最大绝对值归一化</a></p> 
<p id="3%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96" rel="nofollow">3、标准化</a></p> 
<p id="%E5%9B%9B%E3%80%81%E8%AF%AF%E5%B7%AE%E4%BC%B0%E8%AE%A1-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E8%AF%AF%E5%B7%AE%E4%BC%B0%E8%AE%A1" rel="nofollow">四、特征降维</a></p> 
<p id="2%E3%80%81%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9" rel="nofollow">2、特征选择</a></p> 
<p id="2.4%E3%80%81%E4%BD%8E%E6%96%B9%E5%B7%AE%E7%89%B9%E5%BE%81%E8%BF%87%E6%BB%A4-toc" style="margin-left:80px;"><a href="#2.4%E3%80%81%E4%BD%8E%E6%96%B9%E5%B7%AE%E7%89%B9%E5%BE%81%E8%BF%87%E6%BB%A4" rel="nofollow">2.4、低方差特征过滤</a></p> 
<p id="2.5%E3%80%81%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%EF%BC%88Pearson%20Correlation%20Coefficient%EF%BC%89-toc" style="margin-left:80px;"><a href="#2.5%E3%80%81%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%EF%BC%88Pearson%20Correlation%20Coefficient%EF%BC%89" rel="nofollow">2.5、皮尔逊相关系数（Pearson Correlation Coefficient）</a></p> 
<p id="3%E3%80%81%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89" rel="nofollow">3、主成分分析（PCA）</a></p> 
<p></p> 
<hr> 
<h2 id="%E5%89%8D%E8%A8%80"><a id="_12"></a>前言</h2> 
<p><code>tips：这里只是<strong>总结</strong>，不是教程哈。</code></p> 
<p><code>“***”开头的是给好奇心重的宝宝看的，其实不太重要可以跳过。</code></p> 
<p><code>此处以下所有内容均为暂定，因为我还没找到一个好的，让小白（我自己）也能容易理解（更系统、嗯应该是宏观）的讲解顺序与方式。</code></p> 
<p><code>第一文主要简述了一下机器学习大致有哪些东西（当然远远不止这些），对大体框架有了一定了解。接着我们根据机器学习的流程一步步来学习吧，掐掉其他不太用得上我们的步骤，精练起来就4步（数据预处理，特征工程，训练模型，模型评估），其中训练模型则是我们的重头戏，基本上所有算法也都是这一步，so，这个最后写，先把其他三个讲了，然后，在结合这三步来进行算法的学习，兴许会好点（个人拙见）。</code></p> 
<hr> 
<h2 id="%E4%B8%80%E3%80%81%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E7%AE%80%E4%BB%8B">三、特征预处理</h2> 
<h3 id="1%E3%80%81%E5%AE%9A%E4%B9%89">1、定义</h3> 
<p>通过一些转换函数，将特征数据转换成更适合算法模型的特征数据的过程。</p> 
<h4 id="1.1%E3%80%81%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96">1.1、无量纲化</h4> 
<p>也称为数据的规范化，是指不同指标之间由于存在量纲不同致其不具可比性，故首先需将指标进行无量纲化，消除量纲影响后再进行接下来的分析。</p> 
<p>数值数据的无量纲化：主要有两种归一化、标准化</p> 
<p><strong>为什么要进行归一化/标准化？</strong></p> 
<p>特征的单位或者大小相差较大，或者某特征的方法相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征</p> 
<p>特征预处理API</p> 
<pre><code class="language-python">sklearn.preprocessing</code></pre> 
<h3 id="2%E3%80%81%E5%BD%92%E4%B8%80%E5%8C%96">2、归一化</h3> 
<p>tips：归一化也叫离差标准化，（标准化，也叫标准差标准化，Z-Score标准化）</p> 
<h4 id="2.1%E3%80%81%E5%AE%9A%E4%B9%89">2.1、定义</h4> 
<p>将特征缩放到一个特定的范围，通常是[0, 1]。</p> 
<h4 id="2.2%E3%80%81%E5%88%86%E7%B1%BB%E5%92%8C%E5%85%AC%E5%BC%8F">2.2、分类和公式</h4> 
<h5 id="2.2.1%E3%80%81%E7%BA%BF%E6%80%A7%E5%BD%92%E4%B8%80%E5%8C%96">2.2.1、线性归一化</h5> 
<p>这是最常用的归一化，也叫最大-最小归一化或最大-最小规范化。（后面也是用的这种哈）</p> 
<p style="text-align:center;"><img alt="X=\frac{X-min(X)}{max(X)-min(X)}" class="mathcode" src="https://images2.imgbox.com/74/a0/qXRni8Ss_o.png"></p> 
<h5 id="2.2.1%E3%80%81***%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E5%9D%87%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96">2.2.1、***最大最小均值归一化</h5> 
<p style="text-align:center;"><img alt="X=\frac{X-mean(X)}{max(X)-min(X)}" class="mathcode" src="https://images2.imgbox.com/05/60/J6JrtcZr_o.png"></p> 
<h5 id="2.2.1%E3%80%81***%E6%9C%80%E5%A4%A7%E7%BB%9D%E5%AF%B9%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96">2.2.1、***最大绝对值归一化</h5> 
<p style="text-align:center;"><img alt="X=\frac{X}{\left \| X_{max} \right \|}" class="mathcode" src="https://images2.imgbox.com/c4/a9/vmPhCUA5_o.png"></p> 
<h4 id="2.3%E3%80%81API">2.3、API</h4> 
<pre><code class="language-python">sklearn.preprocessing.MinMaxScaler
导入：
from sklearn.preprocessing import MinMaxScaler</code></pre> 
<p><img alt="" height="66" src="https://images2.imgbox.com/a1/ae/r41gBAlE_o.png" width="519"></p> 
<h4 id="2.4%E3%80%81%E5%AE%9E%E4%BE%8B">2.4、实例</h4> 
<p><img alt="" height="760" src="https://images2.imgbox.com/38/a7/bgHyOruF_o.png" width="662"></p> 
<pre><code class="language-python"># 2、实例化一个转换器类
transform = MinMaxScaler()
# transform = MinMaxScaler(feature_range=[2,3])

# 3、调用fit_transform
data_new = transform.fit_transform(data)

print("data_new:\n", data_new)</code></pre> 
<p>转换器中添加feature_range=[2,3]属性，将缩放范围变为[2,3]。</p> 
<p><img alt="" height="422" src="https://images2.imgbox.com/02/07/90qQ4gF5_o.png" width="515"></p> 
<h3 id="3%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96">3、标准化</h3> 
<h4>2.1、定义</h4> 
<p>将特征缩放为均值为0，标准差为1的分布。</p> 
<h4 id="2.2%E3%80%81%E5%85%AC%E5%BC%8F">2.2、公式</h4> 
<p style="text-align:center;"><img alt="X=\frac{X-\mu }{S}" class="mathcode" src="https://images2.imgbox.com/3c/96/TLmUz3HG_o.png">，其中μ是均值，S是标准差</p> 
<h4>2.3、API</h4> 
<pre><code class="language-python">sklearn.preprocessing.StandardScaler
导入：
from sklearn.preprocessing import StandardScaler</code></pre> 
<p><img alt="" height="53" src="https://images2.imgbox.com/82/62/Wpdaz1DF_o.png" width="513"></p> 
<h4>2.4、实例</h4> 
<pre><code class="language-python"># 2、实例化一个转换器类
transform = StandardScaler()
#transform = StandardScaler(feature_range=[2,3])

# 3、调用fit_transform
data_new = transform.fit_transform(data)

print("data_new:\n", data_new)</code></pre> 
<p><img alt="" height="379" src="https://images2.imgbox.com/59/01/Durzfsot_o.png" width="451"></p> 
<p></p> 
<h3>4、***小数定标标准化</h3> 
<p style="text-align:center;"><img alt="X=\frac{X}{10^{lg\left | max(X) \right |}}" class="mathcode" src="https://images2.imgbox.com/54/21/Dlz2bqse_o.png"></p> 
<p>思想：通过移动数据的小数点位置来缩放特征值，使其落在一个较小的范围内，从而提高模型训练的效率和准确性。</p> 
<h3>4、***other</h3> 
<p>其实还有其他的，指数化，对数化，离散化（等频（等深）分箱、等距（等宽）分箱），正规化等。</p> 
<h2 id="%E5%9B%9B%E3%80%81%E8%AF%AF%E5%B7%AE%E4%BC%B0%E8%AE%A1">四、特征降维</h2> 
<h3>1、定义</h3> 
<p>降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程。</p> 
<p>降维的两种方式：特征选择、主成分分析（可以理解一种特征提取的方式）</p> 
<h3 id="2%E3%80%81%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9">2、特征选择</h3> 
<h4>2.1、定义</h4> 
<p>数据中包含冗余或相关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征。</p> 
<h4 id="2.2%E3%80%81%E6%96%B9%E6%B3%95">2.2、方法</h4> 
<p><strong>Filter过滤式：</strong></p> 
<p><strong>                主要探究特征本身特点、特征与特征和目标值之间关联</strong></p> 
<p>        方差选择法：低方差特征过滤</p> 
<p>        相关系数：特征与特征之间的相关程度</p> 
<p><strong>Embedded嵌入式：</strong></p> 
<p><strong>                算法自动选择特征（特征与目标值之间的关联）</strong></p> 
<p>        决策树：信息熵、信息增益</p> 
<p>        正则化：L1，L2</p> 
<p>        深度学习：卷积等</p> 
<h4>2.3、API</h4> 
<pre><code class="language-python">sklearn.feature_selection</code></pre> 
<h4 id="2.4%E3%80%81%E4%BD%8E%E6%96%B9%E5%B7%AE%E7%89%B9%E5%BE%81%E8%BF%87%E6%BB%A4">2.4、<strong>低方差特征过滤</strong></h4> 
<p>删除低方差的一些特征</p> 
<ul><li>特征方差小：某个特征大多样本的值<strong>比较相近</strong></li><li>特征方差大：某个特征很多样本的值都有差别</li></ul> 
<p>API</p> 
<pre><code class="language-python">sklearn.feature_selection.VarianceThreshold
导入：
from sklearn.feature_selection import VarianceThreshold</code></pre> 
<p><img alt="" height="65" src="https://images2.imgbox.com/90/9a/vW0xI3qR_o.png" width="612"></p> 
<p><img alt="" height="813" src="https://images2.imgbox.com/e5/5c/4EZUXtlQ_o.png" width="847"></p> 
<pre><code class="language-python"># 2、实例化一个转换器类
#transform = VarianceThreshold()
transform = VarianceThreshold(threshold=10)

# 3、调用fit_transform
data_new = transform.fit_transform(data)

print("data_new\n", data_new,
      ' \n data_new.shape:', data_new.shape)</code></pre> 
<p> 训练集差异低于threadshold的特征将被删除。默认值是保留非零方差特征，即删除所有样本中具有相同值的特征</p> 
<p><img alt="" height="591" src="https://images2.imgbox.com/c8/2d/jk6B0EwQ_o.png" width="718"></p> 
<h4 id="2.5%E3%80%81%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%EF%BC%88Pearson%20Correlation%20Coefficient%EF%BC%89">2.5、皮尔逊相关系数（Pearson Correlation Coefficient）</h4> 
<p>反映变量之间相关关系密切程度的统计指标.</p> 
<h5 id="2.5.1%E3%80%81%E5%85%AC%E5%BC%8F">2.5.1、公式</h5> 
<p style="text-align:center;"><img alt="r=\frac{n\sum xy-\sum x\sum y}{\sqrt{n\sum x^{2}-(\sum x)^{2}}\sqrt{n\sum y^{2}-(\sum y)^{2}}}" class="mathcode" src="https://images2.imgbox.com/cb/09/gVXyqSoP_o.png"></p> 
<p>相关系数的值介于-1与+1之间:</p> 
<p>当r&gt;0时，表示两变量正相关；<br> r&lt;0时，两变量为负相关；<br> 当|r|=1时，表示两变量为完全相关；<br> 当r=0时，表示两变量间无相关关系；<br> 当0&lt;|r|&lt;1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近0，表示两变量的线性相关越弱。</p> 
<p>一般可按三级划分：|r|&lt;0.4为低度相关；0.4&lt;=|r|&lt;0.7为显著相关；0.7&lt;=|r|&lt;1为高维线性相关</p> 
<h5 id="2.5.2%E3%80%81%E5%AE%9E%E4%BE%8B">2.5.2、实例</h5> 
<p>API</p> 
<pre><code class="language-python">from scipy.stats import pearsonr
from sklearn.feature_selection import VarianceThreshold</code></pre> 
<p><img alt="" height="69" src="https://images2.imgbox.com/b7/9a/RTC3eRG7_o.png" width="605"></p> 
<pre><code class="language-python"># 2、实例化一个转换器类
#transform = VarianceThreshold()
transform = VarianceThreshold(threshold=10)

# 3、调用fit_transform
data_new = transform.fit_transform(data)

print("data_new\n", data_new,
      ' \n data_new.shape:', data_new.shape)

# 计算两个变量之间的相关系数
r = pearsonr(data["pe_ratio"],data["pb_ratio"])

print("相关系数：\n", r)</code></pre> 
<p><img alt="" height="593" src="https://images2.imgbox.com/5a/2f/9xF0DyJY_o.png" width="707"></p> 
<p><img alt="" height="178" src="https://images2.imgbox.com/f4/45/0eFzfO4D_o.png" width="515"></p> 
<p>前面的数为相关系数。</p> 
<h3 id="3%E3%80%81%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89">3、主成分分析（PCA）</h3> 
<h4 id="3.1%E3%80%81%E5%AE%9A%E4%B9%89">3.1、定义</h4> 
<p>旨在利用降维的思想，把多指标转化为少数几个<a href="https://upimg.baike.so.com/doc/1081534-1144480.html" rel="nofollow" title="综合指标">综合指标</a>(即主成分)，其中每个主成分都能够反映原始变量的大部分信息，且所含信息互不重复。</p> 
<p>作用：是数据维数的压缩，尽可能降低原数据的维数（复杂度），损失少量信息</p> 
<h4 id="3.2%E3%80%81%E5%AE%9E%E4%BE%8B">3.2、实例</h4> 
<p>API</p> 
<pre><code class="language-python">sklearn.decomposition.PCA
导入：
from sklearn.decomposition import PCA</code></pre> 
<p><img alt="" height="40" src="https://images2.imgbox.com/d1/c2/1hs3LBiW_o.png" width="398"></p> 
<p><img alt="" height="114" src="https://images2.imgbox.com/a6/bd/8jmCgyX6_o.png" width="258"></p> 
<pre><code class="language-python"># 1、实例化一个转换器类
transform = PCA(n_components=2)  # 4个特征降到2个特征

# 2、调用fit_transform
data_new = transform.fit_transform(data)

print("data_new\n", data_new)




# 1、实例化一个转换器类
transform2 = PCA(n_components=0.95)  # 保留95%的信息

# 2、调用fit_transform
data_new2 = transform2.fit_transform(data)

print("data_new2\n", data_new2)</code></pre> 
<p></p> 
<p><img alt="" height="604" src="https://images2.imgbox.com/b0/8d/8q6hq5R3_o.png" width="647"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b35aaa48a710e403fea447d54c0befc0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">深度学习中的知识蒸馏</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0d05e5c10b6f2a0d8ffa9995edcdab69/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">机器学习（四） -- 模型评估（3）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>