<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>多传感器融合之雷达图像数据集自动生成 - 20220613 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="多传感器融合之雷达图像数据集自动生成 - 20220613" />
<meta property="og:description" content="文章目录 Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications1. Radar Camera Co-Calibration2. ROS pipeline3.Sensor-Fusion Data-Set Generation4. Radar-Only Data-Set Generation5. Sensor Fusion Feature Representation6. 实验结果 Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications Sengupta A, Yoshizawa A, Cao S. Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications[J]. IEEE Robotics and Automation Letters (二区, IF:3.741), 2022.
Code : github.com/radar-lab/autolabelling_radar
本文提出了一种新的多传感器数据集生成方法，利用基于YOLOv3的高精度目标检测从相机自动标记点云数据从一个共同校准的雷达传感器，以生成标记的雷达图像和雷达数据集。
首先校准视觉和雷达传感器，并获得一个雷达到相机的变换矩阵。采用基于密度的聚类方法将采集到的雷达回波信号被不同的目标分离，并利用变换矩阵将聚类质心投影到相机图像上。然后使用匈牙利算法将雷达聚类质心与YOLOv3生成的目标框质心关联起来，并使用预测类进行标记。 典型的基于传感器融合的目标识别方案包括4个阶段，即 (i)数据采集和预处理，(ii)传感器之间的协同校准和关联，(iii)传感器融合特征表示(SFFR)，然后是(iv)分类网络，如图1所示。
首先，同时采集摄像机和毫米波雷达的检测结果，进行预处理，利用协同标定变换矩阵将雷达检测簇投影到摄像机图像平面上。从图像数据中检测到的对象通过边界框进行本地化，边界框内的区域称为感兴趣区域(ROI)。ROI与这些区域内的雷达集群相关联。然后，将每个ROI-Cluster对进行单独的特征提取网络，并将转换后的特征表示从单个传感器的数据进行融合/拼接，得到SFFR。然后，该SFFR被置于全连接多层感知器(MLP)中，直到分类输出层。此外，“回退”网络也必须能够使用传感器的数据单独识别/分类对象，特别是在有不相关的ROI/集群的情况下。这是为了解释由于视场受限、电子故障、遮挡或光照不足导致的单个传感器故障，但仍然能够识别场景中的目标，尽管其置信值低于基于SFFR的分类。 1. Radar Camera Co-Calibration 使用张氏标定程序获取摄像机的内参数。使用一个边缘尺寸为22.14 mm的9 × 6棋盘格图案作为校准板，并使用张氏方法的两个软件实现，即MA TLAB的相机校准工具箱和ROS相机校准器，来估计USB-8MP相机模块的内在参数(包括径向和切向失真参数)，该模块被放置在一个使用3-D打印框架安装的静态三脚架上，离地面1米高。
数据收集设置:(a)运行Ubuntu 18和ROS Melodic的NVidia Jetson Xavier使用UART通过USB集线器从相机-雷达系统捕获数据;(b)两个雷达安装在3-D打印线性轨道上，使用M3螺栓固定，间隔23厘米;©将两个雷达的数据投影到图象上进行数据关联。YOLO输出的包围盒内的雷达投影决定了共校准精度。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/4bfd42176b102e7b0244751ca1633d06/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-13T10:38:36+08:00" />
<meta property="article:modified_time" content="2022-06-13T10:38:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">多传感器融合之雷达图像数据集自动生成 - 20220613</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Automatic_RadarCamera_Dataset_Generation_for_SensorFusion_Applications_1" rel="nofollow">Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications</a></li><li><ul><li><a href="#1_Radar_Camera_CoCalibration_19" rel="nofollow">1. Radar Camera Co-Calibration</a></li><li><a href="#2_ROS_pipeline_23" rel="nofollow">2. ROS pipeline</a></li><li><a href="#3SensorFusion_DataSet_Generation_32" rel="nofollow">3.Sensor-Fusion Data-Set Generation</a></li><li><a href="#4_RadarOnly_DataSet_Generation_38" rel="nofollow">4. Radar-Only Data-Set Generation</a></li><li><a href="#5_Sensor_Fusion_Feature_Representation_45" rel="nofollow">5. Sensor Fusion Feature Representation</a></li><li><a href="#6__49" rel="nofollow">6. 实验结果</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="Automatic_RadarCamera_Dataset_Generation_for_SensorFusion_Applications_1"></a>Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications</h2> 
<blockquote> 
 <p>Sengupta A, Yoshizawa A, Cao S. Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications[J]. IEEE Robotics and Automation Letters (二区, IF:3.741), 2022.<br> Code : <a href="github.com/radar-lab/autolabelling_radar" rel="nofollow">github.com/radar-lab/autolabelling_radar</a></p> 
</blockquote> 
<p>本文提出了一种新的多传感器数据集生成方法，利用基于YOLOv3的高精度目标检测从相机自动标记点云数据从一个共同校准的雷达传感器，以生成标记的雷达图像和雷达数据集。</p> 
<ol><li>首先校准视觉和雷达传感器，并获得一个雷达到相机的变换矩阵。</li><li>采用基于密度的聚类方法将采集到的雷达回波信号被不同的目标分离，并利用变换矩阵将聚类质心投影到相机图像上。</li><li>然后使用匈牙利算法将雷达聚类质心与YOLOv3生成的目标框质心关联起来，并使用预测类进行标记。</li></ol> 
<p><img src="https://images2.imgbox.com/a9/5d/pDxNPzEc_o.png" alt="多传感器数据集自动生成的典型用例和动机，以辅助基于传感器融合的分类。雷达和相机数据使用共同校准的变换矩阵相关联。然后对每个相关的雷达图像对进行特征变换和融合，得到SFFR，用于识别被检测物体的类。请注意，未关联的雷达和摄像机数据可以被置于传感器特定的回退网络中，以确保连续检测，特别是在单个传感器故障的情况下。"><br> 典型的基于传感器融合的目标识别方案包括4个阶段，即 (i)<strong>数据采集和预处理</strong>，(ii)<strong>传感器之间的协同校准和关联</strong>，(iii)<strong>传感器融合特征表示(SFFR)</strong>，然后是(iv)<strong>分类网络</strong>，如图1所示。</p> 
<ul><li>首先，同时采集摄像机和毫米波雷达的检测结果，进行预处理，利用协同标定变换矩阵将雷达检测簇投影到摄像机图像平面上。</li><li>从图像数据中检测到的对象通过边界框进行本地化，边界框内的区域称为感兴趣区域(ROI)。ROI与这些区域内的雷达集群相关联。</li><li>然后，将每个ROI-Cluster对进行单独的特征提取网络，并将转换后的特征表示从单个传感器的数据进行融合/拼接，得到SFFR。</li><li>然后，该SFFR被置于全连接多层感知器(MLP)中，直到分类输出层。此外，“回退”网络也必须能够使用传感器的数据单独识别/分类对象，特别是在有不相关的ROI/集群的情况下。这是为了解释由于视场受限、电子故障、遮挡或光照不足导致的单个传感器故障，但仍然能够识别场景中的目标，尽管其置信值低于基于SFFR的分类。</li></ul> 
<h3><a id="1_Radar_Camera_CoCalibration_19"></a>1. Radar Camera Co-Calibration</h3> 
<p>使用张氏标定程序获取摄像机的内参数。使用一个边缘尺寸为22.14 mm的9 × 6棋盘格图案作为校准板，并使用张氏方法的两个软件实现，即MA TLAB的相机校准工具箱和ROS相机校准器，来估计USB-8MP相机模块的内在参数(包括径向和切向失真参数)，该模块被放置在一个使用3-D打印框架安装的静态三脚架上，离地面1米高。<br> <img src="https://images2.imgbox.com/2a/d0/LkRw1KzE_o.png" alt="在这里插入图片描述"><br> 数据收集设置:(a)运行Ubuntu 18和ROS Melodic的NVidia Jetson Xavier使用UART通过USB集线器从相机-雷达系统捕获数据;(b)两个雷达安装在3-D打印线性轨道上，使用M3螺栓固定，间隔23厘米;©将两个雷达的数据投影到图象上进行数据关联。YOLO输出的包围盒内的雷达投影决定了共校准精度。</p> 
<h3><a id="2_ROS_pipeline_23"></a>2. ROS pipeline</h3> 
<p>雷达和相机的数据是通过ROS管道获取的，主要有三个包，分别是(i) <strong>mmWave_radar</strong>;(2) <strong>usb_webcam</strong>;和(3)<strong>darknet_ros</strong>。</p> 
<p>mmWave_radar包将配置加载到雷达上，缓冲来自雷达的处理过的数据，并将其发布界面显示。</p> 
<p>usb_webcam包从相机(30fps)读取原始图像，并使用估计的相机内部参数来纠正和不扭曲的图像，发布为压缩消息，也伴随着时间戳头。</p> 
<p>darknet_ros包使用OpenCV桥来订阅修正后的图像，并将其归入YOLO分类网络，该网络通过消息输出图像中物体的边界盒和类别，以及图像采集和预测时间戳。</p> 
<h3><a id="3SensorFusion_DataSet_Generation_32"></a>3.Sensor-Fusion Data-Set Generation</h3> 
<p><img src="https://images2.imgbox.com/61/5b/jhk73qz9_o.png" alt="在这里插入图片描述"><br> 第一个数据集由给定帧中相同物体的标记雷达图像对组成，覆盖所有N帧。<br> 首先利用参数将雷达点集合起来形成帧级雷达PCL数据。然后，利用第III-A节估计的Radar-to-Camera变换矩阵T M，将星团的三维质心投影到场景对应的相机图像上。然后确定相应的YOLO边界框和类。然后将YOLO质心和投影的雷达簇质心的像素指数进行匈牙利算法帧内雷达-图像关联。<br> 对于每个相关的YOLO聚类对(i)，边框内的图像区域被裁剪和重塑为64 × 64 × 3 png文件(CropImg)，和(ii)雷达聚类中所有点的X、Y、Z、多普勒和信噪比，以Numpy阵列保存到磁盘，以YOLO类为标签。通过图3中的示例描述了上述中间步骤。生成数据集的步骤在算法1中给出。<br> <img src="https://images2.imgbox.com/68/38/pS9AiSBz_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4_RadarOnly_DataSet_Generation_38"></a>4. Radar-Only Data-Set Generation</h3> 
<p><img src="https://images2.imgbox.com/7f/b1/aEjwlF1z_o.png" alt="在这里插入图片描述"></p> 
<p>第二数据集由雷达数据的标记连续帧组成，以帮助旨在仅使用雷达数据识别目标的类别的应用程序。<br> 如图4所示。就像之前的情况一样，雷达点首先被组合起来以产生帧级3-D雷达PCL数据。在每一帧进行DBSCAN，从不同的目标分离PCL簇。在每一帧(i &gt; 1)，使用匈牙利算法进行帧间聚类-聚类关联，几乎以一种伪跟踪的方式，比较和分配来自前一帧的3-D聚类质心。每一个图3。首先使用DBSCAN将每帧中的雷达PCL分离成簇。然后利用匈牙利赋值法将投影的聚类质心与并行获得的YOLO包围盒质心关联起来。图4所示，较差的照明导致相机辅助YOLO在连续10帧中有8帧没有检测到行人，而雷达可以连续检测到目标。当一个质心第一次被检测到时，一个唯一的航迹ID被分配给它，并且在接下来的帧中任何后续的相关集群都被附加到相同的航迹ID。与此同时，另一个匈牙利算法模块检查一个投影的簇质心是否可以与给定帧内任何可用的YOLO预测相关联。如果YOLO分配是可能的，轨迹ID将在该框架中使用相关的YOLO类信息进行标记。请注意，一个航迹ID可以有雷达探测，例如N个连续帧，其中只有M &lt; N帧可以有相关的基于YOLO的标签。这里的想法是，如果一个物体被雷达连续跟踪了几帧，并且在跟踪期间的任何时候都被图像辅助YOLO识别，那么该跟踪中的整个雷达数据都可以被标记为特定的YOLO类。然而，为了使标记更加可靠，我们在所有帧中使用与给定曲目ID相关联的所有YOLO预测的统计模式()，并将最频繁预测的类分配给一个曲目作为最终标签。然后提取3个连续的帧，从1- 3,2 -4,3-5滑动，生成数据集。到(N-2)-N帧，从每个磁道ID标记为模式(YOLO类)，并保存到磁盘Numpy阵列。请注意，&lt;3帧的对象/轨迹id和/或在整个过程中只有一个YOLO预测的轨迹id都被排除在这个数据集之外。生成数据集的步骤在算法2中给出。<br> <img src="https://images2.imgbox.com/38/0f/GOVGjlBk_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="5_Sensor_Fusion_Feature_Representation_45"></a>5. Sensor Fusion Feature Representation</h3> 
<p><img src="https://images2.imgbox.com/f5/1f/eaGhEjwT_o.png" alt="在这里插入图片描述"></p> 
<p>本研究使用的数据集是使用第三- c节概述的方法生成的。数据集包括10838,2860,13和31雷达图像对V车辆，行人，自行车和摩托车，分别。由于Bicyclists和Motorbikes的类别不平衡，且数据不足，所以我们只使用了V vehicle和Pedestrian数据进行分类任务。此外，为了确保两个类之间的类大小平衡，我们从V车辆类中随机抽样2680个数据点。首先,雷达空间PCL数据,以及多普勒和信噪比的分在一个集群中被用来生成一个12维雷达特征向量,的谎言)在每一个三维的空间差异,b)空间协方差之间的三维双,c) V变异在径向距离,d)多普勒的均值和方差,和e)的意思是,方差和总和的雷达截面(RCS)计算使用信噪比作为RCS = 1 0 l o g R4 +信噪比,在dBsm表示。每个数据点的相关图像大小固定为64 × 64 × 3。神经网络接受两种输入:一张64 × 64 × 3的图像;以及1×12雷达特征向量。经过几个阶段的初始特征转换后，使用i) CNN+Pooling对图像;和ii)雷达特征向量上的全连接密集层，将输出进行级联，形成传感器融合特征表示(SFFR)。然后，该SFFR进行进一步的特征转换，直至分类层，如图5所示。</p> 
<h3><a id="6__49"></a>6. 实验结果</h3> 
<p>数据集被打乱并拆分为80%-5%-15%的训练、验证和测试数据。利用Adam优化器对模型进行训练，目标是最小化稀疏分类交叉熵。基于SFFR的方法对车辆类别的测试准确率为95%，对行人类别的测试准确率为99%。我们还探索了SFFR相对于单个传感器数据的优势。我们使用相同的训练和测试数据来评估三个模型，即(i)如上所述基于SFFR的分类网络，(ii)直接使用Global Max-Pool输出进行进一步特征变换(即不连接雷达特征向量变换)的仅图像分类网络，(iii)仅雷达分类网络，直接使用初步变换输出进行进一步的特征变换(即不拼接图像数据变换)。<br> <img src="https://images2.imgbox.com/10/a5/UPhgQUr2_o.png" alt="在这里插入图片描述"><br> 我们使用基于LSTM的分类网络(图7(a))，在连续3帧上连续提取1 × 12的特征向量，并利用时间特性进行特征变换。同样，由于类别不平衡，我们在本研究中使用了相同数量的V车辆和行人类别的数据点。仅利用雷达数据(3帧连续)，LSTM模型对车辆和行人进行分类的测试精度为≈92%。此外，我们还探索了基于1-D CNN的架构(图7(b))，这是时间数据网络中另一种流行的选择，与基于LSTM的方法相比，精度进一步提高了约1.5%。这里需要注意的另一个现象是，使用多个雷达帧似乎比上一节中只使用一帧雷达分类器提供了更高的分类精度。<br> <img src="https://images2.imgbox.com/0c/4f/5Q5BLXnf_o.png" alt="在这里插入图片描述"><br> ROC曲线及AUC评分如图8所示。<br> <img src="https://images2.imgbox.com/b7/f5/VegrNQyD_o.png" alt="在这里插入图片描述"><br> 们使用了两个样本神经网络来证明数据集的有效性，以及使用传感器融合方法比单一传感器模式的优势(表2)。数据收集期间的一个主要挑战是流量模式的不同性质，这反映在所收集数据的类别不平衡。<br> <img src="https://images2.imgbox.com/13/c4/OufMPlto_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1922bb1a11241b98584b52537c999a46/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python实现将多张图片拼接为一张</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0b8d351930ab2c66735db1391d0f6b1a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python实现文字转图片</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>