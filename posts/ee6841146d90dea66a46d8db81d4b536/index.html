<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>前馈神经网络卷积神经网络的简单知识点 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="前馈神经网络卷积神经网络的简单知识点" />
<meta property="og:description" content=" 目录
1、神经网络
2、前反馈神经网络
3、卷积神经网络
1、神经网络 说明 神经网络（Neural Network）是一种由神经元（neuron）构成的计算模型，它通过构建多个层次结构，并在不同层之间传递数据来实现输入数据到输出结果的转换。神经网络的构建过程就是确定神经元之间的连接方式和权重，并通过不断调整权重来优化网络的性能。 主流的连接方式有全连接、卷积连接（提取局部特征、权重共享）、残差连接（添加跨层的直接连接）、循环连接（添加时间上的循环连接）激活函数的特点 连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以利用数值优化的方法来学习网络参数激活函数及其导函数要尽可能的简单，有利于提高网络计算效率激活函数的的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。（主要是避免梯度消失和梯度爆炸）常见激活函数 Sigmoid型函数 一类S型函数，为两端饱和函数，常用的Sigmoid函数有Logistic函数和Tanh函数（左饱和：x趋于负无穷，其f(x)的导数趋于0） Logistic函数 图像因为logistic函数的性质，使得装备了Logistic激活函数的神经元具有以下两条性质 其输出可以直接看做概率分布其可以看做一个软性门（Soft Gate），用来控制其他神经元的输出信息（类似于一个阈值函数，大于会怎样，小于又会怎样）Tanh函数 图像（公式）Hard-Logistic函数和Hard-Tanh函数 因为Logistic函数和Tanh函数的计算开销较大，然后其图像中间部分又近似线性。所以把其原函数改为分段函数表示，中间写成一段线性函数。图像（g公式）说明 Sigmoid型激活函数会导致一个非稀疏的神经网络Relu函数（Rectified Lineat Unit ,修正线性单元） 图像（公式）优点 神经元只需要进行加、乘和比较的运算，计算比较高效Relu具有很好的稀疏性，大约50%的神经元会处于激活状态相比Sigmoid函数，relu函数为左饱和函数，且在X&gt;0时导数为1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。缺点 输出不是以0为中心的：ReLU函数的输出范围为[0, &#43;∞)，因此其输出值不是以0为中心的，这可能会导致网络优化时出现偏差问题。Dead ReLU问题：当输入值小于等于0时，ReLU函数的导数为0，导致对应的神经元无法再被激活。如果出现大量神经元无法激活的情况，就会出现“Dead ReLU”问题，导致模型性能下降。参数不可共享：由于ReLU函数是基于每个神经元的输入值进行计算的，因此其参数无法共享，这使得模型的参数量相对较大。Relu的变种 带泄露的ReLU(Leaky ReLU) 说明 在输入小于0时候，保持一个很小的梯度，这样当神经元非激活时也能有一个非0的 梯度进行更新参数，避免永远不能被激活图像（公式）带参数的ReLU(Parametric ReLU,PReLU) 说明 引入一个可学习的参数，不同神经元可以有不同的参数图像公式（）Leaky ReLu和PReLU的联系 Leaky ReLU在输入小于0时不是返回0，而是返回一个比较小的值，这个小值是一个固定的超参数，比如0.01。Leaky ReLU可以缓解Dead ReLU问题，但这个超参数需要手动设置，不能根据数据自动学习。PReLU是Leaky ReLU的进一步发展，它将这个超参数变成可学习的参数，从而可以通过反向传播算法自动学习得到最优的值。PReLU在深层神经网络中通常比Leaky ReLU表现更好，因为它可以自适应地调整参数，更好地适应不同的数据集和任务。总的来说，Leaky ReLU和PReLU都是对ReLU的优化和改进，它们都可以缓解Dead ReLU问题，并且具有比ReLU更好的鲁棒性和表现。ELU函数Softplus函数Swish函数GELU函数Maxout函数 说明 Maxout单元也是一种分段线性函数，Maxout单元的输入是上一层神经元的全部原始输出，是一个向量。优点 Maxout单元的优点是，它比一般的激活函数具有更大的表达能力。它可以拟合几乎所有函数，包括线性和非线性函数，而且具有更好的鲁棒性和泛化能力。此外，Maxout单元的训练过程比较简单，可以使用常规的反向传播算法进行优化。缺点 它需要更多的参数来进行训练。每个Maxout单元都有$k$个线性函数，因此需要学习的参数比其他激活函数要多得多。此外，Maxout单元的计算成本也比较高，因为需要计算$k$个线性函数的输出并选择最大值。说明2 Maxout单元的确是一种比较简单粗暴的激活函数，但由于其需要更多的参数和计算成本，因此在实践中并不是最优的选择。在深度学习的实际应用中，我们通常会根据具体任务和数据集的特点选择合适的激活函数，综合考虑模型的性能和计算效率。而在大多数情况下，ReLU仍然是最常用的激活函数之一，因为它简单、高效，而且在大多数任务中表现良好。常见激活函数及其导数 图像常用的神经网络结构 前馈网络 说明 前馈网络可以看作一个函数，实现简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射，这种网络简单，易于实现记忆网络 常见的记忆网络 循环神经网络、Hopfield网络、波尔茨曼机、受限波尔茨曼机说明 记忆网络可以看作一个程序，具有更强的计算和记忆能力图网络 说明 图神经网络具有任意连接的能力，其节点之间的连接关系不仅仅是简单的前向连接或反向连接，还包括节点之间的任意连接，因此其网络结构也比较复杂。优化问题 说明 神经网络的参数学习比线性模型要更加困难，主要原因有两点1）非凸优化问题2）梯度消失问题 2、前反馈神经网络 说明 前馈神经网络（Feedforward Neural Network，也称为多层感知机，Multilayer Perceptron，MLP）是一种最常见的神经网络模型之一。它包含输入层、若干个隐藏层和输出层，其中每个隐藏层由若干个神经元组成，每个神经元都与上一层的所有神经元相连，并根据它们的权重对上一层的输出进行加权求和，然后将结果通过激活函数进行非线性映射，最终传递到下一层。输出层的神经元数量取决于任务的类型，例如二元分类任务需要一个输出神经元，而多类别分类任务则需要多个输出神经元。图像（前馈神经网络的记号）通用近似定理 在神经网络领域中，通用近似定理（Universal Approximation Theorem）是指一个前馈神经网络具有足够的参数，可以以任意精度来逼近一个连续函数。简单来说，就是给定一个连续函数，只要神经网络的结构足够复杂，就可以用神经网络来逼近该函数的值。参数学习（定义参数变化的方向）反向传播 误差反向传播算法的前馈神经网络可以分为以下三步 前馈计算每一层的净输入z(l)和激活值a(l)，直到最后一步反向传播计算每一层的误差项计算每一层参数的偏导数，并更新参数全连接前馈神经网络的缺点 参数太多：这会导致整个神经网络的训练效率非常低，也很容易出现过拟合局部不变性特征：自然图像的物体都具有局部不变性特征，比如尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈神经网络很难提取这些局部不变性特征，一般需要进行数据增强来提高性能 3、卷积神经网络 卷积 一维卷积 说明 一维卷积常用在信号处理中，用于计算信号的延迟累积二维卷积在图像处理中，一幅图像在经过卷积操作后得到结果称为特征映射卷积的变种 说明 在卷积的标准定义下，还可以引入卷积核的滑动步长和零填充来增加卷积的多样性，可以灵活地进行特征提取步长：是指卷积核在滑动时的时间间隔零填充：是在输入向量两端进行补零窄卷积、宽卷积、等宽卷积 图像互相关 说明 在机器学习和图像处理领域，卷积的主要功能是在一个图像（或某种特征）上滑动一个卷积核（及滤波器），通过卷积操作得到一组新的特征，在计算卷积的过程中需要进行卷积核翻转。对比 互相关和卷积的区别仅仅在于卷积核是否进行翻转，因此互相关也可以称为不翻转的卷积说明：在机器学习中所说的卷积计算其实就是互相关操作。因为在神经网络中使用卷积是为了进行特征抽取，卷积核是否进行翻转何其特征的抽取能力无关。说明 卷积神经网络（CNN）是一种具有局部连接、权重共享等特性的深层前馈神经网络卷积神经网络一般是由卷积层、汇聚层（池化层）和全连接层交叉堆叠而成的前馈神经网络卷积神经网络的三个结构上的特点 局部连接、权重共享、汇聚卷积神经网络主要使用在图像和视频分析的各种任务上用卷积来代替全连接 卷积层两个重要性质 局部连接 卷积层和前一层直接的连接数大大减少权重共享 权重共享可以理解为一个卷积核只捕捉输入数据中的一种特定的局部特征，因此，如果要提取多种特征就需要使用多个不同的卷积核。卷积层 说明 卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。特征映射 说明 特征映射为一幅图像（或其他特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征。特征映射 在输入层，特征映射就是其本身如果是灰度图像，就是有一个特征映射，输入层的深度D=1如果是彩色图像，分别有RGB三个颜色通道的特征映射，输入层的深度D=3汇聚层（池化层） 说明 汇聚层也叫子采样层，其作用是进行特征选择，降低特征的数量，从而减少参数数量卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少，如果后面接一个分类器，分类器的输入维度依然很高，很容易出现过拟合。常见的汇聚函数 最大汇聚平均汇聚图像说明 目前主流的卷积网络中，汇聚层仅包含下采样操作，但在早期的一些卷积网络（比如LeNet-5）中，又是也会在汇聚层使用非线性激活函数。卷积网络的整体结构 图像趋势 整体结构趋向于使用更小的卷积核以及更深的结构汇聚层的比例正在逐渐减低，趋向于全卷积网络参数 计算 卷积层参数计算 输出特征图通道数=卷积核个数每个卷积核的权重参数个数=卷积核宽度*卷积核高度*输入特征图的通道数卷积层总参数量=每个卷积核的权重参数个数*卷积核个数&#43;卷积核个数（p偏置）池化层参数计算 池化层的参数通常没有权重参数，只有一个池化核的大小和步长参数。输出的特征图通道数=输入特征图通道数每个池化核的输入大小=池化核宽度*池化核高度*输入特征图通道数池化层的总参数量为0全连接层参数计算 输出特征数=全连接层神经元个数 （特征数是指输入数据的通道数或特征图的深度）每个神经元的权重参数个数=输入特征数。全连接层总参数量=输入特征数*全连接层神经元个数&#43;全连接层神经元个数（偏置项） （全连接层的神经元个数通常是指上一层输出的特征数） " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/ee6841146d90dea66a46d8db81d4b536/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-13T22:09:03+08:00" />
<meta property="article:modified_time" content="2023-06-13T22:09:03+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">前馈神经网络卷积神经网络的简单知识点</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"></p> 
<p id="1%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:0px;"><a href="#1%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">1、神经网络</a></p> 
<p id="2%E3%80%81%E5%89%8D%E5%8F%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:0px;"><a href="#2%E3%80%81%E5%89%8D%E5%8F%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">2、前反馈神经网络</a></p> 
<p id="3%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:0px;"><a href="#3%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">3、卷积神经网络</a></p> 
<hr id="hr-toc"> 
<p></p> 
<ul><li> <h2 id="1%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span style="color:#0d0016;"><strong>1、神经网络</strong></span></h2> 
  <ul><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
    <ul><li>神经网络（Neural Network）是一种由神经元（neuron）构成的计算模型，它通过构建多个层次结构，并在不同层之间传递数据来实现输入数据到输出结果的转换。神经网络的构建过程就是<strong>确定神经元之间的连接方式和权重</strong>，并通过不断调整权重来优化网络的性能。 
      <ul><li>主流的连接方式有<strong>全连接、卷积连接</strong>（提取局部特征、权重共享）、<strong>残差连接</strong>（添加跨层的直接连接）、<strong>循环连接</strong>（添加时间上的循环连接）</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">激活函数的特点</span></strong> 
    <ul><li><strong>连续并可导</strong>（允许少数点上不可导）的<strong><span style="color:#ffaf38;">非线性</span></strong>函数。可导的激活函数可以利用<strong>数值优化</strong>的方法来学习网络参数</li><li>激活函数及其导函数<strong>要尽可能的简单</strong>，有利于提高网络计算效率</li><li><strong>激活函数的的导函数的值域要在一个合适的区间内</strong>，不能太大也不能太小，否则会影响训练的效率和稳定性。（主要是避免<strong>梯度消失和梯度爆炸</strong>）</li></ul></li><li><strong><span style="color:#dc2d1e;">常见激活函数</span></strong> 
    <ul><li>Sigmoid型函数 
      <ul><li><strong>一类<span style="color:#ffaf38;">S</span><span style="color:#ffaf38;">型</span>函数，为两端饱和<span style="color:#ffaf38;">函数</span>，常用的<span style="color:#ffaf38;">Sigmoid</span><span style="color:#ffaf38;">函数有Logistic函数</span>和Tanh函数（左饱和：x趋于负无穷，其f(x)的导数趋于0）</strong> 
        <ul><li><strong><span style="color:#dc2d1e;">Logistic</span></strong><strong><span style="color:#dc2d1e;">函数</span></strong> 
          <ul><li>图像</li><li>因为logistic函数的性质，使得装备了Logistic激活函数的神经元具有以下两条性质 
            <ul><li>其<strong><span style="color:#ffaf38;">输出可以直接看做概率分</span></strong>布</li><li>其可以看做一<strong><span style="color:#ffaf38;">个软性门（Soft Gate），用来</span></strong>控制其他神经元的输出信息（类似于一个阈值函数，大于会怎样，小于又会怎样）</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">Tanh</span></strong><strong><span style="color:#dc2d1e;">函数</span></strong> 
          <ul><li>图像（公式）</li></ul></li><li><strong><span style="color:#dc2d1e;">Hard-Logistic</span></strong><strong><span style="color:#dc2d1e;">函数和Hard-Tanh函数</span></strong> 
          <ul><li>因为Logistic函数和Tanh函数的计算开销较大，然后其图像中间部分又近似线性。所以把其原函数<strong><span style="color:#ffaf38;">改为分段函数表示</span></strong>，中间写成一段线性函数。</li><li>图像（g公式）</li></ul></li><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
          <ul><li>Sigmoid型激活函数会导致<strong><span style="color:#ffaf38;">一个非稀疏</span></strong>的神经网络</li></ul></li></ul></li><li></ul></li><li><strong><span style="color:#dc2d1e;">Relu</span></strong><strong><span style="color:#dc2d1e;">函数（Rectified Lineat Unit ,修正线性单元）</span></strong> 
      <ul><li>图像（公式）</li><li><strong><span style="color:#dc2d1e;">优点</span></strong> 
        <ul><li>神经元只需要进行<strong><span style="color:#ffaf38;">加、乘和比较</span></strong>的运算，计算比较高效</li><li>Relu具有很<strong><span style="color:#ffaf38;">好的稀疏性，大约50%</span></strong>的神经元会处于激活状态</li><li>相比Sigmoid函数，relu函数为左饱和函数，且在X&gt;0时导数为1，在一定程度上<strong><span style="color:#ffaf38;">缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。</span></strong></li></ul></li><li><strong><span style="color:#dc2d1e;">缺点</span></strong> 
        <ul><li><strong><span style="color:#ffaf38;">输出不是以0为中心的：ReLU函数的输出范围为[0, +∞)，</span></strong>因此其输出值不是以0为中心的，这可能会导致网络优化时出现偏差问题。</li><li><strong><span style="color:#ffaf38;">Dead ReLU</span></strong><strong><span style="color:#ffaf38;">问题：</span></strong>当输入值小于等于0时，ReLU函数的导数为0，导致对应的神经元无法再被激活。如果出现大量神经元无法激活的情况，就会出现“Dead ReLU”问题，导致模型性能下降。</li><li><strong><span style="color:#ffaf38;">参数不可共享</span></strong>：由于ReLU函数是基于每个神经元的输入值进行计算的，因此其参数无法共享，这使得模型的参数量相对较大。</li></ul></li><li>R<strong><span style="color:#dc2d1e;">elu</span></strong><strong><span style="color:#dc2d1e;">的变种</span></strong> 
        <ul><li><strong><span style="color:#dc2d1e;">带泄露的ReLU(Leaky ReLU)</span></strong> 
          <ul><li>说明 
            <ul><li>在输入小于0时候，保持一个很小的梯度，这样当神经元非激活时也能有一个非0的 梯度进行更新参数，避免永远不能被激活</li></ul></li><li>图像（公式）</li></ul></li><li><strong><span style="color:#dc2d1e;">带参数的ReLU(Parametric ReLU,PReLU)</span></strong> 
          <ul><li>说明 
            <ul><li>引入一个<strong><span style="color:#ffaf38;">可学习的参数</span></strong>，不同神经元可以有不同的参数</li><li>图像公式（）</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">Leaky ReLu</span></strong><strong><span style="color:#dc2d1e;">和PReLU的联系</span></strong> 
          <ul><li>Leaky ReLU在输入小于0时不是返回0，而是返回一个比较小的值，这个小值是一个固定的超参数，比如0.01。Leaky ReLU可以缓解Dead ReLU问题，但这个超参数需要手动设置，不能根据数据自动学习。PReLU是Leaky ReLU的进一步发展，它将这个超参数变成可学习的参数，从而可以通过反向传播算法自动学习得到最优的值。PReLU在深层神经网络中通常比Leaky ReLU表现更好，因为它可以自适应地调整参数，更好地适应不同的数据集和任务。总的来说，Leaky ReLU和PReLU都是对ReLU的优化和改进，它们都可以缓解Dead ReLU问题，并且具有比ReLU更好的鲁棒性和表现。</li></ul></li><li>ELU函数</li><li>Softplus函数</li><li>Swish函数</li><li>GELU函数</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">Maxout</span></strong><strong><span style="color:#dc2d1e;">函数</span></strong> 
      <ul><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
        <ul><li>Maxout单元也是一种分<strong><span style="color:#ffaf38;">段线性函数</span></strong>，Maxout单元的输入是<strong><span style="color:#ffaf38;">上一层神经元的全部原始输出</span></strong>，是一个向量。</li></ul></li><li><strong><span style="color:#dc2d1e;">优点</span></strong> 
        <ul><li>Maxout单元的优点是，它比一般的激活函数具有更大的表达能力。<strong><span style="color:#ffaf38;">它可以拟合几乎所有函数</span></strong>，包括线性和非线性函数，而且具有更好的鲁棒性和泛化能力。此外，Maxout单元的训练过程比较简单，可以使用常规的反向传播算法进行优化。</li></ul></li><li><strong><span style="color:#dc2d1e;">缺点</span></strong> 
        <ul><li>它需要更多的参数来进行训练。每个Maxout单元都有$k$个线性函数，因此需要学习的<strong><span style="color:#ffaf38;">参数比其他激活函数要多得多</span></strong>。此外，Maxout单元的<strong><span style="color:#ffaf38;">计算成本也比较高</span></strong>，因为需要计算$k$个线性函数的输出并选择最大值。</li></ul></li><li>说明2 
        <ul><li>Maxout单元的确是一种比较简单粗暴的激活函数，但由于其需要更多的参数和计算成本，因此在实践中并不是最优的选择。在深度学习的实际应用中，我们通常会根据具体任务和数据集的特点选择合适的激活函数，综合考虑模型的性能和计算效率。而在大多数情况下，ReLU仍然是最常用的激活函数之一，因为它简单、高效，而且在大多数任务中表现良好。</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">常见激活函数及其导数</span></strong> 
      <ul><li>图像</li></ul></li></ul></li><li><strong>常用的神经网络结构</strong> 
    <ul><li><strong><span style="color:#dc2d1e;">前馈网络</span></strong> 
      <ul><li>说明 
        <ul><li>前馈网络可以看作一个<strong>函数</strong>，实现简单非线性函数的多次复合，<strong>实现输入空间到输出空间的复杂映射</strong>，这种网络简单，易于实现</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">记忆网络</span></strong> 
      <ul><li>常见的记忆网络 
        <ul><li>循环神经网络、Hopfield网络、波尔茨曼机、受限波尔茨曼机</li></ul></li><li>说明 
        <ul><li>记忆网络可以看作一个程序，具有更强的计算和记忆能力</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">图网络</span></strong> 
      <ul><li>说明 
        <ul><li>图神经网络<strong><span style="color:#dc2d1e;">具有任意连接</span></strong>的能力，其节点之间的连接关系不仅仅是简单的前向连接或反向连接，还包括节点之间的任意连接，因此其网络结构也比较复杂。</li></ul></li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">优化问题</span></strong> 
    <ul><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
      <ul><li>神经网络的参数学习比线性模型要更加困难，<strong><span style="color:#ffaf38;">主要原因有两点1）非凸优化问题2）梯度消失问题</span></strong></li><li></ul></li></ul></li></ul></li><li> <h2 id="2%E3%80%81%E5%89%8D%E5%8F%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><strong><span style="color:#dc2d1e;">2、前反馈神经网络</span></strong></h2> 
  <ul><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
    <ul><li>前馈神经网络（Feedforward Neural Network，也称为多层感知机，Multilayer Perceptron，MLP）是一种最常见的神经网络模型之一。它包含<strong><span style="color:#ffaf38;">输入层、若干个隐藏层和输出层</span></strong>，其中每个隐藏层由若干个神经元组成，每个神经元都与上一层的<strong><span style="color:#ffaf38;">所有</span></strong>神经元相连，并根据它们的权重对上一层的输出进行加权求和，<strong><span style="color:#ffaf38;">然后将结果通过激活函数进行非线性映射</span></strong>，最终传递到下一层。输出层的神经元数量取决于任务的类型，例如二元分类任务需要一个输出神经元，而多类别分类任务则需要多个输出神经元。</li></ul></li><li>图像（前馈神经网络的记号）</li><li><strong><span style="color:#dc2d1e;">通用近似定理</span></strong> 
    <ul><li>在神经网络领域中，通用近似定理（Universal Approximation Theorem）是指一个<strong>前馈神经网络具有足够的参数</strong>，可以以任意精度来逼近一个连续函数。简单来说，就是给定一个连续函数，只要神经网络的结构足够复杂，就可以用神经网络来逼近该函数的值。</li></ul></li><li>参数学习（定义参数变化的方向）</li><li><strong><span style="color:#dc2d1e;">反向传播</span></strong> 
    <ul><li>误差反向传播算法的前馈神经网络可以分为以下三步 
      <ul><li>前馈计算每一层的净输入z(l)和激活值a(l)，直到最后一步</li><li>反向传播计算每一层的误差项</li><li>计算每一层参数的偏导数，并更新参数</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">全连接前馈神经网络的缺点</span></strong> 
    <ul><li><strong><span style="color:#dc2d1e;">参数太多：</span></strong>这会导致整个神经网络的训练效率非常低，也很容易出现过拟合</li><li><strong><span style="color:#dc2d1e;">局部不变性特征：</span></strong>自然图像的物体都具有局部不变性特征，比如尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈神经网络<strong><span style="color:#ffaf38;">很难</span></strong>提取这些局部不变性特征，一般需要进行<strong><span style="color:#ffaf38;">数据增强</span></strong>来提高性能</li></ul></li></ul></li><li> <h2 id="3%E3%80%81%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><strong><span style="color:#dc2d1e;">3、卷积神经网络</span></strong></h2> 
  <ul><li><strong><span style="color:#dc2d1e;">卷积</span></strong> 
    <ul><li>一维卷积 
      <ul><li>说明 
        <ul><li>一维卷积常用在信号处理中，用于计算信号的延迟累积</li></ul></li></ul></li><li>二维卷积</li><li>在图像处理中，一幅图像在经过卷积操作后得到结果称为特征映射</li></ul></li><li><strong><span style="color:#dc2d1e;">卷积的变种</span></strong> 
    <ul><li>说明 
      <ul><li>在卷积的标准定义下，还可以引入卷积核的滑动步长和零填充来增加卷积的多样性，可以灵活地进行特征提取</li></ul></li><li><strong><span style="color:#dc2d1e;">步长</span></strong>：是指卷积核在滑动时的时间间隔</li><li><strong><span style="color:#dc2d1e;">零填充</span></strong>：是在输入向量两端进行补零</li><li>窄卷积、宽卷积、等宽卷积 
      <ul><li>图像</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">互相关</span></strong> 
    <ul><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
      <ul><li>在机器学习和图像处理领域，<strong><span style="color:#ffaf38;">卷积的主要功能是在一个图像（或某种特征）上滑动一个卷积核（及滤波器</span></strong>），通过卷积操作得到一组新的特征，在计算<strong><span style="color:#ffaf38;">卷积的过程中需要进行卷积核翻转</span></strong>。</li></ul></li><li><strong><span style="color:#dc2d1e;">对比</span></strong> 
      <ul><li>互相关和卷积的区别<strong><span style="color:#ffaf38;">仅仅在于卷积核是否进行翻转</span></strong>，因此互相关也可以称为<strong><span style="color:#ffaf38;">不翻转的卷积</span></strong></li><li>说明：<strong><span style="color:#ffaf38;">在机器学习中所说的卷积计算其实就是互相关操作</span></strong>。因为在神经网络中使用卷积是为了进行特征抽取，卷积核是否进行翻转何其特征的抽取能力无关。</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
    <ul><li>卷积神经网络（CNN）是一种具有<strong><span style="color:#ffaf38;">局部连接、权重共享</span></strong>等特性的深层前馈神经网络</li><li>卷积神经网络一般是由<strong><span style="color:#ffaf38;">卷积层、汇聚层（池化层）和全连接层</span></strong>交叉堆叠而成的前馈神经网络</li><li>卷积神经网络的三个结构上的特点 
      <ul><li><strong><span style="color:#ffaf38;">局部连接、权重共享、汇聚</span></strong></li></ul></li><li>卷积神经网络主要使用<strong><span style="color:#ffaf38;">在图像和视频分析</span></strong>的各种任务上</li></ul></li><li><strong><span style="color:#dc2d1e;">用卷积来代替全连接</span></strong> 
    <ul><li><strong><span style="color:#dc2d1e;">卷积层两个重要性质</span></strong> 
      <ul><li><strong><span style="color:#dc2d1e;">局部连接</span></strong> 
        <ul><li>卷积层和前一层直接<strong><span style="color:#ffaf38;">的连接数大大减少</span></strong></li></ul></li><li><strong><span style="color:#dc2d1e;">权重共享</span></strong> 
        <ul><li>权重共享可以理解为一个卷积核<strong><span style="color:#ffaf38;">只捕捉输入数据中的一种特定的局部特征</span></strong>，因此，如果要提取多种特征就需要使用多个不同的卷积核。</li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">卷积层</span></strong> 
      <ul><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
        <ul><li>卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。</li></ul></li><li><strong><span style="color:#dc2d1e;">特征映射</span></strong> 
        <ul><li><strong><span style="color:#dc2d1e;">说明</span></strong> 
          <ul><li>特征映射为一幅图像（或其他特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征。</li></ul></li><li><strong><span style="color:#dc2d1e;">特征映射</span></strong> 
          <ul><li>在输入层，特征映射就是其本身</li><li>如果是<strong><span style="color:#ffaf38;">灰度图像</span></strong>，就是有一个特征映射，<strong><span style="color:#ffaf38;">输入层的深度D=1</span></strong></li><li>如果是<strong><span style="color:#ffaf38;">彩色图像</span></strong>，分别有RGB三个颜色通道的特征映射，输入层的<strong><span style="color:#ffaf38;">深度D=3</span></strong></li></ul></li></ul></li></ul></li><li><strong>汇聚层（池化层）</strong> 
      <ul><li>说明 
        <ul><li>汇聚层也叫子采样层，其作用是进行特征选择，降低特征的数量，从而减少参数数量</li><li>卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少，如果后面接一个分类器，分类器的输入维度依然很高，很容易出现过拟合。</li></ul></li><li>常见的汇聚函数 
        <ul><li>最大汇聚</li><li>平均汇聚</li><li><strong><span style="color:#3da8f5;">图像</span></strong></li><li>说明 
          <ul><li>目前主流的卷积网络中，汇聚层仅包含下采样操作，但在早期的一些卷积网络（比如LeNet-5）中，又是也会在汇聚层使用非线性激活函数。</li></ul></li></ul></li></ul></li><li>卷积网络的整体结构 
      <ul><li><strong><span style="color:#3da8f5;">图像</span></strong></li><li>趋势 
        <ul><li>整体结构趋向于使用更小的卷积核以及更深的结构</li><li>汇聚层的比例正在逐渐减低，趋向于全卷积网络</li></ul></li></ul></li></ul></li><li><strong><span style="color:#dc2d1e;">参数 计算</span></strong> 
    <ul><li><strong><span style="color:#dc2d1e;">卷积层参数计算</span></strong> 
      <ul><li>输出特征图通道数=卷积核个数</li><li>每个卷积核的权重参数个数=卷积核宽度*卷积核高度*输入特征图的通道数</li><li>卷积层总参数量=每个卷积核的权重参数个数*卷积核个数+卷积核个数（p偏置）</li></ul></li><li><span style="color:#0d0016;"><strong>池化层参数计算</strong></span> 
      <ul><li><span style="color:#0d0016;"><strong>池化层的参数通常没有权重参数，只有一个池化核的大小和步长参数。</strong></span></li><li><span style="color:#0d0016;"><strong>输出的特征图通道数=输入特征图通道数</strong></span></li><li><span style="color:#0d0016;"><strong>每个池化核的输入大小=池化核宽度*池化核高度*输入特征图通道数</strong></span></li><li><span style="color:#0d0016;"><strong>池化层的总参数量为0</strong></span></li></ul></li><li><span style="color:#0d0016;"><strong>全连接层参数计算</strong></span> 
      <ul><li><span style="color:#0d0016;"><strong>输出特征数=全连接层神经元个数</strong></span> 
        <ul><li><span style="color:#0d0016;"><strong>（特征数是指输入数据的通道数或特征图的深度）</strong></span></li></ul></li><li><span style="color:#0d0016;"><strong>每个神经元的权重参数个数=输入特征数。</strong></span></li><li><span style="color:#0d0016;"><strong>全连接层总参数量=输入特征数*全连接层神经元个数+全连接层神经元个数（偏置项）</strong></span> 
        <ul><li><span style="color:#0d0016;"><strong>（全连接层的神经元个数通常是指上一层输出的特征数）</strong></span></li></ul></li><li></ul></li></ul></li></ul></li></ul> 
<p style="margin-left:0;text-align:justify;"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a99d33531b49f61fcd9807a599fbbaac/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">记一次openssh连接失败及解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5b2d896fc597a67be2ac1fd7f4236ec1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">华为OD机试 - 通过软盘拷贝文件（Java）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>