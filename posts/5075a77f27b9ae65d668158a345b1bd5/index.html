<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习】OCR文本识别 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【深度学习】OCR文本识别" />
<meta property="og:description" content="OCR文字识别定义 OCR（optical character recognition）文字识别是指电子设备（例如扫描仪或数码相机）检查纸上打印的字符，然后用字符识别方法将形状翻译成计算机文字的过程；即，对文本资料进行扫描，然后对图像文件进行分析处理，获取文字及版面信息的过程。如何除错或利用辅助信息提高识别正确率，是OCR最重要的课题。衡量一个OCR系统性能好坏的主要指标有：拒识率、误识率、识别速度、用户界面的友好性，产品的稳定性，易用性及可行性等。
由于扫描仪的普及与广泛应用，OCR软件只需提供与扫描仪的接口，利用扫描仪驱动软件即可。因此，OCR软件主要是由下面几个部分组成。
图像输入、预处理：
图像输入：对于不同的图像格式，有着不同的存储格式，不同的压缩方式。预处理：主要包括二值化，噪声去除，倾斜较正等二值化：
对摄像头拍摄的图片，大多数是彩色图像，彩色图像所含信息量巨大，对于图片的内容，我们可以简单的分为前景与背景，为了让计算机更快的，更好的识别文字，我们需要先对彩色图进行处理，使图片只前景信息与背景信息，可以简单的定义前景信息为黑色，背景信息为白色，这就是二值化图了。噪声去除：
对于不同的文档，我们对噪声的定义可以不同，根据噪声的特征进行去噪，就叫做噪声去除倾斜较正：
由于一般用户，在拍照文档时，都比较随意，因此拍照出来的图片不可避免的产生倾斜，这就需要文字识别软件进行较正。版面分析：
将文档图片分段落，分行的过程就叫做版面分析，由于实际文档的多样性，复杂性，因此，还没有一个固定的，最优的切割模型。字符切割：
由于拍照条件的限制，经常造成字符粘连，断笔，因此极大限制了识别系统的性能，这就需要文字识别软件有字符切割功能。字符识别：
这一研究，已经是很早的事情了，比较早有模板匹配，后来以特征提取为主，由于文字的位移，笔画的粗细，断笔，粘连，旋转等因素的影响，极大影响特征的提取的难度。版面恢复：
人们希望识别后的文字，仍然像原文档图片那样排列着，段落不变，位置不变，顺序不变，的输出到word文档,pdf文档等，这一过程就叫做版面恢复。后处理、校对：
根据特定的语言上下文的关系，对识别结果进行较正，就是后处理。 开发一个OCR文字识别软件系统，其目的很简单，只是要把影像作一个转换，使影像内的图形继续保存、有表格则表格内资料及影像内的文字，一律变成计算机文字，使能达到影像资料的储存量减少、识别出的文字可再使用及分析，当然也可节省因键盘输入的人力与时间。
从影像到结果输出，须经过影像输入、影像前处理、文字特征抽取、比对识别、最后经人工校正将认错的文字更正，将结果输出。
以美团的OCR识别为例
基于深度学习的OCR 文字是不可或缺的视觉信息来源。相对于图像/视频中的其他内容，文字往往包含更强的语义信息，因此对图像中的文字提取和识别具有重大意义。OCR在美团业务中主要起着两方面作用。一方面是辅助录入，比如在移动支付环节通过对银行卡卡号的拍照识别以实现自动绑卡，辅助运营录入菜单中菜品信息，在配送环节通过对商家小票的识别以实现调度核单，如图1所示。另一方面是审核校验，比如在商家资质审核环节对商家上传的身份证、营业执照和餐饮许可证等证件照片进行信息提取和核验以确保该商家的合法性，机器过滤商家上单和用户评价环节产生的包含违禁词的图片。
OCR技术发展历程 传统的OCR基于图像处理（二值化、连通域分析、投影分析等）和统计机器学习（Adaboost、SVM），过去20年间在印刷体和扫描文档上取得了不错的效果。传统的印刷体OCR解决方案整体流程如图所示。
从输入图像到给出识别结果经历了图像预处理、文字行提取和文字行识别三个阶段。其中文字行提取的相关步骤（版面分析、行切分）会涉及大量的先验规则，而文字行识别主要基于传统的机器学习方法。随着移动设备的普及，对拍摄图像中的文字提取和识别成为主流需求，同时对场景中文字的识别需求越来越突出。因此，相比于印刷体场景，拍照文字的识别将面临以下三方面挑战：
成像复杂。噪声、模糊、光线变化、形变。文字复杂。字体、字号、色彩、磨损、笔画宽度任意、方向任意。场景复杂。版面缺失、背景干扰。 对于上述挑战，传统的OCR解决方案存在着以下不足：
通过版面分析（连通域分析）和行切分（投影分析）来生成文本行，要求版面结构有较强的规则性且前背景可分性强（例如黑白文档图像、车牌），无法处理前背景复杂的随意文字（例如场景文字、菜单、广告文字等）。另外，二值化操作本身对图像成像条件和背景要求比较苛刻。通过人工设计边缘方向特征（例如方向梯度直方图）来训练字符识别模型，在字体变化、模糊或背景干扰时，此类单一的特征的泛化能力迅速下降。过度依赖于字符切分的结果，在字符扭曲、粘连、噪声干扰的情况下，切分的错误传播尤其突出。尽管图像预处理模块可有效改善输入图像的质量，但多个独立的校正模块的串联必然带来误差传递。另外由于各模块优化目标独立，它们无法融合到统一的框架中进行。 为了解决上述问题，现有技术在以下三方面进行了改进。
1. 文字行提取 传统OCR（如图3所示）采取自上而下的切分式，但它只适用于版面规则背景简单的情况。该领域还有另外两类思路。
自底向上的生成式方法。该类方法通过连通域分析或最大稳定极值区域（MSER）等方法提取候选区域，然后通过文字/非文字的分类器进行区域筛选，对筛选后的区域进行合并生成文字行，再进行文字行级别的过滤，如图3所示。该类方法的不足是，一方面流程冗长导致的超参数过多，另一方面无法利用全局信息。
基于滑动窗口的方法。该类方法利用通用目标检测的思路来提取文字行信息，利用训练得到的文字行/词语/字符级别的分类器来进行全图搜索。原始的基于滑动窗口方法通过训练文字/背景二分类检测器，直接对输入图像进行多尺度的窗口扫描。检测器可以是传统机器学习模型（Adaboost、Random Ferns），也可以是深度卷积神经网络。 为了提升效率，DeepText、TextBoxes等方法先提取候选区域再进行区域回归和分类，同时该类方法可进行端到端训练，但对多角度和极端宽高比的文字区域召回低。
2. 传统单字识别引擎→基于深度学习的单字识别引擎 由于单字识别引擎的训练是一个典型的图像分类问题，而卷积神经网络在描述图像的高层语义方面优势明显，所以主流方法是基于卷积神经网络的图像分类模型。实践中的关键点在于如何设计网络结构和合成训练数据。对于网络结构，我们可以借鉴手写识别领域相关网络结构，也可采用OCR领域取得出色效果的Maxout网络结构，如图4所示。对于数据合成，需考虑字体、形变、模糊、噪声、背景变化等因素。
表1给出了卷积神经网络的特征学习和传统特征的性能比较，可以看出通过卷积神经网络学习得到的特征鉴别能力更强。
3. 文字行识别流程 传统OCR将文字行识别划分为字符切分和单字符识别两个独立的步骤，尽管通过训练基于卷积神经网络的单字符识别引擎可以有效提升字符识别率，但切分对于字符粘连、模糊和形变的情况的容错性较差，而且切分错误对于识别是不可修复的。因此在该框架下，文本行识别的准确率主要受限于字符切分。假设已训练单字符识别引擎的准确率p=99%，字符切分准确率为q= 95%，则对于一段长度为L的文字行，其识别的平均准确率为P= (pq)的L次方，其中L=10时，P=54.1%。
由于独立优化字符切分提升空间有限，因此有相关方法试图联合优化切分和识别两个任务。现有技术主要可分为基于切分的方法（Segmentation-Based）和不依赖切分的方法（Segmentation- Free）两类方法。
基于切分的方法
该类方法还是保留主动切分的步骤，但引入了动态合并机制，通过识别置信度等信息来指导切分，如图所示。
过切分模块将文字行在垂直于基线方向上分割成碎片，使得其中每个碎片至多包含一个字符。通常来说，过切分模块会将字符分割为多个连续笔划。过切分可以采用基于规则或机器学习的方法。规则方法主要是直接在图像二值化的结果上进行连通域分析和投影分析来确定候补切点位置，通过调整参数可以控制粒度来使得字符尽可能被切碎。基于规则的方法实现简单，但在成像/背景复杂的条件下其效果不好。机器学习方法通过离线训练鉴别切点的二类分类器，然后基于该分类器在文字行图像上进行滑窗检测。
动态合并模块将相邻的笔划根据识别结果组合成可能的字符区域，最优组合方式即对应最佳切分路径和识别结果。直观来看，寻找最优组合方式可转换为路径搜索问题，对应有深度优先和广度优先两种搜索策略。深度优先策略在每一步选择扩展当前最优的状态，因此全局来看它是次优策略，不适合过长的文字行。广度优先策略在每一步会对当前多个状态同时进行扩展，比如在语音识别领域广泛应用的Viterbi解码和Beam Search。但考虑到性能，Beam Search通常会引入剪枝操作来控制路径长度，剪枝策略包含限制扩展的状态数（比如，每一步只扩展TopN的状态）和加入状态约束（比如，合并后字符形状）等。
由于动态合并会产生多个候选路径，所以需要设计合适的评价函数来进行路径选择。评价函数的设计主要从路径结构损失和路径识别打分两方面出发。路径结构损失主要从字符形状特征方面衡量切分路径的合理性，路径识别打分则对应于特定切分路径下的单字平均识别置信度和语言模型分。
该方案试图将字符切分和单字符识别融合在同一个框架下解决，但由于过分割是独立的步骤，因此没有从本质上实现端到端学习。
不依赖切分的方法
该类方法完全跨越了字符切分，通过滑动窗口或序列建模直接对文字行进行识别。
滑窗识别借鉴了滑动窗口检测的思路，基于离线训练的单字识别引擎，对文字行图像从左到右进行多尺度扫描，以特定窗口为中心进行识别。在路径决策上可采用贪心策略或非极大值抑制（NMS）策略来得到最终的识别路径。图6给出了滑窗识别的示意流程。可见滑窗识别存在两个问题：滑动步长的粒度过细则计算代价大，过粗则上下文信息易丢失；无论采用何种路径决策方案，它们对单字识别的置信度依赖较高。
序列学习起源于手写识别、语音识别领域，因为这类问题的共同特点是需要对时序数据进行建模。尽管文字行图像是二维的，但如果把从左到右的扫描动作类比为时序，文字行识别从本质上也可归为这类问题。通过端到端的学习，摒弃矫正/切分/字符识别等中间步骤，以此提升序列学习的效果，这已经成为当前研究的热点。
基于现有技术和美团业务涉及的OCR场景，我们在文字检测和文字行识别采用如图所示的深度学习框架。
基于深度学习的文字检测 对于美团的OCR场景，根据版面是否有先验信息（卡片的矩形区域、证件的关键字段标识）以及文字自身的复杂性（如水平文字、多角度），图像可划分为受控场景（如身份证、营业执照、银行卡）和非受控场景（如菜单、门头图），如图所示。
考虑到这两类场景的特点不同，我们借鉴不同的检测框架。由于受控场景文字诸多约束条件可将问题简化，因此利用在通用目标检测领域广泛应用的Faster R-CNN框架进行检测。而对于非受控场景文字，由于形变和笔画宽度不一致等原因，目标轮廓不具备良好的闭合边界，我们需要借助图像语义分割来标记文字区域与背景区域。
受控场景的文字检测 对于受控场景（如身份证），我们将文字检测转换为对关键字目标（如姓名、身份证号、地址）或关键条目（如银行卡号）的检测问题。基于Faster R-CNN的关键字检测流程如图9所示。为了保证回归框的定位精度，同时提升运算速度，我们对原有框架和训练方式进行了微调。
考虑到关键字或关键条目的类内变化有限，网络结构只采用了3个卷积层。训练过程中提高正样本的重叠率阈值。根据关键字或关键条目的宽高比范围来适配RPN层Anchor的宽高比。 Faster R-CNN框架由RPN（候选区域生成网络）和RCN（区域分类网络）两个子网络组成。RPN通过监督学习的方法提取候选区域，给出的是无标签的区域和粗定位结果。RCN引入类别概念，同时进行候选区域的分类和位置回归，给出精细定位结果。训练时两个子网络通过端到端的方式联合优化。图10以银行卡卡号识别为例，给出了RPN层和RCN层的输出。
对于人手持证件场景，由于证件目标在图像中所占比例过小，直接提取微小候选目标会导致一定的定位精度损失。为了保证高召回和高定位精度，可采用由粗到精的策略进行检测。首先定位卡片所在区域位置，然后在卡片区域范围内进行关键字检测，而区域定位也可采用Faster R-CNN框架，如图11所示。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/5075a77f27b9ae65d668158a345b1bd5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-26T11:18:45+08:00" />
<meta property="article:modified_time" content="2023-05-26T11:18:45+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习】OCR文本识别</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="OCR_0"></a>OCR文字识别定义</h3> 
<p><strong>OCR</strong>（optical character recognition）文字识别是指电子设备（例如扫描仪或数码相机）检查纸上打印的字符，然后用字符识别方法将形状翻译成计算机文字的过程；即，对文本资料进行扫描，然后对图像文件进行分析处理，获取文字及版面信息的过程。如何除错或利用辅助信息提高识别正确率，是OCR最重要的课题。衡量一个OCR系统性能好坏的主要指标有：拒识率、误识率、识别速度、用户界面的友好性，产品的稳定性，易用性及可行性等。</p> 
<p>由于扫描仪的普及与广泛应用，OCR软件只需提供与扫描仪的接口，利用扫描仪驱动软件即可。因此，OCR软件主要是由下面几个部分组成。</p> 
<ol><li>图像输入、预处理：<br> 图像输入：对于不同的图像格式，有着不同的存储格式，不同的压缩方式。预处理：主要包括二值化，噪声去除，倾斜较正等</li><li>二值化：<br> 对摄像头拍摄的图片，大多数是彩色图像，彩色图像所含信息量巨大，对于图片的内容，我们可以简单的分为前景与背景，为了让计算机更快的，更好的识别文字，我们需要先对彩色图进行处理，使图片只前景信息与背景信息，可以简单的定义前景信息为黑色，背景信息为白色，这就是二值化图了。</li><li>噪声去除：<br> 对于不同的文档，我们对噪声的定义可以不同，根据噪声的特征进行去噪，就叫做噪声去除</li><li>倾斜较正：<br> 由于一般用户，在拍照文档时，都比较随意，因此拍照出来的图片不可避免的产生倾斜，这就需要文字识别软件进行较正。</li><li>版面分析：<br> 将文档图片分段落，分行的过程就叫做版面分析，由于实际文档的多样性，复杂性，因此，还没有一个固定的，最优的切割模型。</li><li>字符切割：<br> 由于拍照条件的限制，经常造成字符粘连，断笔，因此极大限制了识别系统的性能，这就需要文字识别软件有字符切割功能。</li><li>字符识别：<br> 这一研究，已经是很早的事情了，比较早有模板匹配，后来以特征提取为主，由于文字的位移，笔画的粗细，断笔，粘连，旋转等因素的影响，极大影响特征的提取的难度。</li><li>版面恢复：<br> 人们希望识别后的文字，仍然像原文档图片那样排列着，段落不变，位置不变，顺序不变，的输出到word文档,pdf文档等，这一过程就叫做版面恢复。</li><li>后处理、校对：<br> 根据特定的语言上下文的关系，对识别结果进行较正，就是后处理。</li></ol> 
<p>开发一个OCR文字识别软件系统，其目的很简单，只是要把影像作一个转换，使影像内的图形继续保存、有表格则表格内资料及影像内的文字，一律变成计算机文字，使能达到影像资料的储存量减少、识别出的文字可再使用及分析，当然也可节省因键盘输入的人力与时间。</p> 
<p>从影像到结果输出，须经过影像输入、影像前处理、文字特征抽取、比对识别、最后经人工校正将认错的文字更正，将结果输出。</p> 
<hr> 
<blockquote> 
 <p>以<a href="https://tech.meituan.com/2018/06/29/deep-learning-ocr.html" rel="nofollow">美团的OCR识别</a>为例</p> 
</blockquote> 
<h3><a id="OCR_33"></a>基于深度学习的OCR</h3> 
<p>文字是不可或缺的视觉信息来源。相对于图像/视频中的其他内容，文字往往包含更强的语义信息，因此对图像中的文字提取和识别具有重大意义。OCR在美团业务中主要起着两方面作用。一方面是辅助录入，比如在移动支付环节通过对银行卡卡号的拍照识别以实现自动绑卡，辅助运营录入菜单中菜品信息，在配送环节通过对商家小票的识别以实现调度核单，如图1所示。另一方面是审核校验，比如在商家资质审核环节对商家上传的身份证、营业执照和餐饮许可证等证件照片进行信息提取和核验以确保该商家的合法性，机器过滤商家上单和用户评价环节产生的包含违禁词的图片。<br> <img src="https://images2.imgbox.com/28/f3/G9ZneuKb_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="OCR_38"></a>OCR技术发展历程</h3> 
<p>传统的OCR基于图像处理（二值化、连通域分析、投影分析等）和统计机器学习（Adaboost、SVM），过去20年间在印刷体和扫描文档上取得了不错的效果。传统的印刷体OCR解决方案整体流程如图所示。<br> <img src="https://images2.imgbox.com/01/fa/9HYEtt8o_o.png" alt="在这里插入图片描述"><br> 从输入图像到给出识别结果经历了图像预处理、文字行提取和文字行识别三个阶段。其中文字行提取的相关步骤（版面分析、行切分）会涉及大量的先验规则，而文字行识别主要基于传统的机器学习方法。随着移动设备的普及，对拍摄图像中的文字提取和识别成为主流需求，同时对场景中文字的识别需求越来越突出。因此，相比于印刷体场景，拍照文字的识别将面临以下三方面挑战：</p> 
<ul><li>成像复杂。噪声、模糊、光线变化、形变。</li><li>文字复杂。字体、字号、色彩、磨损、笔画宽度任意、方向任意。</li><li>场景复杂。版面缺失、背景干扰。</li></ul> 
<p>对于上述挑战，传统的OCR解决方案存在着以下不足：</p> 
<ul><li>通过版面分析（连通域分析）和行切分（投影分析）来生成文本行，要求版面结构有较强的规则性且前背景可分性强（例如黑白文档图像、车牌），无法处理前背景复杂的随意文字（例如场景文字、菜单、广告文字等）。另外，二值化操作本身对图像成像条件和背景要求比较苛刻。</li><li>通过人工设计边缘方向特征（例如方向梯度直方图）来训练字符识别模型，在字体变化、模糊或背景干扰时，此类单一的特征的泛化能力迅速下降。</li><li>过度依赖于字符切分的结果，在字符扭曲、粘连、噪声干扰的情况下，切分的错误传播尤其突出。</li><li>尽管图像预处理模块可有效改善输入图像的质量，但多个独立的校正模块的串联必然带来误差传递。另外由于各模块优化目标独立，它们无法融合到统一的框架中进行。</li></ul> 
<p>为了解决上述问题，现有技术在以下三方面进行了改进。</p> 
<h4><a id="1__59"></a>1. 文字行提取</h4> 
<p>传统OCR（如图3所示）采取自上而下的切分式，但它只适用于版面规则背景简单的情况。该领域还有另外两类思路。</p> 
<ul><li>自底向上的生成式方法。该类方法通过连通域分析或最大稳定极值区域（MSER）等方法提取候选区域，然后通过文字/非文字的分类器进行区域筛选，对筛选后的区域进行合并生成文字行，再进行文字行级别的过滤，如图3所示。该类方法的不足是，一方面流程冗长导致的超参数过多，另一方面无法利用全局信息。<br> <img src="https://images2.imgbox.com/57/e6/3fwlzWnh_o.png" alt="在这里插入图片描述"></li><li>基于滑动窗口的方法。该类方法利用通用目标检测的思路来提取文字行信息，利用训练得到的文字行/词语/字符级别的分类器来进行全图搜索。原始的基于滑动窗口方法通过训练文字/背景二分类检测器，直接对输入图像进行多尺度的窗口扫描。检测器可以是传统机器学习模型（Adaboost、Random Ferns），也可以是深度卷积神经网络。</li></ul> 
<p>为了提升效率，DeepText、TextBoxes等方法先提取候选区域再进行区域回归和分类，同时该类方法可进行端到端训练，但对多角度和极端宽高比的文字区域召回低。</p> 
<h4><a id="2__69"></a>2. 传统单字识别引擎→基于深度学习的单字识别引擎</h4> 
<p>由于单字识别引擎的训练是一个典型的图像分类问题，而卷积神经网络在描述图像的高层语义方面优势明显，所以主流方法是基于卷积神经网络的图像分类模型。实践中的关键点在于如何设计网络结构和合成训练数据。对于网络结构，我们可以借鉴手写识别领域相关网络结构，也可采用OCR领域取得出色效果的Maxout网络结构，如图4所示。对于数据合成，需考虑字体、形变、模糊、噪声、背景变化等因素。</p> 
<p><img src="https://images2.imgbox.com/fc/cf/jOp8SUtX_o.png" alt="在这里插入图片描述"><br> 表1给出了卷积神经网络的特征学习和传统特征的性能比较，可以看出通过卷积神经网络学习得到的特征鉴别能力更强。</p> 
<p><img src="https://images2.imgbox.com/e4/c4/UflFpNby_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="3__78"></a>3. 文字行识别流程</h4> 
<p>传统OCR将文字行识别划分为字符切分和单字符识别两个独立的步骤，尽管通过训练基于卷积神经网络的单字符识别引擎可以有效提升字符识别率，但切分对于字符粘连、模糊和形变的情况的容错性较差，而且切分错误对于识别是不可修复的。因此在该框架下，文本行识别的准确率主要受限于字符切分。假设已训练单字符识别引擎的准确率p=99%，字符切分准确率为q= 95%，则对于一段长度为L的文字行，其识别的平均准确率为P= (pq)的L次方，其中L=10时，P=54.1%。</p> 
<p>由于独立优化字符切分提升空间有限，因此有相关方法试图联合优化切分和识别两个任务。现有技术主要可分为基于切分的方法（Segmentation-Based）和不依赖切分的方法（Segmentation- Free）两类方法。</p> 
<p><strong>基于切分的方法</strong></p> 
<p>该类方法还是保留主动切分的步骤，但引入了动态合并机制，通过识别置信度等信息来指导切分，如图所示。</p> 
<p><img src="https://images2.imgbox.com/ac/63/MlgSCSAT_o.png" alt="在这里插入图片描述"><br> 过切分模块将文字行在垂直于基线方向上分割成碎片，使得其中每个碎片至多包含一个字符。通常来说，过切分模块会将字符分割为多个连续笔划。过切分可以采用基于规则或机器学习的方法。规则方法主要是直接在图像二值化的结果上进行连通域分析和投影分析来确定候补切点位置，通过调整参数可以控制粒度来使得字符尽可能被切碎。基于规则的方法实现简单，但在成像/背景复杂的条件下其效果不好。机器学习方法通过离线训练鉴别切点的二类分类器，然后基于该分类器在文字行图像上进行滑窗检测。</p> 
<p>动态合并模块将相邻的笔划根据识别结果组合成可能的字符区域，最优组合方式即对应最佳切分路径和识别结果。直观来看，寻找最优组合方式可转换为路径搜索问题，对应有深度优先和广度优先两种搜索策略。深度优先策略在每一步选择扩展当前最优的状态，因此全局来看它是次优策略，不适合过长的文字行。广度优先策略在每一步会对当前多个状态同时进行扩展，比如在语音识别领域广泛应用的Viterbi解码和Beam Search。但考虑到性能，Beam Search通常会引入剪枝操作来控制路径长度，剪枝策略包含限制扩展的状态数（比如，每一步只扩展TopN的状态）和加入状态约束（比如，合并后字符形状）等。</p> 
<p>由于动态合并会产生多个候选路径，所以需要设计合适的评价函数来进行路径选择。评价函数的设计主要从路径结构损失和路径识别打分两方面出发。路径结构损失主要从字符形状特征方面衡量切分路径的合理性，路径识别打分则对应于特定切分路径下的单字平均识别置信度和语言模型分。</p> 
<p>该方案试图将字符切分和单字符识别融合在同一个框架下解决，但由于过分割是独立的步骤，因此没有从本质上实现端到端学习。</p> 
<p><strong>不依赖切分的方法</strong></p> 
<p>该类方法完全跨越了字符切分，通过滑动窗口或序列建模直接对文字行进行识别。</p> 
<p>滑窗识别借鉴了滑动窗口检测的思路，基于离线训练的单字识别引擎，对文字行图像从左到右进行多尺度扫描，以特定窗口为中心进行识别。在路径决策上可采用贪心策略或非极大值抑制（NMS）策略来得到最终的识别路径。图6给出了滑窗识别的示意流程。可见滑窗识别存在两个问题：滑动步长的粒度过细则计算代价大，过粗则上下文信息易丢失；无论采用何种路径决策方案，它们对单字识别的置信度依赖较高。</p> 
<p><img src="https://images2.imgbox.com/07/24/VzocJBBd_o.png" alt="在这里插入图片描述"><br> 序列学习起源于手写识别、语音识别领域，因为这类问题的共同特点是需要对时序数据进行建模。尽管文字行图像是二维的，但如果把从左到右的扫描动作类比为时序，文字行识别从本质上也可归为这类问题。通过端到端的学习，摒弃矫正/切分/字符识别等中间步骤，以此提升序列学习的效果，这已经成为当前研究的热点。</p> 
<p>基于现有技术和美团业务涉及的OCR场景，我们在文字检测和文字行识别采用如图所示的深度学习框架。</p> 
<p><img src="https://images2.imgbox.com/41/a3/a2hoPRk2_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_110"></a>基于深度学习的文字检测</h3> 
<p>对于美团的OCR场景，根据版面是否有先验信息（卡片的矩形区域、证件的关键字段标识）以及文字自身的复杂性（如水平文字、多角度），图像可划分为受控场景（如身份证、营业执照、银行卡）和非受控场景（如菜单、门头图），如图所示。<br> <img src="https://images2.imgbox.com/f9/d6/GOFql5Aa_o.png" alt="![在这里插入图片描述](https://img-blog.csdnimg.cn/192e0068d334429db474839133a9f532.png"><br> 考虑到这两类场景的特点不同，我们借鉴不同的检测框架。由于受控场景文字诸多约束条件可将问题简化，因此利用在通用目标检测领域广泛应用的Faster R-CNN框架进行检测。而对于非受控场景文字，由于形变和笔画宽度不一致等原因，目标轮廓不具备良好的闭合边界，我们需要借助图像语义分割来标记文字区域与背景区域。</p> 
<ol><li>受控场景的文字检测</li></ol> 
<p>对于受控场景（如身份证），我们将文字检测转换为对关键字目标（如姓名、身份证号、地址）或关键条目（如银行卡号）的检测问题。基于Faster R-CNN的关键字检测流程如图9所示。为了保证回归框的定位精度，同时提升运算速度，我们对原有框架和训练方式进行了微调。</p> 
<ul><li>考虑到关键字或关键条目的类内变化有限，网络结构只采用了3个卷积层。</li><li>训练过程中提高正样本的重叠率阈值。</li><li>根据关键字或关键条目的宽高比范围来适配RPN层Anchor的宽高比。</li></ul> 
<p><img src="https://images2.imgbox.com/e9/fa/cWFtsDzp_o.png" alt="在这里插入图片描述"><br> Faster R-CNN框架由RPN（候选区域生成网络）和RCN（区域分类网络）两个子网络组成。RPN通过监督学习的方法提取候选区域，给出的是无标签的区域和粗定位结果。RCN引入类别概念，同时进行候选区域的分类和位置回归，给出精细定位结果。训练时两个子网络通过端到端的方式联合优化。图10以银行卡卡号识别为例，给出了RPN层和RCN层的输出。</p> 
<p><img src="https://images2.imgbox.com/cf/c1/gK8SSwDm_o.png" alt="在这里插入图片描述"><br> 对于人手持证件场景，由于证件目标在图像中所占比例过小，直接提取微小候选目标会导致一定的定位精度损失。为了保证高召回和高定位精度，可采用由粗到精的策略进行检测。首先定位卡片所在区域位置，然后在卡片区域范围内进行关键字检测，而区域定位也可采用Faster R-CNN框架，如图11所示。</p> 
<p><img src="https://images2.imgbox.com/a0/14/ig72NK2x_o.png" alt="在这里插入图片描述"><br> 2. 非受控场景的文字检测</p> 
<p>对于菜单、门头图等非受控场景，由于文字行本身的多角度且字符的笔画宽度变化大，该场景下的文字行定位任务挑战很大。由于通用目标检测方法的定位粒度是回归框级，此方法适用于刚体这类有良好闭合边界的物体。然而文字往往由一系列松散的笔画构成，尤其对于任意方向或笔画宽度的文字，仅以回归框结果作为定位结果会有较大偏差。另外刚体检测的要求相对较低，即便只定位到部分主体（如定位结果与真值的重叠率是50%），也不会对刚体识别产生重大影响，而这样的定位误差对于文字识别则很可能是致命的。</p> 
<p>为了实现足够精细的定位，我们利用语义分割中常用的全卷积网络（FCN）来进行像素级别的文字/背景标注，整体流程如图12所示。</p> 
<p><img src="https://images2.imgbox.com/bd/b5/yVIT2egb_o.png" alt="在这里插入图片描述"><br> 多尺度全卷积网络通过对多个阶段的反卷积结果的融合，实现了全局特征和局部特征的联合，进而达到了由粗到精的像素级别标注，适应于任意非受控场景（门头图、菜单图片）。</p> 
<p>基于多尺度全卷积网络得到的像素级标注，通过连通域分析技术可得到一系列连通区域（笔划信息）。但由于无法确定哪些连通域属于同一文字行，因此需要借助单链聚类技术来进行文字行提取。至于聚类涉及的距离度量，主要从连通域间的距离、形状、颜色的相似度等方面提取特征，并通过度量学习自适应地得到特征权重和阈值，如图13所示。</p> 
<p><img src="https://images2.imgbox.com/54/d7/N53VZ8Pd_o.png" alt="在这里插入图片描述"><br> 图14分别给出了在菜单和门头图场景中的全卷积网络定位效果。第二列为全卷积网络的像素级标注结果，第三列为最终文字检测结果。可以看出，全卷积网络可以较好地应对复杂版面或多角度文字定位。</p> 
<p><img src="https://images2.imgbox.com/52/b0/3VWKpR5q_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_149"></a>基于序列学习的文字识别</h3> 
<p>我们将整行文字识别问题归结为一个序列学习问题。利用基于双向长短期记忆神经网络（Bi-directional Long Short-term Memory，BLSTM）的递归神经网络作为序列学习器，来有效建模序列内部关系。为了引入更有效的输入特征，我们采用卷积神经网络模型来进行特征提取，以描述图像的高层语义。此外在损失函数的设计方面，考虑到输出序列与输入特征帧序列无法对齐，我们直接使用结构化的Loss（序列对序列的损失），另外引入了背景（Blank）类别以吸收相邻字符的混淆性。</p> 
<p>整体网络结构分为三层：卷积层、递归层和翻译层，如图15所示。其中卷积层提取特征；递归层既学习特征序列中字符特征的先后关系，又学习字符的先后关系；翻译层实现对时间序列分类结果的解码。</p> 
<p><img src="https://images2.imgbox.com/bb/2d/UrtlBq3i_o.png" alt="在这里插入图片描述"><br> 对于输入的固定高度h0= 36的图像（宽度任意，如W0 = 248），我们通过CNN网络结构提取特征，得到9×62×128的特征图，可将其看作一个长度为62的时间序列输入到RNN层。RNN层有400个隐藏节点，其中每个隐藏节点的输入是9×128维的特征，是对图像局部区域的描述。考虑到对应于某个时刻特征的图像区域，它与其前后内容都具有较强的相关性，所以我们一般采用双向RNN网络，如图16所示。</p> 
<p><img src="https://images2.imgbox.com/41/1d/wSwA5Ai7_o.png" alt="在这里插入图片描述"><br> 双向RNN后接一个全连接层，输入为RNN层（在某个时刻）输出的特征图，输出为该位置是背景、字符表中文字的概率。全连接层后接CTC（联结主义时间分类器）作为损失函数。在训练时，根据每个时刻对应的文字、背景概率分布，得到真值字符串在图像中出现的概率P(ground truth)，将-log(P(ground truth))作为损失函数。在测试时，CTC可以看作一个解码器，将每一时刻的预测结果（当前时刻的最大后验概率对应的字符）联合起来，然后去掉空白和重复的模式，就形成了最终的序列预测结果，如图17所示。</p> 
<p><img src="https://images2.imgbox.com/e0/f9/OVehfYEd_o.png" alt="在这里插入图片描述"><br> 从图17中也可以看出，对应输入序列中的每个字符，LSTM输出层都会产生明显的尖峰，尽管该尖峰未必对应字符的中心位置。换句话说，引入CTC机制后，我们不需要考虑每个字符出现的具体位置，只需关注整个图像序列对应的文字内容，最终实现深度学习的端到端训练与预测。</p> 
<p>由于序列学习框架对训练样本的数量和分布要求较高，我们采用了真实样本+合成样本的方式。真实样本以美团业务来源（例如，菜单、身份证、营业执照）为主，合成样本则考虑了字体、形变、模糊、噪声、背景等因素。</p> 
<p>基于上述序列学习框架，我们给出了在不同场景下的文字行识别结果，如图18所示。其中前两行的图片为验证码场景，第三行为银行卡，第四行为资质证件，第五行为门头图，第六行为菜单。可以看到，识别模型对于文字形变、粘连、成像的模糊和光线变化、背景的复杂等都有较好的健壮性。</p> 
<p><img src="https://images2.imgbox.com/9f/49/tTuUtQZm_o.png" alt="在这里插入图片描述"><br> 基于上述试验，与传统OCR相比，我们在多种场景的文字识别上都有较大幅度的性能提升，如图19所示：</p> 
<p><img src="https://images2.imgbox.com/58/29/NDU5q48f_o.png" alt="在这里插入图片描述"><br> 与传统OCR相比，基于深度学习的OCR在<strong>识别率</strong>方面有了大幅上升。但对于特定的应用场景（营业执照、菜单、银行卡等），条目准确率还有待提升。一方面需要融合基于深度学习的文字检测与传统版面分析技术，以进一步提升限制场景下的检测性能。另一方面需要丰富真实训练样本和语言模型，以提升文字识别准确率。</p> 
<hr> 
<p>如果我们的标签序列，就是真实的数据“水煮肉片22元”，长度设为L<br> 加入blank空格之后，长度为多少？<br> “<em>水_煮_肉_片_2_2_元</em>”<br> 2*L+1</p> 
<p>最终将预测的序列：</p> 
<ol><li>连续相同的字符去重！</li><li>去除空字符！</li></ol> 
<p>所以我们要想我们的预测序列可以经过上述的去重去空格得到正确答案<br> 我们是不是在训练模型的时候，就要给RNN准备各种可能的路径~</p> 
<p>各种可能的路径是不是要根据之前的“<em>水_煮_肉_片_2_2_元</em>”来构建</p> 
<p>为了最终去重去空格可以不会错，那么我们在构建各种可能的路径时候就需要加入一些约束条件！！！</p> 
<ol><li>方向只能向下和向右</li><li>相同的字符之间要有一个空字符</li><li>非空字符不能被跳过</li><li>起点必须从前两个字符开始</li><li>终点必须在结尾两个字符结束</li></ol> 
<p>CTC loss == 求的是<strong>所有可能路径的概率的对数之和最大</strong></p> 
<p>会用到<strong>动态规划思想</strong>，把大问题拆分为小问题，像滚雪球似的求解</p> 
<p>对概率求总和和进行递归求解，动态规划需要<strong>初始条件</strong>和<strong>状态转移方程</strong>，CTC中也有状态转移方程</p> 
<h3><a id="_204"></a>前向传播</h3> 
<ul><li> <p>情况一：第s个字符是blank<br> s=3,t=3,a33状态依赖于a23和a22<br> a33 = (a23+a22)*y_3<br> 推广 <code>At(s) = (At-1(s) + At-1(s-1))*Yt(seq(s))</code></p> </li><li> <p>情况二：第s个字符和第s-2个字符相同<br> 推广 <code>At(s) = (At-1(s) + At-1(s-1))*Yt(seq(s))</code></p> </li><li> <p>情况三：其他情况<br> 推广 <code>At(s) = (At-1(s) + At-1(s-1) + At-1(s-2))*Yt(seq(s))</code></p> </li></ul> 
<h3><a id="_217"></a>后向传播</h3> 
<p>CTC算法特性:</p> 
<ol><li>CTC经常被吐槽一个特性是<strong>条件独立性</strong><br> CTC loss最开始是用在语言识别的，后来OCR<br> 比如一个人去说“三个A”，输出有两个，AAA，triple A<br> 它<strong>不会学到任何语言模型的知识</strong></li><li>CTC算法<strong>不需要训练数据对齐</strong><br> CTC要求对齐的方式是<strong>单调</strong>的，像机器翻译，不适合<br> CTC要求输入和输出是<strong>多对一</strong>的关系，有的任务是需要严格一对一关系，比如词性标注，不适合<br> CTC要求<strong>输出要比输入短</strong></li></ol>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a078e711ca975bca44b5143ec57e8b7c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何理解驱动程序、设备树、platform、device、driver之间的关系</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9a810a18b034897c549df6801bfa3ef6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Android Json文件解析及带关键字文件解压</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>