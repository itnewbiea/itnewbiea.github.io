<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文阅读——SG-Former - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="论文阅读——SG-Former" />
<meta property="og:description" content="SG-Former: Self-guided Transformer with Evolving Token Reallocation
1. Introduction
方法的核心是利用显著性图，根据每个区域的显著性重新分配tokens。显著性图是通过混合规模的自我关注来估计的，并在训练过程中自我进化。直观地说，我们将更多的tokens分配给显著区域，以实现细粒度的关注，而将更少的tokens分配到次要区域，以换取效率和全局感受场。
2. Method
hybrid-scale Transformer block提取混合尺度对象和多粒度信息，指导区域重要性；self-guided Transformer block根据混合尺度Transformer块的显著性信息，在保持显著区域细粒度的同时，对全局信息进行建模。
2.1 Self-Guided Attention
通过将几个tokens合并为一个token聚合来减少序列长度这种减少注意力计算的聚合方法面临两个问题：（i）信息可能在显著区域丢失或与不相关的信息混合，（ii）在次要区域或背景区域，许多标记（序列的较高比例）对于简单语义是冗余的，同时需要大量计算。
输入特征图：，映射为Q、K、V
然后H个相互独立的自注意力头平行的计算自注意力，为了计算注意力后保持特征图大小不变的同时降低计算成本，使用重要性引导聚合模块（IAM）固定Q的长度，但聚合K和V的tokens。
其中是significance map。将S的值生序排列，分为n个子区域。s1是最不重要的，Sn是最重要的。r是聚合率，每r个tokens聚合在一起。在不同重要性的区域设置了不同的聚合率r1，··，rn，使得每个子区域都有一个聚合率，并且子区域越重要，聚合率越小。
IAM的目标是在显著区域将更少的令牌聚合为一（即，保留更多），在背景区域将更多的令牌聚合成一（即保留更少）。
然后：
F是聚合函数。
2.2 Hybrid-scale Attention
H个heads分成h组，每组H/h个heads。
将聚合成一个，Q不聚合，这样A和KV的数量不一样了，然后将QKV分窗口，窗口大小M，Q和KV数量不一样，所以Q的窗口大小是：
计算注意力：
计算significance map：
3 实验结果
反正现在试的，这个模型比VIT快很多，计算量也少很多，但是不知道效果，实验结果还没出来。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/33f15e526b55ece9f796fb24b24d3581/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-31T20:52:41+08:00" />
<meta property="article:modified_time" content="2023-12-31T20:52:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文阅读——SG-Former</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>SG-Former: Self-guided Transformer with Evolving Token Reallocation</strong></p> 
<p></p> 
<p><strong>1. Introduction</strong></p> 
<p class="img-center"><img alt="" height="364" src="https://images2.imgbox.com/92/00/6u5yxia9_o.png" width="378"></p> 
<p>方法的核心是利用显著性图，根据每个区域的显著性重新分配tokens。显著性图是通过混合规模的自我关注来估计的，并在训练过程中自我进化。直观地说，我们将更多的tokens分配给显著区域，以实现细粒度的关注，而将更少的tokens分配到次要区域，以换取效率和全局感受场。</p> 
<p><img alt="" height="499" src="https://images2.imgbox.com/2a/3e/OWx4KDBm_o.png" width="1200"></p> 
<p></p> 
<p><strong>2. Method</strong></p> 
<p><img alt="" height="478" src="https://images2.imgbox.com/c6/13/CD1DBAyg_o.png" width="1200"></p> 
<p>hybrid-scale Transformer block提取混合尺度对象和多粒度信息，指导区域重要性；self-guided Transformer block根据混合尺度Transformer块的显著性信息，在保持显著区域细粒度的同时，对全局信息进行建模。</p> 
<p>2.1 Self-Guided Attention</p> 
<p class="img-center"><img alt="" height="399" src="https://images2.imgbox.com/2f/cf/NzljP3Ct_o.png" width="378"></p> 
<p>通过将几个tokens合并为一个token聚合来减少序列长度这种减少注意力计算的聚合方法面临两个问题：（i）信息可能在显著区域丢失或与不相关的信息混合，（ii）在次要区域或背景区域，许多标记（序列的较高比例）对于简单语义是冗余的，同时需要大量计算。</p> 
<p>输入特征图：<img alt="" height="23" src="https://images2.imgbox.com/da/24/yaPp9QzL_o.png" width="122">，映射为Q、K、V</p> 
<p>然后H个相互独立的自注意力头平行的计算自注意力，为了计算注意力后保持特征图大小不变的同时降低计算成本，使用重要性引导聚合模块（IAM）固定Q的长度，但聚合K和V的tokens。</p> 
<p class="img-center"><img alt="" height="77" src="https://images2.imgbox.com/9b/35/vuLfdF5C_o.png" width="152"></p> 
<p>其中<img alt="" height="19" src="https://images2.imgbox.com/0b/99/aK7okVj1_o.png" width="69">是significance map。将S的值生序排列，分为n个子区域<img alt="" height="20" src="https://images2.imgbox.com/ce/20/rYSNLG3S_o.png" width="72">。s1是最不重要的，Sn是最重要的。r是聚合率，每r个tokens聚合在一起。在不同重要性的区域设置了不同的聚合率r1，··，rn，使得每个子区域都有一个聚合率，并且子区域越重要，聚合率越小。</p> 
<p>IAM的目标是在显著区域将更少的令牌聚合为一（即，保留更多），在背景区域将更多的令牌聚合成一（即保留更少）。</p> 
<p>然后：</p> 
<p class="img-center"><img alt="" height="49" src="https://images2.imgbox.com/ca/90/Ozs9LUTV_o.png" width="137"></p> 
<p>F是聚合函数。</p> 
<p></p> 
<p class="img-center"><img alt="" height="654" src="https://images2.imgbox.com/56/a4/SI3DTXZg_o.png" width="1200"></p> 
<p></p> 
<p>2.2 Hybrid-scale Attention</p> 
<p class="img-center"><img alt="" height="562" src="https://images2.imgbox.com/b1/55/SabnTfVy_o.png" width="457"></p> 
<p></p> 
<p>H个heads分成h组，每组H/h个heads。</p> 
<p>将<img alt="" height="19" src="https://images2.imgbox.com/de/b5/Q4M1wK3i_o.png" width="156">聚合成一个，Q不聚合，这样A和KV的数量不一样了，然后将QKV分窗口，窗口大小M，Q和KV数量不一样，所以Q的窗口大小是<img alt="" height="23" src="https://images2.imgbox.com/70/8b/cUouPS4k_o.png" width="82">：</p> 
<p class="img-center"><img alt="" height="80" src="https://images2.imgbox.com/0a/27/sRPpGLu6_o.png" width="159"></p> 
<p>计算注意力：</p> 
<p class="img-center"><img alt="" height="67" src="https://images2.imgbox.com/09/12/ySh5ruMv_o.png" width="257"></p> 
<p>计算significance map：</p> 
<p class="img-center"><img alt="" height="93" src="https://images2.imgbox.com/db/81/vCOpDOFS_o.png" width="134"></p> 
<p></p> 
<p><strong>3 实验结果</strong></p> 
<p class="img-center"><img alt="" height="1123" src="https://images2.imgbox.com/fe/a7/RCUIaz2M_o.png" width="1200"></p> 
<p class="img-center"><img alt="" height="1164" src="https://images2.imgbox.com/e7/d9/9PULiQ8n_o.png" width="1200"></p> 
<p class="img-center"><img alt="" height="311" src="https://images2.imgbox.com/84/cb/EWcDalLt_o.png" width="338"></p> 
<hr> 
<p>反正现在试的，这个模型比VIT快很多，计算量也少很多，但是不知道效果，实验结果还没出来。</p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8a77ed89dbce4acfc897c926d843437d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">2024年原创深度学习算法项目分享</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/016bf91a8a89091e70f047f9373f9d9c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">HTML5-新增表单input属性</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>