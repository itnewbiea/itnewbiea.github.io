<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>经典模型——Transformer - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="经典模型——Transformer" />
<meta property="og:description" content="文章目录 AbstractIntroductionBackgroundModel ArchitectureEncoder and Decoder StacksBatchNorm与LayerNorm AttentionScaled Dot-Product AttentionMuti-Head AttentionApplications of Attention in our Model Position-wise Feed-Fordward NetworksEmbedding and SoftmaxPositional Encoding Why Self-AttentionConclusion MLP、CNN、RNN后的第四大模型。
Abstract 序列转录模型主要是采用RNN或者CNN。里面常常包含一种编码器和解码器的结构。
仅仅依赖于注意力机制。
该篇文章主要是针对机器翻译做的。后来应用在了不同的领域。
Introduction 问题：
RNN是一个一步步计算的过程，无法并行，计算性能很差；时序信息会随着序列的移动而丢失。 注意力机制很早就和RNN有所结合，更好地实现了编解码器之间的数据交互。
但是本文舍弃了RNN的结构，完全采用注意力机制来完成。
Background 用卷积神经网络对比较长的序列难以建模，需要用很多层卷积扩大感受野。卷积的优势在于有多个输出通道，每个通道可以学一个模式。
因此，本文提出了多头的注意力模型。
Model Architecture 对于序列模型来说，编码器-解码器结构在序列任务中有不错的表现。
对于解码器而言，在循环神经网络中，词是一个个输出的，过去时刻的输出会作为当前时刻的输入，这称为自回归。
对于解码器而言，是可以看到全部的句子的。编码器得到的序列整个交付给解码器。
Encoder and Decoder Stacks Encoder：包含了六个堆积的模块，每个模块有两个子层。每一个模块中，两个子层都有相应的残差连接，随后经过标准化（LayerNorm）。
为了避免残差连接时，通道大小的不一致（需要做投影），本文将其的维度统一设置为512。
BatchNorm与LayerNorm Internal Covariate Shift：训练的过程中，数据的分布在不断地变化，为下一层网络的学习带来了困难。
在训练的时候，对于一个二维矩阵，行代表样本，列代表特征，BatchNorm是将每列做一个标准化（算均值、标准差，Z-score）。在测试的时候，
通常而言，最后会用可学习的参数 γ , β \gamma,\beta γ,β，对得到的标准化的结果做一个线性变换，相当于是改变了这个分布的均值和方差。
这是因为，如果全都统一成标准正态分布，那模型学习到的特征分布就完全被消除了。因此有必要给他微调的机会。
我认为BN层起到的作用应该是一方面限制其分布不要太离谱，有一个基本的雏形，另一方面又不希望都是一个模子里刻出来的。
在测试时，用到的均值和方差的参数是在训练的时候算出来的。
公式为 μ = m μ &#43; ( 1 − m ) μ b a t c h , σ 同 理 \mu = m\mu&#43;(1-m)\mu_{batch},\sigma同理 μ=mμ&#43;(1−m)μbatch​,σ同理。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/1af2e1318a4b549b65fd8b318a871405/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-27T20:47:55+08:00" />
<meta property="article:modified_time" content="2022-06-27T20:47:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">经典模型——Transformer</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Abstract_3" rel="nofollow">Abstract</a></li><li><a href="#Introduction_10" rel="nofollow">Introduction</a></li><li><a href="#Background_19" rel="nofollow">Background</a></li><li><a href="#Model_Architecture_24" rel="nofollow">Model Architecture</a></li><li><ul><li><a href="#Encoder_and_Decoder_Stacks_31" rel="nofollow">Encoder and Decoder Stacks</a></li><li><ul><li><a href="#BatchNormLayerNorm_36" rel="nofollow">BatchNorm与LayerNorm</a></li></ul> 
   </li><li><a href="#Attention_69" rel="nofollow">Attention</a></li><li><ul><li><a href="#Scaled_DotProduct_Attention_74" rel="nofollow">Scaled Dot-Product Attention</a></li><li><a href="#MutiHead_Attention_91" rel="nofollow">Muti-Head Attention</a></li><li><a href="#Applications_of_Attention_in_our_Model_98" rel="nofollow">Applications of Attention in our Model</a></li></ul> 
   </li><li><a href="#Positionwise_FeedFordward_Networks_107" rel="nofollow">Position-wise Feed-Fordward Networks</a></li><li><a href="#Embedding_and_Softmax_122" rel="nofollow">Embedding and Softmax</a></li><li><a href="#Positional_Encoding_125" rel="nofollow">Positional Encoding</a></li></ul> 
  </li><li><a href="#Why_SelfAttention_131" rel="nofollow">Why Self-Attention</a></li><li><a href="#Conclusion_143" rel="nofollow">Conclusion</a></li></ul> 
</div> 
<p></p> 
<p>MLP、CNN、RNN后的第四大模型。</p> 
<h2><a id="Abstract_3"></a>Abstract</h2> 
<p>序列转录模型主要是采用RNN或者CNN。里面常常包含一种编码器和解码器的结构。</p> 
<p>仅仅依赖于注意力机制。</p> 
<p>该篇文章主要是针对机器翻译做的。后来应用在了不同的领域。</p> 
<h2><a id="Introduction_10"></a>Introduction</h2> 
<p>问题：</p> 
<ol><li>RNN是一个一步步计算的过程，无法并行，计算性能很差；</li><li>时序信息会随着序列的移动而丢失。</li></ol> 
<p>注意力机制很早就和RNN有所结合，更好地实现了编解码器之间的数据交互。</p> 
<p>但是本文舍弃了RNN的结构，完全采用注意力机制来完成。</p> 
<h2><a id="Background_19"></a>Background</h2> 
<p>用卷积神经网络对比较长的序列难以建模，需要用很多层卷积扩大感受野。卷积的优势在于有多个输出通道，每个通道可以学一个模式。</p> 
<p>因此，本文提出了多头的注意力模型。</p> 
<h2><a id="Model_Architecture_24"></a>Model Architecture</h2> 
<p>对于序列模型来说，编码器-解码器结构在序列任务中有不错的表现。</p> 
<p>对于解码器而言，在循环神经网络中，词是一个个输出的，过去时刻的输出会作为当前时刻的输入，这称为自回归。</p> 
<p>对于解码器而言，是可以看到全部的句子的。编码器得到的序列整个交付给解码器。</p> 
<h3><a id="Encoder_and_Decoder_Stacks_31"></a>Encoder and Decoder Stacks</h3> 
<p>Encoder：包含了六个堆积的模块，每个模块有两个子层。每一个模块中，两个子层都有相应的残差连接，随后经过标准化（LayerNorm）。</p> 
<p>为了避免残差连接时，通道大小的不一致（需要做投影），本文将其的维度统一设置为512。</p> 
<h4><a id="BatchNormLayerNorm_36"></a>BatchNorm与LayerNorm</h4> 
<p>Internal Covariate Shift：训练的过程中，数据的分布在不断地变化，为下一层网络的学习带来了困难。</p> 
<p>在训练的时候，对于一个二维矩阵，行代表样本，列代表特征，BatchNorm是将每列做一个标准化（算均值、标准差，Z-score）。在测试的时候，</p> 
<p>通常而言，最后会用可学习的参数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         γ 
        
       
         , 
        
       
         β 
        
       
      
        \gamma,\beta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8888799999999999em; vertical-align: -0.19444em;"></span><span style="margin-right: 0.05556em;" class="mord mathdefault">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span style="margin-right: 0.05278em;" class="mord mathdefault">β</span></span></span></span></span>，对得到的标准化的结果做一个线性变换，相当于是改变了这个分布的均值和方差。</p> 
<p>这是因为，如果全都统一成标准正态分布，那模型学习到的特征分布就完全被消除了。因此有必要给他微调的机会。</p> 
<p>我认为BN层起到的作用应该是一方面限制其分布不要太离谱，有一个基本的雏形，另一方面又不希望都是一个模子里刻出来的。</p> 
<p>在测试时，用到的均值和方差的参数是在训练的时候算出来的。</p> 
<p>公式为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         μ 
        
       
         = 
        
       
         m 
        
       
         μ 
        
       
         + 
        
       
         ( 
        
       
         1 
        
       
         − 
        
       
         m 
        
       
         ) 
        
        
        
          μ 
         
         
         
           b 
          
         
           a 
          
         
           t 
          
         
           c 
          
         
           h 
          
         
        
       
         , 
        
       
         σ 
        
       
         同 
        
       
         理 
        
       
      
        \mu = m\mu+(1-m)\mu_{batch},\sigma同理 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">μ</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 0.7777700000000001em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">μ</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">m</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.33610799999999996em;"><span class="" style="top: -2.5500000000000003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span style="margin-right: 0.03588em;" class="mord mathdefault">σ</span><span class="mord cjk_fallback">同</span><span class="mord cjk_fallback">理</span></span></span></span></span>。</p> 
<p>如果输入的是[B,C,H,W]的话，输出的是[C,H,W]。</p> 
<p><img src="https://images2.imgbox.com/54/86/GB5q1KWY_o.png" alt="在这里插入图片描述"></p> 
<p>LayerNorm是针对一个Batch中的一个Sample而言的。其计算的是所有channel中的每一个参数的均值和方差，进行归一化，即只在C维度上进行。</p> 
<p>公式为<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          y 
         
        
          = 
         
         
          
          
            x 
           
          
            − 
           
          
            E 
           
          
            [ 
           
          
            x 
           
          
            ] 
           
          
          
           
           
             V 
            
           
             a 
            
           
             r 
            
           
             [ 
            
           
             x 
            
           
             ] 
            
           
             + 
            
           
             ϵ 
            
           
          
         
        
          ⋅ 
         
        
          γ 
         
        
          + 
         
        
          β 
         
        
       
         y=\frac{x-E[x]}{\sqrt{Var[x]+\epsilon}}\cdot\gamma+\beta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span style="margin-right: 0.03588em;" class="mord mathdefault">y</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 2.557em; vertical-align: -1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.427em;"><span class="" style="top: -2.175em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.935em;"><span class="svg-align" style="top: -3.2em;"><span class="pstrut" style="height: 3.2em;"></span><span class="mord" style="padding-left: 1em;"><span style="margin-right: 0.22222em;" class="mord mathdefault">V</span><span class="mord mathdefault">a</span><span style="margin-right: 0.02778em;" class="mord mathdefault">r</span><span class="mopen">[</span><span class="mord mathdefault">x</span><span class="mclose">]</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mord mathdefault">ϵ</span></span></span><span class="" style="top: -2.8950000000000005em;"><span class="pstrut" style="height: 3.2em;"></span><span class="hide-tail" style="min-width: 1.02em; height: 1.28em;"> 
                   <svg width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"> 
                    <path d="M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,
158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067
c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,
175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71
c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,
-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26
s76,-59,76,-59s76,-60,76,-60z M1001 80H40000v40H1012z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.30499999999999994em;"><span class=""></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span style="margin-right: 0.05764em;" class="mord mathdefault">E</span><span class="mopen">[</span><span class="mord mathdefault">x</span><span class="mclose">]</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.13em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height: 0.7777700000000001em; vertical-align: -0.19444em;"></span><span style="margin-right: 0.05556em;" class="mord mathdefault">γ</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height: 0.8888799999999999em; vertical-align: -0.19444em;"></span><span style="margin-right: 0.05278em;" class="mord mathdefault">β</span></span></span></span></span></span></p> 
<p>LN通常用在NLP中，因为NLP中一个Sample是一个句子，句子中的每一维度都是一个词，因此同一维度的词之间没有共性的特征关系，同时为了做BN还要padding没用的块，所以BN的效果很糟糕。</p> 
<p>因此训练过程中归一化的对象是一个词。</p> 
<p>因为是对每个样本自己来做的，所以没必要在训练过程中算全局的均值和方差。</p> 
<p>Decoder：带掩码。使得训练和预测时的行为保持一致。其输入是Encoder的全部输出。</p> 
<h3><a id="Attention_69"></a>Attention</h3> 
<p>注意力函数一个query和一系列key映射成一个输出的函数。</p> 
<h4><a id="Scaled_DotProduct_Attention_74"></a>Scaled Dot-Product Attention</h4> 
<p>常见的注意力机制有两种，分别是加法和点积。这里采用了点积的形式，因为其算起来比较高效。</p> 
<p>但是这里还除了一个<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           d 
          
         
           k 
          
         
        
       
      
        \sqrt{d_k} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.18278000000000005em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.85722em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.33610799999999996em;"><span class="" style="top: -2.5500000000000003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.03148em;" class="mord mathdefault mtight">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.81722em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.18278000000000005em;"><span class=""></span></span></span></span></span></span></span></span></span>，目的是为了避免softmax这样的函数在训练过程中出现饱和的情况（最后出现当然是好的）。</p> 
<p>这也是该注意力命名的由来。</p> 
<p>接下来讲一下注意力函数<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          A 
         
        
          t 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          t 
         
        
          i 
         
        
          o 
         
        
          n 
         
        
          ( 
         
        
          Q 
         
        
          , 
         
        
          K 
         
        
          , 
         
        
          V 
         
        
          ) 
         
        
          = 
         
        
          s 
         
        
          o 
         
        
          f 
         
        
          t 
         
        
          m 
         
        
          a 
         
        
          x 
         
        
          ( 
         
         
          
          
            Q 
           
           
           
             K 
            
           
             T 
            
           
          
          
           
           
             d 
            
           
             k 
            
           
          
         
        
          ) 
         
        
          V 
         
        
       
         Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span style="margin-right: 0.07153em;" class="mord mathdefault">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span style="margin-right: 0.22222em;" class="mord mathdefault">V</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 2.448331em; vertical-align: -0.93em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span style="margin-right: 0.10764em;" class="mord mathdefault">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.5183309999999999em;"><span class="" style="top: -2.25278em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.85722em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.33610799999999996em;"><span class="" style="top: -2.5500000000000003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.03148em;" class="mord mathdefault mtight">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.81722em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
                   <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                    <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.18278000000000005em;"><span class=""></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathdefault">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413309999999999em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.13889em;" class="mord mathdefault mtight">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.93em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span style="margin-right: 0.22222em;" class="mord mathdefault">V</span></span></span></span></span></span></p> 
<p>其中Q、K、V的每一个shape都是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         n 
        
       
         u 
        
       
         m 
        
       
         _ 
        
       
         s 
        
       
         a 
        
       
         m 
        
       
         p 
        
       
         l 
        
       
         e 
        
       
         , 
        
       
         n 
        
       
         u 
        
       
         m 
        
       
         _ 
        
       
         f 
        
       
         e 
        
       
         a 
        
       
         t 
        
       
         u 
        
       
         r 
        
       
         e 
        
       
         ) 
        
       
      
        (num\_sample,num\_feature) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06em; vertical-align: -0.31em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span><span style="margin-right: 0.02778em;" class="mord">_</span><span class="mord mathdefault">s</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span style="margin-right: 0.01968em;" class="mord mathdefault">l</span><span class="mord mathdefault">e</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord mathdefault">n</span><span class="mord mathdefault">u</span><span class="mord mathdefault">m</span><span style="margin-right: 0.02778em;" class="mord">_</span><span style="margin-right: 0.10764em;" class="mord mathdefault">f</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">u</span><span style="margin-right: 0.02778em;" class="mord mathdefault">r</span><span class="mord mathdefault">e</span><span class="mclose">)</span></span></span></span></span>，可想而知，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Q 
        
        
        
          K 
         
        
          T 
         
        
       
      
        QK^T 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.035771em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathdefault">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413309999999999em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right: 0.13889em;" class="mord mathdefault mtight">T</span></span></span></span></span></span></span></span></span></span></span></span>的元素<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         i 
        
       
         , 
        
       
         j 
        
       
         ) 
        
       
      
        (i,j) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span style="margin-right: 0.05724em;" class="mord mathdefault">j</span><span class="mclose">)</span></span></span></span></span>的含义是，我的第i个样本需要对第j个样本注意多少。</p> 
<p>key,value的shape应该是一致的，query的shape可以和他们不一样。</p> 
<p><img src="https://images2.imgbox.com/d1/79/ypN7Zhp3_o.png" alt="请添加图片描述" width="300"><br> 怎么样做mask？<br> 解码器中，是不能看到后续的内容的，所以让第t个时间的query只看前面的key。这里算还是正常算，就是最后将乘出来的结果t+1之后的数据变成一个很大的负数即可，softmax后就会变为0.</p> 
<h4><a id="MutiHead_Attention_91"></a>Muti-Head Attention</h4> 
<p><img src="https://images2.imgbox.com/0d/dd/gMDBCO98_o.png" alt="请添加图片描述" width="300"><br> 模拟多个输出通道，将输入的内容分割成很多个等大小的通道。</p> 
<p><img src="https://images2.imgbox.com/8c/b7/EwnKeV1R_o.png" alt="请添加图片描述"></p> 
<h4><a id="Applications_of_Attention_in_our_Model_98"></a>Applications of Attention in our Model</h4> 
<p>有三种不同的注意力层。</p> 
<p><img src="https://images2.imgbox.com/95/09/UIplEnTa_o.png" alt="请添加图片描述"></p> 
<p>解码器的第一个注意力层有一个Mask的东西。</p> 
<p>解码器的第二个注意力层的输入：key和value来自于编码器，query来自于上一个注意力层。</p> 
<h3><a id="Positionwise_FeedFordward_Networks_107"></a>Position-wise Feed-Fordward Networks</h3> 
<p>说白了就是一个MLP，作用在最后一层。</p> 
<p>单隐藏层MLP，中间的隐藏层将维度变成2048，之后再变回来。</p> 
<p>公式为：<br> <img src="https://images2.imgbox.com/fe/98/VkFRwBAn_o.png" alt="请添加图片描述"></p> 
<p>pytorch输入的是3d的话，默认是在最后一个维度做计算。</p> 
<p>Attention起的作用的是将序列中的信息抓取出来，做一次汇聚。MLP的作用是映射成我想要的语义空间，因为每一个词都有了完整的序列信息，所以MLP是可以单独做的。</p> 
<p>RNN也用MLP做一个转换，为了保证序列信息的获取，将上一时刻的输出输入到下一时刻的MLP。</p> 
<h3><a id="Embedding_and_Softmax_122"></a>Embedding and Softmax</h3> 
<p>就是将词映射成向量。</p> 
<h3><a id="Positional_Encoding_125"></a>Positional Encoding</h3> 
<p>Attention是没有时序信息的，所以要进行位置编码。</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          P 
         
        
          ( 
         
        
          p 
         
        
          o 
         
        
          s 
         
        
          , 
         
        
          2 
         
        
          i 
         
        
          ) 
         
        
          = 
         
        
          sin 
         
        
          ⁡ 
         
         
          
          
            p 
           
          
            o 
           
          
            s 
           
          
          
          
            1000 
           
           
           
             0 
            
            
             
             
               2 
              
             
               i 
              
             
             
             
               d 
              
              
              
                m 
               
              
                o 
               
              
                d 
               
              
                e 
               
              
                l 
               
              
             
            
           
          
         
        
       
         P(pos,2i)=\sin \frac{pos}{10000^{\frac{2i}{d_{model}}}} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span style="margin-right: 0.13889em;" class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathdefault">i</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 2.11949em; vertical-align: -1.01193em;"></span><span class="mop">sin</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.1075599999999999em;"><span class="" style="top: -2.11em;"><span class="pstrut" style="height: 3.12193em;"></span><span class="mord"><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.1219299999999999em;"><span class="" style="top: -3.5233700000000003em; margin-right: 0.05em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8550857142857142em;"><span class="" style="top: -2.656em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.3448em; margin-left: 0em; margin-right: 0.1em;"><span class="pstrut" style="height: 2.69444em;"></span><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span style="margin-right: 0.01968em;" class="mord mathdefault mtight">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.34963999999999995em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.2255000000000003em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line mtight" style="border-bottom-width: 0.049em;"></span></span><span class="" style="top: -3.384em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.5937428571428571em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.35193em;"><span class="pstrut" style="height: 3.12193em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.79893em;"><span class="pstrut" style="height: 3.12193em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.01193em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          P 
         
        
          ( 
         
        
          p 
         
        
          o 
         
        
          s 
         
        
          , 
         
        
          2 
         
        
          i 
         
        
          + 
         
        
          1 
         
        
          ) 
         
        
          = 
         
        
          cos 
         
        
          ⁡ 
         
         
          
          
            p 
           
          
            o 
           
          
            s 
           
          
          
          
            1000 
           
           
           
             0 
            
            
             
             
               2 
              
             
               i 
              
             
             
             
               d 
              
              
              
                m 
               
              
                o 
               
              
                d 
               
              
                e 
               
              
                l 
               
              
             
            
           
          
         
        
       
         P(pos,2i+1)=\cos \frac{pos}{10000^{\frac{2i}{d_{model}}}} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span style="margin-right: 0.13889em;" class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord">2</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 2.11949em; vertical-align: -1.01193em;"></span><span class="mop">cos</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.1075599999999999em;"><span class="" style="top: -2.11em;"><span class="pstrut" style="height: 3.12193em;"></span><span class="mord"><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.1219299999999999em;"><span class="" style="top: -3.5233700000000003em; margin-right: 0.05em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8550857142857142em;"><span class="" style="top: -2.656em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.3448em; margin-left: 0em; margin-right: 0.1em;"><span class="pstrut" style="height: 2.69444em;"></span><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span style="margin-right: 0.01968em;" class="mord mathdefault mtight">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.34963999999999995em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.2255000000000003em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line mtight" style="border-bottom-width: 0.049em;"></span></span><span class="" style="top: -3.384em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.5937428571428571em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.35193em;"><span class="pstrut" style="height: 3.12193em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.79893em;"><span class="pstrut" style="height: 3.12193em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.01193em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<h2><a id="Why_SelfAttention_131"></a>Why Self-Attention</h2> 
<p><img src="https://images2.imgbox.com/5c/18/TmbGFSiN_o.png" alt="请添加图片描述"><br> 第一个比较的是计算复杂度、第二个是序列事件（并行性的度量）、第三个是最大路径长度（从序列的第一个位置到最后一个位置要走多久，体现信息的糅合性）。</p> 
<p>Q K^T的话，n个样本和n个样本相乘，每次乘d次，所以是这个复杂度。</p> 
<p>循环神经网络的话，进来一个d维度的样本，MLP进行对每一维度进行d次运算，共进行n次，所以是这个数。</p> 
<p>目前来看的话两者的计算复杂度没啥区别。主要是后续的内容：Attention信息不容易丢失且并行度较高。</p> 
<p>但是Attention对于模型的约束更少需要更大的模型和更多的约束才能训练出来。</p> 
<h2><a id="Conclusion_143"></a>Conclusion</h2> 
<p>使用了encoder-decoder的结构，不过将其中的recurrent layers换成了multi-headed self-attention。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/133644c9c3e3b9b2486587c586b96722/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">[Vue warn]: Error in render: “RangeError: Maximum call stack size exceeded“ found in ---＞ ＜App＞ at</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/051d93f849b7397555ccad07f0e58ff4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">syntax error near unexpected token in 解决方案</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>