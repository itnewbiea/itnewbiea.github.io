<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>2022李宏毅机器学习深度学习学习笔记第五周--BERT - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="2022李宏毅机器学习深度学习学习笔记第五周--BERT" />
<meta property="og:description" content="文章目录 前言一、pre-train 模型二、How to fine-tune总结 前言 介绍什么是pre-train模型，有了模型后怎么做fine-tune以及pre-train模型怎么被训练出来。
一、pre-train 模型 pre-train 模型希望把输入的每一个token表示成一个embedding vector，vector包含了token的语义，意思相近的token要有比较相近的embedding ，embedding 里面的某些维度可以看出语义的关联性。
contextualized word embedding和过去模型的不同之处就是过去的模型输入一个token输出一个embedding，现在的是吃一整个句子，把一整个句子都看过以后，了解了上下文才给每个token输出一个embedding。
他的模型架构如下：
怎么让BERT模型变小呢？
除了用一些network压缩方法让network变小以外，network architecture 设计的目标是为了让机器可以读非常长的序列。
比如Transformer-XL可以读将近一本书的token，Reformer和Longformer要做的是减少self-attention（n*n）带来的计算量。
二、How to fine-tune fine-tune部分旨在根据预训练的model添加部分层从而可以解决下游任务，我们有了一个pre-trained model以后，我们希望在它上面再叠一层Task-specific的部分，接下来这个模型就可以用在特定的模型上。
先看一下NLP任务的分类：
根据输入分两类，根据输出分四类，
输入：
输入为一个句子，把输入丢到pre-trained model就可以了，如果输入是多个句子，第一个句子是问题。第二个是文件，在两个句子中间加分隔符SEP。
输出：
1.机器读一整个句子，输出一个class
在输入的时候加一个CLS，在pre-trained的时候要告诉机器看到CLS要产生跟整个句子有关的embedding ，把输出丢到模型中，模型在做分类的问题。在训练的时候没有cls的输入，就把所有输出都读进Task Specific进行分类。
2.每一个token都给他一个class
3.从输入做copy，比如Extraction-based QA
4.General Sequence,怎么把pre-trained的模型用在seq2seq模型里面。
版本一
第四个任务就是生成任务，BERT可以作为一个encoder，需要我们自己去设计一个decoder，但是decoder是没有经过预训练的。
版本二
也可以让pre-training模型当作decoder来使用，其方法就是输入一个[sep]之后让model输出一个东西，再将模型的输出作为模型的输入，以此类推，不断的得到输出结果。
假设有一些Task-specific模型怎么去fine-tune模型。
1.pre-trained模型训练完后就固定住了，变成一个Feature Extractor。
2，把pre-trained和Task-specific的部分接在一起，在fine-tune的时候不止调Task-specific也调pre-trained的模型。
再去微调整个模型，从上图可以看到pre-trained的模型在不同的任务中通过fine-tune以后都会变得不一样，每个任务都需要存一个新的model ，往往非常巨大，就有了Adaptor。只调pre-trained的一部分就好了，在pre-trained model中加入一些Apt，只调Apt的部分。
Weighted features
把不同层的features乘上weight加起来，再把每一层合起来的丢到任务中，w1\w2可以跟着task specific一起被学出来。
为什么需要pre-trained的模型？
pre-trained的模型带给我们比较好的performance，
黑线代表人类的能力，每个点代表每个模型在不同任务上的performance，蓝线代表不同任务的平均。机器以及可以与人相提并论了。
总结 本章在了解了BERT的内容后，具体了解了预训练和微调的内容。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/639eb017282c03d8da78fce26cf58ca5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-22T18:42:50+08:00" />
<meta property="article:modified_time" content="2022-05-22T18:42:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">2022李宏毅机器学习深度学习学习笔记第五周--BERT</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_6" rel="nofollow">前言</a></li><li><a href="#pretrain__12" rel="nofollow">一、pre-train 模型</a></li><li><a href="#How_to_finetune_22" rel="nofollow">二、How to fine-tune</a></li><li><a href="#_59" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_6"></a>前言</h2> 
<p>介绍什么是pre-train模型，有了模型后怎么做fine-tune以及pre-train模型怎么被训练出来。</p> 
<hr> 
<h2><a id="pretrain__12"></a>一、pre-train 模型</h2> 
<p>pre-train 模型希望把输入的每一个token表示成一个embedding vector，vector包含了token的语义，意思相近的token要有比较相近的embedding ，embedding 里面的某些维度可以看出语义的关联性。<br> contextualized word embedding和过去模型的不同之处就是过去的模型输入一个token输出一个embedding，现在的是吃一整个句子，把一整个句子都看过以后，了解了上下文才给每个token输出一个embedding。<br> 他的模型架构如下：<br> <img src="https://images2.imgbox.com/17/23/HsJIFngW_o.png" alt="在这里插入图片描述"><br> 怎么让BERT模型变小呢？<br> <img src="https://images2.imgbox.com/26/2c/J6NieS2A_o.png" alt="在这里插入图片描述"><br> 除了用一些network压缩方法让network变小以外，network architecture 设计的目标是为了让机器可以读非常长的序列。<br> <img src="https://images2.imgbox.com/76/cd/IFvya3vu_o.png" alt="在这里插入图片描述"><br> 比如Transformer-XL可以读将近一本书的token，Reformer和Longformer要做的是减少self-attention（n*n）带来的计算量。</p> 
<h2><a id="How_to_finetune_22"></a>二、How to fine-tune</h2> 
<p>fine-tune部分旨在根据预训练的model添加部分层从而可以解决下游任务，我们有了一个pre-trained model以后，我们希望在它上面再叠一层Task-specific的部分，接下来这个模型就可以用在特定的模型上。<br> 先看一下NLP任务的分类：<br> 根据输入分两类，根据输出分四类，<br> 输入：<img src="https://images2.imgbox.com/64/ca/HuIxrKgv_o.png" alt="在这里插入图片描述"><br> 输入为一个句子，把输入丢到pre-trained model就可以了，如果输入是多个句子，第一个句子是问题。第二个是文件，在两个句子中间加分隔符SEP。<br> <img src="https://images2.imgbox.com/59/83/BnceOtlq_o.png" alt="在这里插入图片描述"><br> 输出：<br> 1.机器读一整个句子，输出一个class<br> <img src="https://images2.imgbox.com/a9/d4/jHPSqiLO_o.png" alt="在这里插入图片描述"><br> 在输入的时候加一个CLS，在pre-trained的时候要告诉机器看到CLS要产生跟整个句子有关的embedding ，把输出丢到模型中，模型在做分类的问题。在训练的时候没有cls的输入，就把所有输出都读进Task Specific进行分类。<br> 2.每一个token都给他一个class<br> <img src="https://images2.imgbox.com/1a/f0/OPJ0yvBN_o.png" alt="在这里插入图片描述"><br> 3.从输入做copy，比如Extraction-based QA<br> 4.General Sequence,怎么把pre-trained的模型用在seq2seq模型里面。<br> 版本一<br> 第四个任务就是生成任务，BERT可以作为一个encoder，需要我们自己去设计一个decoder，但是decoder是没有经过预训练的。<br> <img src="https://images2.imgbox.com/aa/df/92TC8HL2_o.png" alt="在这里插入图片描述"><br> 版本二<br> 也可以让pre-training模型当作decoder来使用，其方法就是输入一个[sep]之后让model输出一个东西，再将模型的输出作为模型的输入，以此类推，不断的得到输出结果。<br> <img src="https://images2.imgbox.com/b2/d9/fU2NDDeI_o.png" alt="在这里插入图片描述"><br> 假设有一些Task-specific模型怎么去fine-tune模型。<br> 1.pre-trained模型训练完后就固定住了，变成一个Feature Extractor。<br> <img src="https://images2.imgbox.com/06/81/egMiEDPP_o.png" alt="在这里插入图片描述"><br> 2，把pre-trained和Task-specific的部分接在一起，在fine-tune的时候不止调Task-specific也调pre-trained的模型。<br> <img src="https://images2.imgbox.com/1d/59/UEKtnqu8_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/11/e7/CrSeQH2q_o.png" alt="在这里插入图片描述"><br> 再去微调整个模型，从上图可以看到pre-trained的模型在不同的任务中通过fine-tune以后都会变得不一样，每个任务都需要存一个新的model ，往往非常巨大，就有了Adaptor。只调pre-trained的一部分就好了，在pre-trained model中加入一些Apt，只调Apt的部分。<br> <img src="https://images2.imgbox.com/98/6b/VOt2BASy_o.png" alt="在这里插入图片描述"><br> Weighted features<br> 把不同层的features乘上weight加起来，再把每一层合起来的丢到任务中，w1\w2可以跟着task specific一起被学出来。<br> <img src="https://images2.imgbox.com/53/44/mgyQui15_o.png" alt="在这里插入图片描述"><br> 为什么需要pre-trained的模型？<br> pre-trained的模型带给我们比较好的performance，<br> <img src="https://images2.imgbox.com/bf/9d/RVEfAr0E_o.png" alt="在这里插入图片描述"><br> 黑线代表人类的能力，每个点代表每个模型在不同任务上的performance，蓝线代表不同任务的平均。机器以及可以与人相提并论了。</p> 
<h2><a id="_59"></a>总结</h2> 
<p>本章在了解了BERT的内容后，具体了解了预训练和微调的内容。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f530bb23b407b58051f1d856a97837ec/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Error: Module build failed (from ./node_modules/@dcloudio/webpack-uni-mp-loader/lib/style.js):Error</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/07ed1a518fbaad008ee0746b30b8fee7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【matlab图像处理】图像直方图操作和matlab画图</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>