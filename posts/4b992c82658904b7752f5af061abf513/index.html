<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pyflink kafka es - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pyflink kafka es" />
<meta property="og:description" content="# -*- coding: utf-8 -*-
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import MapFunction, RuntimeContext, KeyedProcessFunction
from abc import ABC, abstractmethod
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import MapFunction, RuntimeContext, KeyedProcessFunction
from pyflink.datastream.state import MapStateDescriptor
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer
from pyflink.common.typeinfo import Types, TypeInformation
from pyflink.datastream.connectors.elasticsearch import Elasticsearch7SinkBuilder, ElasticsearchEmitter, FlushBackoffType
from pyflink.datastream.connectors import DeliveryGuarantee
from pyflink.common.serialization import SimpleStringSchema
import json
import re
from datetime import datetime
from elasticsearch import Elasticsearch" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/4b992c82658904b7752f5af061abf513/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-12T21:04:20+08:00" />
<meta property="article:modified_time" content="2023-06-12T21:04:20+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pyflink kafka es</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p># -*- coding: utf-8 -*-<br> from pyflink.datastream import StreamExecutionEnvironment<br> from pyflink.datastream.functions import  MapFunction, RuntimeContext, KeyedProcessFunction<br> from abc import ABC, abstractmethod<br> from pyflink.datastream import StreamExecutionEnvironment<br> from pyflink.datastream.functions import  MapFunction, RuntimeContext, KeyedProcessFunction<br> from pyflink.datastream.state import MapStateDescriptor<br> from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer<br> from pyflink.common.typeinfo import Types, TypeInformation<br> from pyflink.datastream.connectors.elasticsearch import Elasticsearch7SinkBuilder, ElasticsearchEmitter, FlushBackoffType<br> from pyflink.datastream.connectors import DeliveryGuarantee<br> from pyflink.common.serialization import SimpleStringSchema<br> import json<br> import re<br> from datetime import datetime<br> from elasticsearch import Elasticsearch<br> from pyflink.datastream.functions import RuntimeContext, FlatMapFunction<br> from pyflink.common.typeinfo import Types<br> from pyflink.datastream import StreamExecutionEnvironment<br> from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer<br> from pyflink.common.serialization import SimpleStringSchema</p> 
<p><br> import re<br> import redis</p> 
<p><br> # 创建 StreamExecutionEnvironment 对象<br> env = StreamExecutionEnvironment.get_execution_environment()<br> env.set_parallelism(1)<br> env.add_jars("file:///root/flink-sql-connector-kafka_2.11-1.14.4.jar")</p> 
<p>TEST_KAFKA_SERVERS = "127.0.0.1:9092"<br> TEST_KAFKA_TOPIC = "topic_elink"<br> TEST_GROUP_ID = "pyflink_group"<br> def get_kafka_customer_properties(kafka_servers: str, group_id: str):<br>     properties = {<!-- --><br>         "bootstrap.servers": kafka_servers,<br>         "fetch.max.bytes": "67108864",<br>         "key.deserializer": "org.apache.kafka.common.serialization.StringDeserializer",<br>         "value.deserializer": "org.apache.kafka.common.serialization.StringDeserializer",<br>         "enable.auto.commit": "false",  # 关闭kafka 自动提交，此处不能传bool 类型会报错<br>         "group.id": group_id,<br>     }<br>     return properties<br> properties = get_kafka_customer_properties(TEST_KAFKA_SERVERS, TEST_GROUP_ID)</p> 
<p><br> class LogEvent:<br>     buss_seq = None<br>     message = None<br>     index_name = None<br>     <br>     def __init__(self, bus_seq,message,index_name):<br>         self.bus_seq = bus_seq<br>         self.message = message<br>         self.index_name= index_name</p> 
<p>    def to_dict(self):<br>         return {<!-- --><br>             "bus_seq": self.bus_seq,<br>             "message": self.message,<br>             "index_name" : self.index_name<br>         }<br>    <br> class MyMapFunction(FlatMapFunction):<br>    def open(self, runtime_context: RuntimeContext):<br>        self.process_id_to_bus_seq = runtime_context.get_map_state(MapStateDescriptor('process_id', Types.STRING(), Types.STRING()))<br>       <br>    def close(self):<br>        pass</p> 
<p>   def flat_map(self,line):<br>       bus_seq=''<br>       process_id=''<br>       message=''<br>       message = line.replace("\n", "")<br>       line = json.loads(message)['message']<br>       if not line.startswith("ES"):<br>           return <br>       if '&lt;Serial&gt;' in line:<br>          try:<br>              pat=re.compile(r"&lt;Serial&gt;(\d+)&lt;/Serial&gt;")<br>              bus_seq=pat.search(line).group(1)<br>              process_id=line.split()[1]<br>              self.process_id_to_bus_seq.put(process_id, bus_seq)<br>          except:<br>              return <br>       process_id=line.split()[1]<br>       if not len(process_id)==6 :<br>           process_id=line.split()[2]<br>       bus_seq=self.process_id_to_bus_seq.get(process_id)<br>       if not bus_seq:<br>           bus_seq='0'<br>       #self.r.delete(process_id)<br>       #log_event = LogEvent(bus_seq.decode('UTF-8'),line)<br>       #LogEvent['bus_seq']=bus_seq.decode('UTF-8')<br>       date_str = datetime.now().strftime("%Y-%m-%d")<br>       index_name='flink-test'+date_str<br>       try:<br>          log_event=LogEvent(bus_seq,line,index_name)<br>       except:<br>           return <br>       yield log_event.to_dict()<br>     <br>   <br> data_stream = env.add_source(<br>         FlinkKafkaConsumer(topics=TEST_KAFKA_TOPIC,<br>                            properties=properties,<br>                            deserialization_schema=SimpleStringSchema()) \<br>             .set_commit_offsets_on_checkpoints(True) \<br>             .set_start_from_latest()<br>     ).name(f"消费{TEST_KAFKA_TOPIC}主题数据")</p> 
<p><br> env.add_jars("file:///root/lib/flink-sql-connector-elasticsearch7-3.0.1-1.16.jar")</p> 
<p>es7_sink = Elasticsearch7SinkBuilder() \<br>     .set_bulk_flush_max_actions(1) \<br>     .set_emitter(ElasticsearchEmitter.dynamic_index('index_name')) \<br>     .set_hosts(['127.0.0.1:9200']) \<br>     .build()<br> def get_line_key(line):<br>           message=''<br>           message = line.replace("\n", "")<br>           line = json.loads(message)['message']<br>           try:<br>               process_id=line.split()[1]<br>               if not len(process_id)==6 :<br>                   process_id=line.split()[2]<br>           except:<br>               process_id='9999'<br>           return    process_id <br> #data_stream.key_by(get_line_key).flat_map(MyMapFunction(),output_type=Types.MAP(Types.STRING(),Types.STRING())).sink_to(es7_sink)<br> data_stream.key_by(get_line_key).flat_map(MyMapFunction(),output_type=Types.MAP(Types.STRING(),Types.STRING())).sink_to(es7_sink)</p> 
<p><br> # 执行任务<br> env.execute('Add "bus_seq" to each line')</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ddc9136ad888c68a39738d48bfaee68b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">合并两个排序的链表C&#43;&#43;</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/593b1f40134da5ff02f674bcdb6febae/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C&#43;&#43;编程(6)——类和对象的基本概念(二)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>