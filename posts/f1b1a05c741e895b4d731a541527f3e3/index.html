<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python爬取贴吧数据_Python爬取某贴吧第一页的所有帖子的标题、连接、作者，将数据储存到txt文件中... - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python爬取贴吧数据_Python爬取某贴吧第一页的所有帖子的标题、连接、作者，将数据储存到txt文件中..." />
<meta property="og:description" content="学习网络爬虫的第二个程序：
#-*- coding:utf-8 -*-
#import re
import urllib
from bs4 import BeautifulSoup
import urlparse #处理url链接的库
import chardet #字符集检测
import sys #解决UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode characters in position 0-15: ordinal not in range(128)
reload(sys)
sys.setdefaultencoding( &#34;utf-8&#34; )
&#34;&#34;&#34;
爬取某贴吧第一页的所有帖子的标题、连接、作者，将数据储存到txt文件中
&#34;&#34;&#34;
def get_content(url):
&#34;&#34;&#34; 获取页面源码&#34;&#34;&#34;
html = urllib.urlopen(url) #获取网站页面的地址
content = html.read() #将页面读取到content变量中
html.close() #关闭页面
#local = &#39;/root/desktop/python/teiba.html&#39;
#urllib.urlretrieve(url,local) #将页面源码下载的本地
#print chardet.detect(content) #检测网页的字符集，依据网页具体内容
return content
def get_author(info):
&#34;&#34;&#34;
提取帖子里面的作者
&#34;&#34;&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/f1b1a05c741e895b4d731a541527f3e3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-21T06:26:16+08:00" />
<meta property="article:modified_time" content="2021-02-21T06:26:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python爬取贴吧数据_Python爬取某贴吧第一页的所有帖子的标题、连接、作者，将数据储存到txt文件中...</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size:16px;"> 
 <p>学习网络爬虫的第二个程序：</p> 
 <p>#-*- coding:utf-8 -*-</p> 
 <p>#import re</p> 
 <p>import urllib</p> 
 <p>from bs4 import BeautifulSoup</p> 
 <p>import urlparse #处理url链接的库</p> 
 <p>import chardet #字符集检测</p> 
 <p>import sys #解决UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-15: ordinal not in range(128)</p> 
 <p>reload(sys)</p> 
 <p>sys.setdefaultencoding( "utf-8" )</p> 
 <p>"""</p> 
 <p>爬取某贴吧第一页的所有帖子的标题、连接、作者，将数据储存到txt文件中</p> 
 <p>"""</p> 
 <p>def get_content(url):</p> 
 <p>""" 获取页面源码"""</p> 
 <p>html = urllib.urlopen(url) #获取网站页面的地址</p> 
 <p>content = html.read() #将页面读取到content变量中</p> 
 <p>html.close() #关闭页面</p> 
 <p>#local = '/root/desktop/python/teiba.html'</p> 
 <p>#urllib.urlretrieve(url,local) #将页面源码下载的本地</p> 
 <p>#print chardet.detect(content) #检测网页的字符集，依据网页具体内容</p> 
 <p>return content</p> 
 <p>def get_author(info):</p> 
 <p>"""</p> 
 <p>提取帖子里面的作者</p> 
 <p>"""</p> 
 <p>soup = BeautifulSoup(info) #转换成soup对象</p> 
 <p>all_author = soup.find_all('div',class_="louzhubiaoshi j_louzhubiaoshi") #提取有关与作者的那行代码</p> 
 <p>for author in all_author:</p> 
 <p>return author['author'] #提取作者的名字</p> 
 <p>def get_tiezi(info):</p> 
 <p>"""</p> 
 <p>提取帖子的标题和链接</p> 
 <p>"""</p> 
 <p>soup = BeautifulSoup(info) #转换成soup对象</p> 
 <p>all_title = soup.find_all('a',class_="j_th_tit")</p> 
 <p>#all_author = soup.find_all('span',class_="tb_icon_author ")</p> 
 <p>#class_="..."中的class带下划线是因为class是python的一个关键字，他还是表示class="..."</p> 
 <p>print len(all_title) #打印获取的链接个数</p> 
 <p>#print len(all_author)</p> 
 <p>f = file("data.txt","w+") #创建并打开文件，文件路径和这个Python执行文件的路径一样</p> 
 <p>for title in all_title:</p> 
 <p>#print title['title']</p> 
 <p>url_tiezi = urlparse.urljoin("http://tieba.baidu.com",title['href']) #合并为全路径</p> 
 <p>#print type(url_tiezi)</p> 
 <p>info1 = get_content(url_tiezi) #进入帖子里面</p> 
 <p>author = get_author(info1) #找到作者</p> 
 <p>if author == None: #处理识别不了的作者</p> 
 <p>author = "I don't know the author!"</p> 
 <p>print author</p> 
 <p>li = [str(title['title']+"\n"),url_tiezi+"\n",str(author+"\n\n")] #转换为字符串</p> 
 <p>f.writelines(li) #储存到文件中</p> 
 <p>f.close() #关闭文件</p> 
 <p>return 0</p> 
 <p>info = get_content('http://tieba.baidu.com/f?kw=%B3%A4%BD%AD%B4%F3%D1%A7&amp;fr=home') #贴吧地址</p> 
 <p>print get_tiezi(info)</p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2f246476e1ebfc05e243aa9724496afd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python字符串定界符_python字符串</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7468989d068612129954b4cf50fc861b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Keil5中出现UNDEFINED SYMBOL</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>