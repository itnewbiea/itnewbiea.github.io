<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大模型微调样本构造的trick - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大模型微调样本构造的trick" />
<meta property="og:description" content="来自：包包算法笔记
进NLP群—&gt;加入NLP交流群
开局一道面试题。
面试官：大模型微调如何组织训练样本？
你：大模型训练一问一答，一指令一输出，问题和指令可以作为prompt输入，答案作为输出，计算loss的部分要屏蔽掉pad token。
面试官：多轮对话如何组织训练样本呢？
你：假设多轮为Q1A1/Q2A2/Q3A3，那么可以转化成 Q1—&gt;A1, Q1A1Q2-&gt;A2, Q1A1Q2A2Q3-&gt;A3三条训练样本。
面试官：这样的话一个session变成了三条数据，并且上文有依次重复的情况，这样会不会有啥问题？
你：数据中大部分都是pad token，训练数据利用效率低下。另外会有数据重复膨胀的问题，训练数据重复膨胀为 session数量*平均轮次数，且上文有重复部分，训练效率也会低下。
面试官：你也意识到了，有什么改进的方法吗？
你：有没有办法能一次性构造一个session作为训练样本呢？（思索）
面试官：提示你下，限制在decoder-only系列的模型上，利用模型特性，改进样本组织形式。
对于这个问题，我们思考下decoder-only模型有啥特点，第一点很关键的是其attention形式是casual的，casual简单理解就是三角阵，单个token只能看到其上文的信息。
如图所示：
其二是postion_id是只有token次序含义而无需特定指代信息，（区别于GLM模型需要postion_id来标识生成span的位置等特殊的要求）。
有了这两点我们就可以设想，如果构造多轮对话样本的input为 Q1 A1 &lt;eos&gt; Q2 A2 &lt;eos&gt; Q3 A3 &lt;eos&gt;，在计算loss的时候，只需要计算 A1 &lt;eos&gt; A2 &lt;eos&gt; 和 A3 &lt;eos&gt;部分，岂不是就可以进行session级别的训练了？
嗯为什么原来的chatglm不能用这种形式呢，虽然prefix attention可以推广为适应多轮训练的prefix attention形式，如图：
但是由于其postition id 无法简单按次序推广，故不能高效训练，这也是chatglm初代的很大的一个问题，导致后续微调的效果都比较一般。
现在chatglm2的代码针对这两个问题已经进行了改善，可以认为他就是典型的decoder-only模型了，具体表现为推断时候attention 是casual attention的形式，position id也退化为token次序增长。
那么好了，万事具备，只欠东风。我们据此实现了chatglm2-6b的代码微调。其核心代码逻辑为处理样本组织的逻辑，其他的就是大模型微调，大同小异了。
conversation = &#39;&#39; input_ids = [] labels = [] eos_id = tokenizer.eos_token_id turn_idx = 0 for sentence in examples[prompt_column][i]: sentence_from = sentence[&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/4b88d050e07fab4d1b5e8e427266656e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-10T11:00:08+08:00" />
<meta property="article:modified_time" content="2023-07-10T11:00:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大模型微调样本构造的trick</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p>来自：包包算法笔记</p> 
 <p><strong><strong><strong><strong><strong>进NLP群—&gt;</strong><strong><a href="" rel="nofollow">加入NLP交流群</a></strong></strong></strong></strong></strong></p> 
 <p>开局一道面试题。<br></p> 
 <hr> 
 <p>面试官：<strong>大模型微调如何组织训练样本？</strong></p> 
 <p>你：大模型训练一问一答，一指令一输出，问题和指令可以作为prompt输入，答案作为输出，计算loss的部分要屏蔽掉pad token。</p> 
 <p>面试官：<strong>多轮对话如何组织训练样本呢？</strong></p> 
 <p>你：假设多轮为Q1A1/Q2A2/Q3A3，那么可以转化成 Q1—&gt;A1, Q1A1Q2-&gt;A2, Q1A1Q2A2Q3-&gt;A3三条训练样本。</p> 
 <p>面试官：这样的话一个session变成了三条数据，并且上文有依次重复的情况，这样会不会有啥问题？</p> 
 <p>你：数据中大部分都是pad token，训练数据利用效率低下。另外会有数据重复膨胀的问题，训练数据重复膨胀为 session数量*平均轮次数，且上文有重复部分，训练效率也会低下。</p> 
 <p>面试官：你也意识到了，有什么改进的方法吗？</p> 
 <p>你：有没有办法能一次性构造一个session作为训练样本呢？（思索）</p> 
 <p>面试官：提示你下，限制在decoder-only系列的模型上，利用模型特性，改进样本组织形式。</p> 
 <p>对于这个问题，我们思考下decoder-only模型有啥特点，第一点很关键的是其attention形式是casual的，casual简单理解就是三角阵，单个token只能看到其上文的信息。</p> 
 <p>如图所示：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f7/13/RFIdiK6v_o.png" alt="97fc6f52337a2515c105b688ae2e2271.png"></p> 
 <p>其二是postion_id是只有token次序含义而无需特定指代信息，（区别于GLM模型需要postion_id来标识生成span的位置等特殊的要求）。</p> 
 <p>有了这两点我们就可以设想，如果构造多轮对话样本的input为  Q1 A1 &lt;eos&gt; Q2 A2 &lt;eos&gt; Q3 A3 &lt;eos&gt;，在计算loss的时候，只需要计算 A1 &lt;eos&gt; A2 &lt;eos&gt; 和 A3 &lt;eos&gt;部分，岂不是就可以进行session级别的训练了？</p> 
 <p>嗯为什么原来的chatglm不能用这种形式呢，虽然prefix attention可以推广为适应多轮训练的prefix attention形式，如图：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9e/be/iUB6BFqO_o.jpg" alt="5034292552c83bf23ac2acb226d23e2d.jpeg"></p> 
 <p>但是由于其postition id 无法简单按次序推广，故不能高效训练，这也是chatglm初代的很大的一个问题，导致后续微调的效果都比较一般。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ad/bc/4k9G1W69_o.jpg" alt="21fe5b6fc028fb180ea5d7a16ff4f095.jpeg"><br></p> 
 <p>现在chatglm2的代码针对这两个问题已经进行了改善，可以认为他就是典型的decoder-only模型了，具体表现为推断时候attention 是casual attention的形式，position id也退化为token次序增长。</p> 
 <p>那么好了，万事具备，只欠东风。我们据此实现了chatglm2-6b的代码微调。其核心代码逻辑为处理样本组织的逻辑，其他的就是大模型微调，大同小异了。<br></p> 
 <pre class="has"><code class="language-makefile">conversation = ''
input_ids = []
labels = []
eos_id = tokenizer.eos_token_id
turn_idx = 0
for sentence in examples[prompt_column][i]:
    sentence_from = sentence["from"].lower()
    sentence_value = '[Round {}]\n\n问：'.format(turn_idx) + sentence["value"] + '\n\n答：' if sentence_from == 'human' else sentence["value"]+'\n\n'
    conversation += sentence_value
    sentence_ids = tokenizer.encode(sentence_value, add_special_tokens=False)  
    label = copy.deepcopy(sentence_ids) if sentence_from != 'human' else [-100] * len(sentence_ids)               
    input_ids += sentence_ids 
    labels += label
    if sentence_from != 'human':
        input_ids += [eos_id]
        labels += [eos_id]
        turn_idx += 1
input_ids = tokenizer.encode('') + input_ids #add gmask bos 
labels =  [-100] * 2 + labels# #add padding
pad_len = max_seq_length - len(input_ids)
input_ids = input_ids + [eos_id] * pad_len 
labels = labels + [-100] * pad_len</code></pre> 
 <p>其中有几个关键的地方，就是在开头要加上 bos和gmask，遵循模型原来的逻辑。问答提示词和轮次prompt，还有两个\n保持和原模型保持一致，最后屏蔽掉pad部分的loss计算。</p> 
 <p>实测训练效果如下：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/85/bd/l11y1WTn_o.jpg" alt="477e874452e28bd487005b7ccefd339c.jpeg"></p> 
 <p>同样的数据在chatglm1上 train loss只能降到2.x左右，同时评估测试集结果，在同样的数据上rouge等指标都有不小的提升。</p> 
 <p><strong>我们再仔细回顾下，对话session级别训练和拆开训练从原理上有啥区别？</strong></p> 
 <p>1. session级别训练，效果之一为等价batchsize变大（1个batch可以塞下更多样本），且同一通对话产生的样本在一个bs内。</p> 
 <p>2. session级别的不同轮次产生的梯度是求平均的，拆开轮次构造训练是求和的，这样除了等价于lr会变大，还会影响不同轮次token权重的分配，另外还会影响norm的计算。</p> 
 <p>我们用一个简化地例子定量分析下，我们假设两条训练样本分为 </p> 
 <p>1.问：A 答：xx</p> 
 <p>2.问: A 答：xx 问: B 答：xx  问: C 答：xx</p> 
 <p>则session级别训练影响梯度为 (Ga+(Ga + Gb + Gc)/3 )/2。对 A，B，C影响的权重分别为，2/3 1/6 1/6。</p> 
 <p>拆开训练为 (Ga+Ga+ (Ga + Gb)/2 +(Ga + Gb + Gc)/3)/4。对 A，B，C影响的权重分别为，17/24 5/24 1/12。</p> 
 <p>从上面的权重分布来看，session级别靠后的轮次影响权重要比拆开更大。这也是更合理的，因为大部分场景下，开场白都是趋同和重复的。</p> 
 <p><strong>一点小福利，以上面试题对应的ChatGLM2-6B 微调完整的代码地址为：</strong></p> 
 <p>https://github.com/SpongebBob/Finetune-ChatGLM2-6B</p> 
 <p>实现了对于 ChatGLM2-6B 模型的全参数微调，主要改进点在多轮对话的交互组织方面，使用了更高效的session级别高效训练，训练效果相比原版ChatGLM-6B有较大提升。</p> 
 <p>这可能是目前全网效果最好的ChatGLM2-6B全参数微调代码。</p> 
 <hr> 
 <p style="text-align:center;"><strong><strong><strong><strong><strong>进NLP群—&gt;</strong><strong><a href="" rel="nofollow">加入NLP交流群</a></strong></strong></strong></strong></strong></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/68fcd15320391b07503b783c28feaa43/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Windows PCL点云库不同版本的切换，以及不同版本计算结果不一致的阐述</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bc0705d0eac2ec0eb4cb5a7d1a91beae/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">js 内容去掉html标签</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>