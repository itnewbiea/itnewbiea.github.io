<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>流媒体学习之路(机器学习应用)——了解我们的网络模型与CC算法 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="流媒体学习之路(机器学习应用)——了解我们的网络模型与CC算法" />
<meta property="og:description" content="流媒体学习之路(机器学习应用)——了解我们的网络模型与CC算法 —— 我正在的github给大家开发一个用于做实验的项目 —— github.com/qw225967/Bifrost 目标：可以让大家熟悉各类Qos能力、带宽估计能力，提供每个环节关键参数调节接口并实现一个json全配置，提供全面的可视化算法观察能力。 欢迎大家使用 —— 文章目录 流媒体学习之路(机器学习应用)——了解我们的网络模型与CC算法一、导读二、基于学习的模型与算法2.1 丢包检测算法2.1.1 无线网络丢包模型2.1.2 多信道重排序丢包模型2.1.3 光脉冲网络切换模型 2.2 延迟检测算法2.2.1 延迟原因 2.3 队列长度控制算法2.3.1 论文提到的场景2.3.2 排队算法 三、基于无监督学习的算法3.1 丢包算法3.2 延迟算法3.3 基于窗口更新的算法 3.4 基于队列长度的算法四、实验 一、导读 最近在做机器学习与网络相关的内容，偶然间看到了一篇清华大学2020年发布的机器学习与网络相关综述论文，抽时间阅读了一下，感觉有所收获与大家分享——When Machine Learning Meets Congestion Control: A Survey and Comparison。在展开说的之前老规矩先给大家粘上他们的摘要翻译：
机器学习（ML）已经在许多不同的应用程序中得到了显著的普及。它提供的高度灵活性、适应性和计算能力扩展了在包括网络运营和管理在内的多个领域中使用的传统方法。许多调查都探讨了ML在网络环境中的应用，如流量工程、性能优化和网络安全。许多ML方法侧重于聚类、分类、回归和强化学习（RL）。本研究的创新和贡献在于对基于学习的拥塞控制（CC）方法的详细总结和比较。与通常基于规则的传统CC算法相比，从历史经验中学习的能力是非常可取的。从文献中可以观察到，RL是基于学习的CC算法中的一个关键趋势。在本文中，我们探索了基于RL的CC算法的性能，并提出了当前基于RL的CC算法存在的问题。我们概述了与基于学习的CC算法相关的挑战和趋势。
二、基于学习的模型与算法 2.1 丢包检测算法 随机、聚簇丢包模型相关的算法：
算法场景输入输出决策树 [1]Wireless networks单向延迟 &#43; 包间延迟链路丢包 \ 拥塞丢包贝叶斯 [2]Networks with Reordered events丢失数据的RTT重排序丢包 \ 拥塞丢包Hidden Markov模型 [3]Optica Burst Switching两次突发发送之间成功接收包数竞争丢包 \ 拥塞丢包DT\Bagging\Boosting 神经网络 [4]Wireless networks队列延迟 &#43; 到达时间差 &#43; 包队列无线网络丢包 \ 拥塞丢包决策树\决策树集合\套袋\随机森林\额外树\多层\感知器\K-最近邻 [5］Wireless networks单向延迟的最大值、最小值、标准差 &#43; 包间间隔的最大值、最小值、标准差无线丢包 \ 拥塞丢包 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/359e491b479a97f027afb80b66713a24/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-16T15:50:37+08:00" />
<meta property="article:modified_time" content="2023-11-16T15:50:37+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">流媒体学习之路(机器学习应用)——了解我们的网络模型与CC算法</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="CC_0"></a>流媒体学习之路(机器学习应用)——了解我们的网络模型与CC算法</h2> 
<pre><code>——
我正在的github给大家开发一个用于做实验的项目 —— github.com/qw225967/Bifrost

目标：可以让大家熟悉各类Qos能力、带宽估计能力，提供每个环节关键参数调节接口并实现一个json全配置，提供全面的可视化算法观察能力。

欢迎大家使用
——
</code></pre> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#CC_0" rel="nofollow">流媒体学习之路(机器学习应用)——了解我们的网络模型与CC算法</a></li><li><a href="#_14" rel="nofollow">一、导读</a></li><li><a href="#_24" rel="nofollow">二、基于学习的模型与算法</a></li><li><ul><li><a href="#21__25" rel="nofollow">2.1 丢包检测算法</a></li><li><ul><li><a href="#211__37" rel="nofollow">2.1.1 无线网络丢包模型</a></li><li><a href="#212__45" rel="nofollow">2.1.2 多信道重排序丢包模型</a></li><li><a href="#213__48" rel="nofollow">2.1.3 光脉冲网络切换模型</a></li></ul> 
   </li><li><a href="#22__67" rel="nofollow">2.2 延迟检测算法</a></li><li><ul><li><a href="#221__75" rel="nofollow">2.2.1 延迟原因</a></li></ul> 
   </li><li><a href="#23__94" rel="nofollow">2.3 队列长度控制算法</a></li><li><ul><li><a href="#231__99" rel="nofollow">2.3.1 论文提到的场景</a></li><li><a href="#232__105" rel="nofollow">2.3.2 排队算法</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_134" rel="nofollow">三、基于无监督学习的算法</a></li><li><ul><li><a href="#31__144" rel="nofollow">3.1 丢包算法</a></li><li><a href="#32__157" rel="nofollow">3.2 延迟算法</a></li><li><a href="#33__172" rel="nofollow">3.3 基于窗口更新的算法</a></li></ul> 
  </li><li><a href="#34__223" rel="nofollow">3.4 基于队列长度的算法</a></li><li><a href="#_249" rel="nofollow">四、实验</a></li></ul> 
</div> 
<p></p> 
<hr color="#000000" size='1"'> 
<h2><a id="_14"></a>一、导读</h2> 
<p><img src="https://images2.imgbox.com/f6/8c/7dCo3Kyz_o.png" alt="在这里插入图片描述"></p> 
<p>  最近在做机器学习与网络相关的内容，偶然间看到了一篇清华大学2020年发布的机器学习与网络相关综述论文，抽时间阅读了一下，感觉有所收获与大家分享——When Machine Learning Meets Congestion Control: A Survey and Comparison。在展开说的之前老规矩先给大家粘上他们的摘要翻译：</p> 
<hr> 
<p>  机器学习（ML）已经在许多不同的应用程序中得到了显著的普及。它提供的高度灵活性、适应性和计算能力扩展了在包括网络运营和管理在内的多个领域中使用的传统方法。许多调查都探讨了ML在网络环境中的应用，如流量工程、性能优化和网络安全。许多ML方法侧重于聚类、分类、回归和强化学习（RL）。本研究的创新和贡献在于对基于学习的拥塞控制（CC）方法的详细总结和比较。与通常基于规则的传统CC算法相比，从历史经验中学习的能力是非常可取的。从文献中可以观察到，RL是基于学习的CC算法中的一个关键趋势。在本文中，我们探索了基于RL的CC算法的性能，并提出了当前基于RL的CC算法存在的问题。我们概述了与基于学习的CC算法相关的挑战和趋势。</p> 
<hr> 
<h2><a id="_24"></a>二、基于学习的模型与算法</h2> 
<h3><a id="21__25"></a>2.1 丢包检测算法</h3> 
<p><strong>随机、聚簇</strong>丢包模型相关的算法：</p> 
<table><thead><tr><th>算法</th><th>场景</th><th>输入</th><th>输出</th></tr></thead><tbody><tr><td>决策树 [1]</td><td>Wireless networks</td><td>单向延迟 + 包间延迟</td><td>链路丢包 \ 拥塞丢包</td></tr><tr><td>贝叶斯 [2]</td><td>Networks with Reordered events</td><td>丢失数据的RTT</td><td>重排序丢包 \ 拥塞丢包</td></tr><tr><td>Hidden Markov模型 [3]</td><td>Optica Burst Switching</td><td>两次突发发送之间成功接收包数</td><td>竞争丢包 \ 拥塞丢包</td></tr><tr><td>DT\Bagging\Boosting 神经网络 [4]</td><td>Wireless networks</td><td>队列延迟 + 到达时间差 + 包队列</td><td>无线网络丢包 \ 拥塞丢包</td></tr><tr><td>决策树\决策树集合\套袋\随机森林\额外树\多层\感知器\K-最近邻 [5］</td><td>Wireless networks</td><td>单向延迟的最大值、最小值、标准差 + 包间间隔的最大值、最小值、标准差</td><td>无线丢包 \ 拥塞丢包</td></tr></tbody></table> 
<h4><a id="211__37"></a>2.1.1 无线网络丢包模型</h4> 
<p>  在上述的表格中，丢包相关的算法模型有一大半是为了检测无线网络丢包的，其中几篇论文对无线网络丢包的产生机制解释为：</p> 
<ol><li>无线链路故障——例如：无线设备损坏等；</li><li>用户移动——例如：移动导致的多个不同无线设备间切换；</li><li>信号干扰、条件差——例如：偏远地区以及强磁场区域；</li></ol> 
<p>  针对上述描述的几个场景，我们可以这样理解：无线网络的数据包与无线信号强相关，而无线信号又与地域以及物理设备息息相关，因此整个无线传输过程伴随着很强的随机性以及聚簇性。正因为每次一丢就一大片一大片的，很多论文都会考虑使用包接收间隔来区分是否产生了排队，只要包接收间隔没有明显变化的丢包就会被归类于无线网络丢包。</p> 
<h4><a id="212__45"></a>2.1.2 多信道重排序丢包模型</h4> 
<p>  多信道重排序丢包是由于不同路由之间链路的网络状态变化导致的。例如：当某些包从一个高延迟的路由转向一个低延迟的路由时，假设其他路由的延迟不变就会产生乱序的情况（但大概率在p2p的模式下发生比较明显）。这类情况也是随机而又聚簇的形式，假设这类路由出现明显的乱序也会出现和无线丢包类似的情况（这个在现代的流媒体传输中尤为重要，因为流媒体传输要保证绝对的时序性）。</p> 
<h4><a id="213__48"></a>2.1.3 光脉冲网络切换模型</h4> 
<p>  光脉冲网络有一个很明显的特点，就是节点处的大量数据包短时间聚合。同时光信号与电信号之间的转换会在整个传输链路中不断出现。当数据聚集开始，超过光电转换信号，数据不会进行缓存而是直接被丢弃，由此形成了一个随机而又聚簇的丢包网络。</p> 
<p>  基于上述的几个模型特点，下图就是这些网络类型最常用的机器学习算法的控制图，基本是利用了对应网络的丢包特点来做明确的区分：<br> <img src="https://images2.imgbox.com/69/5c/MhPKIlUW_o.png" alt="在这里插入图片描述"><br>   由于区别上述网络造成的丢包与普通网络拥塞丢包的方式比较明确，就是使用接包间隔来确定是否产生排队，因此仅仅使用 <strong>决策树</strong> 这种方式也能实现比较明显的区分，但是网络的随机性导致我们几乎无法完全确定我们所做出的决策100%正确，因此根据这种随机性，我们很自然的考虑到了使用 <strong>贝叶斯定理</strong> 来提高准确度。而使用 <strong>隐马尔科夫模型</strong> 来区分光切换网络是因为作者发现光切换的 故障间突发数（NBBF） 遵循高斯分布，通过其特征值的变化我们可以使用迭代算法、隐马尔可夫模型进行推算，从而实现拥塞丢包与OBS丢包的检测。</p> 
<p>[1] I. El Khayat, P. Geurts, and G. Leduc, “Improving tcp in wireless net- works with an adaptive machine-learnt classifier of packet loss causes,” in International Conference on Research in Networking. Springer, 2005, pp. 549–560.</p> 
<p>[2] N. Fonseca and M. Crovella, “Bayesian packet loss detection for TCP,” in INFOCOM 2005. 24th Annual Joint Conference of the IEEE Computer and Communications Societies, 13-17 March 2005, Miami, FL, USA, 2005, pp. 1826–1837. [Online]. Available: https://doi.org/10.1109/INFCOM.2005.1498462</p> 
<p>[3] A. Jayaraj, T. Venkatesh, and C. S. R. Murthy, “Loss classification in optical burst switching networks using machine learning techniques: improving the performance of TCP,” IEEE J. Sel. Areas Commun., vol. 26, no. 6-Supplement, pp. 45–54, 2008. [Online]. Available: https://doi.org/10.1109/JSACOCN.2008.033508</p> 
<p>[4] “Enhancement of tcp over wired/wireless networks with packet loss classifiers inferred by supervised learning,” Wireless Networks, vol. 16, no. 2, pp. 273–290, 2010.</p> 
<p>[5] P. Geurts, I. E. Khayat, and G. Leduc, “A machine learning approach to improve congestion control over wireless computer networks,” in Proceedings of the 4th IEEE International Conference on Data Mining (ICDM 2004), 1-4 November 2004, Brighton, UK, 2004, pp. 383–386. [Online]. Available: https://doi.org/10.1109/ICDM.2004.10063</p> 
<h3><a id="22__67"></a>2.2 延迟检测算法</h3> 
<table><thead><tr><th>算法</th><th>场景</th><th>算法目的</th></tr></thead><tbody><tr><td>固定份额专家框架 [1]</td><td>Delay-sensitive networks</td><td>根据RTT的预测调整提高吞吐量</td></tr><tr><td>固定份额专家框架+加权移动平均[2]</td><td>Networks with fluctuating time scales</td><td>提高RTT的预测准确度</td></tr><tr><td>贝叶斯定理 [3]</td><td>Real-time video applications and wireless networks</td><td>通过RTT数据估计来调整发送码率</td></tr><tr><td>线性回归模型 [4]</td><td>Interactive video applications</td><td>根据统计模型计算出RTT，控制发送速率</td></tr></tbody></table> 
<h4><a id="221__75"></a>2.2.1 延迟原因</h4> 
<ul><li>硬件延迟：有线网络中，电信号传输和光信号传输的能力不同，因此自身传输信息的时间也不同；无线网络中，则是受无线信号传输的能力影响；</li><li>数据包传输距离：端到端的传输距离必然会增加数据传输的延迟；</li><li>网络路由跳数：数据包在网络中传输会进行多次跳转，每次跳转都需要消耗一定的时间，跳转次数越多延迟越大；</li><li>网络排队：数据量较大时，路由设备转发性能跟不上时，会产生明显的延迟；</li></ul> 
<p>  根据上述的特点，当数据产生拥塞时就会出现延迟持续变大的情况（可以理解为排队发送），这可以很明确的区分出物理硬件限制和区域限制，硬件和传输距离造成的延迟虽然大，但是整体是稳定的。</p> 
<p><img src="https://images2.imgbox.com/c9/69/eFoFH5mb_o.png" alt="在这里插入图片描述"></p> 
<p>[1] B. A. A. Nunes, K. Veenstra, W. Ballenthin, S. Lukin, and K. Obraczka,<br> “A machine learning framework for tcp round-trip time estimation,” EURASIP Journal on Wireless Communications and Networking, vol. 2014, no. 1, p. 47, 2014.<br> [2] Y. Edalat, J. S. Ahn, and K. Obraczka, “Smart experts for network state estimation,” IEEE Trans. Network and Service Management, vol. 13, no. 3, pp. 622–635, 2016. [Online]. Available: https: //doi.org/10.1109/TNSM.2016.2586506<br> [3] T. Dai, X. Zhang, and Z. Guo, “Learning-based congestion control for internet video communication over wireless networks,” in IEEE International Symposium on Circuits and Systems, ISCAS 2018, 27-30 May 2018, Florence, Italy, 2018, pp. 1–5. [Online]. Available: https://doi.org/10.1109/ISCAS.2018.8351530<br> [4] T. Dai, X. Zhang, Y. Zhang, and Z. Guo, “Statistical learning based congestion control for real-time video communication,” CoRR, vol. abs/1905.05998, 2019. [Online]. Available: http://arxiv.org/abs/1905. 05998</p> 
<h3><a id="23__94"></a>2.3 队列长度控制算法</h3> 
<p>  以上两种算法思想都属于比较老旧的，在整个拥塞算法发展中，很多工程师发现在发送拥塞的过程中最明显的特征就是会出现网络排队，而这个排队的队列可以作为有效的检测指标。为什么拥塞必然会发生排队呢？因为大部分的路由器和交换机都会存在缓存队列用以应付在大数据量转发能力受限时做临时存储，这样排队的过程中我们可以明显的观察到数据在不断地延迟增大。</p> 
<p><img src="https://images2.imgbox.com/e2/f5/HFEWG3w2_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="231__99"></a>2.3.1 论文提到的场景</h4> 
<ul><li>ATM网络：是一种异步传输模式（Asynchronous Transfer Mode）的通信网络。它是一种基于分组交换的网络技术，用于在高速数据传输中实现多种类型的通信服务。ATM网络通过将数据划分为固定大小的小数据包（称为单元）来传输数据。每个单元都包含了目标地址和控制信息，以确保数据在网络中的可靠传输。ATM网络具有高带宽、低延迟和灵活的服务质量控制等特点，因此在广域网和局域网等各种通信场景中得到广泛应用。</li><li>AQM是主动队列管理（Active Queue Management）的缩写，是一种网络拥塞控制的技术。它通过在网络路由器中实施一系列算法和策略来监测和管理网络中的队列长度，以避免网络拥塞的发生。AQM技术可以根据网络的拥塞程度动态地调整数据包的传输速率，从而提高网络的性能和吞吐量。AQM技术的目标是在保持网络稳定和公平性的同时，尽可能地减少数据包的丢失和延迟。常见的AQM算法包括RED（Random Early Detection）、ECN（Explicit Congestion Notification）和PI（Proportional Integral）等。这些算法通过监测队列长度、丢包率和延迟等指标来判断网络的拥塞状态，并根据预设的策略进行拥塞控制。AQM技术在各种网络环境中都得到了广泛的应用，包括有线网络和无线网络等。</li><li>NDN是命名数据网络（Named Data Networking）的缩写。它是一种新兴的网络架构，旨在改进当前基于IP的互联网架构。NDN通过将数据命名为网络中的基本单位，而不是以IP地址为基础，来实现更高效的数据传输和内容交付。在NDN中，数据包被命名为唯一的名称，而不是根据源和目的地的IP地址进行路由。这种命名方式使得数据包可以在网络中进行缓存和共享，从而提高数据传输的效率和可靠性。</li></ul> 
<h4><a id="232__105"></a>2.3.2 排队算法</h4> 
<p>  由上述的网络发展衍生出的拥塞控制的算法有以下几种：</p> 
<p>  Proportional Integral Derivative (PID)：PID是一种常用的队列长度管理技术，通过计算丢包概率来维护队列长度，以达到目标阈值。</p> 
<p>  Q学习（Q learning）：Q学习算法利用队列长度作为状态参数，通过学习不同策略来调整拥塞窗口大小，以最大化链路利用率。</p> 
<p>  Actor-Critic（AC）：AC算法基于演员-评论家框架，通过学习评估函数和策略函数来管理队列长度，以优化路由器决策和资源分配。</p> 
<p>  深度强化学习（Deep Reinforcement Learning，DRL）：DRL算法如DQL、DDPG和PPO等利用神经网络来表示状态和动作空间，通过学习调整队列长度以最大化吞吐量和最小化延迟。</p> 
<p>  论文中举例了它收集到的一些相关论述：</p> 
<table><thead><tr><th>算法</th><th>场景</th><th>描述</th></tr></thead><tbody><tr><td>神经网络算法[1][2]</td><td>在ATM网络中</td><td>通过预测未来的流量值来控制发送速率</td></tr><tr><td>神经模糊算法[3]</td><td>在ATM网络中</td><td>利用估计的平均队列长度计算丢包率，并控制发送速率</td></tr><tr><td>线性最小均方误差估计算法[4]</td><td>在支持AQM的网络中</td><td>建立长期流量流之间的关系，以估计未来的流量</td></tr><tr><td>归一化最小均方误差算法[5]</td><td>在支持AQM的网络中</td><td>采用自适应技术估计瞬时队列长度</td></tr><tr><td>深度置信网络算法[6]</td><td>在NDN中</td><td>基于对待处理兴趣表项的预测，计算平均队列长度</td></tr></tbody></table> 
<p>[1] B. Hariri and N. Sadati, “Nn-red: an aqm mechanism based on neural networks,” Electronics Letters, vol. 43, no. 19, pp. 1053–1055, 2007.<br> [2] W. Jang, J. Byun, and M. L. Hambaba, “An intelligent architecture for ATM traffic congestion control,” Journal of Intelligent and Fuzzy Systems, vol. 5, no. 2, pp. 155–165, 1997. [Online]. Available: https://doi.org/10.3233/IFS- 1997- 5206<br> [3] S.-J. Lee and C.-L. Hou, “A neural-fuzzy system for congestion control in atm networks,” IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 30, no. 1, pp. 2–9, 2000.<br> [4] Y. Gao, G. He, and J. C. Hou, “On exploiting traffic predictability in active queue management,” in Proceedings IEEE INFOCOM 2002, The 21st Annual Joint Conference of the IEEE Computer and Communications Societies, New York, USA, June 23-27, 2002, 2002, pp. 1630–1639. [Online]. Available: https://doi.org/10.1109/INFCOM. 2002.1019416<br> [5] A. Jain, A. Karandikar, and R. Verma, “An adaptive prediction based approach for congestion estimation in active queue management (APACE),” in Proceedings of the Global Telecommunications Conference, 2003. GLOBECOM ’03, San Francisco, CA, USA, 1-5 December 2003, 2003, pp. 4153–4157. [Online]. Available: https://doi.org/10.1109/GLOCOM.2003.1259009<br> [6] T. Liu, M. Zhang, J. Zhu, R. Zheng, R. Liu, and Q. Wu, “ACCP: adaptive congestion control protocol in named data networking based on deep learning,” Neural Computing and Applications, vol. 31, no. 9, pp. 4675–4683, 2019. [Online]. Available: https: //doi.org/10.1007/s00521- 018- 3408- 2</p> 
<h2><a id="_134"></a>三、基于无监督学习的算法</h2> 
<p>  无监督学习算法具备以下优势：</p> 
<ul><li>无需标注数据：无监督学习算法不需要标注的训练数据，可以直接从未标注的数据中学习模式和结构。这使得算法更加灵活，可以适应各种不同类型的数据。</li><li>发现隐藏模式：无监督学习算法可以发现数据中的隐藏模式和结构，而不需要事先知道要寻找的特定模式。这有助于发现数据中的潜在关联和规律。</li><li>数据探索和可视化：无监督学习算法可以帮助进行数据探索和可视化，从而更好地理解数据的特征和分布。这有助于发现数据中的异常值、聚类和分类等。</li><li>降维和特征提取：无监督学习算法可以用于降维和特征提取，从而减少数据的维度和复杂性。这有助于提高算法的效率和准确性。</li><li>适应不确定性和变化：无监督学习算法对于不确定性和变化的数据具有较强的适应能力。它们可以自动调整模型和参数，以适应数据的变化和漂移。</li></ul> 
<h3><a id="31__144"></a>3.1 丢包算法</h3> 
<p>  论文中提到了两种基于非监督学习的丢包算法：</p> 
<table><thead><tr><th>算法</th><th>场景</th><th>输入</th></tr></thead><tbody><tr><td>隐马尔可夫模型[1][2]</td><td>Wired/wireless networks</td><td>使用延迟-丢包对将数据聚类成几个组，并为每个组分配特定的发送速率</td></tr><tr><td>期望最大化聚类[3]</td><td>OBS网络</td><td>将丢包聚类为争用丢包和拥塞丢包，并分别调整环境</td></tr></tbody></table> 
<p>[1] J. Liu, I. Matta, and M. Crovella, “End-to-end inference of loss nature in a hybrid wired/wireless environment,” 2003.<br> [2] D. Barman and I. Matta, “Model-based loss inference by tcp over heterogeneous networks,” in Proceedings of WiOpt, 2004, pp. 364–73.<br> [3] A. Jayaraj, T. Venkatesh, and C. S. R. Murthy, “Loss classification in optical burst switching networks using machine learning techniques: improving the performance of TCP,” IEEE J. Sel. Areas Commun., vol. 26, no. 6-Supplement, pp. 45–54, 2008. [Online]. Available: https://doi.org/10.1109/JSACOCN.2008.033508</p> 
<h3><a id="32__157"></a>3.2 延迟算法</h3> 
<p>  论文中只提到了一种基于非监督学习的延迟算法：</p> 
<table><thead><tr><th>算法</th><th>场景</th><th>输入</th></tr></thead><tbody><tr><td>K-means[1][2]</td><td>Vehicular ad hoc networks</td><td>将数据根据消息大小、消息的有效性、车辆和RSU之间的距离、消息类型和消息发送方向等特征聚类，并为每个聚类分配发送速率。</td></tr></tbody></table> 
<hr> 
<pre><code>注释：

Vehicular ad hoc networks (VANETs) 是指车辆之间通过无线通信建立的自组织网络。
这些网络允许车辆之间进行直接通信，以实现交通信息共享、车辆安全和智能交通系统等应用。
VANETs 可以提供实时的车辆位置和速度信息，以及交通拥堵、事故和路况等信息，从而提高交通效率和安全性。
VANETs 还可以支持车辆与基础设施（如交通信号灯和路边设备）之间的通信，以实现更高级的交通管理和服务。
</code></pre> 
<hr> 
<h3><a id="33__172"></a>3.3 基于窗口更新的算法</h3> 
<p><img src="https://images2.imgbox.com/b0/b0/3fDptYQD_o.png" alt="在这里插入图片描述"></p> 
<p>  基于窗口更新的算法在传统的拥塞控制中应用广泛，相关的tcp拥塞控制算法都会使用到窗口更新。它可以有效的控制传输数据量，但是网络剧烈变化的情况下可能会引起吞吐量的明显下降。</p> 
<p>  文中提到了挺多的基于窗口变化的论文：</p> 
<p>注：AWNs是指Ad hoc Wireless Networks，即自组织无线网络。这是一种没有固定基础设施的移动无线节点集合，资源有限，处理能力有限，移动性不可预测，并且具有高度动态性的网络。</p> 
<table><thead><tr><th>算法</th><th>场景</th><th>输入</th></tr></thead><tbody><tr><td>演员评论家策略[1]</td><td>ATM networks</td><td>使用演员-评论家算法来最小化数据包丢失率并保持视频/语音质量</td></tr><tr><td>Q函数学习法+sarsa值函数强化学习[2]</td><td>SDN</td><td>训练基于Q学习的离线策略方法和基于Sarsa的在线策略方法来控制拥塞。两种算法都实现了良好的链路利用率</td></tr><tr><td>深度Q学习法[3]</td><td>NDN</td><td>通过多样化的内容学习最佳CC策略在NDN中</td></tr><tr><td>DDPG算法(Deep Deterministic Policy Gradient)[4]</td><td>卫星通信网络</td><td>提出了一种智能算法，用于改善低地球轨道卫星通信的性能。</td></tr><tr><td>Fuzzy Kanerva-based Q Learning[5]</td><td>物理网</td><td>减少存储算法历史记录所需的内存量，以支持更大的状态空间和动作空间。</td></tr><tr><td>Q学习[6]</td><td>有线网络中的有缓冲的链路</td><td>输入的数据包括确认间隔时间、数据包发送间隔时间、当前往返时延（RTT）、最小往返时延、慢启动阈值和拥塞窗口大小的比率，以获取调整信息。</td></tr><tr><td>有限行动集学习自动机[7]</td><td>AWNs</td><td>输入数据，包括 ACK 的到达间隔时间和复制数据包并输出窗口大小</td></tr><tr><td>连续动作集学习自动机[8]</td><td>AWNs</td><td>维持动作概率分布</td></tr><tr><td>Q学习[9]</td><td>AWNs</td><td>虑吞吐量和 RTT状态空间到行动空间的投影</td></tr><tr><td>深度Q学习法[10]</td><td>无线网络</td><td>输入由 CWND、RTT 和到达间隔时间，然后输出发送速率</td></tr><tr><td>连续动作集学习自动机[11]</td><td>无线网络：多跳、单跳，如无线局域网、蜂窝网络和卫星网络。</td><td>维持动作概率分布</td></tr><tr><td>演员评论家策略[12]</td><td>具有时域流量变化的网络</td><td>基于此设计多智能体拥塞控制器演员-评论家框架</td></tr><tr><td>演员评论家策略[13]</td><td>具有时域流量变化的网络</td><td>应用于基于LSTM的表示网络，与相比，显示出有效性和优越性著名的 MPTCP CC 算法，例如 wVegas</td></tr><tr><td>异步优势演员-评论家算法[14]</td><td>类似游戏、视频传输流</td><td>提出一种支持延迟的部分动作学习方法以及部分奖励</td></tr><tr><td>Q学习[15]</td><td>连续或剧烈的空间变化网络</td><td>基于Kanerva抽象状态空间和动作空间编码</td></tr><tr><td>近端策略优化[16]</td><td>视频直播等互联网服务，虚拟现实和物联网</td><td>检测网络和数据模式（例如延迟）以获得必要的调整</td></tr><tr><td>Q学习[17]</td><td>动态网络</td><td>检测平均数据包到达间隔、平均ACK调整CWND大小的间隔和平均RTT</td></tr><tr><td>异步优势演员-评论家算法[18]</td><td>流量规模多样化的网络</td><td>采用RL算法配置初始窗口和 CC 政策</td></tr></tbody></table> 
<p>[1] A. A. Tarraf, I. W. Habib, and T. N. Saadawi, “Reinforcement learning- based neural network congestion controller for atm networks,” in Pro- ceedings of MILCOM’95, vol. 2. IEEE, 1995, pp. 668–672.<br> [2] R. Jin, J. Li, X. Tuo, W. Wang, and X. Li, “A congestion control method of SDN data center based on reinforcement learning,” Int. J. Communication Systems, vol. 31, no. 17, 2018. [Online]. Available: https://doi.org/10.1002/dac.3802<br> [3] D. Lan, X. Tan, J. Lv, Y. Jin, and J. Yang, “A deep reinforcement learning based congestion control mechanism for NDN,” in 2019 IEEE International Conference on Communications, ICC 2019, Shanghai,China, May 20-24, 2019, 2019, pp. 1–7. [Online]. Available:<br> https://doi.org/10.1109/ICC.2019.8761737<br> [4] Z. Ji, “Self-learning congestion control of mptcp in satellites communi- cations,” in IWCMC 2019, 2019.<br> [5] W. Li, F. Zhou, W. Meleis, and K. R. Chowdhury, “Learning-based and data-driven TCP design for memory-constrained iot,” in International Conference on Distributed Computing in Sensor Systems, DCOSS 2016, Washington, DC, USA, May 26-28, 2016, 2016, pp. 199–205. [Online]. Available: https://doi.org/10.1109/DCOSS.2016.8<br> [6] Y. Kong, H. Zang, and X. Ma, “Improving TCP congestion control with machine intelligence,” in Proceedings of the 2018 Workshop on Network Meets AI &amp; ML, NetAI@SIGCOMM 2018, Budapest, Hungary, August 24, 2018, 2018, pp. 60–66. [Online]. Available: https://doi.org/10.1145/3229543.3229550<br> [7] V. Badarla, B. S. Manoj, and C. S. R. Murthy, “Learning- tcp: A novel learning automata based reliable transport protocol for ad hoc wireless networks,” in 2nd International Conference on Broadband Networks (BROADNETS 2005), 3-7 October 2005, Boston, Massachusetts, USA, 2005, pp. 521–530. [Online]. Available: https://doi.org/10.1109/ICBN.2005.1589652<br> [8] V. Badarla and C. S. R. Murthy, “Learning-tcp: A stochastic approach for efficient update in TCP congestion window in ad hoc wireless networks,” J. Parallel Distributed Comput., vol. 71, no. 6, pp. 863–878, 2011. [Online]. Available: https://doi.org/10.1016/j.jpdc.2010.12.012<br> [9] H. Jiang, Y. Luo, Q. Zhang, M. Yin, and C. Wu, “Tcp-gvegas with prediction and adaptation in multi-hop ad hoc networks,” Wireless Networks, vol. 23, no. 5, pp. 1535–1548, 2017.<br> [10] K. Xiao, S. Mao, and J. K. Tugnait, “Tcp-drinc: Smart congestion control based on deep reinforcement learning,” IEEE Access, vol. 7, pp. 11892–11904, 2019. [Online]. Available: https://doi.org/10.1109/ ACCESS.2019.2892046<br> [11] V. Badarla and C. S. R. Murthy, “A novel learning based solution for efficient data transport in heterogeneous wireless networks,” Wireless Networks, vol. 16, no. 6, pp. 1777–1798, 2010.<br> [12] K. Hwang, M. Hsiao, C. Wu, and S. Tan, “Multi-agent congestion control for high-speed networks using reinforcement co-learning,” in Advances in Neural Networks - ISNN 2005, Second International Symposium on Neural Networks, Chongqing, China, May 30 - June 1, 2005, Proceedings, Part III, 2005, pp. 379–384. [Online]. Available: https://doi.org/10.1007/11427469 61<br> [13] Z. Xu, J. Tang, C. Yin, Y. Wang, and G. Xue, “Experience-driven congestion control: When multi-path TCP meets deep reinforcement learning,” IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1325–1336, 2019. [Online]. Available: https://doi.org/10.1109/JSAC.2019.2904358<br> [14] M. Shaio, S. Tan, K. Hwang, and C. Wu, “A reinforcement learning approach to congestion control of high-speed multimedia networks,” Cybernetics and Systems, vol. 36, no. 2, pp. 181–202, 2005. [Online]. Available: https://doi.org/10.1080/01969720590897224<br> [15] W. Li, F. Zhou, W. Meleis, and K. R. Chowdhury, “Dynamic generalization kanerva coding in reinforcement learning for TCP congestion control design,” in Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, AAMAS 2017, Sa ̃o Paulo, Brazil, May 8-12, 2017, 2017, pp. 1598–1600. [Online]. Available: http://dl.acm.org/citation.cfm?id=3091375<br> [16] Z. Li, P. Liu, C. Xu, H. Duan, and W. Wang, “Reinforcement<br> learning-based variable speed limit control strategy to reduce traffic congestion at freeway recurrent bottlenecks,” IEEE Trans. Intell. Transp. Syst., vol. 18, no. 11, pp. 3204–3217, 2017. [Online]. Available: https://doi.org/10.1109/TITS.2017.2687620<br> [17] W. Li, F. Zhou, K. R. Chowdhury, and W. Meleis, “QTCP: adaptive congestion control with reinforcement learning,” IEEE Trans. Network Science and Engineering, vol. 6, no. 3, pp. 445–458, 2019. [Online]. Available: https://doi.org/10.1109/TNSE.2018.2835758<br> [18] X. Nie, Y. Zhao, Z. Li, G. Chen, K. Sui, J. Zhang, Z. Ye, and D. Pei, “Dynamic TCP initial windows and congestion control schemes through reinforcement learning,” IEEE J. Sel. Areas Commun., vol. 37, no. 6, pp. 1231–1247, 2019. [Online]. Available: https://doi.org/10.1109/JSAC.2019.2904350</p> 
<h2><a id="34__223"></a>3.4 基于队列长度的算法</h2> 
<p><img src="https://images2.imgbox.com/f2/ba/GqZmod5A_o.png" alt="在这里插入图片描述"></p> 
<p>  基于队列长度的非监督学习算法是通过队列排队的判断来确定拥塞产生与否的，但是在不同的网络传输中有很多网络不会进行排队就产生丢包，这里大部分还是针对存在队列排队的网络。</p> 
<p>关键算法：</p> 
<ul><li>PID：PID是比例-积分-微分（Proportional-Integral-Derivative）的缩写，是一种常用的控制算法，广泛应用于工程和自动化系统中。它是一种反馈控制算法，通过比较期望设定值与实际过程变量之间的误差信号来连续计算。PID控制器根据三个组成部分（比例、积分和微分）调整控制输出。</li></ul> 
<table><thead><tr><th>算法</th><th>场景</th><th>输入</th></tr></thead><tbody><tr><td>PID控制器[1]</td><td>支持主动队列管理的网络</td><td>使用PID通过稳定路由器队列长度来适应网络中的参数</td></tr><tr><td>自适应神经元PID[2]</td><td>支持主动队列管理的网络</td><td>给定不同的流量负载、场景、RTT、瓶颈链路容量，维护目标队列长度附近的队列长度</td></tr><tr><td>Q学习[3]</td><td>支持主动队列管理的网络</td><td>使用 RL 根据流量历史记录优化路由器决策</td></tr><tr><td>神经元PL[4]</td><td>支持主动队列管理的网络</td><td>基于队列管理控制队列长度，最大化链路利用率</td></tr><tr><td>神经网络PID控制器[5]</td><td>支持主动队列管理的网络</td><td>根据学习率，计算丢弃概率</td></tr><tr><td>Q学习[6]</td><td>容错网络</td><td>利用拥塞状态来支持拥塞通知</td></tr></tbody></table> 
<p>[1] J. Sun, S. Chan, K.-T. Ko, G. Chen, and M. Zukerman, “Neuron pid: a robust aqm scheme,” in Proceedings of ATNAC, vol. 2006. Citeseer, 2006, pp. 259–262.<br> [2] J. Sun and M. Zukerman, “An adaptive neuron AQM for a stable internet,” in NETWORKING 2007. Ad Hoc and Sensor Networks, Wireless Networks, Next Generation Internet, 6th International IFIP-TC6 Networking Conference, Atlanta, GA, USA, May 14- 18, 2007, Proceedings, 2007, pp. 844–854. [Online]. Available: https://doi.org/10.1007/978-3-540-72606-7 72<br> [3] S. Masoumzadeh, G. Taghizadeh, K. Meshgi, and S. Shiry, “Deep blue: A fuzzy q-learning enhanced active queue management scheme,” in 2009 International Conference on Adaptive and Intelligent Systems. IEEE, 2009, pp. 43–48.<br> [4] C. Zhou, D. Di, Q. Chen, and J. Guo, “An adaptive aqm algorithm based on neuron reinforcement learning,” in 2009 IEEE International<br> Conference on Control and Automation. IEEE, 2009, pp. 1342–1346.<br> [5] Q.YanandQ.Lei,“Anewactivequeuemanagementalgorithmbasedon self-adaptive fuzzy neural-network pid controller,” in 2011 International Conference on Internet Technology and Applications. IEEE, 2011, pp. 1–4.<br> [6] A. P. Silva, K. Obraczka, S. Burleigh, and C. M. Hirata, “Smart congestion control for delay- and disruption tolerant networks,” in 13th Annual IEEE International Conference on Sensing, Communication, and Networking, SECON 2016, London, United Kingdom, June 27-30, 2016, 2016, pp. 1–9. [Online]. Available: https://doi.org/10.1109/ SAHCN.2016.7733018</p> 
<h2><a id="_249"></a>四、实验</h2> 
<p>  文章中还对几个算进行了对比仿真实验，具体的算法分析就不写了（看这文章确实累，有点长，粘点结果混一下）。</p> 
<p><img src="https://images2.imgbox.com/3b/6e/ALB36gSK_o.png" alt="在这里插入图片描述"></p> 
<p>  文章根据上表的几个场景进行对比仿真实验，相关结果：<br> <img src="https://images2.imgbox.com/c5/60/HZf7sV9r_o.png" alt="在这里插入图片描述"></p> 
<p>CWND：在三种基于 RL 的 CC 算法中，它们在三种场景中的差异很小，如图 8 所示。此外，我们观察到基于 RL 的 CC 算法的 CWND 的大小远大于基于规则的 CC场景 I 和场景 II 中的算法，都具有预期的高 BPD。而在场景三中，这四种算法之间没有太大区别。从图 13 到图 15，CDF 图显示了 CWND 在三种情况下的分布。正如预期的那样，在场景 I 和场景 II 中，当应用基于 RL 的 CC 算法时，CWND 的大小往往会更大。</p> 
<p>吞吐量：理论上，由于场景一和场景二中基于强化学习的 CC 算法的 CWND 平均值有所增加，基于强化学习的 CC 算法的吞吐量有望超过 NewReno 的吞吐量。如图9所示，我们的猜测得到了验证。在场景I和场景II中，当使用基于RL的CC算法时，吞吐量得到提高。而在场景 III 中，基于强化学习的 CC 算法没有表现出任何优势。对于吞吐量的详细分布和时间线，图 16 至图 21 中的更多图表补充了结果和解释。</p> 
<p>RTT：NewReno的RTT小且稳定，代表了RTT的标杆。在三种场景中，与NewReno相比，基于RL的CC算法的网络中RTT更高，如图10、25、26和27所示。由于CWND的增加在基于RL的CC算法中更为激进，因此可以理解RTT 较高。然而，从图22到图24可以看出，与场景一和场景二的吞吐量增量相比，RTT的增量是有限的。</p> 
<p>丢包率：如图11所示，NewReno的丢包率几乎为零，而基于RL的CC算法的网络丢包率极低。此外，从图 28 到图 30，分布信息显示基于 RL 的 CC 算法中的丢包率有所增加。考虑到基于 RL 的 CC 算法的攻击性，有限的丢包率是可以理解的。</p> 
<p>（文章还有很多实验结果，这里不一一展示，有兴趣自己去看吧。）</p> 
<p>  最后：<br>   由于传统CC算法在动态网络中的局限性，基于学习的CC算法最近在学术界出现了趋势。在本文中，我们回顾了基于学习的 CC 算法的最新技术，并针对不同的基于 RL 的 CC 算法进行了模拟，作为基于学习的 CC 算法的代表。仿真结果表明，在高带宽、低延迟网络等不同场景下，基于强化学习的 CC 算法比传统 CC 算法表现出更好的性能。我们提出并讨论了当前基于强化学习的 CC 算法在实际部署中的局限性，并概述了一些可能在未来研究中使用的方法。我们确定了与基于学习的 CC 算法相关的挑战和趋势，包括处理与基于 RL 的 CC 算法相关的工程问题。未来，网络环境预计将日益复杂。鉴于此，显然需要解决这种复杂性和灵活性。为了提高性能和鲁棒性，需要进一步研究来处理计算时间、数据存储和预先设计的参数等问题。我们认为需要具有通用学习平台的轻量级且高效的基于学习的模型，这将是未来的研究重点。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b4ae0d2c9bddcf124a6547b11cf6e1fb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C&#43;&#43;常用格式化输出转换</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c4607f39107308651d640b854d094976/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MOS管的那些事——很火的MOS管电路工作原理及详解，建议收藏！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>