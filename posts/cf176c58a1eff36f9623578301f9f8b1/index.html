<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection" />
<meta property="og:description" content="目录
一、论文阅读笔记：
1、摘要：
2、主要贡献点：
3、方法：
3.1 网络的总体框架图：
3.2 Transformer-based Information Propagation Path (TIPP)
3.3 Intra-Modal/Cross-Scale Self-Attention (IMSA/CSSA)
Q1: MHSA计算复杂度较高
A1:Patch-wise Token Re-Embedding (PTRE)
Q2:目前的MHSA只考虑空间视图上的特征对齐，而忽略了通道视图的潜在值
A2:View-Mixed Attention (VMA)
3.4 Inter-Modal Cross-Attention (IMCA)
4、实验：
二、代码复现
1、实验细节：
2、数据集：
3、实验步骤：
3.1 将对应的代码和数据集上传到服务器上
3.2 将根目录下的datasets.py里的路径更改成自己的路径
3.3 更改对应的./configs/rgbd-2dataset.py中的路径信息
3.4 将预训练模型从给出的网址上下载下来，上传到对应文件夹 (/root/autodl-tmp/CAVER/pretrained)
3.5 安装对应的module
3.6 在两个数据集上的训练结果
三、BUGS
问题1：assert path.endswith(&#34;.jpg&#34;) or path.endswith(&#34;.png&#34;) or path,endswith(&#34;.bmp&#34;) AssertionError
问题2： TypeError: FormatCode() got an key word &#39;verify&#39;
​编辑
​编辑 问题3：RuntimeError: CuDA error: no kernel imade is available for execution on the deviceCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrectFor debugging consider passing CUDA LAUNCH BLOCKING=1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/cf176c58a1eff36f9623578301f9f8b1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-29T09:30:36+08:00" />
<meta property="article:modified_time" content="2023-12-29T09:30:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A" rel="nofollow">一、论文阅读笔记：</a></p> 
<p id="1%E3%80%81%E6%91%98%E8%A6%81%EF%BC%9A-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E6%91%98%E8%A6%81%EF%BC%9A" rel="nofollow">1、摘要：</a></p> 
<p id="2%E3%80%81%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE%E7%82%B9%EF%BC%9A-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE%E7%82%B9%EF%BC%9A" rel="nofollow">2、主要贡献点：</a></p> 
<p id="3%E3%80%81%E6%96%B9%E6%B3%95%EF%BC%9A-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E6%96%B9%E6%B3%95%EF%BC%9A" rel="nofollow">3、方法：</a></p> 
<p id="3.1%20%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%BB%E4%BD%93%E6%A1%86%E6%9E%B6%E5%9B%BE%EF%BC%9A-toc" style="margin-left:80px;"><a href="#3.1%20%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%BB%E4%BD%93%E6%A1%86%E6%9E%B6%E5%9B%BE%EF%BC%9A" rel="nofollow">3.1 网络的总体框架图：</a></p> 
<p id="3.2%C2%A0Transformer-based%20Information%20Propagation%20Path%20(TIPP)-toc" style="margin-left:80px;"><a href="#3.2%C2%A0Transformer-based%20Information%20Propagation%20Path%20%28TIPP%29" rel="nofollow">3.2 Transformer-based Information Propagation Path (TIPP)</a></p> 
<p id="3.3%20Intra-Modal%2FCross-Scale%20Self-Attention%20(IMSA%2FCSSA)-toc" style="margin-left:80px;"><a href="#3.3%20Intra-Modal%2FCross-Scale%20Self-Attention%20%28IMSA%2FCSSA%29" rel="nofollow">3.3 Intra-Modal/Cross-Scale Self-Attention (IMSA/CSSA)</a></p> 
<p id="Q1%3A%20MHSA%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E8%BE%83%E9%AB%98-toc" style="margin-left:160px;"><a href="#Q1%3A%20MHSA%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E8%BE%83%E9%AB%98" rel="nofollow">Q1: MHSA计算复杂度较高</a></p> 
<p id="A1%3APatch-wise%20Token%20Re-Embedding%20(PTRE)-toc" style="margin-left:160px;"><a href="#A1%3APatch-wise%20Token%20Re-Embedding%20%28PTRE%29" rel="nofollow">A1:Patch-wise Token Re-Embedding (PTRE)</a></p> 
<p id="Q2%3A%E7%9B%AE%E5%89%8D%E7%9A%84MHSA%E5%8F%AA%E8%80%83%E8%99%91%E7%A9%BA%E9%97%B4%E8%A7%86%E5%9B%BE%E4%B8%8A%E7%9A%84%E7%89%B9%E5%BE%81%E5%AF%B9%E9%BD%90%EF%BC%8C%E8%80%8C%E5%BF%BD%E7%95%A5%E4%BA%86%E9%80%9A%E9%81%93%E8%A7%86%E5%9B%BE%E7%9A%84%E6%BD%9C%E5%9C%A8%E5%80%BC%E3%80%82-toc" style="margin-left:160px;"><a href="#Q2%3A%E7%9B%AE%E5%89%8D%E7%9A%84MHSA%E5%8F%AA%E8%80%83%E8%99%91%E7%A9%BA%E9%97%B4%E8%A7%86%E5%9B%BE%E4%B8%8A%E7%9A%84%E7%89%B9%E5%BE%81%E5%AF%B9%E9%BD%90%EF%BC%8C%E8%80%8C%E5%BF%BD%E7%95%A5%E4%BA%86%E9%80%9A%E9%81%93%E8%A7%86%E5%9B%BE%E7%9A%84%E6%BD%9C%E5%9C%A8%E5%80%BC%E3%80%82" rel="nofollow">Q2:目前的MHSA只考虑空间视图上的特征对齐，而忽略了通道视图的潜在值</a></p> 
<p id="A1%3AView-Mixed%20Attention%20(VMA)-toc" style="margin-left:160px;"><a href="#A1%3AView-Mixed%20Attention%20%28VMA%29" rel="nofollow">A2:View-Mixed Attention (VMA)</a></p> 
<p id="%C2%A03.4%20Inter-Modal%20Cross-Attention%20(IMCA)-toc" style="margin-left:80px;"><a href="#%C2%A03.4%20Inter-Modal%20Cross-Attention%20%28IMCA%29" rel="nofollow"> 3.4 Inter-Modal Cross-Attention (IMCA)</a></p> 
<p id="4%E3%80%81%E5%AE%9E%E9%AA%8C%EF%BC%9A-toc" style="margin-left:40px;"><a href="#4%E3%80%81%E5%AE%9E%E9%AA%8C%EF%BC%9A" rel="nofollow">4、实验：</a></p> 
<p id="%E4%BA%8C%E3%80%81%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0" rel="nofollow">二、代码复现</a></p> 
<p id="1%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82%EF%BC%9A-toc" style="margin-left:120px;"><a href="#1%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82%EF%BC%9A" rel="nofollow">1、实验细节：</a></p> 
<p id="2%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A-toc" style="margin-left:120px;"><a href="#2%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A" rel="nofollow">2、数据集：</a></p> 
<p id="3%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4%EF%BC%9A-toc" style="margin-left:120px;"><a href="#3%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4%EF%BC%9A" rel="nofollow">3、实验步骤：</a></p> 
<p id="3.1%20%E5%B0%86%E5%AF%B9%E5%BA%94%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E4%BC%A0%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A-toc" style="margin-left:160px;"><a href="#3.1%20%E5%B0%86%E5%AF%B9%E5%BA%94%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E4%BC%A0%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A" rel="nofollow">3.1 将对应的代码和数据集上传到服务器上</a></p> 
<p id="3.2%20%E5%B0%86%E6%A0%B9%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84datasets.py%E9%87%8C%E7%9A%84%E8%B7%AF%E5%BE%84%E6%9B%B4%E6%94%B9%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E8%B7%AF%E5%BE%84-toc" style="margin-left:160px;"><a href="#3.2%20%E5%B0%86%E6%A0%B9%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84datasets.py%E9%87%8C%E7%9A%84%E8%B7%AF%E5%BE%84%E6%9B%B4%E6%94%B9%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E8%B7%AF%E5%BE%84" rel="nofollow">3.2 将根目录下的datasets.py里的路径更改成自己的路径</a></p> 
<p id="3.3%20%E6%9B%B4%E6%94%B9%E5%AF%B9%E5%BA%94%E7%9A%84.%2Fconfigs%2Frgbd-2dataset.py%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84%E4%BF%A1%E6%81%AF%EF%BC%88%E8%BF%99%E4%B8%80%E9%83%A8%E5%88%86%E6%9C%89%E7%96%91%E9%97%AE%EF%BC%89-toc" style="margin-left:160px;"><a href="#3.3%20%E6%9B%B4%E6%94%B9%E5%AF%B9%E5%BA%94%E7%9A%84.%2Fconfigs%2Frgbd-2dataset.py%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84%E4%BF%A1%E6%81%AF%EF%BC%88%E8%BF%99%E4%B8%80%E9%83%A8%E5%88%86%E6%9C%89%E7%96%91%E9%97%AE%EF%BC%89" rel="nofollow">3.3 更改对应的./configs/rgbd-2dataset.py中的路径信息</a></p> 
<p id="3.3%20%E5%B0%86%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8E%E7%BB%99%E5%87%BA%E7%9A%84%E7%BD%91%E5%9D%80%E4%B8%8A%E4%B8%8B%E8%BD%BD%E4%B8%8B%E6%9D%A5%EF%BC%8C%E4%B8%8A%E4%BC%A0%E5%88%B0%E5%AF%B9%E5%BA%94%E6%96%87%E4%BB%B6%E5%A4%B9%20(%2Froot%2Fautodl-tmp%2FCAVER)%2Fpretrained-toc" style="margin-left:160px;"><a href="#3.3%20%E5%B0%86%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8E%E7%BB%99%E5%87%BA%E7%9A%84%E7%BD%91%E5%9D%80%E4%B8%8A%E4%B8%8B%E8%BD%BD%E4%B8%8B%E6%9D%A5%EF%BC%8C%E4%B8%8A%E4%BC%A0%E5%88%B0%E5%AF%B9%E5%BA%94%E6%96%87%E4%BB%B6%E5%A4%B9%20%28%2Froot%2Fautodl-tmp%2FCAVER%29%2Fpretrained" rel="nofollow">3.4 将预训练模型从给出的网址上下载下来，上传到对应文件夹 (/root/autodl-tmp/CAVER/pretrained)</a></p> 
<p id="3.3%20%E5%AE%89%E8%A3%85%E5%AF%B9%E5%BA%94%E7%9A%84module-toc" style="margin-left:160px;"><a href="#3.3%20%E5%AE%89%E8%A3%85%E5%AF%B9%E5%BA%94%E7%9A%84module" rel="nofollow">3.5 安装对应的module</a></p> 
<p id="3.4%20%E5%9C%A8%E4%B8%A4%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C-toc" style="margin-left:160px;"><a href="#3.4%20%E5%9C%A8%E4%B8%A4%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C" rel="nofollow">3.6 在两个数据集上的训练结果</a></p> 
<p id="%E4%B8%89%E3%80%81BUGS-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81BUGS" rel="nofollow">三、BUGS</a></p> 
<p id="%E9%97%AE%E9%A2%981%EF%BC%9Aassert%20path.endswith(%22.jpg%22)%20or%20path.endswith(%22.png%22)%20or%20path%2Cendswith(%22.bmp%22)%20AssertionError-toc" style="margin-left:120px;"><a href="#%E9%97%AE%E9%A2%981%EF%BC%9Aassert%20path.endswith%28%22.jpg%22%29%20or%20path.endswith%28%22.png%22%29%20or%20path%2Cendswith%28%22.bmp%22%29%20AssertionError" rel="nofollow">问题1：assert path.endswith(".jpg") or path.endswith(".png") or path,endswith(".bmp") AssertionError</a></p> 
<p id="%E9%97%AE%E9%A2%982%EF%BC%9A%20TypeError%3A%20FormatCode()%20got%20an%20key%20word%20'verify'-toc" style="margin-left:120px;"><a href="#%E9%97%AE%E9%A2%982%EF%BC%9A%20TypeError%3A%20FormatCode%28%29%20got%20an%20key%20word%20'%20rel=">问题2： TypeError: FormatCode() got an key word 'verify'</a></p> 
<p id="%E2%80%8B%E7%BC%96%E8%BE%91-toc" style="margin-left:120px;"><a href="#%E2%80%8B%E7%BC%96%E8%BE%91" rel="nofollow">​编辑</a></p> 
<p id="%E2%80%8B%E7%BC%96%E8%BE%91%20%E9%97%AE%E9%A2%983%EF%BC%9ARuntimeError%3A%20CuDA%20error%3A%20no%20kernel%20imade%20is%20available%20for%20execution%20on%20the%20deviceCUDA%20kernel%20errors%20might%20be%20asynchronously%20reported%20at%20some%20other%20API%20call%2Cso%20the%20stacktrace%20below%20might%20be%20incorrectFor%20debugging%20consider%20passing%20CUDA%20LAUNCH%20BLOCKING%3D1.-toc" style="margin-left:120px;"><a href="#%E2%80%8B%E7%BC%96%E8%BE%91%20%E9%97%AE%E9%A2%983%EF%BC%9ARuntimeError%3A%20CuDA%20error%3A%20no%20kernel%20imade%20is%20available%20for%20execution%20on%20the%20deviceCUDA%20kernel%20errors%20might%20be%20asynchronously%20reported%20at%20some%20other%20API%20call%2Cso%20the%20stacktrace%20below%20might%20be%20incorrectFor%20debugging%20consider%20passing%20CUDA%20LAUNCH%20BLOCKING%3D1." rel="nofollow">​编辑 问题3：RuntimeError: CuDA error: no kernel imade is available for execution on the deviceCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrectFor debugging consider passing CUDA LAUNCH BLOCKING=1.</a></p> 
<p id="%E5%9B%9B%E3%80%81%E6%8F%90%E9%97%AE-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E6%8F%90%E9%97%AE" rel="nofollow">四、提问</a></p> 
<hr id="hr-toc"> 
<hr> 
<p></p> 
<p>发表期刊及年份：TIP 2023</p> 
<p>代码网址：https://github.com/lartpang/CAVER</p> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A"><strong>一、论文阅读笔记：</strong></h2> 
<h3 id="1%E3%80%81%E6%91%98%E8%A6%81%EF%BC%9A">1、摘要：</h3> 
<p>        大多数现有的双模态（RGB-D 和 RGB-T）显着对象检测方法利用卷积操作并构建复杂的交织融合结构来实现跨模态信息集成。卷积操作的固有局部连通性将基于卷积的方法的性能限制在天花板上。在这项工作中，我们从全局信息对齐和转换的角度重新思考这些任务。具体来说，所提出的跨模态视图混合转换器 (CAVER) 级联了几个跨模态集成单元来构建自上而下的基于转换器的信息传播路径。CAVER 将多尺度和多模态特征集成视为建立在新颖的视图混合注意力机制之上的序列到序列上下文传播和更新过程。此外，考虑到输入令牌数量的二次复杂度w.r.t，我们设计了一种无参数补丁令牌重新嵌入策略来简化操作。RGB-D 和 RGB-T SOD 数据集的广泛实验结果表明，当配备所提出的组件时，这种简单的双流编码器-解码器框架可以超越最近最先进的方法。代码和预训练模型将在链接处可用。</p> 
<h3 id="2%E3%80%81%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE%E7%82%B9%EF%BC%9A">2、主要贡献点：</h3> 
<ol><li>我们引入了transformer从序列到序列的角度重新思考双模态SOD建模，从而获得更好的可解释性。</li><li>我们构建了一个自顶向下的基于transformer的信息传播路径，通过视图混合注意块增强，可以对齐RGB和深度/热模态的特征，充分利用空间和通道视图之间的模态间和模态内信息。</li><li>我们通过使用patch-wise令牌重新嵌入来提高注意中的矩阵操作，提高了transformer对多尺度特征和高分辨率特征的效率。在卷积前馈网络的帮助下，可以进一步增强特征的局部性，并且可以充分感知和探索全局和局部上下文中的关键线索。</li><li>大量的实验表明，该模型在7个RGB-D SOD数据集和3个RGB-T SOD数据集上的性能优于最近的方法。</li></ol> 
<h3 id="3%E3%80%81%E6%96%B9%E6%B3%95%EF%BC%9A">3、方法：</h3> 
<h4 id="3.1%20%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%BB%E4%BD%93%E6%A1%86%E6%9E%B6%E5%9B%BE%EF%BC%9A">3.1 网络的总体框架图：</h4> 
<p>    <img alt="" height="667" src="https://images2.imgbox.com/33/54/JZQGttoi_o.png" width="1200"></p> 
<h4 id="3.2%C2%A0Transformer-based%20Information%20Propagation%20Path%20(TIPP)">3.2 Transformer-based Information Propagation Path (TIPP)</h4> 
<p>        基于transformer的信息传播路径主要是四个CMUI模块之间自上而下地信息交互和传播，前三个CMUI模块（即CMUI1、CMUI2、CMUI3）都是有三个输入（RGB流、depth/thermal流，来自下一个CMUI的输出 <img alt="{f_{rgb-d/t}^{i+1}}" class="mathcode" src="https://images2.imgbox.com/7b/3c/eLhO8xbP_o.png">），但是最后一个CMUI4模块只有两个输入（即RGB流、depth/thermal流）</p> 
<h4 id="3.3%20Intra-Modal%2FCross-Scale%20Self-Attention%20(IMSA%2FCSSA)">3.3 Intra-Modal/Cross-Scale Self-Attention (IMSA/CSSA)</h4> 
<p>        IMSA和CSSA两者的结构式相同的，但相较于自注意机制而言两者在FFN（前馈神经网络）和MHSA（多头注意力机制）两部分做出了改进。如下图所示：</p> 
<p class="img-center"><img alt="" height="530" src="https://images2.imgbox.com/af/ca/ZJ5sLdDU_o.png" width="359"></p> 
<p>其中如上两部分改进的原因：</p> 
<h6 id="Q1%3A%20MHSA%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E8%BE%83%E9%AB%98">Q1: MHSA计算复杂度较高</h6> 
<p>MHSA单头的计算公式：</p> 
<p style="text-align:center;"><img alt="" height="131" src="https://images2.imgbox.com/9b/74/P773dRBf_o.png" width="681"></p> 
<p style="text-align:center;">        Qh、Kh、Vh 分别是单个头部的查询、键和值。Wq , Wk, Wv 是对应的投影矩阵。Z = [Y1,., Yh ]<img alt="W^{^{o}}" class="mathcode" src="https://images2.imgbox.com/b6/45/2fAfgdzJ_o.png"> ，<img alt="W^{^{o}}" class="mathcode" src="https://images2.imgbox.com/7a/d7/Erl4Qjni_o.png">是一个输出投影层。</p> 
<p style="text-align:center;">        注意矩阵<img alt="Q_{h}K_{h}^{T}" class="mathcode" src="https://images2.imgbox.com/47/30/eVu2IhGc_o.png">的点积运算具有输入序列长度的二次复杂度w.r.t，即N^2，这限制了它处理多尺度高分辨率特征。</p> 
<h6 id="A1%3APatch-wise%20Token%20Re-Embedding%20(PTRE)">A1:Patch-wise Token Re-Embedding (PTRE)</h6> 
<p style="text-align:justify;">        与MHSA相比，PTRE被应用于将矩阵运算从<strong>像素形式</strong>提高到<strong>逐块形式</strong>，从而将复杂度降低了p^2倍。这里，p^2是Patch-wise中的元素数量。具体做法就是将1D序列折叠成2D的形式，部分和总体图如下面两图：</p> 
<p style="text-align:center;"><img alt="" height="416" src="https://images2.imgbox.com/0a/8f/rERgkNQq_o.png" width="789"></p> 
<p style="text-align:center;"><img alt="" height="711" src="https://images2.imgbox.com/42/5b/9g5Bho6I_o.png" width="774"></p> 
<h6 id="Q2%3A%E7%9B%AE%E5%89%8D%E7%9A%84MHSA%E5%8F%AA%E8%80%83%E8%99%91%E7%A9%BA%E9%97%B4%E8%A7%86%E5%9B%BE%E4%B8%8A%E7%9A%84%E7%89%B9%E5%BE%81%E5%AF%B9%E9%BD%90%EF%BC%8C%E8%80%8C%E5%BF%BD%E7%95%A5%E4%BA%86%E9%80%9A%E9%81%93%E8%A7%86%E5%9B%BE%E7%9A%84%E6%BD%9C%E5%9C%A8%E5%80%BC%E3%80%82">Q2:目前的MHSA只考虑空间视图上的特征对齐，而忽略了通道视图的潜在值</h6> 
<h6 id="A1%3AView-Mixed%20Attention%20(VMA)">A2:View-Mixed Attention (VMA)</h6> 
<p>        具体做法如下，值得注意的是，计算空间和通道的Z值时，计算方式不一样。</p> 
<p class="img-center"><img alt="" height="76" src="https://images2.imgbox.com/04/3d/q93Py9jL_o.png" width="504"></p> 
<p><img alt="\Rightarrow" class="mathcode" src="https://images2.imgbox.com/bb/97/6s5jipW5_o.png"> </p> 
<p class="img-center"><img alt="" height="75" src="https://images2.imgbox.com/35/f5/M3AYv3fh_o.png" width="473"></p> 
<p class="img-center"><img alt="" height="667" src="https://images2.imgbox.com/66/e3/hoHChPBX_o.png" width="468"></p> 
<h4 id="%C2%A03.4%20Inter-Modal%20Cross-Attention%20(IMCA)"> 3.4 Inter-Modal Cross-Attention (IMCA)</h4> 
<p>        交叉注意机制和自注意机制两者之间的区别就是，自注意机制的Q、K、V是同源的，即来自同一个X，而交叉注意机制的Q、K、V是不同源的，如在IMCA中Q来自rgb流，但是K和V来自d/t流。</p> 
<p class="img-center"><img alt="" height="604" src="https://images2.imgbox.com/78/05/WPBYJclD_o.png" width="587"></p> 
<p class="img-center"><img alt="" height="175" src="https://images2.imgbox.com/da/65/5NcmbZwH_o.png" width="630"></p> 
<p>        d/t流与上述公式类似</p> 
<h3 id="4%E3%80%81%E5%AE%9E%E9%AA%8C%EF%BC%9A">4、实验：</h3> 
<p class="img-center"><img alt="" height="750" src="https://images2.imgbox.com/39/23/KIlwYk4Q_o.png" width="1200"></p> 
<p class="img-center"><img alt="" height="761" src="https://images2.imgbox.com/1a/e8/5hZDyN5f_o.png" width="1200"></p> 
<h2 id="%E4%BA%8C%E3%80%81%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0">二、代码复现</h2> 
<h5 id="1%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%86%E8%8A%82%EF%BC%9A">1、实验细节：</h5> 
<p>        在AutoDL平台上租的服务器，服务器型号为：RTX A4000(16GB)，同时使用的相关配置为：PyTorch  1.11.0 Python  3.8(ubuntu20.04) Cuda  11.3</p> 
<h5 id="2%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A">2、数据集：</h5> 
<p>        并未采用论文里的数据集，而是采用rsdds_1500数据集和rsdds_113数据集</p> 
<h5 id="3%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4%EF%BC%9A">3、实验步骤：</h5> 
<h6 id="3.1%20%E5%B0%86%E5%AF%B9%E5%BA%94%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E4%BC%A0%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A">3.1 将对应的代码和数据集上传到服务器上</h6> 
<h6 id="3.2%20%E5%B0%86%E6%A0%B9%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84datasets.py%E9%87%8C%E7%9A%84%E8%B7%AF%E5%BE%84%E6%9B%B4%E6%94%B9%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E8%B7%AF%E5%BE%84">3.2 将根目录下的datasets.py里的路径更改成自己的路径</h6> 
<p>        由于datasets.py中有多个数据集的定义，我只更改了第一个数据集的内容，具体是（更改的地方标红）</p> 
<blockquote> 
 <p>_RGBD_SOD_ROOT = <span style="color:#fe2c24;">"/root/autodl-tmp/CAVER/rsdds"</span><br> _RGBT_SOD_ROOT = "&lt;rgbtsod root&gt;"</p> 
 <p># RGB-D SOD<br><span style="color:#fe2c24;">rsdds</span>= dict(<br>     image=dict(path=f"{_RGBD_SOD_ROOT}<span style="color:#fe2c24;">/TrainDataset/RGB</span>", suffix="<span style="color:#fe2c24;">.bmp</span>"),<br>     depth=dict(path=f"{_RGBD_SOD_ROOT}<span style="color:#fe2c24;">/TrainDataset/depth</span>", suffix="<span style="color:#fe2c24;">.tiff</span>"),<br>     mask=dict(path=f"{_RGBD_SOD_ROOT}<span style="color:#fe2c24;">/TrainDataset/GT</span>", suffix="<span style="color:#fe2c24;">.png</span>"),<br> )</p> 
</blockquote> 
<h6 id="3.3%20%E6%9B%B4%E6%94%B9%E5%AF%B9%E5%BA%94%E7%9A%84.%2Fconfigs%2Frgbd-2dataset.py%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84%E4%BF%A1%E6%81%AF%EF%BC%88%E8%BF%99%E4%B8%80%E9%83%A8%E5%88%86%E6%9C%89%E7%96%91%E9%97%AE%EF%BC%89">3.3 更改对应的./configs/rgbd-2dataset.py中的路径信息</h6> 
<p>由于使用的是ResNet101d训练模型，所以需要更改对应的./configs/rgbd-2dataset.py中的路径信息。</p> 
<p><img alt="" height="771" src="https://images2.imgbox.com/e2/c0/QQIgixu0_o.png" width="1200"></p> 
<p>具体做法如下(由于数据集中图片较少，只进行了训练，没有测试，所以两者都是rsdds_113)：</p> 
<blockquote> 
 <p>data = dict(<br>     train=dict(<br>         name=[<br>             "<span style="color:#fe2c24;">rsdds_113</span>",<br>         ],<br>         shape=dict(h=256, w=256),<br>     ),<br>     test=dict(<br>         name=[<br>             "<span style="color:#fe2c24;">rsdds_113</span>"<br>         ],<br>         shape=dict(h=256, w=256),<br>     ),</p> 
</blockquote> 
<h6 id="3.3%20%E5%B0%86%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%BB%8E%E7%BB%99%E5%87%BA%E7%9A%84%E7%BD%91%E5%9D%80%E4%B8%8A%E4%B8%8B%E8%BD%BD%E4%B8%8B%E6%9D%A5%EF%BC%8C%E4%B8%8A%E4%BC%A0%E5%88%B0%E5%AF%B9%E5%BA%94%E6%96%87%E4%BB%B6%E5%A4%B9%20(%2Froot%2Fautodl-tmp%2FCAVER)%2Fpretrained">3.4 将预训练模型从给出的网址上下载下来，上传到对应文件夹 (/root/autodl-tmp/CAVER/pretrained)</h6> 
<p>预训练模型网址：</p> 
<p>Pre-trained parameters: <a href="https://github.com/lartpang/CAVER/releases/tag/rgbd-rgbt-models" title="Release All pretrained parameters. · lartpang/CAVER · GitHub">Release All pretrained parameters. · lartpang/CAVER · GitHub</a></p> 
<p><img alt="" height="216" src="https://images2.imgbox.com/cc/0e/MWxqpQYj_o.png" width="1092"></p> 
<p></p> 
<h6 id="3.3%20%E5%AE%89%E8%A3%85%E5%AF%B9%E5%BA%94%E7%9A%84module">3.5 安装对应的module</h6> 
<h6 id="3.4%20%E5%9C%A8%E4%B8%A4%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C">3.6 在两个数据集上的训练结果</h6> 
<p>rsdds_113</p> 
<p><img alt="" height="249" src="https://images2.imgbox.com/c7/51/fmzOGMus_o.png" width="1200"></p> 
<p> rsdds_1500<img alt="" height="443" src="https://images2.imgbox.com/e2/9b/lQ8mWY8v_o.png" width="1200"></p> 
<h2 id="%E4%B8%89%E3%80%81BUGS">三、BUGS</h2> 
<h5 id="%E9%97%AE%E9%A2%981%EF%BC%9Aassert%20path.endswith(%22.jpg%22)%20or%20path.endswith(%22.png%22)%20or%20path%2Cendswith(%22.bmp%22)%20AssertionError"><strong>问题1：</strong>assert path.endswith(".jpg") or path.endswith(".png") or path,endswith(".bmp") AssertionError</h5> 
<p><img alt="" height="633" src="https://images2.imgbox.com/42/27/aDX6jMHo_o.png" width="1200"></p> 
<p><strong>解决办法：</strong>原来对应的datasets.py中的后缀suffix没有更改，原来对应的是如上图，但是我数据集里的三个文件夹中对应的图片格式与之不同，更改成对应的后缀即可 ，如下图：</p> 
<blockquote> 
 <p>_RGBD_SOD_ROOT = "/root/autodl-tmp/CAVER/rsdds"<br> _RGBT_SOD_ROOT = "&lt;rgbtsod root&gt;"</p> 
 <p># RGB-D SOD<br> rsdds= dict(<br>     image=dict(path=f"{_RGBD_SOD_ROOT}/TrainDataset/RGB", suffix="<span style="color:#fe2c24;">.bmp</span>"),<br>     depth=dict(path=f"{_RGBD_SOD_ROOT}/TrainDataset/depth", suffix="<span style="color:#fe2c24;">.tiff</span>"),<br>     mask=dict(path=f"{_RGBD_SOD_ROOT}/TrainDataset/GT", suffix="<span style="color:#fe2c24;">.png</span>"),<br> )</p> 
</blockquote> 
<h5 id="%E9%97%AE%E9%A2%982%EF%BC%9A%20TypeError%3A%20FormatCode()%20got%20an%20key%20word%20'verify'">问题2： TypeError: FormatCode() got an key word 'verify'</h5> 
<h5 id="%E2%80%8B%E7%BC%96%E8%BE%91"><img alt="" height="268" src="https://images2.imgbox.com/e6/74/ldTdDXf8_o.png" width="1200"></h5> 
<p><strong>解决办法：</strong>我对应文件夹里的预训练模型上传出现问题，对应的.pth文件大小为0Bytes，重新上传即可</p> 
<h5 id="%E2%80%8B%E7%BC%96%E8%BE%91%20%E9%97%AE%E9%A2%983%EF%BC%9ARuntimeError%3A%20CuDA%20error%3A%20no%20kernel%20imade%20is%20available%20for%20execution%20on%20the%20deviceCUDA%20kernel%20errors%20might%20be%20asynchronously%20reported%20at%20some%20other%20API%20call%2Cso%20the%20stacktrace%20below%20might%20be%20incorrectFor%20debugging%20consider%20passing%20CUDA%20LAUNCH%20BLOCKING%3D1."><img alt="" height="198" src="https://images2.imgbox.com/a1/2c/SIqXTrlf_o.png" width="1069"> 问题3：RuntimeError: CuDA error: no kernel imade is available for execution on the deviceCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrectFor debugging consider passing CUDA LAUNCH BLOCKING=1.</h5> 
<p><img alt="" height="671" src="https://images2.imgbox.com/32/13/UIMq5MEL_o.png" width="1200"></p> 
<p><strong>解决办法：</strong>由于为了直接快速安装各种包，我直接使用 <code>pip install -r requirements.txt</code> 安装各种包，但是由于txt文件里有torch和torchvision的版本，与无使用的版本不同，导致我原先的版本被卸载，重新安装了对应的版本，但是由于安装是从阿里云镜像里安装的cpu版本，导致出现这个问题。所以直接换一个服务器，然后再将<code>requirements.txt中对应的</code>torch和torchvision的版本要求删掉，或者一步一步根据错误提示安装对应的包即可。</p> 
<h2 id="%E5%9B%9B%E3%80%81%E6%8F%90%E9%97%AE">四、提问</h2> 
<p>问题1:SOD（显著性目标检测）和图像分割之间的区别</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/65ad049ce2dbb3514b11688a7e88bb10/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">计算机网络【Cookie和session机制】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e3e7c0dbd601d8f36c8473608f1dffe6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">机器视觉在智能交通与无人驾驶领域的应用及前景</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>