<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>多模态大模型系列论文（ALBEF、BLIP、BLIP-2） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="多模态大模型系列论文（ALBEF、BLIP、BLIP-2）" />
<meta property="og:description" content="1. ALBEF: ALign the image and text BEfore Fusing 1.1 论文与代码链接： ​​​​​​https://arxiv.org/abs/2107.07651 GitHub - salesforce/ALBEF: Code for ALBEF: a new vision-language pre-training method 1.2 目标任务： 视觉-文本 融合任务，如图文检索、视觉问答、NLVR （natural language vision reasoning）等
1.3 当前方法问题： 1）没有对齐视觉的 tokens 和 文字的 tokens, 因此给 多模编码器进行图文交互学习时带来挑战
2）训练多模模型，利用到了互联网上爬取的数据，这些数据中往往存在大量噪声，传统的图文特征融合训练模式（如 MLM, masked language modeling） 可能过拟合到噪声文本上，从而影响模型的泛化性能。
1.4 本文解决方案： 1） 通过跨模态 attention 的方式引入对比损失，在图文特征融合前对齐图像和文本表征，相对与大多数传统方案来说，不需要在高清图片上进行框级别的标注。
2）提出一种 动量蒸馏 （momentum distillation） 的方案，即通过自训练(self-training)的方式从动量模型提供的伪标签中进行学习。
在训练过程中，通过参数移动平均的方式更新动量模型，并利用动量模型生成伪标签(pseudo-targets) 作为额外的监督信息。利用动量蒸馏的方式，模型将不在惩罚模型合理的输出，即使这个输出与网络标签不一致，提升从网络噪声数据中学习的能力。
1.5 实验结果： 1）在图文检索任务中，本方案优于在大规模数据集中预训练的方案（CLIP &amp; ALIGN）
2) 在 VQA 和 NLVR 任务中，本方案相对 SOTA 算法（VILIA）分别获得了 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/b104671034eeb456ffaac5114cfdc9b1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-25T21:06:54+08:00" />
<meta property="article:modified_time" content="2023-06-25T21:06:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">多模态大模型系列论文（ALBEF、BLIP、BLIP-2）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1. ALBEF: ALign the image and text BEfore Fusing</h2> 
<h3><strong>1.1 论文与代码链接</strong>：</h3> 
<h3><a href="https://arxiv.org/abs/2107.07651" rel="nofollow" title="​​​​​​https://arxiv.org/abs/2107.07651">​​​​​​https://arxiv.org/abs/2107.07651</a></h3> 
<h3><a href="https://github.com/salesforce/ALBEF" title="GitHub - salesforce/ALBEF: Code for ALBEF: a new vision-language pre-training method">GitHub - salesforce/ALBEF: Code for ALBEF: a new vision-language pre-training method</a></h3> 
<h3><img alt="" height="457" src="https://images2.imgbox.com/b0/26/2ELgEUne_o.png" width="980"></h3> 
<p></p> 
<h3><strong>1.2 目标任务：</strong></h3> 
<p> 视觉-文本 融合任务，如图文检索、视觉问答、NLVR （natural language vision reasoning）等</p> 
<h3></h3> 
<h3><strong>1.3 当前方法问题：</strong></h3> 
<p>1）没有对齐视觉的 tokens 和 文字的 tokens, 因此给 多模编码器进行图文交互学习时带来挑战</p> 
<p>2）训练多模模型，利用到了互联网上爬取的数据，这些数据中往往存在大量噪声，传统的图文特征融合训练模式（如 MLM, masked language modeling） 可能过拟合到噪声文本上，从而影响模型的泛化性能。</p> 
<h3></h3> 
<h3><strong>1.4 本文解决方案：</strong></h3> 
<p>1） 通过跨模态 attention 的方式引入对比损失，在图文特征融合前对齐图像和文本表征，相对与大多数传统方案来说，不需要在高清图片上进行框级别的标注。</p> 
<p>2）提出一种 动量蒸馏 （momentum distillation） 的方案，即通过自训练(self-training)的方式从动量模型提供的伪标签中进行学习。</p> 
<p>在训练过程中，通过参数移动平均的方式更新动量模型，并利用动量模型生成伪标签(pseudo-targets) 作为额外的监督信息。利用动量蒸馏的方式，模型将不在惩罚模型合理的输出，即使这个输出与网络标签不一致，提升从网络噪声数据中学习的能力。</p> 
<h3></h3> 
<h3>1.5 实验结果：</h3> 
<p>1）在图文检索任务中，本方案优于在大规模数据集中预训练的方案（CLIP &amp; ALIGN）</p> 
<p>2) 在 VQA 和 NLVR 任务中，本方案相对 SOTA 算法（VILIA）分别获得了 2.37% 和 3.84% 的指标提升，而且获得了更快的推理速度。</p> 
<p></p> 
<h2>2. BLIP (Bootstrapping Language- Image Pretraining)</h2> 
<h3><strong>2.1 论文与代码链接</strong>：</h3> 
<p><a href="https://arxiv.org/abs/2201.12086" rel="nofollow" title="https://arxiv.org/abs/2201.12086">https://arxiv.org/abs/2201.12086</a></p> 
<p><a href="https://github.com/salesforce/BLIP" title="GitHub - salesforce/BLIP: PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation">GitHub - salesforce/BLIP: PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></p> 
<h3><img alt="" height="528" src="https://images2.imgbox.com/02/86/TyBYTOCe_o.png" width="1200"></h3> 
<p>  </p> 
<h3><strong>2.2 目标任务：</strong></h3> 
<p> 视觉-文本 融合任务，如图文检索、视觉问答、NLVR （natural language vision reasoning）等</p> 
<h3><strong>2.3 当前方法问题：</strong></h3> 
<p>1）当前 视觉-语言 预训练（VLP）推动了 视觉语言预训练任务的性能，然而大多数现有的预训练模型或者擅长基于理解的任务（分类）或者基于生成的任务之一。encoder-based 架构不擅长生成类任务，encoder-decoder 架构不擅长分类相关任务(如 图文跨模态检索)</p> 
<p>2）当前 VLP 模型的性能提升依赖于扩大图文对训练集，这些图文对通常是从互联网上爬取的，所以噪声相对较大。</p> 
<h3>2.4 本文解决方案：</h3> 
<p>提出一种新的 VLP 框架，可以在视觉-语言的 理解任务 和 生成任务 之间灵活转换，而且可以通过booststraping 的方式有效利用噪声数据，即构造了一个 captioner 用于生成captions，一个 filters 移除噪声 captions。具体如下：</p> 
<p>1）提出一种多模混合 encoder-decoder 架构 (MED)：可以作为独立的编码器，也可以分别作为 基于图像的文本编码器和解码器。通过联合三种视觉-语言 的目标进行学习：图文对比学习、图文匹配 和 基于图像的语言建模(image-conditioned language modeling)。</p> 
<p></p> 
<h3>2.5 实验结果：</h3> 
<p>1）在图文检索任务中，本方案相较 SOTA， top1 recall 提升了 2.7%</p> 
<p>2）在 image caption 任务中，CIDEr 指标提升 2.8%</p> 
<p>2) 在 VQA 本方案相对 SOTA 算法获得了 1.6% 的VQA score 指标提升</p> 
<p></p> 
<h2>3. BLIP -2 (Bootstrapping Language- Image Pretraining)</h2> 
<h3><strong>3.1 论文与代码链接</strong>：</h3> 
<p><a href="https://export.arxiv.org/pdf/2301.12597v1.pdf" rel="nofollow" title="https://export.arxiv.org/pdf/2301.12597v1.pdf">https://export.arxiv.org/pdf/2301.12597v1.pdf</a></p> 
<p><a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2" title="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">https://github.com/salesforce/LAVIS/tree/main/projects/blip2</a></p> 
<h3><img alt="" height="291" src="https://images2.imgbox.com/44/3d/bD9CyMUu_o.png" width="777"></h3> 
<h3><img alt="" height="300" src="https://images2.imgbox.com/dc/67/Ka9Zt22A_o.png" width="629"></h3> 
<h3><strong>3.2 目标任务：</strong></h3> 
<p> 视觉-文本 融合任务，如图文检索、视觉问答、NLVR （natural language vision reasoning）等</p> 
<h3><strong>3.3 当前方法问题：</strong></h3> 
<p>由于模型越来越大，VLP 预训练成本变得越来越高。</p> 
<h3>3.4 本文解决方案</h3> 
<p>充分利用大模型原始能力，不做预训练，而通过设计一个轻量级的 Querying transformer（Q-former） 连接视觉大模型和语言大模型。Q-former 通过两阶段方式进行训练：</p> 
<p>阶段 1：固定图像编码器，学习视觉-语言(vision-language)一致性的表征</p> 
<p>阶段 2： 固定语言大模型，提升视觉到语言(vision-to-language)的生成能力</p> 
<p></p> 
<p>参考文献：</p> 
<ol><li>Li, Junnan, et al. “Align before Fuse: Vision and Language Representation Learning with Momentum Distillation.” in NeuraIPS 2021.</li><li>Li, Junnan, et al. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, in ICML 2022.</li><li>Li, Junnan, et al. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models, arxiv preprint 2023.</li></ol>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/02a97a239fbf8907b6199b672f7577b7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【MySql】用户管理——用户管理|权限管理</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f06b5486b966eb0d31f2867c75a28622/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Redis为什么快？（面试常问）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>