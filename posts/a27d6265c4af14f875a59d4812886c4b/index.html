<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>OVD (Open-Vocabulary Detection) - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="OVD (Open-Vocabulary Detection)" />
<meta property="og:description" content="目标检测是计算机视觉中一个非常重要的基础任务，与常见的的图像分类 / 识别任务不同，目标检测需要模型在给出目标的类别之上，进一步给出目标的位置和大小信息，在 CV 三大任务（识别、检测、分割）中处于承上启下的关键地位。
当前大火的多模态 GPT-4 在视觉能力上只具备目标识别的能力，还无法完成更高难度的目标检测任务。而识别出图像或视频中物体的类别、位置和大小信息，是现实生产中众多人工智能应用的关键，例如自动驾驶中的行人车辆识别、安防监控应用中的人脸锁定、医学图像分析中的肿瘤定位等等。
已有的目标检测方法如 YOLO 系列、R-CNN 系列等目标检测算法在科研人员的不断努力下已经具备很高的目标检测精度与效率，但由于现有方法需要在模型训练前就定义好待检测目标的集合（闭集），导致它们无法检测训练集合之外的目标，比如一个被训练用于检测人脸的模型就不能用于检测车辆；另外，现有方法高度依赖人工标注的数据，当需要增加或者修改待检测的目标类别时，一方面需要对训练数据进行重新标注，另一方面需要对模型进行重新训练，既费时又费力。
一个可能的解决方案是，收集海量的图像，并人工标注 Box 信息与语义信息，但这将需要极高的标注成本，而且使用海量数据对检测模型进行训练也对科研工作者提出了严峻的挑战，如数据的长尾分布问题与人工标注的质量不稳定等因素都将影响检测模型的性能表现。
发表于 CVPR 2021 的文章 OVR-CNN [1] 提出了一种全新的目标检测范式：开放词集目标检测（Open-Vocabulary Detection，OVD，亦称为开放世界目标检测），来应对上文提到的问题，即面向开放世界未知物体的检测场景。
OVD 由于能够在无需人工扩充标注数据量的情形下识别并定位任意数量和类别目标的能力，自提出后吸引了学术界与工业界的持续关注，也为经典的目标检测任务带来了新的活力与新的挑战，有望成为目标检测的未来新范式。
具体地，OVD 技术不需要人工标注海量的图片来增强检测模型对未知类别的检测能力，而是通过将具有良好泛化性的无类别（class-agnostic）区域检测器与经过海量无标注数据训练的跨模态模型相结合，通过图像区域特征与待检测目标的描述性文字进行跨模态对齐来扩展目标检测模型对开放世界目标的理解能力。
跨模态和多模态大模型工作近期的发展非常迅速，如 CLIP [2]、ALIGN [3] 与 R2D2 [4] 等，而它们的发展也促进了 OVD 的诞生与 OVD 领域相关工作的快速迭代与进化。
OVD 技术涉及两大关键问题的解决：1）如何提升区域 (Region) 信息与跨模态大模型之间的适配；2）如何提升泛类别目标检测器对新类别的泛化能力。从这两个角度出发，下面将详细介绍一些 OVD 领域的相关工作。
OVD 的基础概念：OVD 的使用主要涉及到 few-shot 和 zero-shot 两大类场景，few-shot 是指有少量人工标注训练样本的目标类别，zero-shot 则是指不存在任何人工标注训练样本的目标类别。在常用的学术评测数据集 COCO、LVIS 上，数据集会被划分为 Base 类和 Novel 类，其中 Base 类对应 few-shot 场景，Novel 类对应 zero-shot 场景。如 COCO 数据集包含 65 种类别，常用的评测设定是 Base 集包含 48 种类别，few-shot 训练中只使用这 48 个类别。Novel 集包含 17 种类别，在训练时完全不可见。测试指标主要参考 Novel 类的 AP50 数值进行比较。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/a27d6265c4af14f875a59d4812886c4b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-05T15:00:37+08:00" />
<meta property="article:modified_time" content="2023-06-05T15:00:37+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">OVD (Open-Vocabulary Detection)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>目标检测是计算机视觉中一个非常重要的基础任务，与常见的的图像分类 / 识别任务不同，目标检测需要模型在给出目标的类别之上，进一步给出目标的位置和大小信息，在 CV 三大任务（识别、检测、分割）中处于承上启下的关键地位。</p> 
<p>当前大火的多模态 GPT-4 在视觉能力上只具备目标识别的能力，还无法完成更高难度的目标检测任务。而识别出图像或视频中物体的类别、位置和大小信息，是现实生产中众多人工智能应用的关键，例如自动驾驶中的行人车辆识别、安防监控应用中的人脸锁定、医学图像分析中的肿瘤定位等等。</p> 
<p>已有的目标检测方法如 YOLO 系列、R-CNN 系列等目标检测算法在科研人员的不断努力下已经具备很高的目标检测精度与效率，但由于现有方法需要在模型训练前就定义好待检测目标的集合（闭集），导致它们无法检测训练集合之外的目标，比如一个被训练用于检测人脸的模型就不能用于检测车辆；另外，现有方法高度依赖人工标注的数据，当需要增加或者修改待检测的目标类别时，一方面需要对训练数据进行重新标注，另一方面需要对模型进行重新训练，既费时又费力。</p> 
<p>一个可能的解决方案是，收集海量的图像，并人工标注 Box 信息与语义信息，但这将需要极高的标注成本，而且使用海量数据对检测模型进行训练也对科研工作者提出了严峻的挑战，如数据的长尾分布问题与人工标注的质量不稳定等因素都将影响检测模型的性能表现。</p> 
<p>发表于 CVPR 2021 的文章 OVR-CNN [1] 提出了一种全新的目标检测范式：开放词集目标检测（Open-Vocabulary Detection，OVD，亦称为开放世界目标检测），来应对上文提到的问题，即面向开放世界未知物体的检测场景。</p> 
<p>OVD 由于能够在无需人工扩充标注数据量的情形下识别并定位任意数量和类别目标的能力，自提出后吸引了学术界与工业界的持续关注，也为经典的目标检测任务带来了新的活力与新的挑战，有望成为目标检测的未来新范式。</p> 
<p>具体地，OVD 技术不需要人工标注海量的图片来增强检测模型对未知类别的检测能力，而是通过将具有良好泛化性的无类别（class-agnostic）区域检测器与经过海量无标注数据训练的跨模态模型相结合，通过图像区域特征与待检测目标的描述性文字进行跨模态对齐来扩展目标检测模型对开放世界目标的理解能力。</p> 
<p>跨模态和多模态大模型工作近期的发展非常迅速，如 CLIP [2]、ALIGN [3] 与 R2D2 [4] 等，而它们的发展也促进了 OVD 的诞生与 OVD 领域相关工作的快速迭代与进化。</p> 
<p>OVD 技术涉及两大关键问题的解决：1）如何提升区域 (Region) 信息与跨模态大模型之间的适配；2）如何提升泛类别目标检测器对新类别的泛化能力。从这两个角度出发，下面将详细介绍一些 OVD 领域的相关工作。</p> 
<p><img alt="" height="673" src="https://images2.imgbox.com/6c/ae/4FznXX9j_o.png" width="1071">OVD 的基础概念：OVD 的使用主要涉及到 few-shot 和 zero-shot 两大类场景，few-shot 是指有少量人工标注训练样本的目标类别，zero-shot 则是指不存在任何人工标注训练样本的目标类别。在常用的学术评测数据集 COCO、LVIS 上，数据集会被划分为 Base 类和 Novel 类，其中 Base 类对应 few-shot 场景，Novel 类对应 zero-shot 场景。如 COCO 数据集包含 65 种类别，常用的评测设定是 Base 集包含 48 种类别，few-shot 训练中只使用这 48 个类别。Novel 集包含 17 种类别，在训练时完全不可见。测试指标主要参考 Novel 类的 AP50 数值进行比较。</p> 
<ul><li> <p>论文地址：https://arxiv.org/pdf/2011.10678.pdf</p> </li><li> <p>代码地址：https://github.com/alirezazareian/ovr-cnn</p> </li></ul> 
<p>OVR-CNN 是 CVPR 2021 的 Oral-Paper，也是 OVD 领域的开山之作。它的二阶段训练范式，影响了后续很多的 OVD 工作。如下图所示，第一阶段主要使用 image-caption pairs 对视觉编码器进行预训练，其中借助 BERT (参数固定) 来生成词掩码，并与加载 ImageNet 预训练权重的 ResNet50 进行弱监督的 Grounding 匹配，作者认为弱监督会让匹配陷入局部最优，于是加入多模态 Transformer 进行词掩码预测来增加鲁棒性。</p> 
<p><img alt="" height="524" src="https://images2.imgbox.com/51/4a/YKLagSH8_o.png" width="1074">第二阶段的训练流程与 Faster-RCNN 类似，区别点在于，特征提取的 Backbone 来自于第一阶段预训练得到的 ResNet50 的 1-3 层，RPN 后依然使用 ResNet50 的第四层进行特征加工，随后将特征分别用于 Box 回归与分类预测。分类预测是 OVD 任务区别于常规检测的关键标志，OVR-CNN 中将特征输入一阶段训练得到的 V2L 模块 (参数固定的图向量转词向量模块) 得到一个图文向量，随后与标签词向量组进行匹配，对类别进行预测。在二阶段训练中，主要使用 Base 类对检测器模型进行框回归训练与类别匹配训练。由于 V2L 模块始终固定，配合目标检测模型定位能力向新类别迁移，使得检测模型能够识别并定位到全新类别的目标。 </p> 
<p><img alt="" height="438" src="https://images2.imgbox.com/93/78/U1XPu2OJ_o.png" width="1077">如下图所示，OVR-CNN 在 COCO 数据集上的表现远超之前的 Zero-shot 目标检测算法。</p> 
<p><img alt="" height="555" src="https://images2.imgbox.com/bd/eb/gncF554e_o.png" width="785"></p> 
<p><strong>论文2：RegionCLIP: Region-based Language-Image Pretraining</strong></p> 
<ul><li> <p>论文地址：https://arxiv.org/abs/2112.09106</p> </li><li> <p>代码地址：https://github.com/microsoft/RegionCLIP</p> </li></ul> 
<p>OVR-CNN 中使用 BERT 与多模态 Transfomer 进行 iamge-text pairs 预训练，但随着跨模态大模型研究的兴起，科研工作者开始利用 CLIP，ALIGN 等更强大的跨模态大模型对 OVD 任务进行训练。检测器模型本身主要针对 Proposals，即区域信息进行分类识别，发表于 CVPR 2022 的 RegionCLIP [5] 发现当前已有的大模型，如 CLIP，对裁剪区域的分类能力远低于对原图本身的分类能力，为了改进这一点，RegionCLIP 提出了一个全新的两阶段 OVD 方案。 </p> 
<p><img alt="" height="801" src="https://images2.imgbox.com/6c/1d/pDH2X2Wf_o.png" width="1018">第一阶段，数据集主要使用 CC3M，COCO-caption 等图文匹配数据集进行区域级别的蒸馏预训练。具体地：</p> 
<p>1. 将原先存在于长文本中的词汇进行提取，组成 Concept Pool，进一步形成一组关于 Region 的简单描述，用于训练。</p> 
<p>2. 利用基于 LVIS 预训练的 RPN 提取 Proposal Regions，并利用原始 CLIP 对提取到的不同 Region 与准备好的描述进行匹配分类，并进一步组装成伪造的语义标签。</p> 
<p>3. 将准备好的 Proposal Regions 与语义标签在新的 CLIP 模型上进行 Region-text 对比学习，进而得到一个专精于 Region 信息的 CLIP 模型。</p> 
<p>4. 在预训练中，新的 CLIP 模型还会通过蒸馏策略学习原始 CLIP 的分类能力，以及进行全图级别的 image-text 对比学习，来维持新的 CLIP 模型对完整图像的表达能力。</p> 
<p>第二阶段，将得到的预训练模型在检测模型上进行迁移学习。</p> 
<p><img alt="" height="525" src="https://images2.imgbox.com/6a/da/vEkjGLBH_o.png" width="1068">RegionCLIP 进一步拓展了已有跨模态大模型在常规检测模型上的表征能力，进而取得了更加出色的性能，如下图所示，RegionCLIP 相比 OVR-CNN 在 Novel 类别上取得了较大提升。RegionCLIP 通过一阶段的预训练有效地的提升了区域 (Region) 信息与多模态大模型之间的适应能力，但 CORA 认为其使用更大参数规模的跨模态大模型进行一阶段训练时，训练成本将会非常高昂。</p> 
<p><img alt="" height="430" src="https://images2.imgbox.com/00/d4/4tcANfpW_o.png" width="1062"><strong>论文3：CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching</strong> </p> 
<ul><li> <p>论文地址：https://arxiv.org/abs/2303.13076</p> </li><li> <p>代码地址：https://github.com/tgxs002/CORA</p> </li></ul> 
<p>CORA [6] 已被收录于 CVPR 2023，为了克服其所提出当前 OVD 任务所面临的两个阻碍，设计了一个类 DETR 的 OVD 模型。如其文章标题所示，模型主要包含了 Region Prompting 与 Anchor Pre-Matching 两个策略。前者通过 Prompt 技术来优化基于 CLIP 的区域分类器所提取的区域特征，进而缓解整体与区域的分布差距，后者通过 DETR 检测方法中的锚点预匹配策略来提升 OVD 模型对新类别物体定位能力的泛化性。</p> 
<p> <img alt="" height="714" src="https://images2.imgbox.com/28/e6/fXNBNgxj_o.png" width="668"></p> 
<p>CLIP 原始视觉编码器的整体图像特征与区域特征之间存在分布差距，进而导致检测器的分类精度较低（这一点与 RegionCLIP 的出发点类似）。因此，CORA 提出 Region Prompting 来适应 CLIP 图像编码器，提高对区域信息的分类性能。具体地，首先通过 CLIP 编码器的前 3 层将整幅图像编码成一个特征映射，然后由 RoI Align 生成锚点框或预测框，并将其合并成区域特征。随后由 CLIP 图像编码器的第四层进行编码。为了缓解 CLIP 图像编码器的全图特征图与区域特征之间存在分布差距，设置了可学习的 Region Prompts 并与第四层输出的特征进行组合，进而生成最终的区域特征用来与文本特征进行匹配，匹配损失使用了朴素的交叉熵损失，且训练过程中与 CLIP 相关的参数模型全都冻结。   whaosoft <a href="https://so.csdn.net/so/search?q=aiot&amp;spm=1001.2101.3001.7020" title="aiot">aiot</a> <a href="http://143ai.com/" rel="nofollow" title="http://143ai.com">http://143ai.com</a> </p> 
<p><img alt="" height="871" src="https://images2.imgbox.com/28/db/u3pAB9ew_o.png" width="990">CORA 是一个类 DETR 的检测器模型，类似于 DETR，其也使用了锚点预匹配策略来提前生成候选框用于框回归训练。具体来说，锚点预匹配是将每个标签框与最接近的一组锚点框进行匹配，以确定哪些锚点框应该被视为正样本，哪些应该被视为负样本。这个匹配过程通常是基于 IoU（交并比）进行的，如果锚点框与标签框的 IoU 超过一个预定义的阈值，则将其视为正样本，否则将其视为负样本。CORA 表明该策略能够有效提高对新类别定位能力的泛化性。</p> 
<p>但是使用锚点预匹配机制也会带来一些问题，比如只有在至少有一个锚点框与标签框形成匹配时，才可正常进行训练。否则，该标签框将被忽略，同时阻碍模型的收敛。进一步，即使标签框获得了较为准确的锚点框，由于 Region Classifier 的识别精度有限，进而导致该标签框仍可能被忽略，即标签框对应的类别信息没有与基于 CLIP 训练的 Region Classifier 形成对齐。因此，CORA 用 CLIP-Aligned 技术利用 CLIP 的语义识别能力，与预训练 ROI 的定位能力，在较少人力情形下对训练数据集的图像进行重新标注，使用这种技术，可以让模型在训练中匹配更多的标签框。</p> 
<p><img alt="" height="417" src="https://images2.imgbox.com/ca/2e/yHomibud_o.png" width="1058"></p> 
<p>相比于 RegionCLIP，CORA 在 COCO 数据集上进一步提升了 2.4 的 AP50 数值。</p> 
<p><strong>总结与展望</strong></p> 
<p>OVD 技术不仅与当前流行的跨 / 多模态大模型的发展紧密联系，同时也承接了过去科研工作者对目标检测领域的技术累积，是传统 AI 技术与面向通用 AI 能力研究的一次成功衔接。OVD 更是一项面向未来的全新目标检测技术，可以预料到的是，OVD 可以检测并定位任意目标的能力，也将反过来推进多模态大模型的进一步发展，有希望成为多模态 AGI 发展中的重要基石。当下，多模态大模型的训练数据来源是网络上的海量粗糙信息对，即文本图像对或文本语音对。若利用 OVD 技术对原本粗糙的图像信息进行精准定位，并辅助预测图像的语义信息来筛选语料，将会进一步提升大模型预训练数据的质量，进而优化大模型的表征能力与理解能力。</p> 
<p>一个很好的例子便是 SAM (Segment Anything)[7]，SAM 不仅让科研工作者们看到了通用视觉大模型未来方向，也引发了很多思考。值得注意的是，OVD 技术可以很好的接入 SAM，来增强 SAM 的语义理解能力，自动的生成 SAM 需要的 box 信息，从而进一步解放人力。同样的对于 AIGC (人工智能生成内容)，OVD 技术同样可以增强与用户之间的交互能力，如当用户需要指定一张图片的某一个目标进行变化，或对该目标生成一句描述的时候，可以利用 OVD 的语言理解能力与 OVD 对未知目标检测的能力实现对用户描述对象的精准定位，进而实现更高质量的内容生成。当下 OVD 领域的相关研究蓬勃发展，OVD 技术对未来通用 AI 大模型能够带来的改变值得期待。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f0be2473c1558dfe7877fa7254997619/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">自学Python能做哪些副业？我一般不告诉别人</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6f402e096ac8b910d61df8df13216809/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">为什么 Spring和IDEA 都不推荐使用@Autowired注解?</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>