<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【Paper Note】Swin Transformer: Hierarchical ViT using Shifted Windows - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【Paper Note】Swin Transformer: Hierarchical ViT using Shifted Windows" />
<meta property="og:description" content="Swin Transformer: Hierarchical ViT using Shifted Windows 论文信息概述核心思想整体结构名词解释与vit区别 模型处理过程概括Patch EmbeddingBasicLayerPatch MergingSwin Transform BlockWindow AttentionShifted Window Attention小结 模型使用及代码模型使用环境配置SwinT 代码Patch EmbeddingPatch MergingMask 论文信息 论文全名：Swin transformer: Hierarchical vision transformer using shifted windows发表期刊/会议：Proceedings of the IEEE/CVF international conference on computer vision论文链接：https://arxiv.org/abs/2103.14030引用：Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows.Proceedings of the IEEE/CVF international conference on computer vision. 2021: 10012-10022. 概述 1.SwinTransformer想设计一个可以作为密集预测任务的Transformer Backbone，其采用PatchMerging的策略，构建了层次化的特征，使得其可以作为密集预测任务的Backbone。
2.同时考虑到密集预测任务中，tokens数目太多导致计算量过大的问题，其采用一种在local window内部计算Self-Attention的机制去降低计算复杂度，使得整体计算复杂度由O(N^2)降低至O(N)水平。
3.为了弥补Local Self-Attention带来了远程依赖关系缺失的问题，其创新性地采用了Shift Window操作，引入了不同window之间的关系，并且在精度以及速度上都超越了简单的Sliding Window的方法。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/7d49f04ca5ddcc60a802edee6dc939c0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-04T16:52:18+08:00" />
<meta property="article:modified_time" content="2023-07-04T16:52:18+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Paper Note】Swin Transformer: Hierarchical ViT using Shifted Windows</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>Swin Transformer: Hierarchical ViT using Shifted Windows</h4> 
 <ul><li><a href="#_2" rel="nofollow">论文信息</a></li><li><a href="#_8" rel="nofollow">概述</a></li><li><a href="#_13" rel="nofollow">核心思想</a></li><li><a href="#_40" rel="nofollow">整体结构</a></li><li><ul><li><a href="#_49" rel="nofollow">名词解释</a></li><li><a href="#vit_56" rel="nofollow">与vit区别</a></li></ul> 
  </li><li><a href="#_64" rel="nofollow">模型处理过程</a></li><li><ul><li><a href="#_65" rel="nofollow">概括</a></li><li><a href="#Patch_Embedding_70" rel="nofollow">Patch Embedding</a></li><li><a href="#BasicLayer_73" rel="nofollow">BasicLayer</a></li><li><ul><li><a href="#Patch_Merging_76" rel="nofollow">Patch Merging</a></li><li><a href="#Swin_Transform_Block_85" rel="nofollow">Swin Transform Block</a></li><li><ul><li><a href="#Window_Attention_104" rel="nofollow">Window Attention</a></li><li><a href="#Shifted_Window_Attention_126" rel="nofollow">Shifted Window Attention</a></li><li><a href="#_152" rel="nofollow">小结</a></li></ul> 
   </li></ul> 
  </li></ul> 
  </li><li><a href="#_165" rel="nofollow">模型使用及代码</a></li><li><ul><li><a href="#_166" rel="nofollow">模型使用</a></li><li><ul><li><a href="#_167" rel="nofollow">环境配置</a></li><li><a href="#SwinT_169" rel="nofollow">SwinT</a></li></ul> 
   </li><li><a href="#_174" rel="nofollow">代码</a></li><li><ul><li><a href="#Patch_Embedding_181" rel="nofollow">Patch Embedding</a></li><li><a href="#Patch_Merging_197" rel="nofollow">Patch Merging</a></li><li><a href="#Mask_246" rel="nofollow">Mask</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_2"></a>论文信息</h2> 
<ul><li>论文全名：Swin transformer: Hierarchical vision transformer using shifted windows</li><li>发表期刊/会议：Proceedings of the IEEE/CVF international conference on computer vision</li><li>论文链接：https://arxiv.org/abs/2103.14030</li><li>引用：Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows.Proceedings of the IEEE/CVF international conference on computer vision. 2021: 10012-10022.</li></ul> 
<h2><a id="_8"></a>概述</h2> 
<p>1.SwinTransformer想设计一个可以作为<strong>密集预测任务的Transformer Backbone</strong>，其采用<strong>PatchMerging的策略</strong>，构建了层次化的特征，使得其可以作为密集预测任务的Backbone。<br> 2.同时考虑到密集预测任务中，<strong>tokens数目太多导致计算量过大的问题</strong>，其采用一种在<strong>local window内部计算Self-Attention</strong>的机制去降低计算复杂度，使得整体计算复杂度由O(N^2)降低至O(N)水平。<br> 3.为了弥补<strong>Local Self-Attention带来了远程依赖关系缺失</strong>的问题，其创新性地采用了<strong>Shift Window</strong>操作，<strong>引入了不同window之间的关系</strong>，并且在精度以及速度上都超越了简单的Sliding Window的方法。</p> 
<h2><a id="_13"></a>核心思想</h2> 
<p>Swin Transformer就是想让 Vision Transformer像卷积神经网络一样，也能够<strong>分成几个 block（分组计算），也能做层级式的特征提取</strong>，从而导致提出来的特征有多尺度的概念</p> 
<p><strong>分组计算的复杂度优势</strong></p> 
<ul><li>原生 Transformer 对 N 个 token 做 Self-Attention ，复杂度为 NxN，<br> 0 Swin Transformer 将 <strong>N 个 token 拆为 N/n 组， 每组 n （n设为常数）个token 进行计算，复杂度降为 [N*nxn]</strong> ，考虑到 n 是常数，那么复杂度其实为N。</li></ul> 
<p>分组计算导致的问题和解决方式</p> 
<ul><li>其一是分组后 Transformer 的视野局限于 n 个token，看不到全局信息 
  <ul><li>对于问题一，Swin Transformer 的解决方案即 Hierarchical，每个 stage 后对 2x2 组的特征向量进行融合和压缩（空间尺寸HxW变成0.5Hx0.5W，特征维度C-&gt;4C-&gt;2C ），这样视野就和 CNN-based 的结构一样，随着 stage 逐渐变大。</li></ul> </li><li>其二是组与组之间的信息缺乏交互。 
  <ul><li>对于问题二，Swin Transformer 的解决方法是 Shifted Windows，</li></ul> </li></ul> 
<p>整个Swin Transformer其实最重要的就两个点：</p> 
<ul><li><strong>相对位置信息</strong> 
  <ul><li>核心点在于可以把每种相对位置信息和att对应的一行信息对应上</li></ul> </li><li><strong>移动窗口注意力机制</strong> 
  <ul><li>移动窗口注意力机制核心点在于mask，mask矩阵的生成是通过窗口索引tensor相减得到的；</li></ul> </li></ul> 
<p>综合就是两个优点：</p> 
<ol><li>相比于ViT，Swin Transfomer 计算<strong>复杂度大幅度降低</strong>，具有输入图像大小<strong>线性计算复杂度</strong>。</li><li>Swin Transformer随着深度加深，逐渐合并图像块来构建<strong>层次化Transformer</strong>，可以作为通用的视觉骨干网络，应用于图像分类、目标检测和语义分割等任务。</li></ol> 
<p>Swin transformer和viT的架构不同之处：<br> <img src="https://images2.imgbox.com/95/b0/nFdDsKfw_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_40"></a>整体结构</h2> 
<p><img src="https://images2.imgbox.com/18/0e/tYmKnnzU_o.png" alt="在这里插入图片描述"><br> 上图有四个stage，每个stage都会缩小输入特征图的分辨率，像CNN一样逐层扩大感受野。<br> 流程解释：</p> 
<ul><li>在输入开始的时候，做了一个<strong>Patch Embedding，将图片切成一个个图块</strong>，并嵌入到Embedding。</li><li>在每个Stage里，由<strong>Patch Merging和多个Block组成</strong></li><li><strong>Patch Merging</strong>模块主要在每个<strong>Stage一开始降低图片分辨率</strong></li><li>Block具体结构如右图所示，主要是<strong>LayerNorm（LN），MLP(Multilayer Perceptron多层感知器)，Window Attention 和 Shifted Window Attention</strong>组成</li></ul> 
<h3><a id="_49"></a>名词解释</h3> 
<p>假设输入图片的尺寸为224X224，先划分成多个大小为4x4像素的小片，每个小片之间没有交集。</p> 
<ul><li>patch：224/4=56，那么一共可以划分56x56个小片。每一个小片就叫一个patch，</li><li>token：每一个patch将会被对待成一个token。所以patch=token。</li><li>window：而一张图被划分为7x7个window，每个window之间也没有交集。那么每个window就会包含8x8个patch</li></ul> 
<h3><a id="vit_56"></a>与vit区别</h3> 
<ol><li><strong>patch大小</strong>：与ViT一样对于输入的图像信息先做一个PatchEmbed操作将图像进行切分后打成多个patches传入进行后续的处理，但与ViT不同的是初始的切分不再以16 * 16的大小，而是以4 * 4的大小（为了看到更多细节）</li><li><strong>PatchMerging</strong>且后续通过PatchMerging的操作不断增加尺寸，进而可以得到多尺度信息便于在目标检测和语义分割中的使用</li><li><strong>位置编码</strong>：ViT在输入会给embedding进行位置编码。<br> Swin-T这里则是作为一个可选项（self.ape），Swin-T是在计算Attention的时候做了一个相对位置编码</li><li>分类：ViT会单独加上一个可学习参数，作为分类的token。<br> Swin-T则是直接做平均，输出分类，有点类似CNN最后的全局平均池化层</li></ol> 
<h2><a id="_64"></a>模型处理过程</h2> 
<h3><a id="_65"></a>概括</h3> 
<p><strong>PatchEmbed</strong>将图像换分为多个patches，<br> 之后接入多个<strong>BasicLayer</strong>进行处理(默认是和上述结构图一致，4个虚线框中的结构)，<br> 再然后将结果做<strong>avgpool输出计算结果</strong>，<br> 最后再进行<strong>分类</strong>操作（所以这里与ViT中不一样的是并没有采用一个cls token来进行分类而是对多个tokens取均值参与最终的分类运算）</p> 
<h3><a id="Patch_Embedding_70"></a>Patch Embedding</h3> 
<p>不能直接将一整幅图片作为一个patch，所以需要对图像进行切分然后处理为一个patch，但与ViT不同的是，Swin-T不在以16*16作为一个切割大小，而是以4 * 4作为切分大小，并通过后续的Patch Merging操作不断增大每个Patch的大小，进而实现多尺度变化</p> 
<h3><a id="BasicLayer_73"></a>BasicLayer</h3> 
<p>生成Patch之后就进入Swin- Transformer的核心模块部分了，每个basiclayer主要是由<strong>若干个Swin-Transformer Block和一个Patch Merging</strong></p> 
<h4><a id="Patch_Merging_76"></a>Patch Merging</h4> 
<ul><li>作用：是在每个Stage开始前做降采样，用于<strong>缩小分辨率，调整通道数</strong> ，类似于CNN中Pooling层。进而形成层次化的设计，同时也能节省一定运算量。</li><li>启发：在做Window Attention这个操作时，数据的维度变换是和CNN是有些相似的地方的，当然SwinTransformer的初衷也是想让Transformer能像CNN一样能够<strong>分成多个Block，进而在不同层级的Block之间提取到分辨率不同的特征信息</strong>，</li><li>实现：SwinTransformer引入了Patch Merging操作来实现，<strong>类似于CNN的池化的操作</strong><br> 在CNN中，则是在每个Stage开始前用stride=2的卷积/池化层来降低分辨率。<br> 每次降采样是两倍，因此在行方向和列方向上，间隔2选取元素。<br> 然后拼接在一起作为一整个张量，最后展开。此时通道维度会变成原先的4倍（因为H,W各缩小2倍），此时再通过一个全连接层再调整通道维度为原来的两倍</li></ul> 
<h4><a id="Swin_Transform_Block_85"></a>Swin Transform Block</h4> 
<p>这部分是整个程序的核心，它由<strong>窗口多头自注意层（window multi-head self-attention, W-MSA）和移位窗口多头自注意层（shifted-window multi-head self-attention, SW-MSA）组成</strong><br> 包含了论文中的很多知识点，涉及到相对位置编码、mask、window self-attention、shifted window self-attention</p> 
<p>整体流程如下：</p> 
<ul><li>输入到该stage的特征 z的l-1 先<strong>经过LN进行归一化</strong></li><li>再<strong>经过W-MSA进行特征的学习</strong>，</li><li>接着的是一个<strong>残差操作得到 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           z 
          
          
           
            
           
             ^ 
            
           
          
            l 
           
          
         
        
          z\hat{}^l 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9334em;"></span><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6944em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9334em;"><span class="" style="top: -3.1473em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span></span></span></span></span></span></span></span></span>的估计值（头上带个帽子就是估计值的意思）</strong>。</li><li>接着是<strong>一个LN，一个MLP以及一个残差</strong>，得到这一层的输出特征<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           z 
          
         
           l 
          
         
        
       
         z^l 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8491em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em;">l</span></span></span></span></span></span></span></span></span></span></span></span>。</li><li><strong>SW-MSA层的结构和W-MSA层类似</strong>，不同的是计算特征部分分别使用了SW-MSA和W-MSA，<br> 可以从上面的源码中看出它们除了shifted的这个bool值不同之外，其它的值是保持完全一致的。这一部分可以表示为式(2)</li></ul> 
<p><img src="https://images2.imgbox.com/28/7f/WkICN47H_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/d6/c0/3leLeBKQ_o.png" alt="在这里插入图片描述"><br> Swin Transformer使用<strong>window self-attention降低了计算复杂度</strong>，为了保证不重叠窗口之间有联系，采用了<strong>shifted window self-attention的方式重新计算一遍窗口偏移之后的自注意力</strong>，所以<strong>Swin Transformer Block都是成对出现的 (W-MSA + SW-MSA为一对)</strong> ，不同大小的Swin Transformer的Block个数也都为偶数，Block的数量不可能为奇数。</p> 
<h5><a id="Window_Attention_104"></a>Window Attention</h5> 
<p>传统的Transformer都是基于全局来计算注意力的，因此计算复杂度十分高。<br> 而<strong>Swin Transformer则将注意力的计算限制在每个窗口内</strong>，进而减少了计算量。<br> Window Attention与传统的Attention主要区别是<strong>在原始计算Attention的公式中的Q,K时加入了相对位置编码</strong></p> 
<p><img src="https://images2.imgbox.com/5f/df/zRfisLux_o.png" alt="在这里插入图片描述"></p> 
<p>绝对位置编码是在进行self-attention计算之前为每一个token添加一个可学习的参数，<br> <strong>相对位置编码</strong>如上式所示，是在进行self-attention计算时，在计算过程中添加一个可学习的相对位置参数B。</p> 
<p>实际上这里在参与Attention计算的B 是relative_position_bias_table这个可学习的参数，而relative_position_index则是作为一个index去取relative_position_bias_table中的值来参与运算<br> 有了相对位置索引(relative_position_index)之后，后续将相对位置bias(relative_position_bias_table)加入<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Q 
        
        
        
          K 
         
        
          T 
         
        
       
      
        QK^T 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0358em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span></span></span></span></span> 中<br> 这里比较难理解的就是relative_position_index的生成代码，如下图所示为整个relative_position_index的生成过程：</p> 
<p>假设window_size = 2*2即每个窗口有4个token [M=2] ，如图1所示，在计算self-attention时，每个token都要与所有的token计算QK值，如图2所示，当位置1的token计算self-attention时，要计算位置1与位置(1,2,3,4)的QK值，即以位置1的token为中心点，中心点位置坐标(0,0)，其他位置计算与当前位置坐标的偏移量。</p> 
<p><img src="https://images2.imgbox.com/48/60/uqitVv4I_o.png" alt="在这里插入图片描述"><br> 第一行就是以蓝色为中心的坐标，第二行是以紫色框为中心各颜色框的坐标，以此类推</p> 
<p>下图没有明确的计算过程但更加清晰<br> <img src="https://images2.imgbox.com/9d/bd/wV3Gsgub_o.png" alt="在这里插入图片描述"><br> 然后再最后一维上进行求和，展开成一个一维坐标，并注册为一个不参与网络学习的变量</p> 
<h5><a id="Shifted_Window_Attention_126"></a>Shifted Window Attention</h5> 
<p>前面的Window Attention是在每个窗口下计算注意力的，为了更好的和其他window进行信息交互，Swin Transformer还引入了shifted window操作。</p> 
<p>shifted window也就是把左侧的“规则”windows变为右侧“不规则”的windows，因为这样就能实现左侧“规则”windows之间的“信息交流”</p> 
<p><img src="https://images2.imgbox.com/5d/b1/T3AeBWJJ_o.png" alt="在这里插入图片描述"></p> 
<p>左边是没有重叠的Window Attention，而右边则是将窗口进行移位的Shift Window Attention。可以看到移位后的窗口包含了原本相邻窗口的元素。但这也引入了一个新问题，即window的个数翻倍了，由原本四个窗口变成了9个窗口。</p> 
<p>为此论文提出了一种针对于shifted window Attention更加高效的计算方式，如下图所示,为论文提供的高效计算shifted window Attention的示意图</p> 
<p>在实际代码里，我们是通过对特征图移位，并给Attention设置mask来间接实现的。能在保持原有的window个数下，最后的计算结果等价。</p> 
<p><img src="https://images2.imgbox.com/fa/04/B0UjFBKX_o.png" alt="在这里插入图片描述"></p> 
<ol><li>将特征数据进行cyclic shift操作，这个操作具体的代码中是使用的torch.roll实现的，如下图，通过将A B C三个区域的数据移动到如图的位置，那么整个窗口的划分就变得大小一致了<br> <img src="https://images2.imgbox.com/64/c5/uWB3SMmP_o.png" alt="在这里插入图片描述">2. Attention Mask：通过设置合理的mask，让Shifted Window Attention在与Window Attention相同的窗口个数下，达到等价的计算结果。得到大小一致的窗口之后，再进行带掩码的MSA操作，因为shift之后windows的大小都一致，所以在进行Attention计算时就比较好<strong>并行计算</strong>，同时通过掩码的作用，<strong>原本不属于同一个窗口的数据进行Attention之后也不会得到较高的注意力</strong>(比如蓝天和草原之间的Attention值就不会高)。</li></ol> 
<p>如下图，window_size=2,shift_size=-1，最左侧方块所示，我们分别对这9个方块编号为0～8，那么经过roll处理以后，每个区域的位置分布就如第二个方块所示；</p> 
<p>再以window_size在每个window内做带掩码的MSA，具体而言就是相同编号的区域做MSA时就没有mask，<strong>不同区域之间做MSA就需要有掩码</strong>，例如</p> 
<p>右下侧的那个window内一共有4个区域的数据(8,6,2,0)，那么区域8的Q只和区域8的K^ T相乘时才不带掩码，与其他区域的K^T相乘都需要带掩码，计算结果就如右下侧的红色框中所示：<br> <img src="https://images2.imgbox.com/44/ab/KYeULQ76_o.png" alt="在这里插入图片描述"><br> 3. reverse cyclic shift<br> 把之前cyclic shift的shift参数设置成对应的正数就行</p> 
<h5><a id="_152"></a>小结</h5> 
<p>首先我们对Shift Window后的每个窗口都给上index，并且做一个roll操作（window_size=2, shift_size=-1）</p> 
<p><img src="https://images2.imgbox.com/fd/59/rsdNEzWp_o.png" alt="在这里插入图片描述"></p> 
<p>希望在计算Attention的时候，让具有相同index QK进行计算，而忽略不同index QK计算结果。<br> 而要想在原始四个窗口下得到正确的结果，我们就必须给Attention的结果加入一个mask（如下图最右边所示）<br> 最后正确的结果如下图所示</p> 
<p><img src="https://images2.imgbox.com/17/ec/2Htxf41Y_o.png" alt="在这里插入图片描述"><br> 引入window这一个概念，将CNN的局部性引入，还能控制模型整体计算量。<br> 在Shift Window Attention部分，用一个mask和移位操作，很巧妙的实现计算等价。</p> 
<h2><a id="_165"></a>模型使用及代码</h2> 
<h3><a id="_166"></a>模型使用</h3> 
<h4><a id="_167"></a>环境配置</h4> 
<p>环境配置参考<a href="https://zhuanlan.zhihu.com/p/443418635" rel="nofollow">Swin Transformer算法环境配置（语义分割）</a></p> 
<h4><a id="SwinT_169"></a>SwinT</h4> 
<p>Swin-Transformer最核心的部分制成了一个类似于nn.Conv2D的接口并命名为SwinT。其输入、输出数据形状完全和Conv2D(CNN)一样，这极大的方便了使用Transformer来编写模型代码。</p> 
<p>参考<a href="https://zhuanlan.zhihu.com/p/452209913" rel="nofollow">SwinT-让Swin-Transformer的使用变得和CNN一样方便快捷</a></p> 
<h3><a id="_174"></a>代码</h3> 
<p>代码讲解参考<a href="https://blog.csdn.net/weixin_54546190/article/details/124422937">Swin-Transformer（原理 + 代码）详解</a></p> 
<p>非常详细的原理和代码展示<a href="https://blog.csdn.net/qq_39478403/article/details/120042232">【深度学习】详解 Swin Transformer (SwinT)</a></p> 
<h4><a id="Patch_Embedding_181"></a>Patch Embedding</h4> 
<ul><li> <p>Patch Partition<br> 作用：将RGB图转为非重叠的patch块。这里的patch尺寸为 4x4，乘上对应的RGB通道可得大小为4 x 4 x3=48。</p> </li><li> <p>Linear Embedding<br> 作用：将处理好的patch投影到指定的维度，这里embed_dim=96。</p> </li><li> <p>核心代码实现<br> 通过设定固定大小（4*4）的patch进行卷积，实现Patch Partition，再设定输出通道实现 Linear Embedding</p> </li></ul> 
<pre><code class="prism language-cpp">self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Conv2d</span><span class="token punctuation">(</span>in_c<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>patch_size<span class="token punctuation">,</span>stride<span class="token operator">=</span>patch_size<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>norm <span class="token operator">=</span> <span class="token function">norm_layer</span><span class="token punctuation">(</span>embed_dim<span class="token punctuation">)</span> <span class="token keyword">if</span> norm_layer <span class="token keyword">else</span> nn<span class="token punctuation">.</span><span class="token function">Identity</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<h4><a id="Patch_Merging_197"></a>Patch Merging</h4> 
<p>作用：将传入矩阵划分为2 x 2 大小的窗口，每个窗口的对应位置（例如下图中的同色块[^3]）相merge，再对merge后的四个特征矩阵相concatenate。最后经过layer normalization和linear layer降维。<br> <img src="https://images2.imgbox.com/1e/b9/cgKc8j75_o.png" alt="在这里插入图片描述"><br> Layer normalization和Linear layer的初始化</p> 
<pre><code class="prism language-cpp">self<span class="token punctuation">.</span>norm <span class="token operator">=</span> <span class="token function">norm_layer</span><span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> dim<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>reduction <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Linear</span><span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> dim<span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>False<span class="token punctuation">)</span>

</code></pre> 
<p>其中由图可知，每一层通道在传递给LayerNorm时都是原通道的4倍。传递给Linear时同理，Linear的输入为原通道的4倍，输出为原通道的2倍。</p> 
<p>Merging的实现</p> 
<pre><code class="prism language-cpp">  def <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        x<span class="token operator">:</span> B<span class="token punctuation">,</span> H<span class="token operator">*</span>W<span class="token punctuation">,</span> C
        <span class="token string">""</span>"
        B<span class="token punctuation">,</span> L<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        assert L <span class="token operator">==</span> H <span class="token operator">*</span> W<span class="token punctuation">,</span> <span class="token string">"input feature has wrong size"</span>

        x <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token function">view</span><span class="token punctuation">(</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C<span class="token punctuation">)</span>

        <span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">padding</span></span>
        # 如果输入feature map的H，W不是<span class="token number">2</span>的整数倍，需要进行padding
        pad_input <span class="token operator">=</span> <span class="token punctuation">(</span>H <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token function">or</span> <span class="token punctuation">(</span>W <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> pad_input<span class="token operator">:</span>
            <span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">to</span> <span class="token expression">pad the last <span class="token number">3</span> dimensions<span class="token punctuation">,</span> starting from the last dimension <span class="token operator">and</span> moving forward<span class="token punctuation">.</span></span></span>
            # <span class="token punctuation">(</span>C_front<span class="token punctuation">,</span> C_back<span class="token punctuation">,</span> W_left<span class="token punctuation">,</span> W_right<span class="token punctuation">,</span> H_top<span class="token punctuation">,</span> H_bottom<span class="token punctuation">)</span>
            # 注意这里的Tensor通道是<span class="token punctuation">[</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C<span class="token punctuation">]</span>，所以会和官方文档有些不同
            x <span class="token operator">=</span> F<span class="token punctuation">.</span><span class="token function">pad</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> W <span class="token operator">%</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> H <span class="token operator">%</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        x0 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token double-colon punctuation">::</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token double-colon punctuation">::</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">]</span>  # <span class="token punctuation">[</span>B<span class="token punctuation">,</span> H<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> W<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> C<span class="token punctuation">]</span>
        x1 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token double-colon punctuation">::</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token double-colon punctuation">::</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">]</span>  # <span class="token punctuation">[</span>B<span class="token punctuation">,</span> H<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> W<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> C<span class="token punctuation">]</span>
        x2 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token double-colon punctuation">::</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token double-colon punctuation">::</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">]</span>  # <span class="token punctuation">[</span>B<span class="token punctuation">,</span> H<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> W<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> C<span class="token punctuation">]</span>
        x3 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token double-colon punctuation">::</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token double-colon punctuation">::</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">]</span>  # <span class="token punctuation">[</span>B<span class="token punctuation">,</span> H<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> W<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> C<span class="token punctuation">]</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token function">cat</span><span class="token punctuation">(</span><span class="token punctuation">[</span>x0<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> x3<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  # <span class="token punctuation">[</span>B<span class="token punctuation">,</span> H<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> W<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token operator">*</span>C<span class="token punctuation">]</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token function">view</span><span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> C<span class="token punctuation">)</span>  # <span class="token punctuation">[</span>B<span class="token punctuation">,</span> H<span class="token operator">/</span><span class="token number">2</span><span class="token operator">*</span>W<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token operator">*</span>C<span class="token punctuation">]</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span><span class="token function">norm</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span><span class="token function">reduction</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>  # <span class="token punctuation">[</span>B<span class="token punctuation">,</span> H<span class="token operator">/</span><span class="token number">2</span><span class="token operator">*</span>W<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>C<span class="token punctuation">]</span>

        <span class="token keyword">return</span> x

</code></pre> 
<p>其中12-17行的作用是对行数或者列数是奇数的层进行扩充；<br> 19-24完成的是Merging操作，即每隔2行2列取一次元素并将这些元素沿最后一个维度（通道维度）concat</p> 
<h4><a id="Mask_246"></a>Mask</h4> 
<p>构建Mask是为了以后SW-MSA移动后窗口只对连续部分做self-attention，整个构建过程分为两步。</p> 
<pre><code class="prism language-cpp">  def <span class="token function">create_mask</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">calculate</span> <span class="token expression">attention mask <span class="token keyword">for</span> SW<span class="token operator">-</span>MSA</span></span>
        # 保证Hp和Wp是window_size的整数倍，起到了padding的作用
        Hp <span class="token operator">=</span> <span class="token keyword">int</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token function">ceil</span><span class="token punctuation">(</span>H <span class="token operator">/</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size
        Wp <span class="token operator">=</span> <span class="token keyword">int</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token function">ceil</span><span class="token punctuation">(</span>W <span class="token operator">/</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size
        # 拥有和feature map一样的通道排列顺序，方便后续window_partition
        img_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token function">zeros</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> Hp<span class="token punctuation">,</span> Wp<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>  # <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> Hp<span class="token punctuation">,</span> Wp<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
        h_slices <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token function">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token function">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token function">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> None<span class="token punctuation">)</span><span class="token punctuation">)</span>
        w_slices <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token function">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token function">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token function">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> None<span class="token punctuation">)</span><span class="token punctuation">)</span>
        cnt <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> h in h_slices<span class="token operator">:</span>
            <span class="token keyword">for</span> w in w_slices<span class="token operator">:</span>
                img_mask<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> cnt
                cnt <span class="token operator">+=</span> <span class="token number">1</span>
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e38e96c9112e6749ff6bdce4f318b3dd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Ambari-2.7.5在麒麟V10操作系统中的集群部署（一）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/56da0fce73eadb52a849232a46080031/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">趣谈MySQL 多个%等模糊查询</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>