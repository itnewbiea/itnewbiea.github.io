<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于LSTM神经网络的股票预测（Python&#43;pytorch） - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于LSTM神经网络的股票预测（Python&#43;pytorch）" />
<meta property="og:description" content="👨‍🎓个人主页：研学社的博客 💥💥💞💞欢迎来到本博客❤️❤️💥💥a
🏆博主优势：🌞🌞🌞博客内容尽量做到思维缜密，逻辑清晰，为了方便读者。
⛳️座右铭：行百里者，半于九十。
📋📋📋本文目录如下：🎁🎁🎁
目录
💥1 概述
📚2 运行结果
🎉3 参考文献
🌈4 Python代码实现
💥1 概述 为了解决循环神经网络存在的梯度消失等问题，专家学者经过研究提出了长短期记忆（Long-Short Term Memory ， LSTM ）神经网络。 LSTM 神经网络从本质上来说是一种特殊的循环神经网络，能够用来处理长期依赖问题。经过大量的研究结论证明 LSTM 神经网络已经解决了循环神经网络无法解决的许多问题，在时间序列预测问题上获得了更进一步的成功 [54] 。 LSTM 神经网络增加了一个信息存储记忆单元，可以维持一个持续信息流，使得梯度不会消失或爆炸。同时构造了遗忘门（Forget Gate ），输入门（ Input Gate ），输出门（ Output Gate ）分别对该记忆单元进行控制。这三个门就像滤波器一样，遗忘门控制记忆单元状态信息的舍弃与保留，输入门更新记忆单元状态，输出门控制 LSTM 单元输出。 LSTM 神经网络拥有着与循环神经网络相似的结构，不同之处在于它是通过内部的各个模块协同工作的[55] 。 LSTM 神经网络结构图如下。 LSTM 神经网络的优点主要表现在以下三方面： （1 ）解决了循环神经网络存在的长期依赖问题，能够处理时间滞后很长的数据序列。如果在某个时刻下信息较为重要，那么它对应的遗忘门位置会一直保留在接近于 1 的数值。这样就可以让这个时刻的信息一直往下传递下去而不被丢失，这就是 LSTM 神经网络能够处理长序列的原因之一。 （2）收敛性好。遗忘门对上一时刻的记忆进行控制，再加上当前输入产生当前时刻的新记忆。“门”结构的加入，提供了控制网络中信息传递的工具，让 LSTM神经网络能够记忆较为长期的信息。因此，通过遗忘门便可以对先前记忆进行处理进而对网络的输出产生影响[56] 。 （3）不易发生梯度消失或爆炸。循环神经网络在基于时间的反向传播中存在激活函数导数的乘数，与之相比 LSTM 神经网络其相应导数不是以乘积的形式存在，而是通过累加的方式进行计算。这一改变使得梯度消失和爆炸问题获得解决，同时不易陷入局部最优。 📚2 运行结果 # 归一化，便与训练 train_data_numpy = np.array(train_data) train_mean = np.mean(train_data_numpy) train_std = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/31bbe43b301117bb003f6088c3961fb6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-30T14:35:54+08:00" />
<meta property="article:modified_time" content="2023-05-30T14:35:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于LSTM神经网络的股票预测（Python&#43;pytorch）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p> 👨‍🎓<strong>个人主页：<strong><a href="https://blog.csdn.net/weixin_46039719?type=blog" title="研学社的博客">研学社的博客</a></strong> </strong>   </p> 
</blockquote> 
<blockquote> 
 <p>💥💥💞💞<strong>欢迎来到本博客</strong>❤️❤️💥💥a</p> 
 <p><strong><strong>🏆博主优势：</strong>🌞🌞🌞</strong><span style="color:#fe2c24;">博客内容尽量做到思维缜密，逻辑清晰，为了方便读者。</span></p> 
 <p></p> 
 <p></p> 
 <p>⛳️<strong>座右铭：</strong><span style="color:#a2e043;">行百里者，半于九十。</span></p> 
 <p></p> 
 <p>📋📋📋<u><strong>本文目录如下：</strong></u>🎁🎁🎁</p> 
 <p id="main-toc"><strong>目录</strong></p> 
 <p id="%F0%9F%92%A51%20%E6%A6%82%E8%BF%B0-toc" style="margin-left:40px;"><a href="#%F0%9F%92%A51%20%E6%A6%82%E8%BF%B0" rel="nofollow">💥1 概述</a></p> 
 <p id="%F0%9F%93%9A2%20%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;"><a href="#%F0%9F%93%9A2%20%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C" rel="nofollow">📚2 运行结果</a></p> 
 <p id="%F0%9F%8E%893%C2%A0%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-toc" style="margin-left:40px;"><a href="#%F0%9F%8E%893%C2%A0%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE" rel="nofollow">🎉3 参考文献</a></p> 
 <p id="%F0%9F%8C%884%20Python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#%F0%9F%8C%884%20Python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">🌈4 Python代码实现</a></p> 
 <hr id="hr-toc"> 
 <p></p> 
</blockquote> 
<h3 id="%F0%9F%92%A51%20%E6%A6%82%E8%BF%B0"><strong>💥1 概述</strong></h3> 
<div style="text-align:justify;"> 
 <span style="color:#000000;">为了解决循环神经网络存在的梯度消失等问题，专家学者经过研究提出了长短期记忆（Long-Short Term Memory</span> 
 <span style="color:#000000;">，</span> 
 <span style="color:#000000;">LSTM</span> 
 <span style="color:#000000;">）神经网络。</span> 
 <span style="color:#000000;">LSTM </span> 
 <span style="color:#000000;">神经网络从本质上来说是一种特殊的循环神经网络，能够用来处理长期依赖问题。经过大量的研究结论证明 LSTM </span> 
 <span style="color:#000000;">神经网络已经解决了循环神经网络无法解决的许多问题，在时间序列预测问题上获得了更进一步的成功 [54]</span> 
 <span style="color:#000000;">。</span> 
 <span style="color:#000000;">LSTM </span> 
 <span style="color:#000000;">神经网络增加了一个信息存储记忆单元，可以维持一个持续信息流，使得梯度不会消失或爆炸。同时构造了遗忘门（Forget Gate</span> 
 <span style="color:#000000;">），输入门（</span> 
 <span style="color:#000000;">Input Gate</span> 
 <span style="color:#000000;">），输出门（</span> 
 <span style="color:#000000;">Output Gate</span> 
 <span style="color:#000000;">）分别对该记忆单元进行控制。这三个门就像滤波器一样，遗忘门控制记忆单元状态信息的舍弃与保留，输入门更新记忆单元状态，输出门控制 LSTM </span> 
 <span style="color:#000000;">单元输出。</span> 
 <span style="color:#000000;">LSTM </span> 
 <span style="color:#000000;">神经网络拥有着与循环神经网络相似的结构，不同之处在于它是通过内部的各个模块协同工作的[55]</span> 
 <span style="color:#000000;">。</span> 
 <span style="color:#000000;">LSTM </span> 
 <span style="color:#000000;">神经网络结构图如下。 </span> 
</div> 
<p style="text-align:justify;"><img alt="" height="1185" src="https://images2.imgbox.com/07/e2/0jo5vkrq_o.png" width="1200"></p> 
<div style="text-align:justify;"> 
 <span style="color:#000000;">LSTM </span> 
 <span style="color:#000000;">神经网络的优点主要表现在以下三方面： </span> 
</div> 
<div style="text-align:justify;"> 
 <span style="color:#000000;">（1</span> 
 <span style="color:#000000;">）解决了循环神经网络存在的长期依赖问题，能够处理时间滞后很长的数据序列。如果在某个时刻下信息较为重要，那么它对应的遗忘门位置会一直保留在接近于 1 </span> 
 <span style="color:#000000;">的数值。这样就可以让这个时刻的信息一直往下传递下去而不被丢失，这就是 LSTM </span> 
 <span style="color:#000000;">神经网络能够处理长序列的原因之一。 </span> 
</div> 
<div style="text-align:justify;"> 
 <span style="color:#000000;">（2）收敛性好。遗忘门对上一时刻的记忆进行控制，再加上当前输入产生当前时刻的新记忆。“门”结构的加入，提供了控制网络中信息传递的工具，让 LSTM神经网络能够记忆较为长期的信息。因此，通过遗忘门便可以对先前记忆进行处理进而对网络的输出产生影响[56]</span> 
 <span style="color:#000000;">。 </span> 
</div> 
<div style="text-align:justify;"> 
 <span style="color:#000000;">（3）不易发生梯度消失或爆炸。循环神经网络在基于时间的反向传播中存在激活函数导数的乘数，与之相比 LSTM </span> 
 <span style="color:#000000;">神经网络其相应导数不是以乘积的形式存在，而是通过累加的方式进行计算。这一改变使得梯度消失和爆炸问题获得解决，同时不易陷入局部最优。</span> 
</div> 
<p> </p> 
<h3 id="%F0%9F%93%9A2%20%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C"><strong>📚</strong><strong>2 运行结果</strong></h3> 
<blockquote> 
 <pre># 归一化，便与训练
train_data_numpy = np.array(train_data)
train_mean = np.mean(train_data_numpy)
train_std  = np.std(train_data_numpy)
train_data_numpy = (train_data_numpy - train_mean) / train_std
train_data_tensor = torch.Tensor(train_data_numpy)

# 创建 dataloader
train_set = TrainSet(train_data_tensor)
train_loader = DataLoader(train_set, batch_size=10, shuffle=True)</pre> 
</blockquote> 
<p></p> 
<blockquote> 
 <pre>for i in range(DAYS_BEFORE, len(all_series)):
    x = all_series[i - DAYS_BEFORE:i]
    # 将 x 填充到 (bs, ts, is) 中的 timesteps
    x = torch.unsqueeze(torch.unsqueeze(x, dim=0), dim=2)
    
    if torch.cuda.is_available():
        x = x.cuda()

    y = rnn(x)
    
    if i &lt; test_start:
        generate_data_train.append(torch.squeeze(y.cpu()).detach().numpy() * train_std + train_mean)
    else:
        generate_data_test.append(torch.squeeze(y.cpu()).detach().numpy() * train_std + train_mean)
        
plt.figure(figsize=(12,8))
plt.plot(df_index[DAYS_BEFORE: TRAIN_END], generate_data_train, 'b', label='generate_train', )
plt.plot(df_index[TRAIN_END:], generate_data_test, 'k', label='generate_test')
plt.plot(df_index, all_series.clone().numpy()* train_std + train_mean, 'r', label='real_data')
plt.legend()
plt.show()</pre> 
</blockquote> 
<p><img alt="" height="965" src="https://images2.imgbox.com/0c/72/XMjae4dF_o.png" width="1200"></p> 
<blockquote> 
 <pre>plt.figure(figsize=(10,16))

plt.subplot(2,1,1)
plt.plot(df_index[100 + DAYS_BEFORE: 130 + DAYS_BEFORE], generate_data_train[100: 130], 'b', label='generate_train')
plt.plot(df_index[100 + DAYS_BEFORE: 130 + DAYS_BEFORE], (all_series.clone().numpy()* train_std + train_mean)[100 + DAYS_BEFORE: 130 + DAYS_BEFORE], 'r', label='real_data')
plt.legend()

plt.subplot(2,1,2)
plt.plot(df_index[TRAIN_END + 50: TRAIN_END + 80], generate_data_test[50:80], 'k', label='generate_test')
plt.plot(df_index[TRAIN_END + 50: TRAIN_END + 80], (all_series.clone().numpy()* train_std + train_mean)[TRAIN_END + 50: TRAIN_END + 80], 'r', label='real_data')
plt.legend()

plt.show()</pre> 
 <p> </p> 
</blockquote> 
<p><img alt="" height="886" src="https://images2.imgbox.com/b2/23/GC0pvxx8_o.png" width="1200"></p> 
<blockquote> 
 <p><img alt="" height="903" src="https://images2.imgbox.com/3e/a4/KRIQ3llw_o.png" width="1200"> </p> 
</blockquote> 
<blockquote> 
 <p> </p> 
 <pre>print(len(all_series_test2))
print(len(df_index))
print(len(iter_series))

plt.figure(figsize=(12,8))
plt.plot(df_index[ : len(iter_series)], iter_series, 'b', label='generate_train')
plt.plot(df_index, all_series_test2.clone().numpy() * train_std + train_mean, 'r', label='real_data')
plt.legend()
plt.show()</pre> 
</blockquote> 
<p> <img alt="" height="989" src="https://images2.imgbox.com/93/7e/7v9hZNeh_o.png" width="1200"></p> 
<p> </p> 
<h3 id="%F0%9F%8E%893%C2%A0%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><strong><strong>🎉3</strong></strong><strong><strong> 参考文献</strong></strong></h3> 
<blockquote> 
 <p>部分理论来源于网络，如有侵权请联系删除。</p> 
</blockquote> 
<p>[1]曹彦彦. LSTM模型优化及其在股指预测中的应用研究[D].东北财经大学,2022.DOI:10.27006/d.cnki.gdbcu.2022.000020.</p> 
<p>[2]张杰. 基于LSTM的股票预测实证分析[D].山东大学,2020.DOI:10.27272/d.cnki.gshdu.2020.002958.</p> 
<p>[3]隋金城. 基于LSTM神经网络的股票预测研究[D].青岛科技大学,2020.DOI:10.27264/d.cnki.gqdhc.2020.000423.</p> 
<h3 id="%F0%9F%8C%884%20Python%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk0MDMzNzYwOA==&amp;action=getalbum&amp;album_id=2591810113208958977#wechat_redirect" rel="nofollow" title="🌈">🌈</a><strong><strong>4 Python代码实现</strong></strong></h3>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/64d02492e0ecf17e6ad963fe4f15a8d8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">electron24整合vite4&#43;vue3创建跨端桌面程序</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6faf4a77931735dd3467cf707670603d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">IDEA 2020 超简单下载安装 使用教程，三步教你完成</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>