<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用Hadoop 的 Java API 操纵 HDFS 文件系统 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="使用Hadoop 的 Java API 操纵 HDFS 文件系统" />
<meta property="og:description" content="0x00: 说明 使用 Java 操作 HDFS 文件系统可以使用其对应的Java API，即对应三个 jar 依赖包：
hadoop-common.jar (该文件在 hadoop-2.10.1.tar.gz 压缩包中的 \share\hadoop\common 目录下)hadoop-hdfs.jar (该文件在 hadoop-2.10.1.tar.gz 压缩包中的 \share\hadoop\hdfs 目录下)hadoop-client.jar (该文件在 hadoop-2.10.1.tar.gz 压缩包中的 \share\hadoop\hdfs 目录下) 这三个 jar 包的具体名字可能根据你所安装的版本进行变化，在本文档中这三个文件名称具体如下：
此处我不推荐直接通过 jar 包引入依赖支持，本文中也将不在撰写 jar 的方式引入，因为这三个jar还依赖于其他的依赖库，例如 woodstox-core 等一系列jar包，过程较为繁琐，因此本文中使用 pom 表引入对应配置，使用pom表来管理依赖后，子依赖会自动导入，不在需要我们手动引入，较为方便，pom表依赖配置如下：
&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.10.1&lt;/version&gt; &lt;/dependency&gt; 在本文中我将在宿主机使用 IDEA 创建对应工程调用虚拟机的 Java API 接口，创建基本 Java 工程在本文中将不在赘述，但请注意，在创建工程时请将系统编译方式选择为 Maven，如下图所示：
0x01: 依赖导入 注意，在第三步引入后你会发现他是处于红色的错误状态，此时点击第四步刷新依赖后即可
引入依赖完成后如下图所示，但因为版本问题存在相对较多的安全的漏洞提示，鼠标放在上方即可显示相关的CVE漏洞编号，但我们仅仅只是学习用途，该问题我们直接忽略
该步骤完成后 pom 表中无红色文本内容时则代表已完成依赖引入" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/d105ba8605f6689270c543ee520c0316/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-27T19:12:06+08:00" />
<meta property="article:modified_time" content="2023-04-27T19:12:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用Hadoop 的 Java API 操纵 HDFS 文件系统</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3 id="main-toc">0x00: 说明</h3> 
<p id="u1b2b57e4">使用 Java 操作 HDFS 文件系统可以使用其对应的Java API，即对应三个 jar 依赖包：</p> 
<ul><li id="u52e30681">hadoop-common.jar (该文件在 hadoop-2.10.1.tar.gz 压缩包中的 <code>\share\hadoop\common</code> 目录下)</li><li id="u4dd1be76">hadoop-hdfs.jar (该文件在 hadoop-2.10.1.tar.gz 压缩包中的 <code>\share\hadoop\hdfs</code> 目录下)</li><li id="u8d2a35ce">hadoop-client.jar (该文件在 hadoop-2.10.1.tar.gz 压缩包中的 <code>\share\hadoop\hdfs</code> 目录下)</li></ul> 
<p id="u73078f68"><span style="color:#be191c;">这三个 jar 包的具体名字可能根据你所安装的版本进行变化，在本文档中这三个文件名称具体如下：</span></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/5c/d0/dXGvqFwh_o.png"></p> 
<p id="u67368451"><span style="color:#be191c;"><strong>此处我不推荐直接通过 jar 包引入依赖支持，本文中也将不在撰写 jar 的方式引入，因为这三个jar还依赖于其他的依赖库，例如 </strong><code><strong>woodstox-core</strong></code><strong> 等一系列jar包，过程较为繁琐，因此本文中使用 pom 表引入对应配置，使用pom表来管理依赖后，子依赖会自动导入，不在需要我们手动引入，较为方便，pom表依赖配置如下：</strong></span></p> 
<pre><code class="language-XML">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
  &lt;version&gt;2.10.1&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
  &lt;version&gt;2.10.1&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
  &lt;version&gt;2.10.1&lt;/version&gt;
&lt;/dependency&gt;</code></pre> 
<p id="ue7caa476">在本文中我将在宿主机使用 IDEA 创建对应工程调用虚拟机的 Java API 接口，创建基本 Java 工程在本文中将不在赘述，但请注意，在创建工程时请将系统编译方式选择为 Maven，如下图所示：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/bc/ee/15KigtO9_o.png"></p> 
<h3 id="bofMc">0x01: 依赖导入</h3> 
<p id="ube12efba">注意，在第三步引入后你会发现他是处于红色的错误状态，此时点击第四步刷新依赖后即可</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/3a/fa/6IfE8K6f_o.png"></p> 
<p id="u6d0d0754">引入依赖完成后如下图所示，但因为版本问题存在相对较多的安全的漏洞提示，鼠标放在上方即可显示相关的CVE漏洞编号，但我们仅仅只是学习用途，该问题我们直接忽略</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/c5/6b/EmMlcr6t_o.png"></p> 
<p id="u431c0152">该步骤完成后 pom 表中无红色文本内容时则代表已完成依赖引入</p> 
<h3 id="IebX0">0x02: HDFS 文件创建</h3> 
<p id="u717e5a65">这里给出一段示例代码，你可以用来验证 HDFS 的 Java API 是否能正常工作。在开始前，你需要了解如下几个注意点：</p> 
<ol><li id="u545dc818">我们在访问 hdfs 的时候会进行一个权限认证认证的过程，取用户名的过程是这样的：</li></ol> 
<p id="u9f181830">首先读取 <code>HADOOP_USER_NAME</code>系统环境变量，如果不为空，那么拿它作用户名</p> 
<p id="u7f6cf046">如果为空，则读取 <code>HADOOP_USER_NAME</code> 这个 java 环境变量，如果继续为空，则抛出异常</p> 
<p id="u4884bf96">因此在下方代码中，我在第 19-20 行中加了环境变量配置，如果你不想进行该配置，则可以在第 27 行文件系统配置中进行修改，添加一个user参数，代码如下:</p> 
<pre><code class="language-java">// 链接到文件系统
FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.234.129:9000"), conf, "root");</code></pre> 
<ol><li id="u6c48fe8f">该段代码运行后可能会提示有找不到 logger 和 log4j 未正常初始化的配置，如下图，但因为我们此处仅作学习用途，该警告我们可直接忽略。</li></ol> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/71/a5/WhSzrhBL_o.png"></p> 
<p id="ua4d217fe"></p> 
<p id="u59ce1dde"><strong>完整实例代码如下：</strong></p> 
<pre><code class="language-java">package org.example;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Properties;


public class Main {

    public static void main(String[] args) {

        // 配置环境变量
        Properties properties = System.getProperties();
        properties.setProperty("HADOOP_USER_NAME", "root");

        // 创建配置
        Configuration conf = new Configuration();

        try {

            // 链接到文件系统
            FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.234.129:9000"), conf);

            // 创建文件
            FSDataOutputStream fsDataOutputStream = fileSystem.create(new Path("/Tianxidev/test/1.txt"));

            // 写入文件内容到输出流
            fsDataOutputStream.writeUTF("Hi HDFS!");

            // 输出流提交 HDFS
            fsDataOutputStream.flush();

            // 关闭输出流
            fsDataOutputStream.close();

            // 关闭文件系统
            fileSystem.close();

        } catch (IOException | URISyntaxException e) {
            throw new RuntimeException(e);
        }

    }

}</code></pre> 
<p id="u891faef2">运行后，我们在虚拟机上执行命令 <code>hdfs dfs -ls /</code> 查看 HDFS 文件系统根目录，发现已多出一个我们代码中创建的文件夹，使用 <code>hdfs dfs -cat /Tianxidev/test/1.txt</code> 查看我们创建的文件内容发现写入内容为预期文本，此时则代表 HDFS 的 Java API 工作正常。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/98/07/IEMx7FCf_o.png"></p> 
<p id="u6280e9bf"></p> 
<h3 id="HsYco">0x03: HDFS 文件夹创建</h3> 
<p id="u7eac43af">此处仅提供完整样例代码不在提供运行截图，样例代码已经过验证</p> 
<pre><code class="language-java">package org.example;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Properties;


public class Main {

    public static void main(String[] args) {

        // 配置环境变量
        Properties properties = System.getProperties();
        properties.setProperty("HADOOP_USER_NAME", "root");

        // 创建配置
        Configuration conf = new Configuration();

        try {

            // 链接到文件系统
            FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.234.129:9000"), conf);

            // 创建文件夹
            fileSystem.mkdirs(new Path("/Tianxidev/test1"));

            // 关闭文件系统
            fileSystem.close();

        } catch (IOException | URISyntaxException e) {
            throw new RuntimeException(e);
        }

    }

}</code></pre> 
<h3 id="rsPrm">0x04: 本地文件上传 HDFS</h3> 
<p id="ucfb3699d">此处仅提供完整样例代码不在提供运行截图，样例代码已经过验证</p> 
<pre><code class="language-java">package org.example;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Properties;


public class Main {

    public static void main(String[] args) {

        // 配置环境变量
        Properties properties = System.getProperties();
        properties.setProperty("HADOOP_USER_NAME", "root");

        // 创建配置
        Configuration conf = new Configuration();

        try {

            // 链接到文件系统
            FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.234.129:9000"), conf);

            // 上传本地文件
            fileSystem.copyFromLocalFile(new Path("D:\\temp\\hdfs_test_1.txt"),new Path("/Tianxidev/test1"));

            // 关闭文件系统
            fileSystem.close();

        } catch (IOException | URISyntaxException e) {
            throw new RuntimeException(e);
        }

    }

}</code></pre> 
<h3 id="D9hML">0x05: 读取 HDFS 文件内容</h3> 
<p id="u203095ae">此处仅提供完整样例代码不在提供运行截图，样例代码已经过验证</p> 
<pre><code class="language-java">package org.example;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Properties;


public class Main {

    public static void main(String[] args) {

        // 配置环境变量
        Properties properties = System.getProperties();
        properties.setProperty("HADOOP_USER_NAME", "root");

        // 创建配置
        Configuration conf = new Configuration();

        try {

            // 链接到文件系统
            FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.234.129:9000"), conf);

            // 打开文件
            FSDataInputStream fsDataInputStream = fileSystem.open(new Path("/Tianxidev/test/1.txt"));

            // 打印文件内容
            System.out.println("文件内容: " + fsDataInputStream.readUTF());

            // 关闭输入流
            fsDataInputStream.close();

            // 关闭文件系统
            fileSystem.close();

        } catch (IOException | URISyntaxException e) {
            throw new RuntimeException(e);
        }

    }

}</code></pre> 
<h3 id="bZaK1">0x06: HDFS 文件下载本地</h3> 
<p id="u3c9a81c2">此处仅提供完整样例代码不在提供运行截图，样例代码已经过验证</p> 
<p id="ufa4bc8b7"><span style="color:#be191c;">请注意，因为win系统文件系统结构原因，会抛出如下报错：</span></p> 
<pre><code class="language-java">java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. </code></pre> 
<p id="u4507be68"><strong><span style="color:#be191c;">出现该报错时需要你引入 winutils 支持，winutil.exe 主要用于模拟linux下的目录环境，你可以在互联网上寻找该配置教程，如果你不想麻烦，也可以选择通过 API 读取文件内容然后在使用 Java 文件相关API方法将文件流作为文件写入系统。</span></strong></p> 
<pre><code class="language-java">package org.example;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Properties;


public class Main {

    public static void main(String[] args) {

        // 配置环境变量
        Properties properties = System.getProperties();
        properties.setProperty("HADOOP_USER_NAME", "root");

        // 创建配置
        Configuration conf = new Configuration();

        try {

            // 链接到文件系统
            FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.234.129:9000"), conf);

            // 下载文件到本地
            fileSystem.copyToLocalFile(new Path("/Tianxidev/test1/hdfs_test_1.txt"),new Path("D:\\temp\\hdfs_test_1.txt"));

            // 关闭文件系统
            fileSystem.close();

        } catch (IOException | URISyntaxException e) {
            throw new RuntimeException(e);
        }

    }

}</code></pre> 
<h3 id="vf9UE">0x07: 远程删除 HDFS 文件或文件夹</h3> 
<p id="ua2fee69c">此处仅提供完整样例代码不在提供运行截图，样例代码已经过验证</p> 
<pre><code class="language-java">package org.example;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.Properties;


public class Main {

    public static void main(String[] args) {

        // 配置环境变量
        Properties properties = System.getProperties();
        properties.setProperty("HADOOP_USER_NAME", "root");

        // 创建配置
        Configuration conf = new Configuration();

        try {

            // 链接到文件系统
            FileSystem fileSystem = FileSystem.get(new URI("hdfs://192.168.234.129:9000"), conf);

            // b: true 递归删除目录 false 删除指定文件
            fileSystem.delete(new Path("/Tianxidev/test1/hdfs_test_1.txt"),false);
            fileSystem.delete(new Path("/Tianxidev"),true);

            // 关闭文件系统
            fileSystem.close();

        } catch (IOException | URISyntaxException e) {
            throw new RuntimeException(e);
        }

    }

}</code></pre> 
<p id="u151b6aba"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d0e05940689936cad50b203056816d31/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">VMware安装Windows server 2016</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ac6b13126653b70e453d923ad805a82f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue3有了哪些提升</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>