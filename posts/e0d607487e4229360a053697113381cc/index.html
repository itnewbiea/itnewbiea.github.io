<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>EDTER: Edge Detection with Transformer—边缘检测，效果远超之前的研究 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="EDTER: Edge Detection with Transformer—边缘检测，效果远超之前的研究" />
<meta property="og:description" content="EDTER: Edge Detection with Transformer（EDTER: 基于 Transformer 的边缘检测） 期刊合集：最近五年，包含顶刊，顶会，学报&gt;&gt;网址
文章来源：CVPR 2022
研究背景 卷积神经网络通过逐步探索上下文和语义特征，在边缘检测方面取得了重大进展，然而，局部细节随着感受野的扩大而逐渐被抑制。基于 Transformer 在 长期依赖关系 方面出色的表现，文章提出一种基于 Transformer 的边缘检测器，通过利用完整的图像上下文信息和详细的局部线索来提取清晰的物体边界和有意义的边缘。
EDTER 分两阶段进行：在阶段 I 中，使用 global transformer encoder 捕获粗粒度图像补丁上的远距离全局上下文；第 II 阶段， local transformer encoder 在细粒度补丁上工作，以挖掘近距离局部线索，每个 Transformer 编码器后面都有一个 双向多级聚合解码器，用于提高分辨率。最后，通过特征融合模块将全局上下文和局部线索结合起来，并输入决策头（head）进行边缘预测。
问题引入 边缘检测是计算机视觉中最基本的问题之一，具有广泛的应用，如图像分割，物体检测，视频物体分割。对于给定的输入图像，边缘检测的目的是提取精确的物体边界和视觉显著的边缘，但由于复杂的背景、不一致的注释等因素，它具有挑战性。
首先，考虑到计算量的问题，Transformer 通常应用于尺寸相对较大的 patch，而粗粒度的 patch 不利于学习边缘的精确特征，在不增加计算负担的情况下对细粒度补丁执行自我关注是至关重要的。其次，如图 (d) 所示，从相交的薄物体（有多个物体的情况）中提取精确的边缘是具有挑战性的，因此，有必要设计一个有效的 decoder 用于生成边缘感知的高分辨率特征的编码器。
论文分析 文章贡献如下：
1）提出了边缘检测变压器( EDTER )，用于检测自然图像中的物体轮廓和有意义的边缘。
2）EDTER 设计用于有效地探索远程全局上下文( 阶段 I )和捕获细粒度局部线索( 阶段 II )。
4）提出了一种新的双向多级聚合( BiMLA ) decoder 来促进 Transformer 中的信息流。
3）为了有效地整合全局信息和局部信息，使用特征融合模块( FFM )来融合从阶段 I 和阶段 II 提取的线索。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/e0d607487e4229360a053697113381cc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-27T17:39:05+08:00" />
<meta property="article:modified_time" content="2023-03-27T17:39:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">EDTER: Edge Detection with Transformer—边缘检测，效果远超之前的研究</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="EDTER_Edge_Detection_with_TransformerEDTER__Transformer__0"></a>EDTER: Edge Detection with Transformer（EDTER: 基于 Transformer 的边缘检测）</h2> 
<p>期刊合集：最近五年，包含顶刊，顶会，学报&gt;&gt;<a href="https://blog.csdn.net/JJxiao520/article/details/127701543">网址</a><br> 文章来源：<a href="https://paperswithcode.com/paper/edter-edge-detection-with-transformer" rel="nofollow">CVPR 2022</a></p> 
<h3><a id="_4"></a>研究背景</h3> 
<p> 卷积神经网络通过逐步探索上下文和语义特征，在边缘检测方面取得了重大进展，然而，局部细节随着感受野的扩大而逐渐被抑制。基于 Transformer 在 <strong>长期依赖关系</strong> 方面出色的表现，文章提出一种基于 Transformer 的边缘检测器，通过利用完整的图像上下文信息和详细的局部线索来提取清晰的物体边界和有意义的边缘。<br>  EDTER 分两阶段进行：在阶段 I 中，使用 global transformer encoder 捕获粗粒度图像补丁上的远距离全局上下文；第 II 阶段， local transformer encoder 在细粒度补丁上工作，以挖掘近距离局部线索，每个 Transformer 编码器后面都有一个 <strong>双向多级聚合解码器</strong>，用于提高分辨率。最后，通过特征融合模块将全局上下文和局部线索结合起来，并输入决策头（head）进行边缘预测。</p> 
<h3><a id="_7"></a>问题引入</h3> 
<p> 边缘检测是计算机视觉中最基本的问题之一，具有广泛的应用，如图像分割，物体检测，视频物体分割。对于给定的输入图像，边缘检测的目的是提取精确的物体边界和视觉显著的边缘，但由于复杂的背景、不一致的注释等因素，它具有挑战性。</p> 
<p><img src="https://images2.imgbox.com/5a/f5/GIuAVDuK_o.png" alt="在这里插入图片描述"><br>  首先，考虑到计算量的问题，Transformer 通常应用于尺寸相对较大的 patch，而粗粒度的 patch 不利于学习边缘的精确特征，在不增加计算负担的情况下对细粒度补丁执行自我关注是至关重要的。其次，如图 (d) 所示，从相交的薄物体（有多个物体的情况）中提取精确的边缘是具有挑战性的，因此，有必要设计一个有效的 decoder 用于生成边缘感知的高分辨率特征的编码器。</p> 
<h3><a id="_12"></a>论文分析</h3> 
<p>文章贡献如下：</p> 
<p>1）提出了边缘检测变压器( EDTER )，用于检测自然图像中的物体轮廓和有意义的边缘。</p> 
<p>2）EDTER 设计用于有效地探索远程全局上下文( 阶段 I )和捕获细粒度局部线索( 阶段 II )。</p> 
<p>4）提出了一种新的双向多级聚合( BiMLA ) decoder 来促进 Transformer 中的信息流。</p> 
<p>3）为了有效地整合全局信息和局部信息，使用特征融合模块( FFM )来融合从阶段 I 和阶段 II 提取的线索。</p> 
<h4><a id="_22"></a>网络框架</h4> 
<h4><a id="1Overview_23"></a>1、Overview</h4> 
<p><img src="https://images2.imgbox.com/43/9e/sKl5C3fg_o.png" alt="在这里插入图片描述"><br>  首先在阶段 I，将输入图像分割为粗粒度补丁序列，并使用全局转换器编码器（Global Transformer Enconder）来学习全局上下文信息，然后再采用双向的多级聚合( BiMLA )解码器生成高分辨率特征。在阶段 II 中，通过不重叠的滑动窗口采样，将整个图像分割成多个细粒度斑块序列。然后，依次对每个序列执行一个局部变压器编码器，以捕获短程局部线索，将所有的局部线索整合到一个局部的BiMLA解码器中，实现像素级的特征映射。最后，通过特征融合模块( FFM )对全局特征和局部特征<strong>进行融合</strong>，然后输入决策头预测最终的边缘图。</p> 
<h4><a id="2Review_Vision_Transformer_26"></a>2、Review Vision Transformer</h4> 
<p> 本文中的框架所使用的变压器编码器遵循视觉变压器 ( ViT )，如下所述。</p> 
<p><strong>Image Partition：</strong><br>  ViT 的第一步工作，就是将一个( X ∈ R <sup>H×W ×3</sup>) 的二维图像转换为一维图像补丁序列（可以理解为转变为列向量或者行向量）。具体来说，将图像 X 均匀地分割成一组大小为 P × P 的扁平图像斑块序列，得到 H / P × W / P 的 vision tokens。然后，利用可学习线性投影将序列映射到隐藏的嵌入空间中。（经过投影的特征称之为补丁嵌入）。此外，为了保留这些位置信息，将标准的可学习一维位置嵌入添加到补丁嵌入中。最后，将组合好的嵌入 (记为z<sup>0</sup> ) 被馈送到 Transformer encoder。</p> 
<p><strong>Transformer Encoder：</strong><br>  一个标准 Transformer encoder 是由 L 个 <strong>Transformer 块</strong>组成，每个块都有一个<strong>多头自注意力</strong> 操作( MSA )、一个多层感知机 ( MLP )和 两个 Layernorm 步骤( LN )，另外，每块还应用了一个残差连接（<a href="https://blog.csdn.net/JJxiao520/article/details/129087353">什么是ResNet50</a>），MSA <strong>并行执行</strong> M 个自注意力并输出它们串联 (concat) 的结果。<br> <img src="https://images2.imgbox.com/bc/74/qYdWCwTv_o.png" alt="在这里插入图片描述"><br> <strong>MSA 并行执行 M 自注意力并输出它们 concat 的结果</strong>：具体细节如下：</p> 
<p> 在第 m 个自注意力中，给定第( l−1 )个变压器块的输出 z <sup>l−1</sup> ∈ R <sup>N×C</sup>，则查询 Q ∈ R<sup> N×U</sup>，键 K ∈ R<sup> N×U</sup>，值 V ∈ R<sup> N×U</sup>，计算如下：<br> <img src="https://images2.imgbox.com/be/b3/OxaectML_o.png" alt="在这里插入图片描述" width="600"><br> 其中<img src="https://images2.imgbox.com/bf/de/SIrKDCnh_o.png" alt="在这里插入图片描述">属于是参数矩阵，C 是嵌入的维数，U 是 Q，K，V 的维数（<font color="red">三个矩阵的维数是一样的</font>，LN 是 layernorm 操作）。然后，根据序列中两个元素之间的成对相似度计算第 m 个自注意力的输出：<br> <img src="https://images2.imgbox.com/e6/02/M7y1jCFd_o.png" alt="在这里插入图片描述" width="450"><br> 其中 y<sub>sa</sub><sup>m</sup> 为注意权重，d 代表的是矩阵的维数，最后，MSA 就可以表示为：<br> <img src="https://images2.imgbox.com/75/7b/bCCRwenM_o.png" alt="在这里插入图片描述" width="600"><br> 其中 y<sub>msa</sub> 为 MSA 的输出，W<sub>O</sub> ∈ R<sup>M·U×C</sup> 为投影参数，[·] 作用为拼接。</p> 
<h4><a id="3_Stage_I_Global_Context_Modeling_44"></a>3、 Stage I: Global Context Modeling</h4> 
<p> 由于图像中的边和边界具有鲁棒语义，捕捉抽象的线索和整个图像的整体背景是至关重要的。</p> 
<p>  <strong>第一个阶段</strong>：通过 globalTranformer encoder Ge 和 global decoder Gd 来提取粗粒度补丁上的<strong>全局上下文特征</strong>。具体来说，首先将输入图像分割成大小为 16×16 的粗粒度补丁序列，然后生成嵌入 z<sup>0</sup><sub>g</sub> 作为编码器的输入，全局 Transformer 编码器 G<sub>E</sub> 在嵌入 z<sup>0</sup><sub>g</sub> 上工作，用于计算全局注意力。<br> <img src="https://images2.imgbox.com/0b/41/QoJjkrnY_o.png" alt="在这里插入图片描述" width="600"><br>  计算得到的全局上下文特征 z<sub>g</sub> 序列经过全局解码器 Gd <strong>上采样</strong> 到高分辨率特征进行合并。</p> 
<p><strong>BiMLA Decoder：</strong><br>  生成<strong>边缘感知的像素级表示</strong> 对于精确的边缘检测至关重要。文章希望可以设计出一个实用解码器（ decoder ），用来鼓励 Transformer encoder 计算边缘感知的注意，并以可学习的方式对注意力进行上采样，受多级特征融合的影响，作者提出了 <strong>双向多级聚合解码器</strong>（BiMLA）。<br> <img src="https://images2.imgbox.com/63/c7/lJ7PY03I_o.png" alt="在这里插入图片描述"></p> 
<p> 在 BiMLA 中，设计了一种双向特征聚合策略，包括自顶向下路径（Top-down path）和自底向上路径（Bottom-up path），以促进 Transformer coder 中的信息流。具体来说，将 L<sub>g</sub> 个 <strong>Transformer 块</strong>统一划分为 4 组，并将每组最后一块的嵌入特征 { z6g, z12g, z18g, z24g } 作为输入。然后将它们 reshape 成 3D 特征图（ H/16×W/16×C ）。对于自顶向下的路径，采用相同的设计（一个 1×1 卷积和一个 3×3 卷积层）作用于 reshaped feature 得到四个输出 features ( t6,t12,t18,t24 )，遵循 SETR-MLA 的方法。同样地，自底向上的路径从最底层逐步到顶层，分配 3×3 卷积层到多级特征上，最后产生另外四个输出 features ( b6,b12,b18,b24 )。与通过双线性操作对特征进行上采样的 SETR-MLA 不同，每个聚合特征通过一个反卷积 block，包含 4×4 和 16×16 两个反卷积层，每个反卷积之后加入 批归一化 BN 和 ReLU 。双向路径的八个特征图（包含top-down path 和 Bottom-up path 得来的特征）进行 concat 操作得到一个 tensor。BIMLA 使用额外的卷积层来平滑 features，<strong>包括三层 3×3 卷积层和一个 1×1 卷积层+BN+ReLU</strong>。<br> <img src="https://images2.imgbox.com/14/f9/Uy25fJgO_o.png" alt="在这里插入图片描述" width="550"><br> 其中 f<sub>g</sub> 为像素级全局特征，g<sub>D</sub>为 全局 BiMLA 解码器（global BiMLA decoder）。</p> 
<h4><a id="4Stage_II_Local_Refinement_58"></a>4、Stage II: Local Refinement</h4> 
<p> 对于像素级预测，特别是边缘检测，探索细粒度的上下文特征是必不可少的。理想情况下的边缘宽度为 1 像素（ <strong>要求较高</strong> ），而 16×16 补丁不利于提取薄边缘。将像素作为 tokens 可能是一种直观的补救方法，但由于计算成本很高，实际上这是不可行的。文章给出的解决方案：使用一个不重叠的滑动窗口对图像进行采样，然后计算采样区域内的注意力，由于窗口中的 patches 的数量是固定的，计算复杂度和图像大小成<strong>线性关系</strong>。</p> 
<p> 因此，文章建议在阶段 II 中捕获短程细粒度上下文特征，如图，在图像 X ∈ R<sup>H×W ×3</sup> 上执行大小为 H / 2 ×W / 2 的<strong>不重叠滑动窗口</strong>，将输入图像 X 分解为序列{ X<sup>1</sup>, X<sup>2</sup>, X<sup>3</sup>, X<sup>4</sup> }。对于每个窗口，将其分割为大小为 8×8 的细粒度补丁（分得更小了），并通过共享的 local transformer encoder R<sub>E</sub> 计算关注。然后将所有窗口的注意值进行 concat 操作，得到<img src="https://images2.imgbox.com/db/87/2gp5mxO2_o.png" alt="在这里插入图片描述" width="300">，为了进一步节省计算资源，将 Lr = 12，也就意味着 local tranformer decoder 由12 个transformer blocks组成，与全局 BiMLA 类似，选择{ Zr3,Zr6,Zr9,Zr12 } 输入到局部 BiMLA R<sub>D</sub>生成局部高分辨率特征。<br> <img src="https://images2.imgbox.com/52/2f/GCv1VCQL_o.png" alt="在这里插入图片描述" width="550"><br> 其中 f<sub>r</sub> 为局部特征。与全局的 BiMLA 区别在于，局部的 BiMLA 中的 3×3 卷积层替换为 1×1 卷积层，目的是为了避免填充操作造成的人工边缘（操作过多）。</p> 
<p><img src="https://images2.imgbox.com/36/d9/WcIsLLzS_o.png" alt="在这里插入图片描述"><br> <strong>Feature Fusion Module：</strong><br>  通过一个特征融合模块 ( FFM ) 整合来自于两个 levels 的上下文信息，并利用局部决策头来预测边缘。FFM 将全局上下文作为先验知识并对局部上下文进行调节，生成包含全局上下文和细粒度局部细节的融合特征（<strong>这个是文章需要的</strong>）。FFM 由空间特征 transform block和两个 3×3 卷积层组成，然后进行 BN 和 ReLU 操作，前者用于调节，后者用于平滑。然后将融合特征输入局部决策头 R<sub>H</sub>，预测出边缘图E<sub>r</sub>。<br> <img src="https://images2.imgbox.com/3d/45/imdPSp2w_o.png" alt="在这里插入图片描述" width="550"><br> 其中 R<sub>H</sub> 是局部决策头，由 1×1 卷积层和 sigmoid 操作组成。</p> 
<h4><a id="5Network_Training_70"></a>5、Network Training</h4> 
<p> 为了训练两阶段框架 EDTER，首先优化第一阶段以生成代表整个图像上下文信息的全局特征。然后，修正阶段 I 的参数，训练阶段 II生成边缘图。</p> 
<p><strong>Loss Function：</strong> 给定边缘图 E 和对应的实际真相 Y，损失计算为：<br> <img src="https://images2.imgbox.com/13/91/RJysnXJ9_o.png" alt="在这里插入图片描述" width="600"><br> 其中，α = | Y<sup>−</sup> | / (| Y<sup>−</sup> | +| Y<sup>+</sup>|) 表示负像素样本的百分比，其中 |·| 表示像素个数。在实践中，将多个标签归一化为范围为 [0,1] 的边缘概率映射，然后使用阈值 η 来选择像素，如果概率值大于 η，则像素标记为正样本；否则，表示为负样本。</p> 
<p><strong>Training Stage I：</strong><br>  首先将全局决策头（head）合并到全局特征图上，生成边缘图：<br> <img src="https://images2.imgbox.com/22/fd/8GDBQC9E_o.png" alt="在这里插入图片描述" width="400"><br> 其中 G<sub>H</sub> 表示全局决策头，由 1×1 卷积层和 sigmoid 层组成。此外，获得了多个侧输出S1g, S2g，…，通过对全局 BiMLA 解码器提取的中间特征t6, t12, t18, t24和b6, b12, b18, b24进行相同的设计(4×4反卷积层和16×16反卷积层)，逐步强制编码器强调边缘感知注意。</p> 
<p><img src="https://images2.imgbox.com/e7/e0/0fApJlNM_o.png" alt="在这里插入图片描述"><br>  阶段 I 通过最小化每个边缘图与实际真相之间的损失来优化。阶段 I 的损失函数表达式为：<br> <img src="https://images2.imgbox.com/b9/81/T4zJ7dI6_o.png" alt="在这里插入图片描述" width="600"><br> 其中L<sup>E</sup><sub>g</sub>为E<sub>g</sub>的损失，L<sup>side</sup><sub>g</sub>为侧损失，λ 为平衡 L<sup>E</sup><sub>g</sub> 和 L<sup>side</sup><sub>g</sub> 的权重。</p> 
<p><strong>Training Stage II：</strong><br>  阶段 I 训练完之后，确定其参数然后进入阶段 II 。<br> <img src="https://images2.imgbox.com/53/80/gA6C8YWV_o.png" alt="在这里插入图片描述" width="600"><br> 其中 L<sup>E</sup><sub>r</sub> 和 L<sup>side</sup><sub>r</sub> 分别为 Er 和 侧输出 的损失。</p> 
<h3><a id="_92"></a>实验结果</h3> 
<p><img src="https://images2.imgbox.com/72/f4/Z4eFGPEz_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/75/76/gT2rLDce_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6a/88/O85C6OzG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cc/99/QL7GTgel_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c1/55/kRdKaQak_o.png" alt="在这里插入图片描述"><br> BSDS500测试集中三个挑战性样本的定性比较：<br> <img src="https://images2.imgbox.com/94/49/gzwC0IYA_o.png" alt="在这里插入图片描述"><br> 所有的结果都是用单一的比例输入计算出来的：<br> <img src="https://images2.imgbox.com/2a/91/gdzSfOJy_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b7/f5/g2TR7bCs_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_105"></a>总结</h3> 
<p> 限制：该算法提取的边缘宽度占据多个像素，与理想的边缘宽度存在差距，在不进行任何预处理的情况下，生成清晰和细致的边缘是努力的方向。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f8cfba4ee34f31cc2b7d63df394e7097/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">pnpm : 无法加载文件 C:\Users\admin\AppData\Roaming\npm\pnpm.ps1，因为在此系 统上禁止运行脚本。</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7746997ce69b59e7e678ed8f58b55fd3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">HCL模拟器中Server设备启动失败的解决办法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>