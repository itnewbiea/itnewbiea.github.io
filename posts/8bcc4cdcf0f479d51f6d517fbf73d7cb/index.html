<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>我的开源项目 - 使用OnnxRuntime在CPU端部署RTMPose玩转实时2D姿态估计 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="我的开源项目 - 使用OnnxRuntime在CPU端部署RTMPose玩转实时2D姿态估计" />
<meta property="og:description" content="1 RTMPose RTMPose论文地址：https://arxiv.org/abs/2303.07399。
RTMPose是一种Top-Down范式的2D姿态估计框架，魔魔魔魔改Simcc，更加轻量化且更加有效，更加具有工业应用特质。
RTMPose的亮点主打的就是工业级别的推理速度和精度，这在他的论文摘要也是着重突出，可以仔细看他的论文摘要，
Recent studies on 2D pose estimation have achieved excellent performance on public benchmarks, yet its application in the industrial community still suffers from heavy model parameters and high latency. In order to bridge this gap, we empirically explore key factors in pose estimation including paradigm, model architecture, training strategy, and deployment, and present a high-performance real-time multi-person pose estimation framework, RTMPose, based on MMPose. Our RTMPose-m achieves 75." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/8bcc4cdcf0f479d51f6d517fbf73d7cb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-28T17:32:56+08:00" />
<meta property="article:modified_time" content="2023-04-28T17:32:56+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">我的开源项目 - 使用OnnxRuntime在CPU端部署RTMPose玩转实时2D姿态估计</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="1_RTMPose_0"></a>1 RTMPose</h3> 
<p>RTMPose论文地址：<a href="https://arxiv.org/abs/2303.07399" rel="nofollow">https://arxiv.org/abs/2303.07399</a>。</p> 
<p>RTMPose是一种Top-Down范式的2D姿态估计框架，魔魔魔魔改Simcc，更加轻量化且更加有效，更加具有工业应用特质。</p> 
<p>RTMPose的亮点主打的就是工业级别的推理速度和精度，这在他的论文摘要也是着重突出，可以仔细看他的论文摘要，</p> 
<blockquote> 
 <p>Recent studies on 2D pose estimation have achieved excellent performance on public benchmarks, yet its application in the industrial community still suffers from heavy model parameters and high latency. In order to bridge this gap, we empirically explore key factors in pose estimation including paradigm, model architecture, training strategy, and deployment, and present a high-performance real-time multi-person pose estimation framework, RTMPose, based on MMPose. Our RTMPose-m achieves 75.8% AP on COCO with 90+ FPS on an Intel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-l achieves 67.0% AP on COCO-WholeBody with 130+ FPS. To further evaluate RTMPose’s capability in critical real-time applications, we also report the performance after deploying on the mobile device. Our RTMPose-s achieves 72.2% AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existing open-source libraries. Code and models are released at <a href="https://github.com/open-mmlab/mmpose/tree/1.x/projects/rtmpose">this https URL</a>.</p> 
</blockquote> 
<p>从论文摘要的介绍，RTMPose-m 模型在 COCO 上达到 <strong>75.8％AP</strong> 的同时，能在 Intel i7-11700 CPU 上用 ONNXRuntime 达到 <strong>90+FPS</strong>，在 NVIDIA GTX 1660 Ti GPU 上用 TensorRT 达到 **430+FPS。**RTMPose-s 以 <strong>72.2%AP</strong> 的性能，在手机端 Snapdragon865 芯片上用 ncnn 部署达到 <strong>70+FPS</strong>。</p> 
<p><strong>这么强，我肯定要拥有啊！！！！</strong></p> 
<p>RTMPose已经集成到MMPose中，Github地址：<a href="https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmpose">https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmpose</a></p> 
<p>当时仔细看了一下RTMPose的README.md文档，其关于模型部署的教程深度依赖MMDeploy，而我个人认为深度集成MMDeploy会让人望而却步（当然这只是个人看法，勿喷），而我本身有比较多的本地部署和服务器部署模型的经验，所以在本文中我们不依赖MMDeploy，而是使用OnnxRuntime CPU C++ SDK对RTMDetnano+RTMPose-m导出的onnx模型进行本地CPU的部署。<strong>无需GPU，照样进行实时2D姿态估计，就我测试的老掉牙的i5-7400 4H也能实时，还不快冲啊！</strong></p> 
<p>这里也感谢RTMPose的作者镜佬(<a href="https://www.zhihu.com/people/jing-zi-64" rel="nofollow">镜佬知乎主页</a>)对我的这个例子的Pr的光速Merge。</p> 
<h3><a id="2_OnnxRuntimeCPURTMDetnanoRTMPose_20"></a>2 使用OnnxRuntime在CPU端部署RTMDetnano+RTMPose</h3> 
<p>好了，在本小节中会详细介绍如何使用OnnxRuntime在CPU端进行RTMDetnano+RTMPose模型的部署，在本教程中会实现基于RTMDetnano+RTMPose的一个Top-Down的2D姿态估计示例，由RTMDetnano检测人，然后根据检测框裁剪相应的图片区域喂给RTMPose进行姿态估计，进行了一个简单的跳帧检测的实时2D姿态估计的C++类，好了，就让我们愉快的开始吧。</p> 
<p>本文代码示例已开源：<a href="https://github.com/HW140701/RTMPose-Deploy">https://github.com/HW140701/RTMPose-Deploy</a>，并且提供了预编译包，懒得编译的直接可以下载预编译包运行，当然你的Windows电脑上需要有VC runtime。感兴趣的佬可以小点一波<strong>star</strong>，感谢。</p> 
<p>本文代码主要展示基于RTMDetnano+RTMPose的数据前后处理方式，大家感兴趣的可以借鉴。</p> 
<p>本文示例代码已提交pr到MMPose dev1.x：<a href="https://github.com/open-mmlab/mmpose/pull/2316">https://github.com/open-mmlab/mmpose/pull/2316</a>。</p> 
<h4><a id="21_OnnxOnnx_30"></a>2.1 下载Onnx模型、转换Onnx模型</h4> 
<p>从RTMPose的<a href="https://github.com/open-mmlab/mmpose/blob/dev-1.x/projects/rtmpose/README_CN.md">README.md</a> 文档中找到提供的默认导出的onnx模型示例，下载地址为：</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>download<span class="token punctuation">.</span>openmmlab<span class="token punctuation">.</span>com<span class="token operator">/</span>mmpose<span class="token operator">/</span>v1<span class="token operator">/</span>projects<span class="token operator">/</span>rtmpose<span class="token operator">/</span>rtmpose<span class="token operator">-</span>cpu<span class="token punctuation">.</span><span class="token builtin">zip</span>
</code></pre> 
<p>解压缩完成之后会看到RTMDetnano+RTMPose的onnx模型，名称为end2end.onnx。</p> 
<p>不过这里的RTMPose是coco17数据集的17个人体关键点，如果你是需要其他的RTMPose onnx模型，请参照RTMPose的<a href="https://github.com/open-mmlab/mmpose/blob/dev-1.x/projects/rtmpose/README_CN.md">README.md</a> 进行onnx模型导出。</p> 
<p>然后目标检测器是用的RTMDetnano，当然你也可以使用其他的目标检测器，比如各种的YOOOOOOLOOOOO，个人认为目标检测器对后面姿态估计的影响很小，当然这个目标检测器不是智障检测器。</p> 
<h4><a id="22_RTMDetnanoRTMPoseTopDown2D_44"></a>2.2 基于RTMDetnano+RTMPose实现一个Top-Down的2D姿态估计例子</h4> 
<h5><a id="221_RTMDetnano_46"></a>2.2.1 基于RTMDetnano的目标检测</h5> 
<p>由于上述链接提供的RTMDetnano是在batch_size、image_height、image_width都具有动态维度，所以在实现的时候并没有固定输入图片的宽和高。</p> 
<p>而用于输入图片归一化的image_mean和image_std的值来自于<a href="https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmpose-cpu.zip" rel="nofollow">https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmpose-cpu.zip</a> 压缩包里面各个模型对应的<strong>pipeline.json</strong>文件。</p> 
<p>更加详细的可参考仓库代码。</p> 
<p>在输入图片之后，经过推理，在本仓库会选择类别为0且置信度最大的检测框作为后面姿态估计需要检测的区域，也就是说目前本示例只是对图片中目标检测概率最大的人进行姿态估计，不过多人姿态估计扩展起来也比较简单，没有什么很大的任务量。</p> 
<h5><a id="222_RTMPose_56"></a>2.2.2 基于RTMPose的姿态估计</h5> 
<p>在目标检测完成之后，根据检测框和仿射变换将该检测框的区域进行裁剪，因为RTMPose需要的输入维度为1x3x256x192，然后将裁剪的图片经过预处理之后喂给RTMPose进行识别，得到17个关键点在256x192上的坐标，然后通过反仿射变换将坐标反算到原输入图片上，得到正确的坐标。</p> 
<p>更加详细的可参考仓库代码。</p> 
<h5><a id="223_RTMPoseTracker_62"></a>2.2.3 实时视频流姿态估计：RTMPoseTracker</h5> 
<p>在RTMDetnano和RTMPoseTracker的推理类都构建完成之后，我们构建了一个简单的应对实时视频流检测的RTMPoseTracker，在RTMPoseTracker默认每10帧才进行一次目标检测，这样可以极大的降低单帧推理延时，以达到实时2D姿态估计的性能。</p> 
<p>感兴趣可以访问我的个人站：<a href="https://www.stubbornhuang.com/" rel="nofollow">https://www.stubbornhuang.com/</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a02d0d369e0411851787ff0b89c66f27/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android 11.0 源码中,锁屏界面使用 密码 锁屏方式</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/374cf84994a4b93bbe0b4ccdf7a931bf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【模电知识总结】MOS管</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>