<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>通用性和精度全拉满！谷歌最新发布最强零样本深度估计！ - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="通用性和精度全拉满！谷歌最新发布最强零样本深度估计！" />
<meta property="og:description" content=" 来源：3D视觉工坊
在公众号「3D视觉工坊」后台，回复「原论文」可获取论文pdf、主页链接
这里给大家推荐下深度估计微信群，扫码入群，方便大家沟通交流：
1. 笔者个人体会 由于室内和室外场景之间的RGB和深度的巨大变化，以及未知的相机固有特性导致的深度尺度模糊，zero-shot度量深度估计非常难。
今天笔者将为大家分享谷歌最新发布的工作DMD，是单目绝对深度估计的最新SOTA扩散模型。DMD在室内zero-shot数据集上实现了25%的相对误差(REL)减少，在室外zero-shot数据集上实现了33%的相对误差减少。
下面一起来阅读一下这项工作~
2. 效果展示 对比方案主要是之前的ZoeDepth，在室内室外都取得了非常好的定性效果。
DMD相对于ZoeDepth在所有zero-shot基准上都有大幅度的提高，可惜没开源。这里也推荐工坊推出的新课程《单目深度估计方法：算法梳理与代码实现》。
3. 具体原理是什么？ DMD做了很多创新，例如使用对数尺度深度参数化来实现室内和室外场景的联合建模，调节视野(FOV)来处理尺度模糊性，并在训练期间综合增强FOV，以概括训练数据集中有限的相机固有特性。具体贡献如下：
1、室内外联合建模：不是将深度线性映射到[ - 1 , 1]，而是在对数尺度上对深度进行参数化，以更公平地分配室内外场景之间的表示能力。
2、处理不同的相机内参：为了避免对训练相机内参的过拟合，提出通过裁剪和未裁剪(用噪声填充)来增加训练数据，以模拟不同的视野( FOV )。进一步对垂直FOV进行条件限制，消除深度尺度的歧义。
3、多样化的训练数据：使用了多样化的微调混合，这在单独使用NYU和KITTI数据集的微调上显著提高了性能。
4、推理延迟：使用扩散的v参数化而不是通常使用的e参数化，这使得在推理过程中使用最少1个去噪步骤。
4. 和其他SOTA方法对比如何？ 在室内(第一张表)和室外(第二张表)场景下，定量比较DMD和当前SOTA的零样本度量深度估计。DMD在两个领域上都大幅度地提高了性能。
对更多实验结果和文章细节感兴趣的读者，可以阅读一下论文原文~
下载 在公众号「3D视觉工坊」后台，回复「 3dcv」,即可获取工业3D视觉、SLAM、自动驾驶、三维重建、事件相机、无人机等近千余篇最新顶会论文；巴塞罗那自治大学和慕尼黑工业大学3D视觉和视觉导航精品课件；相机标定、结构光、三维重建、SLAM，深度估计、模型部署、3D目标检测等学习资料。
3D视觉方向交流群成立啦
目前工坊已经建立了3D视觉方向多个社群，包括SLAM、工业3D视觉、自动驾驶、三维重建、无人机方向，细分群包括：
[工业3D视觉]相机标定、立体匹配、三维点云、结构光、机械臂抓取、缺陷检测、6D位姿估计、相位偏折术、Halcon、摄影测量、阵列相机、光度立体视觉等。
[SLAM]视觉SLAM、激光SLAM、语义SLAM、滤波算法、多传感器融合、多传感器标定、动态SLAM、MOT SLAM、NeRF SLAM、机器人导航等。
[自动驾驶]深度估计、Transformer、毫米波|激光雷达|视觉摄像头传感器、多传感器标定、多传感器融合、自动驾驶综合群等、3D目标检测、路径规划、轨迹预测、3D点云分割、模型部署、车道线检测、Occupancy、目标跟踪等。
[三维重建]NeRF、多视图几何、OpenMVS、MVSNet、colmap、纹理贴图等
[无人机]四旋翼建模、无人机飞控等
除了这些，还有求职、硬件选型、视觉产品落地、最新论文、3D视觉最新产品、3D视觉行业新闻等交流群
大家可以添加小助理微信：dddvisiona，备注：加群&#43;方向&#43;学校|公司, 小助理会拉你入群。
添加小助理微信：dddvisiona，加群&#43;方向&#43;学校|公司，拉你入群 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/dbfa15165a102c52d24dfa3414d1b222/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-04T07:01:25+08:00" />
<meta property="article:modified_time" content="2024-01-04T07:01:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">通用性和精度全拉满！谷歌最新发布最强零样本深度估计！</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:left;">来源：3D视觉工坊</p> 
 <p style="text-align:left;">在公众号「<strong>3D视觉工坊</strong>」后台，回复「原论文」可获取论文pdf、主页链接</p> 
 <p style="text-align:left;">这里给大家推荐下<strong>深度估计</strong>微信群，扫码入群，方便大家沟通交流：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1a/5b/HxgAJhXG_o.jpg" alt="29845aa52a125118835cd029ba16be35.jpeg"></p> 
 <h3>1. 笔者个人体会</h3> 
 <p>由于室内和室外场景之间的RGB和深度的巨大变化，以及未知的相机固有特性导致的深度尺度模糊，zero-shot度量深度估计非常难。</p> 
 <p>今天笔者将为大家分享谷歌最新发布的工作DMD，是单目绝对深度估计的最新SOTA扩散模型。DMD在室内zero-shot数据集上实现了25%的相对误差(REL)减少，在室外zero-shot数据集上实现了33%的相对误差减少。</p> 
 <p>下面一起来阅读一下这项工作~</p> 
 <h3>2. 效果展示</h3> 
 <p>对比方案主要是之前的ZoeDepth，在室内室外都取得了非常好的定性效果。</p> 
 <img src="https://images2.imgbox.com/8c/13/44y3aQrK_o.jpg" alt="4d995579482c7c00b27564f808f526fd.jpeg"> 
 <img src="https://images2.imgbox.com/2d/45/Dv4jjBEg_o.jpg" alt="fbd71c87be9e1a9545a872055830b622.jpeg"> 
 <p>DMD相对于ZoeDepth在所有zero-shot基准上都有大幅度的提高，可惜没开源。这里也推荐工坊推出的新课程《<a href="" rel="nofollow">单目深度估计方法：算法梳理与代码实现》</a>。</p> 
 <img src="https://images2.imgbox.com/f5/8e/3dkwPzhG_o.jpg" alt="e5668fa90498fabea4e818def1db1ff5.jpeg"> 
 <h3>3. 具体原理是什么？</h3> 
 <p>DMD做了很多创新，例如使用对数尺度深度参数化来实现室内和室外场景的联合建模，调节视野(FOV)来处理尺度模糊性，并在训练期间综合增强FOV，以概括训练数据集中有限的相机固有特性。具体贡献如下：</p> 
 <p>1、室内外联合建模：不是将深度线性映射到[ - 1 , 1]，而是在对数尺度上对深度进行参数化，以更公平地分配室内外场景之间的表示能力。</p> 
 <p>2、处理不同的相机内参：为了避免对训练相机内参的过拟合，提出通过裁剪和未裁剪(用噪声填充)来增加训练数据，以模拟不同的视野( FOV )。进一步对垂直FOV进行条件限制，消除深度尺度的歧义。</p> 
 <p>3、多样化的训练数据：使用了多样化的微调混合，这在单独使用NYU和KITTI数据集的微调上显著提高了性能。</p> 
 <p>4、推理延迟：使用扩散的v参数化而不是通常使用的e参数化，这使得在推理过程中使用最少1个去噪步骤。</p> 
 <h3>4. 和其他SOTA方法对比如何？</h3> 
 <p>在室内(第一张表)和室外(第二张表)场景下，定量比较DMD和当前SOTA的零样本度量深度估计。DMD在两个领域上都大幅度地提高了性能。</p> 
 <img src="https://images2.imgbox.com/ae/b7/Qze2H9Vc_o.jpg" alt="2c4ffe6c10317c3b5d5f50240a648ad4.jpeg"> 
 <p>对更多实验结果和文章细节感兴趣的读者，可以阅读一下论文原文~</p> 
 <h3>下载</h3> 
 <p style="text-align:left;">在公众号「3D视觉工坊」后台，回复「 3dcv」,即可获取工业3D视觉、SLAM、自动驾驶、三维重建、事件相机、无人机等近千余篇最新顶会论文；巴塞罗那自治大学和慕尼黑工业大学3D视觉和视觉导航精品课件；相机标定、结构光、三维重建、SLAM，深度估计、模型部署、3D目标检测等学习资料。</p> 
 <p style="text-align:left;">3D视觉方向交流群成立啦</p> 
 <p style="text-align:left;">目前工坊已经建立了3D视觉方向多个社群，包括<strong>SLAM、工业3D视觉、自动驾驶、三维重建、无人机</strong>方向，<strong>细分群</strong>包括：</p> 
 <p style="text-align:left;">[工业3D视觉]相机标定、立体匹配、三维点云、结构光、机械臂抓取、缺陷检测、6D位姿估计、相位偏折术、Halcon、摄影测量、阵列相机、光度立体视觉等。</p> 
 <p style="text-align:left;">[SLAM]视觉SLAM、激光SLAM、语义SLAM、滤波算法、多传感器融合、多传感器标定、动态SLAM、MOT SLAM、NeRF SLAM、机器人导航等。</p> 
 <p style="text-align:left;">[自动驾驶]深度估计、Transformer、毫米波|激光雷达|视觉摄像头传感器、多传感器标定、多传感器融合、自动驾驶综合群等、3D目标检测、路径规划、轨迹预测、3D点云分割、模型部署、车道线检测、Occupancy、目标跟踪等。</p> 
 <p style="text-align:left;">[三维重建]NeRF、多视图几何、OpenMVS、MVSNet、colmap、纹理贴图等</p> 
 <p style="text-align:left;">[无人机]四旋翼建模、无人机飞控等</p> 
 <p style="text-align:left;">除了这些，还有求职、硬件选型、视觉产品落地、最新论文、3D视觉最新产品、3D视觉行业新闻等交流群</p> 
 <p style="text-align:left;">大家可以添加小助理微信：<strong>dddvisiona</strong>，备注：加群+方向+学校|公司, 小助理会拉你入群。</p> 
 <img src="https://images2.imgbox.com/5e/68/BB0j75LV_o.png" alt="5795905a68c03d35a03f795c1a2c6a7d.png"> 
 <figcaption>
   添加小助理微信：dddvisiona，加群+方向+学校|公司，拉你入群 
   
 </figcaption> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/a2/1d/Eds2MoJj_o.jpg" alt="d1aaa2198f51a57d9011b2a1d7137340.jpeg"><img src="https://images2.imgbox.com/6f/37/zyLkQ0ZP_o.jpg" alt="20345f56ad183fe21dc3767a824b6ecc.jpeg"><img src="https://images2.imgbox.com/a8/66/pnHATbzh_o.jpg" alt="a5d909e11e289d8677a20855b8444953.jpeg"><img src="https://images2.imgbox.com/77/49/HfjCwg8y_o.jpg" alt="153612991fc9f03b92f79706ba8e6856.jpeg"></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/be163f3a46a51a12011f11a256415061/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【二叉树】654.最大二叉树</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2eca47d04134031b5f9b18a1742d157e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">最新DAU-FI Net，突破多类缺陷分割性能边界</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>