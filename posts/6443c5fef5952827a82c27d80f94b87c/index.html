<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>浙大三维视觉团队提出 Neural Body，单目RGB视频重建人体三维模型，无需预训练网络... - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="浙大三维视觉团队提出 Neural Body，单目RGB视频重建人体三维模型，无需预训练网络..." />
<meta property="og:description" content="点击蓝字
关注我们
AI TIME欢迎每一位AI爱好者的加入！
今天，我们介绍一篇2021 CVPR的人体自由视角合成的论文：Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans，该论文由浙江大学CAD&amp;CG国家重点实验室/浙大三维视觉实验室提出。
1.论文链接：arxiv.org/abs/2012.1583
2.论文代码：github.com/zju3dv/neura
3.Project page: zju3dv.github.io/neural
论文的两个demo
1. 从稀疏视角视频中生成的自由视角视频
2. 从单目视频重建出3D human shape
一、引言
1.1 论文的问题描述
论文希望生成动态人体的自由视角视频，这有很多应用，包括电影工业，体育直播和远程视频会议。大家在春晚或一些综艺类节目应该已经见过子弹时间的特效。
1.2 当前方法在这个问题的局限性
现在效果最好的视角合成方法主要是NeRF [3] 这个方向的论文，但他们有两个问题：
需要非常稠密视角来训练视角合成网络。比如NeRF论文中，一般用了100多个视角来训练网络。
NeRF的训练视角
对于静态物体，这是可以做到的，一般是对着静止的物体拍一圈图片。但如果对于动态场景来说，我们无法要求演员静止来让我们给他拍一圈图。为了拍摄稠密多视角视频，之前的方法需要昂贵的相机阵列来进行捕捉。这套设备特别昂贵，而且还很不方便，一般只在电影拍摄时用到。
http://henrybetts.co.uk/an-attempt-at-bullet-time/
NeRF只能处理静态场景。现在大部分视角合成工作是对于每个静态场景训一个网络，对于动态场景，上百帧需要训上百个网络，这成本很高。
1.3 我们的观察和对问题的解决
在稀疏视角下，单帧的信息不足以恢复正确的3D scene representation。论文的key idea是是通过整合时序信息来获得足够多的3D shape observation。大家平时应该能有这种感觉，观看一个动态的物体更能想象他的3维形状。我们论文用的就是这个intuition。
这里整合时序信息的实现用的是latent variable model。具体来说，我们定义了一组隐变量，从同一组隐变量中生成不同帧的场景，这样就把不同帧观察到的信息和一组隐变量关联在了一起。经过训练，我们就能把视频各个帧的信息整合到这组隐变量中，也就整合了时序信息。
二、 论文方法
2.1 Prerequisite: Neural radiance fields
论文使用neural radiance field (NeRF) [3] 来表示一个3维场景。对于一个场景的任意一点，NeRF取出3维坐标和视角方向，输入到一个MLP中，输出color和density。这个表示能通过volume rendering从图片中很好的优化场景的3D模型。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/6443c5fef5952827a82c27d80f94b87c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-09T18:45:00+08:00" />
<meta property="article:modified_time" content="2021-06-09T18:45:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">浙大三维视觉团队提出 Neural Body，单目RGB视频重建人体三维模型，无需预训练网络...</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align: right"><strong>点击蓝字</strong></p> 
 <p><img src="https://images2.imgbox.com/60/79/GdzIylJW_o.png"></p> 
 <p><strong>关注我们</strong></p> 
 <p>AI TIME欢迎每一位AI爱好者的加入！</p> 
 <p>今天，我们介绍一篇2021 CVPR的人体自由视角合成的论文：Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans，该论文由浙江大学CAD&amp;CG国家重点实验室/浙大三维视觉实验室提出。<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/91/fe/6RAodRb7_o.png"></p> 
 <p style="text-align: left">1.论文链接：arxiv.org/abs/2012.1583</p> 
 <p style="text-align: left">2.论文代码：github.com/zju3dv/neura</p> 
 <p style="text-align: left">3.Project page: zju3dv.github.io/neural</p> 
 <p style="text-align: left">论文的两个demo</p> 
 <p>1. 从稀疏视角视频中生成的自由视角视频</p> 
 <p>2. 从单目视频重建出3D human shape</p> 
 <p style="text-align: center"><strong>一、引言</strong></p> 
 <p>1.1 论文的问题描述</p> 
 <p>论文希望生成动态人体的自由视角视频，这有很多应用，包括电影工业，体育直播和远程视频会议。大家在春晚或一些综艺类节目应该已经见过子弹时间的特效。</p> 
 <p>1.2 当前方法在这个问题的局限性</p> 
 <p>现在效果最好的视角合成方法主要是NeRF [3] 这个方向的论文，但他们有两个问题：</p> 
 <ul><li><p>需要非常稠密视角来训练视角合成网络。比如NeRF论文中，一般用了100多个视角来训练网络。</p></li></ul> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/df/77/2g3avKZo_o.png"></p> 
 <p style="text-align: center">NeRF的训练视角</p> 
 <p>对于静态物体，这是可以做到的，一般是对着静止的物体拍一圈图片。但如果对于动态场景来说，我们无法要求演员静止来让我们给他拍一圈图。为了拍摄稠密多视角视频，之前的方法需要昂贵的相机阵列来进行捕捉。这套设备特别昂贵，而且还很不方便，一般只在电影拍摄时用到。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/a8/a7/4i8bbQYb_o.png">http://henrybetts.co.uk/an-attempt-at-bullet-time/</p> 
 <ul><li><p>NeRF只能处理静态场景。现在大部分视角合成工作是对于每个静态场景训一个网络，对于动态场景，上百帧需要训上百个网络，这成本很高。</p></li></ul> 
 <p>1.3 我们的观察和对问题的解决</p> 
 <p>在稀疏视角下，单帧的信息不足以恢复正确的3D scene representation。论文的key idea是是通过整合时序信息来获得足够多的3D shape observation。大家平时应该能有这种感觉，观看一个动态的物体更能想象他的3维形状。我们论文用的就是这个intuition。</p> 
 <p>这里整合时序信息的实现用的是latent variable model。具体来说，我们定义了一组隐变量，从同一组隐变量中生成不同帧的场景，这样就把不同帧观察到的信息和一组隐变量关联在了一起。经过训练，我们就能把视频各个帧的信息整合到这组隐变量中，也就整合了时序信息。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/95/a9/jiRDkKMR_o.png"></p> 
 <p style="text-align: center">二、 论文方法<br></p> 
 <p>2.1 Prerequisite: Neural radiance fields</p> 
 <p>论文使用neural radiance field (NeRF) [3] 来表示一个3维场景。对于一个场景的任意一点，NeRF取出3维坐标和视角方向，输入到一个MLP中，输出color和density。这个表示能通过volume rendering从图片中很好的优化场景的3D模型。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/3d/de/Hbr0x4ux_o.png">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</p> 
 <p>这里的网络是一个global implicit function。global的意识是NeRF只用一个MLP网络去记录场景中所有3维点的颜色和几何信息。</p> 
 <p>2.2 Neural radiance fields on local latent codes</p> 
 <p>我们方法首先将NeRF改成一个local implicit function，也就是让NeRF从一组local latent codes中生成场景。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/7a/4b/Lr0Ks2RB_o.png">Neural radiance fields on local latent codes</p> 
 <ul><li><p>Local latent codes的生成。比如对于某一帧，论文定义了一组离散的local latent codes。为了摆放这些latent codes，需要对这一帧预测一个人的粗糙的human mesh，比如SMPL模型。这里论文用了浙大三维视觉实验室开源的EasyMocap来检测每一帧的SMPL模型，代码地址为：zju3dv/EasyMocap。</p></li></ul> 
 <p>然后我们在SMPL模型的mesh vertex上摆放latent codes，就像图中最左边的样子。<br></p> 
 <ul><li><p>NeRF on local latent codes。Neural Body将local latent codes输入到一个3维卷积神经网络，得到一个latent code volume，这样就能通过插值得到空间中的任意一点的latent code。然后Neural Body把latent code送入NeRF的网络中，就能得到相应的color和density。这里说NeRF是local implicit function的原因是，相比于原来NeRF用一个MLP记录整个场景的信息，这里场景的信息记录在local latent codes。比如脸部和手部的latent codes分别记录脸和手的appearance和shape information。</p></li></ul> 
 <p>2.3 Structured latent codes</p> 
 <p>为了从同一组latent codes中生成不同帧的场景，我们将这些latent codes摆放在一个可驱动人体模型的mesh vertex上，也就得到了structured latent codes。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/00/ca/X8CsH3R8_o.png">Structured latent codes</p> 
 <p>我们通过人体模型驱动structured latent codes的空间位置</p> 
 <p>2.4 从同一组structured latent codes生成不同帧的场景</p> 
 <p>对于不同的视频帧，Neural Body将latent codes摆放为对应的人体姿态，然后输入到NeRF中得到对应帧的3维场景。像NeRF一样，论文用volume rendering从图片中优化网络参数。通过在整段视频上训练，Neural Body实现了时序信息的整合。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/c7/3f/hAH6uQlA_o.png"></p> 
 <p style="text-align: center">三、 实验分析<br></p> 
 <h4>3.1 多视角视频上的结果</h4> 
 <p>论文采集一个叫做ZJU-MoCap的多视角数据集，在上面和NeRF [3], Neural Volumes [4], Neural Textures [5], NHR [6] 比较。数值结果上Neural Body远远超过之前的方法。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/27/ec/pMtKU4Wd_o.png"></p> 
 <p>在可视化结果上，Neural Body在视角合成和三维重建的结果也远远超出了之前的方法。</p> 
 <p>PIFuHD [7] 是很出名的三维人体重建的工作。他是在大量的ground truth 3d data上训练重建网络。需要注意的是，我们的网络不需要ground truth 3d data，只需要在目标视频上进行训练。对于复杂的人体姿态，PIFuHD的泛化能力有限。我们重建的结果更符合图片中观察到的人体姿态。</p> 
 <p>3.2 单目视频上的结果</p> 
 <p>输入一段人在摄像机前转一圈的视频，Neural Body就能恢复出3维人体模型。</p> 
 <p>3.2 Ablation study</p> 
 <p>论文在camera view和视频长度上做了ablation study。</p> 
 <ul><li><p>输入的camera view数量越多，视角合成的效果就越好。</p></li></ul> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/18/76/EPpT71q7_o.png"></p> 
 <ul><li><p>在视频上训练比在单帧上训练的效果要好，但在很长的视频上训练会导致模型效果下降。</p></li></ul> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/c2/19/4G53zpnH_o.png"></p> 
 <p style="text-align: center">四. 参考文献</p> 
 <p>[1] 4DREPLAY. 4dreplay.com/</p> 
 <p>[2] Orts-Escolano, Sergio, et al. Holoportation: Virtual 3d teleportation in real-time. In UIST, 2016.</p> 
 <p>[3] Mildenhall, Ben, et al. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.</p> 
 <p>[4] Lombardi, Stephen, et al. Neural volumes: Learning dynamic renderable volumes from images. In SIGGRAPH, 2019.</p> 
 <p>[5] Thies, Justus, et al. Deferred Neural Rendering: Image Synthesis using Neural Textures. In SIGGRAPH, 2019.</p> 
 <p>[6] Wu, Minye, et al. "Multi-View Neural Human Rendering." In CVPR, 2020.</p> 
 <p>[7] Saito, Shunsuke, et al. PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization. In CVPR, 2020.</p> 
 <p>[8] Alldieck, Thiemo, et al. Video based reconstruction of 3d people models. In CVPR, 2018.</p> 
 <p style="text-align: left">AI TIME欢迎AI领域学者投稿，期待大家剖析学科历史发展和前沿技术。针对热门话题，我们将邀请专家一起论道。同时，我们也长期招募优质的撰稿人，顶级的平台需要顶级的你，</p> 
 <p>请将简历等信息发至yun.he@aminer.cn！</p> 
 <p>微信联系:AITIME_HY</p> 
 <p>AI TIME是清华大学计算机系一群关注人工智能发展，并有思想情怀的青年学者们创办的圈子,旨在发扬科学思辨精神，邀请各界人士对人工智能理论、算法、场景、应用的本质问题进行探索，加强思想碰撞，打造一个知识分享的聚集地。</p> 
 <p><img src="https://images2.imgbox.com/d8/6c/DvM2k0EW_o.png"></p> 
 <p>我知道你<strong>在看</strong>哟<br></p> 
 <p><img src="https://images2.imgbox.com/07/7a/awrRpQB0_o.png"></p> 
 <p><img src="https://images2.imgbox.com/1c/92/v9YXkymy_o.gif"></p> 
 <p>点击 <strong>阅读原文</strong> 了解更多精彩</p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/24829ec810281e9db16b485654877c6d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Latex/CSDN字母输入对照表</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7d6b13ce165eb7b93d596a089092d2f4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">蜻蜓算法(DA)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>