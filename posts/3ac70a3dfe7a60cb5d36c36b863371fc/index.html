<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习——全维度动态卷积ODConv - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习——全维度动态卷积ODConv" />
<meta property="og:description" content="ODConv(OMNI-DIMENSIONAL DYNAMIC CONVOLUTION)是一种关注了空域、输入通道、输出通道等维度上的动态性的卷积方法，因此被称为全维度动态卷积。
part1. 什么是动态卷积 动态卷积就是对卷积核进行线性加权
第一篇提出动态卷积的文章也是在SE之后，他提出目前的注意力机制模型主要都是在特征图上做工作，而动态卷积它对多个卷积核进行线性加权，加权值则与输入有关，这就使得动态卷积具有输入依赖性。
也就是说，对于不同的输入，我们使用不同的卷积核。之后对这些不同的卷积核，进行注意力加权。
看这组图片，这是CondConv: Conditionally Parameterized Convolutions for Efﬁcient Inference的作者提出的。
在两种方式的对比下，发现他们的作用是一样的，但是图b中的方法计算量就像是NLnet重复计算的attention map一样，计算量太大，不适合把卷积放在这里去实现所谓的动态，所以作者提出了方法a，也就是condconv。
part2. 动态卷积和注意力机制有什么差别 动态卷积和注意力机制在神经网络中都是常用的技术，但具有不同的作用和目的。
动态卷积是指在卷积过程中，卷积核的权重不是固定的，而是可以根据输入数据的不同而动态调整。这样可以使卷积核能够更好地适应输入数据的特征，提高卷积网络的性能。
注意力机制是一种重要的神经网络模块，可以使神经网络在处理序列数据时能够更好地关注与当前任务相关的信息。注意力机制可以根据输入数据中的关键信息，给予不同的权重，在传递信息时更多地关注这些重要信息。
因此，动态卷积和注意力机制虽然都可以提高神经网络的性能，但其作用不同。动态卷积是加强了特征的适应性，而注意力机制则是更好地关注当前任务需要的信息。
part3.ODConv ODConv的发现其实比较像CA和GC这种注意力机制，都是发现了已有的东西的不足（或许是忽略的什么，或许是发现某些计算不必要）从而提出的改进。
ODConv发现：现有的工作采用单个注意力，输入对于输出卷积核有相同的注意力值，但其他三个维度（关于空间大小、输入通道数和输出通道数） 卷积核的空间维度、输入通道维度以及输出通道维度）都被忽略了。 受此启发，作者提出了全维动态卷积（ODConv）
如下图所示
ODConv在任何卷积和内部采用并行策略，从四个维度来学习卷积核内部的注意力值，从而获得全维度的卷积核注意力值。
下图课一直观的看出，采用了SE的ODConv(b)和普通的动态卷积的对比。
也就是说，ODConv添加了卷积核的空间维度、输入通道维度以及输出通道维度的特征学习。
动态卷积和注意力机制虽然都可以提高神经网络的性能，但其作用不同。动态卷积是加强了特征的适应性，而注意力机制则是更好地关注当前任务需要的信息。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/3ac70a3dfe7a60cb5d36c36b863371fc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-03T22:45:21+08:00" />
<meta property="article:modified_time" content="2023-08-03T22:45:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习——全维度动态卷积ODConv</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>ODConv(OMNI-DIMENSIONAL DYNAMIC CONVOLUTION)是一种关注了空域、输入通道、输出通道等维度上的动态性的卷积方法，因此被称为全维度动态卷积。</p> 
<h3><a id="part1__4"></a>part1. 什么是动态卷积</h3> 
<p>动态卷积就是对卷积核进行线性加权</p> 
<p>第一篇提出动态卷积的文章也是在SE之后，他提出目前的注意力机制模型主要都是在特征图上做工作，而动态卷积它对多个卷积核进行线性加权，加权值则与输入有关，这就使得动态卷积具有输入依赖性。</p> 
<p>也就是说，对于不同的输入，我们使用不同的卷积核。之后对这些不同的卷积核，进行注意力加权。</p> 
<p><img src="https://images2.imgbox.com/2a/5f/v34f2iex_o.png" alt="在这里插入图片描述"><br> 看这组图片，这是<mark>CondConv: Conditionally Parameterized Convolutions for Efﬁcient Inference</mark>的作者提出的。<br> 在两种方式的对比下，<strong>发现他们的作用是一样的</strong>，但是图b中的方法计算量就像是NLnet重复计算的attention map一样，计算量太大，不适合把卷积放在这里去实现所谓的动态，所以作者提出了方法a，也就是condconv。</p> 
<h3><a id="part2__18"></a>part2. 动态卷积和注意力机制有什么差别</h3> 
<blockquote> 
 <p>动态卷积和注意力机制在神经网络中都是常用的技术，但具有不同的作用和目的。</p> 
 <p>动态卷积是指在卷积过程中，卷积核的权重不是固定的，而是可以根据输入数据的不同而动态调整。这样可以使卷积核能够更好地适应输入数据的特征，提高卷积网络的性能。</p> 
 <p>注意力机制是一种重要的神经网络模块，可以使神经网络在处理序列数据时能够更好地关注与当前任务相关的信息。注意力机制可以根据输入数据中的关键信息，给予不同的权重，在传递信息时更多地关注这些重要信息。</p> 
 <p>因此，动态卷积和注意力机制虽然都可以提高神经网络的性能，但其作用不同。动态卷积是加强了特征的适应性，而注意力机制则是更好地关注当前任务需要的信息。</p> 
</blockquote> 
<h3><a id="part3ODConv_29"></a>part3.ODConv</h3> 
<p>ODConv的发现其实比较像CA和GC这种注意力机制，都是发现了已有的东西的不足（或许是忽略的什么，或许是发现某些计算不必要）从而提出的改进。</p> 
<p>ODConv发现：现有的工作采用单个注意力，输入对于输出卷积核有相同的注意力值，但其他三个维度（关于空间大小、输入通道数和输出通道数） 卷积核的空间维度、输入通道维度以及输出通道维度）都被忽略了。 受此启发，作者提出了全维动态卷积（ODConv）</p> 
<p>如下图所示<br> ODConv在任何卷积和内部采用并行策略，从四个维度来学习卷积核内部的注意力值，从而获得全维度的卷积核注意力值。<br> <img src="https://images2.imgbox.com/40/5b/4NlMEHBE_o.png" alt="在这里插入图片描述"><br> 下图课一直观的看出，采用了SE的ODConv(b)和普通的动态卷积的对比。<br> 也就是说，ODConv添加了卷积核的空间维度、输入通道维度以及输出通道维度的特征学习。<br> <img src="https://images2.imgbox.com/c5/c1/s9VgEUZM_o.png" alt="在这里插入图片描述"></p> 
<p><strong>动态卷积和注意力机制虽然都可以提高神经网络的性能，但其作用不同。动态卷积是加强了特征的适应性，而注意力机制则是更好地关注当前任务需要的信息。</strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4f02df098517431d3ae7d316512b8b44/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java：解决报错非法字符: ‘\ufeff‘以及什么是BOM</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6b83cd4301c62844c0998f82b9d547b2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">YOLOv5S网络框架设计-CSC&amp;C3&amp;SPPF模块</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>