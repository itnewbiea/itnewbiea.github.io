<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PPO算法逐行代码详解 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PPO算法逐行代码详解" />
<meta property="og:description" content="前言：本文会从理论部分、代码部分、实践部分三方面进行PPO算法的介绍。其中理论部分会介绍PPO算法的推导流程，代码部分会给出PPO算法的各部分的代码以及简略介绍，实践部分则会通过debug代码调试的方式从头到尾的带大家看清楚应用PPO算法在cartpole环境上进行训练的整体流程，进而帮助大家将理论与代码实践相结合，更好的理解PPO算法。
文章目录 1. 理论部分2. 代码部分2.1 神经网络的定义2.2 PPO算法的定义2.3 on policy算法的训练代码 3. 实践部分 1. 理论部分 2. 代码部分 这里使用的是《动手学强化学习》中提供的代码，我将这本书中的代码整理到了github上，并且方便使用pycharm进行运行和调试。代码地址：https://github.com/zxs-000202/dsx-rl
代码核心部分整体上可以分为三部分，分别是关于神经网络的类的定义(PolicyNet,ValueNet)，关于PPO算法的类的定义(PPO)，on policy算法训练流程的定义(train_on_policy_agent)。
2.1 神经网络的定义 策略网络actor采用两层全连接层，第一层采用relu激活函数，第二层采用softmax函数。
class PolicyNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): super(PolicyNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, action_dim) def forward(self, x): x = F.relu(self.fc1(x)) return F.softmax(self.fc2(x), dim=1) 价值网络critic采用两层全连接层，第一层和第二层均采用relu激活函数，第二层最后输出的维度是1，表示t时刻某个状态s的价值V。
class ValueNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim): super(ValueNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, 1) def forward(self, x): x = F." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/3a81f3630dc966964f5088b3a377f45f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-12T17:43:53+08:00" />
<meta property="article:modified_time" content="2023-10-12T17:43:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PPO算法逐行代码详解</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>前言：本文会从理论部分、代码部分、实践部分三方面进行PPO算法的介绍。其中理论部分会介绍PPO算法的推导流程，代码部分会给出PPO算法的各部分的代码以及简略介绍，实践部分则会通过debug代码调试的方式从头到尾的带大家看清楚应用PPO算法在cartpole环境上进行训练的整体流程，进而帮助大家将理论与代码实践相结合，更好的理解PPO算法。</p> 
</blockquote> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><ul><li><a href="#1__4" rel="nofollow">1. 理论部分</a></li><li><a href="#2__24" rel="nofollow">2. 代码部分</a></li><li><ul><li><a href="#21__29" rel="nofollow">2.1 神经网络的定义</a></li><li><a href="#22_PPO_57" rel="nofollow">2.2 PPO算法的定义</a></li><li><a href="#23_on_policy_118" rel="nofollow">2.3 on policy算法的训练代码</a></li></ul> 
    </li><li><a href="#3__151" rel="nofollow">3. 实践部分</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h4><a id="1__4"></a>1. 理论部分</h4> 
<p><img src="https://images2.imgbox.com/64/32/HjPi8bRt_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b4/45/r1rsuxNS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6e/36/KQfH0mZ7_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/f4/3f/qZ13dTSb_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3c/98/XGvjWpF7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f3/68/HKYOKKOm_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7b/8e/igMsfDkc_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/59/67/yMDGRlir_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/b4/e6/GMXPfYjO_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/69/83/HBsOaS6S_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2__24"></a>2. 代码部分</h4> 
<p>这里使用的是《动手学强化学习》中提供的代码，我将这本书中的代码整理到了github上，并且方便使用pycharm进行运行和调试。代码地址：<a href="https://github.com/zxs-000202/dsx-rl">https://github.com/zxs-000202/dsx-rl</a></p> 
<p>代码核心部分整体上可以分为三部分，分别是关于神经网络的类的定义(<code>PolicyNet</code>,<code>ValueNet</code>)，关于PPO算法的类的定义(<code>PPO</code>)，on policy算法训练流程的定义(<code>train_on_policy_agent</code>)。</p> 
<h5><a id="21__29"></a>2.1 神经网络的定义</h5> 
<p>策略网络<code>actor</code>采用两层全连接层，第一层采用relu激活函数，第二层采用<code>softmax</code>函数。</p> 
<pre><code class="prism language-py"><span class="token keyword">class</span> <span class="token class-name">PolicyNet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PolicyNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p>价值网络<code>critic</code>采用两层全连接层，第一层和第二层均采用<code>relu</code>激活函数，第二层最后输出的维度是1，表示<code>t</code>时刻某个状态<code>s</code>的价值<code>V</code>。</p> 
<pre><code class="prism language-py"><span class="token keyword">class</span> <span class="token class-name">ValueNet</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ValueNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="22_PPO_57"></a>2.2 PPO算法的定义</h5> 
<p>这部分是PPO算法最核心的部分。本部分包括神经网络的初始化、相关参数的定义，如何根据状态s选择动作，以及<strong>网络参数是如何更新的</strong>。整体的代码如下：</p> 
<pre><code class="prism language-py"><span class="token keyword">class</span> <span class="token class-name">PPO</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">''' PPO算法,采用截断方式 '''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> actor_lr<span class="token punctuation">,</span> critic_lr<span class="token punctuation">,</span>
                 lmbda<span class="token punctuation">,</span> epochs<span class="token punctuation">,</span> eps<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>actor <span class="token operator">=</span> PolicyNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic <span class="token operator">=</span> ValueNet<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>actor_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                lr<span class="token operator">=</span>actor_lr<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>critic_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                 lr<span class="token operator">=</span>critic_lr<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>lmbda <span class="token operator">=</span> lmbda
        self<span class="token punctuation">.</span>epochs <span class="token operator">=</span> epochs  <span class="token comment"># 一条序列的数据用来训练轮数</span>
        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps  <span class="token comment"># PPO中截断范围的参数</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device

    <span class="token keyword">def</span> <span class="token function">take_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        probs <span class="token operator">=</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
        action_dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributions<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span>probs<span class="token punctuation">)</span>
        action <span class="token operator">=</span> action_dist<span class="token punctuation">.</span>sample<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> action<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> transition_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>
        states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                              dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        actions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        rewards <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        next_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'next_states'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                   dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        dones <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>transition_dict<span class="token punctuation">[</span><span class="token string">'dones'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                             dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        td_target <span class="token operator">=</span> rewards <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span>
                                                                       dones<span class="token punctuation">)</span>
        td_delta <span class="token operator">=</span> td_target <span class="token operator">-</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span>
        advantage <span class="token operator">=</span> rl_utils<span class="token punctuation">.</span>compute_advantage<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gamma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>lmbda<span class="token punctuation">,</span>
                                               td_delta<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        old_log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>
                                                            actions<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> actions<span class="token punctuation">)</span><span class="token punctuation">)</span>
            ratio <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>log_probs <span class="token operator">-</span> old_log_probs<span class="token punctuation">)</span>
            surr1 <span class="token operator">=</span> ratio <span class="token operator">*</span> advantage
            surr2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>ratio<span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">,</span>
                                <span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span> <span class="token operator">*</span> advantage  <span class="token comment"># 截断</span>
            actor_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span>torch<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>surr1<span class="token punctuation">,</span> surr2<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># PPO损失函数</span>
            critic_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">,</span> td_target<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            actor_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            critic_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="23_on_policy_118"></a>2.3 on policy算法的训练代码</h5> 
<p>我们知道<code>on policy</code>算法与<code>off policy</code>算法的区别就在与进行采样的网络和用来参数更新的训练网络是否是一个网络。PPO是一种<code>on policy</code>的算法，<code>on policy</code>算法要求训练的网络参数一更新就需要重新进行采样然后训练。但PPO有点特殊，它是利用重要性采样方法来实现数据的多次利用，提高了数据的利用效率。</p> 
<p>从下面的代码中可以看到每当一个episode的数据采样完毕之后都会执行<code>agent.update(transition_dict)</code>。在<code>update</code>中会将当前采样得到的数据（也就是当前episode的每个t时刻的 [ 当前状态，动作，奖励，是否结束，下一个状态 ] 这个五元组）通过重要性采样的方法进行多次的神经网络参数的更新，这个具体过程我会在第三部分实践部分进行详细说明。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">train_on_policy_agent</span><span class="token punctuation">(</span>env<span class="token punctuation">,</span> agent<span class="token punctuation">,</span> num_episodes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    return_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> tqdm<span class="token punctuation">(</span>total<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>num_episodes <span class="token operator">/</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> desc<span class="token operator">=</span><span class="token string">'Iteration %d'</span> <span class="token operator">%</span> i<span class="token punctuation">)</span> <span class="token keyword">as</span> pbar<span class="token punctuation">:</span>
            <span class="token keyword">for</span> i_episode <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>num_episodes <span class="token operator">/</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                episode_return <span class="token operator">=</span> <span class="token number">0</span>
                transition_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'states'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'actions'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'next_states'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'rewards'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'dones'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
                state <span class="token operator">=</span> env<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>
                done <span class="token operator">=</span> <span class="token boolean">False</span>
                <span class="token keyword">while</span> <span class="token keyword">not</span> done<span class="token punctuation">:</span>
                    action <span class="token operator">=</span> agent<span class="token punctuation">.</span>take_action<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
                    next_state<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done<span class="token punctuation">,</span> _ <span class="token operator">=</span> env<span class="token punctuation">.</span>step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'states'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'actions'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'next_states'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>next_state<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'rewards'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>reward<span class="token punctuation">)</span>
                    transition_dict<span class="token punctuation">[</span><span class="token string">'dones'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>done<span class="token punctuation">)</span>
                    state <span class="token operator">=</span> next_state
                    episode_return <span class="token operator">+=</span> reward
                return_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>episode_return<span class="token punctuation">)</span>
                agent<span class="token punctuation">.</span>update<span class="token punctuation">(</span>transition_dict<span class="token punctuation">)</span>
                <span class="token keyword">if</span> <span class="token punctuation">(</span>i_episode <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">'episode'</span><span class="token punctuation">:</span> <span class="token string">'%d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>num_episodes <span class="token operator">/</span> <span class="token number">10</span> <span class="token operator">*</span> i <span class="token operator">+</span> i_episode <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                      <span class="token string">'return'</span><span class="token punctuation">:</span> <span class="token string">'%.3f'</span> <span class="token operator">%</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>return_list<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
                pbar<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> return_list
</code></pre> 
<h4><a id="3__151"></a>3. 实践部分</h4> 
<p>该部分带领大家从代码运行流程上从头到尾过一遍。<br> 本次PPO算法训练应用的gym环境是<code>CartPole-v0</code>，如下图。该gym环境的状态空间是四个连续值用来表示杆所处的状态，而动作空间是两个离散值用来表示给杆施加向左或者向右的力（也就是<code>action</code>为0或者1）。<br> <img src="https://images2.imgbox.com/78/98/0m5vsE8f_o.png" alt="在这里插入图片描述"><br> 代码中前面的部分是网络、算法、训练参数的定义，真正开始训练是在<code>return_list = rl_utils.train_on_policy_agent(env, agent, num_episodes)</code>。我们在这里打一个断点进行调试。<br> <img src="https://images2.imgbox.com/f8/36/4bsyR8OC_o.png" alt="在这里插入图片描述"><br> 比较重要的训练参数是<code>episode</code>为500，<code>epoch</code>为10。表示一共采样500个<code>episode</code>的数据，每个<code>episode</code>数据采样完毕后再<code>update</code>阶段进行10次的梯度下降参数更新。<br> <img src="https://images2.imgbox.com/cd/fd/hBLtdNU1_o.png" alt="在这里插入图片描述"><br> 执行到这里我们跳到<code>take_action</code>里面看看<code>agent</code>是如何选取动作的。<br> <img src="https://images2.imgbox.com/ef/21/UEPv4DKs_o.png" alt="在这里插入图片描述"><br> 可以看到首先将输入的state转换为tensor，然后因为actor网络的最后一层是softmax函数，所以通过actor网络输出两个执行两个动作可能性的大小，然后通过<code>action_dist = torch.distributions.Categorical(probs) action = action_dist.sample()</code>根据可能性大小进行采样最后得到这次选择动作1进行返回。</p> 
<p>第一个episode中我们采样到的数据如下：<br> <img src="https://images2.imgbox.com/04/65/ycW5FAjr_o.png" alt="在这里插入图片描述"><br> 可以看到第一个<code>episode</code>执行了41个<code>step</code>之后<code>done</code>了。<br> 接下来我们跳进<code>update</code>函数中看看具体如何用这个<code>episode</code>采样的数据进行神经网络参数的更新。<br> <img src="https://images2.imgbox.com/92/da/QrMzXDur_o.png" alt="在这里插入图片描述"><br> 首先从<code>transition_dict</code>中将采样的41个<code>step</code>的数据取出来存到<code>tensor</code>中方便之后进行运算。<br> <img src="https://images2.imgbox.com/fc/70/OUSdbN79_o.png" alt="在这里插入图片描述"><br> 接下来计算<code>td_target</code>，<code>td_delta</code>，<code>advantage</code>，<code>old_log_probs</code>。<br> 其中<code>td_target</code>表示t时刻的得到的奖励值<code>r</code>加上根据<code>t+1</code>时刻<code>critic</code>估计的状态价值<code>V</code>。用公式表示的话就是：</p> 
<pre><code class="prism language-py">td_target <span class="token operator">=</span> rewards <span class="token operator">+</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>next_states<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> dones<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/5d/aa/q6O8OWO6_o.png" alt="在这里插入图片描述"><br> <code>td_delta</code>表示的是<code>td_target</code>和<code>t</code>时刻采用<code>critic</code>估计的状态价值之间的差值。用公式表示的话就是：</p> 
<pre><code class="prism language-py">td_delta <span class="token operator">=</span> td_target <span class="token operator">-</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/c6/ed/siLJMBfV_o.png" alt="在这里插入图片描述"><br> <code>advantage</code>则表示的是某状态下采取某动作的优势值，也就是下面PPO算法公式中的<code>A</code>。</p> 
<pre><code class="prism language-py">advantage <span class="token operator">=</span> rl_utils<span class="token punctuation">.</span>compute_advantage<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gamma<span class="token punctuation">,</span> self<span class="token punctuation">.</span>lmbda<span class="token punctuation">,</span> td_delta<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/d5/d1/T4GNB3qJ_o.png" alt="在这里插入图片描述"><br> <code>old_log_probs</code>则表示的是旧策略下某个状态下采取某个动作的概率值的对数值。对应下面PPO算法公式中的<code>clip</code>函数中分母。</p> 
<pre><code class="prism language-py">old_log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>actions<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/0c/9f/PogpmiF8_o.png" alt="在这里插入图片描述"></p> 
<p>到这里我们计算出这些值的<code>shape</code>如下图所示。接下来我们就要进入一个循环中，循环<code>epoch=10</code>次进行参数的神经网络参数的更新。<br> <img src="https://images2.imgbox.com/ca/02/PwswLfx5_o.png" alt="在这里插入图片描述"></p> 
<p>具体而言首先在每个循环中计算一次当前的<code>actor</code>参数下对应的<code>state</code>和<code>action</code>的概率值的对数值，也就是下面分式中的分子。<br> 这里用到的高中对数数学公式有：<br> <img src="https://images2.imgbox.com/53/04/KZuhNIh4_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ee/53/6J0eOd43_o.png" alt="在这里插入图片描述"><br> 通过上面两个公式可以计算得到<br> <img src="https://images2.imgbox.com/6a/91/ZIqNpnCW_o.png" alt="在这里插入图片描述"></p> 
<p>然后计算这个分式的值<code>ratio</code>。<br> 之后结合下面的公式和代码可以看出<code>surr1</code>和<code>surr2</code>分别是下图公式中对应的值。<br> <img src="https://images2.imgbox.com/d9/2a/ULFRl1uE_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-py">log_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> actions<span class="token punctuation">)</span><span class="token punctuation">)</span>
ratio <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>log_probs <span class="token operator">-</span> old_log_probs<span class="token punctuation">)</span>
surr1 <span class="token operator">=</span> ratio <span class="token operator">*</span> advantage
surr2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>ratio<span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">,</span> <span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span> <span class="token operator">*</span> advantage  <span class="token comment"># 截断</span>
</code></pre> 
<p>然后计算<code>actor</code>的<code>loss</code>值和<code>critic</code>的<code>loss</code>值</p> 
<pre><code class="prism language-py">actor_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span>torch<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>surr1<span class="token punctuation">,</span> surr2<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># PPO损失函数</span>
critic_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>states<span class="token punctuation">)</span><span class="token punctuation">,</span> td_target<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>计算得到的数据的<code>shape</code>如下图：<br> <img src="https://images2.imgbox.com/50/2b/wbRHToph_o.png" alt="在这里插入图片描述"><br> 最后对<code>actor</code>和<code>critic</code>进行梯度下降进行参数更新。</p> 
<pre><code class="prism language-py">self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
ctor_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
critic_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>至此第一次循环执行完毕，接下来还需要再进行九次这种循环，把当前<code>episode</code>的41个<code>step</code>的数据再进行九次<code>actor</code>和<code>critic</code>参数的更新。</p> 
<p>然后<code>update</code>相当于执行完毕了。<img src="https://images2.imgbox.com/c5/f2/6Es1NOx3_o.png" alt="在这里插入图片描述"><br> 接下来一个<code>episode</code>的训练过程相当于结束了，我们一共有500个<code>episode</code>需要训练，再循环499次就完成了整个训练的流程。</p> 
<p>还有一部分比较重要的就是通过<code>GAE</code>计算优势函数<code>Advantage</code>的代码。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">compute_advantage</span><span class="token punctuation">(</span>gamma<span class="token punctuation">,</span> lmbda<span class="token punctuation">,</span> td_delta<span class="token punctuation">)</span><span class="token punctuation">:</span>
    td_delta <span class="token operator">=</span> td_delta<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    advantage_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    advantage <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token keyword">for</span> delta <span class="token keyword">in</span> td_delta<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        advantage <span class="token operator">=</span> gamma <span class="token operator">*</span> lmbda <span class="token operator">*</span> advantage <span class="token operator">+</span> delta
        advantage_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>advantage<span class="token punctuation">)</span>
    advantage_list<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>advantage_list<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

</code></pre> 
<p>这部分就留给大家结合第一部分理论部分最后一张图来进行理解了。</p> 
<p><strong>参考资料</strong></p> 
<ol><li>磨菇书easy-rl <a href="https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5" rel="nofollow">https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5</a></li><li>《动手学强化学习》 <a href="https://hrl.boyuai.com/chapter/2/ppo%E7%AE%97%E6%B3%95" rel="nofollow">https://hrl.boyuai.com/chapter/2/ppo%E7%AE%97%E6%B3%95</a></li></ol>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4110c476858c8452b34af2f4823688e6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">micropython ESP32-S3点亮板载RGB灯珠</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/25e76ce44302b72e0b138d05772dd83e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">太顶了！文心大模型落地文旅行业不仅能业生成潮玩、还可补文物残卷！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>