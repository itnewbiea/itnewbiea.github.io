<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Android FFmpeg Camera2 推流直播 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Android FFmpeg Camera2 推流直播" />
<meta property="og:description" content="前言 自己花了点时间实现了一个使用FFmpeg将Camera2视频数据推送到RTMP服务的简单Demo，在这里分享下，里面用到知识很多都是之前博客中用到的，难度不大。
效果图 1、 定义方法 定义了三个JNI方法
public class FFmpegHandler { private FFmpegHandler() { } private static class SingletonInstance { private static final FFmpegHandler INSTANCE = new FFmpegHandler(); } public static FFmpegHandler getInstance() { return SingletonInstance.INSTANCE; } static { System.loadLibrary(&#34;ffmpeg-handler&#34;); } //初始化参数 public native int init(String outUrl); //推流，将Y、U、V数据分开传递 public native int pushCameraData(byte[] buffer,int ylen,byte[] ubuffer,int ulen,byte[] vbuffer,int vlen); //结束 public native int close(); } 2、Camera2实时数据 具体使用可查看Android音视频(一) Camera2 API采集数据
将ImageReader作为预览请求的Target之一，这样我们就可以将预览的数据拿到在onImageAvailable中进行处理推送。
mImageReader = ImageReader." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/4896f017ec4cffe481bc02e5a75071ae/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-02T00:38:29+08:00" />
<meta property="article:modified_time" content="2023-03-02T00:38:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Android FFmpeg Camera2 推流直播</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="kdocs-document"> 
 <h2 style=""><span class="kdocs-bold" style="font-weight:bold;">前言</span></h2> 
 <p style="">自己花了点时间实现了一个使用FFmpeg将Camera2视频数据推送到RTMP服务的简单Demo，在这里分享下，里面用到知识很多都是之前博客中用到的，难度不大。</p> 
 <p style=""></p> 
 <h2 style="">效果图</h2> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:640px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:75.0%;height:0;"> 
    <img src="https://images2.imgbox.com/8d/6d/F7j8H04S_o.jpg" style="margin-left:;display:block;width:640px;margin-top:-75.0%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <h2 style="text-align:left;"><span class="kdocs-bold" style="font-weight:bold;">1、 定义方法</span></h2> 
 <p style="text-align:null;">定义了三个JNI方法</p> 
 <pre class="kdocs-java"><code class="language-java">public class FFmpegHandler {
    private FFmpegHandler() {
    }

    private static class SingletonInstance {
        private static final FFmpegHandler INSTANCE = new FFmpegHandler();
    }

    public static FFmpegHandler getInstance() {
        return SingletonInstance.INSTANCE;
    }


    static {
        System.loadLibrary("ffmpeg-handler");
    }

    //初始化参数
    public native int init(String outUrl);

    //推流，将Y、U、V数据分开传递
    public native int pushCameraData(byte[] buffer,int ylen,byte[] ubuffer,int ulen,byte[] vbuffer,int vlen);

    //结束
    public native int close();
}</code></pre> 
 <h2 style="text-align:left;"><span class="kdocs-bold" style="font-weight:bold;">2、Camera2实时数据</span></h2> 
 <p style="text-align:null;">具体使用可查看Android音视频(一) Camera2 API采集数据</p> 
 <p style="text-align:null;">将ImageReader作为预览请求的Target之一，这样我们就可以将预览的数据拿到在onImageAvailable中进行处理推送。</p> 
 <pre class="kdocs-java"><code class="language-java">mImageReader = ImageReader.newInstance(640, 480,ImageFormat.YUV_420_888, 1); 
mImageReader.setOnImageAvailableListener(mOnImageAvailableListener, mBackgroundHandler);</code></pre> 
 <pre class="kdocs-java"><code class="language-java">Surface imageSurface = mImageReader.getSurface();

mPreviewRequestBuilder = mCameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW);

mPreviewRequestBuilder.addTarget(surface);
mPreviewRequestBuilder.addTarget(imageSurface);</code></pre> 
 <p style="text-align:null;">将获取的Image数据解析为YUV数据，Y、U、V数据分别存储。具体请看 <a class="kdocs-link" style="color:#0A6CFF;" href="https://blog.csdn.net/qq_39312146/article/details/129252235" target="_blank" rel="noopener noreferrer">YUV数据格式与YUV_420_888</a>。</p> 
 <p style="text-align:null;">目前这块暂时这样写着，网上的博客都比较旧了，有点不太合适，我想应该还会有更好的方法，后面再做优化。（或者这块你有什么好的处理方法，欢迎留言）。</p> 
 <pre class="kdocs-java"><code class="language-java">private final ImageReader.OnImageAvailableListener mOnImageAvailableListener
            = new ImageReader.OnImageAvailableListener() {

        @Override
        public void onImageAvailable(ImageReader reader) {
        
            Image image = reader.acquireLatestImage();

            if (image == null) {
                return;
            }

            final Image.Plane[] planes = image.getPlanes();

            int width = image.getWidth();
            int height = image.getHeight();
            
            // Y、U、V数据
            byte[] yBytes = new byte[width * height];
            byte uBytes[] = new byte[width * height / 4];
            byte vBytes[] = new byte[width * height / 4];
            
            //目标数组的装填到的位置
            int dstIndex = 0;
            int uIndex = 0;
            int vIndex = 0;

            int pixelsStride, rowStride;
            for (int i = 0; i &lt; planes.length; i++) {
                pixelsStride = planes[i].getPixelStride();
                rowStride = planes[i].getRowStride();

                ByteBuffer buffer = planes[i].getBuffer();

                //如果pixelsStride==2，一般的Y的buffer长度=640*480，UV的长度=640*480/2-1
                //源数据的索引，y的数据是byte中连续的，u的数据是v向左移以为生成的，两者都是偶数位为有效数据
                byte[] bytes = new byte[buffer.capacity()];
                buffer.get(bytes);

                int srcIndex = 0;
                if (i == 0) {
                    //直接取出来所有Y的有效区域，也可以存储成一个临时的bytes，到下一步再copy
                    for (int j = 0; j &lt; height; j++) {
                        System.arraycopy(bytes, srcIndex, yBytes, dstIndex, width);
                        srcIndex += rowStride;
                        dstIndex += width;
                    }
                } else if (i == 1) {
                    //根据pixelsStride取相应的数据
                    for (int j = 0; j &lt; height / 2; j++) {
                        for (int k = 0; k &lt; width / 2; k++) {
                            uBytes[uIndex++] = bytes[srcIndex];
                            srcIndex += pixelsStride;
                        }
                        if (pixelsStride == 2) {
                            srcIndex += rowStride - width;
                        } else if (pixelsStride == 1) {
                            srcIndex += rowStride - width / 2;
                        }
                    }
                } else if (i == 2) {
                    //根据pixelsStride取相应的数据
                    for (int j = 0; j &lt; height / 2; j++) {
                        for (int k = 0; k &lt; width / 2; k++) {
                            vBytes[vIndex++] = bytes[srcIndex];
                            srcIndex += pixelsStride;
                        }
                        if (pixelsStride == 2) {
                            srcIndex += rowStride - width;
                        } else if (pixelsStride == 1) {
                            srcIndex += rowStride - width / 2;
                        }
                    }
                }
            }
            // 将YUV数据交给C层去处理。
            FFmpegHandler.getInstance().pushCameraData(yBytes, yBytes.length, uBytes, uBytes.length, vBytes, vBytes.length);
            image.close();
        }

    };</code></pre> 
 <h2 style="text-align:left;"><span class="kdocs-bold" style="font-weight:bold;">3、初始化FFmpeg</span></h2> 
 <p style="text-align:null;">直播推送的过程整体就是一个先将视频数据编码，再将编码后的数据写入数据流中推送给服务器的过程。</p> 
 <p style="text-align:null;">下面初始化的过程就是准备好数据编码器和一条已经连上服务器的数据流。</p> 
 <pre class="kdocs-cpp"><code class="language-cpp">JNIEXPORT jint JNICALL Java_com_david_camerapush_ffmpeg_FFmpegHandler_init
        (JNIEnv *jniEnv, jobject instance, jstring url) {

    const char *out_url = (*jniEnv)-&gt;GetStringUTFChars(jniEnv, url, 0);

    //计算yuv数据的长度
    yuv_width = width;
    yuv_height = height;
    y_length = width * height;
    uv_length = width * height / 4;
    
    //output initialize
    int ret = avformat_alloc_output_context2(&amp;ofmt_ctx, NULL, "flv", out_url);
    if (ret &lt; 0) {
        LOGE("avformat_alloc_output_context2 error");
    }

    //初始化H264编码器
    pCodec = avcodec_find_encoder(AV_CODEC_ID_H264);
    if (!pCodec) {
        LOGE("Can not find encoder!\n");
        return -1;
    }

    pCodecCtx = avcodec_alloc_context3(pCodec);
    //编码器的ID号，这里为264编码器
    pCodecCtx-&gt;codec_id = pCodec-&gt;id;

    //像素的格式，也就是说采用什么样的色彩空间来表明一个像素点，这里使用YUV420P
    pCodecCtx-&gt;pix_fmt = AV_PIX_FMT_YUV420P;
    //编码器编码的数据类型
    pCodecCtx-&gt;codec_type = AVMEDIA_TYPE_VIDEO;
    //编码目标的视频帧大小，以像素为单位
    pCodecCtx-&gt;width = width;
    pCodecCtx-&gt;height = height;
    //帧频
    pCodecCtx-&gt;framerate = (AVRational) {15, 1};
    //时间基
    pCodecCtx-&gt;time_base = (AVRational) {1, 15};
    //目标的码率，即采样的码率；显然，采样码率越大，视频大小越大
    pCodecCtx-&gt;bit_rate = 400000;
    pCodecCtx-&gt;gop_size = 50;
    /* Some formats want stream headers to be separate. */
    if (ofmt_ctx-&gt;oformat-&gt;flags &amp; AVFMT_GLOBALHEADER)
        pCodecCtx-&gt;flags |= AV_CODEC_FLAG_GLOBAL_HEADER;

    //H264 codec param
    pCodecCtx-&gt;qcompress = 0.6;
    //最大和最小量化系数
    pCodecCtx-&gt;qmin = 10;
    pCodecCtx-&gt;qmax = 51;
    //Optional Param
    //两个非B帧之间允许出现多少个B帧数
    //设置0表示不使用B帧，b 帧越多，图片越小
    pCodecCtx-&gt;max_b_frames = 0;
    AVDictionary *param = 0;
    //H.264
    if (pCodecCtx-&gt;codec_id == AV_CODEC_ID_H264) {
        av_dict_set(&amp;param, "preset", "superfast", 0); //x264编码速度的选项
        av_dict_set(&amp;param, "tune", "zerolatency", 0);
    }

    // 打开编码器
    if (avcodec_open2(pCodecCtx, pCodec, &amp;param) &lt; 0) {
        LOGE("Failed to open encoder!\n");
        return -1;
    }

    // 新建传输流，即将要直播的视频流
    video_st = avformat_new_stream(ofmt_ctx, pCodec);
    if (video_st == NULL) {
        return -1;
    }
    video_st-&gt;time_base = (AVRational) {25, 1};
    video_st-&gt;codecpar-&gt;codec_tag = 0;
    avcodec_parameters_from_context(video_st-&gt;codecpar, pCodecCtx);
    
    // 打开数据流，表示与rtmp服务器连接
    int err = avio_open(&amp;ofmt_ctx-&gt;pb, out_url, AVIO_FLAG_READ_WRITE);
    if (err &lt; 0) {
        LOGE("Failed to open output：%s", av_err2str(err));
        return -1;
    }

    //Write File Header
    avformat_write_header(ofmt_ctx, NULL);
    av_init_packet(&amp;enc_pkt);

    return 0;
}</code></pre> 
 <h2 style="text-align:left;"><span class="kdocs-bold" style="font-weight:bold;">4、开始传输</span></h2> 
 <p style="text-align:null;">对 YUV 数据编码，并将编码后数据写入准备好的直播流中。</p> 
 <pre class="kdocs-cpp"><code class="language-cpp">JNIEXPORT jint JNICALL Java_com_david_camerapush_ffmpeg_FFmpegHandler_pushCameraData
        (JNIEnv *jniEnv, jobject instance, jbyteArray yArray, jint yLen, jbyteArray uArray, jint uLen, jbyteArray vArray, jint vLen) {
    jbyte *yin = (*jniEnv)-&gt;GetByteArrayElements(jniEnv, yArray, NULL);
    jbyte *uin = (*jniEnv)-&gt;GetByteArrayElements(jniEnv, uArray, NULL);
    jbyte *vin = (*jniEnv)-&gt;GetByteArrayElements(jniEnv, vArray, NULL);

    int ret = 0;

    // 初始化Frame
    pFrameYUV = av_frame_alloc();
    int picture_size = av_image_get_buffer_size(pCodecCtx-&gt;pix_fmt, pCodecCtx-&gt;width,
                                                pCodecCtx-&gt;height, 1);
    uint8_t *buffers = (uint8_t *) av_malloc(picture_size);


    //将buffers的地址赋给AVFrame中的图像数据，根据像素格式判断有几个数据指针
    av_image_fill_arrays(pFrameYUV-&gt;data, pFrameYUV-&gt;linesize, buffers, pCodecCtx-&gt;pix_fmt,pCodecCtx-&gt;width, pCodecCtx-&gt;height, 1);

    // Frame中数据填充
    memcpy(pFrameYUV-&gt;data[0], yin, (size_t) yLen); //Y
    memcpy(pFrameYUV-&gt;data[1], uin, (size_t) uLen); //U
    memcpy(pFrameYUV-&gt;data[2], vin, (size_t) vLen); //V
    pFrameYUV-&gt;pts = count;
    pFrameYUV-&gt;format = AV_PIX_FMT_YUV420P;
    pFrameYUV-&gt;width = yuv_width;
    pFrameYUV-&gt;height = yuv_height;

    //初始化AVPacket
    enc_pkt.data = NULL;
    enc_pkt.size = 0;

    //开始编码YUV数据
    ret = avcodec_send_frame(pCodecCtx, pFrameYUV);
    if (ret != 0) {
        LOGE("avcodec_send_frame error");
        return -1;
    }
    //获取编码后的H264数据
    ret = avcodec_receive_packet(pCodecCtx, &amp;enc_pkt);
    if (ret != 0 || enc_pkt.size &lt;= 0) {
        LOGE("avcodec_receive_packet error %s", av_err2str(ret));
        return -2;
    }
    
    enc_pkt.stream_index = video_st-&gt;index;
    enc_pkt.pts = count * (video_st-&gt;time_base.den) / ((video_st-&gt;time_base.num) * fps);
    enc_pkt.dts = enc_pkt.pts;
    enc_pkt.duration = (video_st-&gt;time_base.den) / ((video_st-&gt;time_base.num) * fps);
    enc_pkt.pos = -1;

    // 往直播流写数据
    ret = av_interleaved_write_frame(ofmt_ctx, &amp;enc_pkt);
    if (ret != 0) {
        LOGE("av_interleaved_write_frame failed");
    }
    count++;
    
    //释放内存，Java写多了经常会忘记这块**
    av_packet_unref(&amp;enc_pkt);
    av_frame_free(&amp;pFrameYUV);
    av_free(buffers);
    (*jniEnv)-&gt;ReleaseByteArrayElements(jniEnv, yArray, yin, 0);
    (*jniEnv)-&gt;ReleaseByteArrayElements(jniEnv, uArray, uin, 0);
    (*jniEnv)-&gt;ReleaseByteArrayElements(jniEnv, vArray, vin, 0);

    return 0;
}</code></pre> 
 <p style="text-align:null;">这是Demo运行后的结果，推送视频OK，但是可能会有2到3秒的延迟（可能也跟网速有关）。目前就做到这种程度，后面会优化延迟、音频直播、音视频同步等都会慢慢加上去。</p> 
 <p style="text-align:null;"></p> 
 <h2 style="text-align:left;">源码地址 </h2> 
 <blockquote class="kdocs-blockquote" style=""> 
  <a class="kdocs-link" style="color:#0A6CFF;" href="https://github.com/David1840/CameraPush" target="_blank" rel="noopener noreferrer">https://github.com/David1840/CameraPush</a> 
 </blockquote> 
 <p style=""></p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/95f808696222ebd581acd6020e6e0f24/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java中的死锁及其解决方案</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4583e41ab4ea3b2ba690303a9bf80962/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">How bad are small triangles on GPU and why?</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>