<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Pytorch从零开始实现Transformer (from scratch) - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Pytorch从零开始实现Transformer (from scratch)" />
<meta property="og:description" content="Pytorch从零开始实现Transformer 前言一、Transformer架构介绍1. Embedding2. Multi-Head AttentionQuery，Key，Value 3. Transformer BlockLayerNormFeed Forward 4. Decoder Block 二、Transformer代码实现0. 导入库1. Word Embedding2. Positional Encoding3. Multi-Head Attention4. Transformer Encoder5. Transformer DecoderDecoder attentionEncoder decoder attention 6. Overrall Transformer7. Test Code 总结日志参考文献 前言 2023年可以说是AI备受关注的一年，ChatGPT（GPT3.5）的问世引起了世界的轰动，近期GPT4的出现更是颠覆了各界对深度学习的认知，AI不止停留在特定任务的学习上，诸如此类的大模型已经初步符合人们最开始对AI的幻想——通用模型，通用模型可以处理多种场景下的难题。总所周知这些大模型背后的基础就是Transformer，于2017年提出后就席卷了整个学术界，各领域学者纷纷投入到Transformer的研究中，比如用Transformer取代ResNet作为Backbone取得更好的下游任务，或是单纯就Transformer的网络架构进行改进。“树要长多高，根就要扎多深”，在Transformer构建的金碧辉煌的大厦逐步建起时，不能忘记其最原初的根本。
正值论文写作稍微告一段落，有时间深入理解Transformer工作原理，复现Transformer和Transformer相关问题在工作面试中也是比较常见的，对于深度学习初学者，从零开始实现Transformer即Code the Transformer from scratch可能会是一个很艰巨的任务，甚至望而却步，不可否认在写这篇博客前我也是这样。Transformer的衍生比如BERT、Vision Transformer这种比较大的模型，下意识就会被这个“大”所吓到，觉得凭自己贫瘠的代码功底应该是难以实现。可实际做起来并没有想象那么难，“Just do it！”
希望每位读者看完这篇文章后能手撕Transformer或者对其有更深入的理解。
一、Transformer架构介绍 Transformer的模型流程图如上，取自原论文 Attention is all you need，目前引用量是69364。论文于2017年发表，由Google Brain和Google Research团队研发。据我所知，Transformer最初就是设计出来解决NLP领域问题的，比如机器翻译。
Tip：对于学习的建议是首先观看视频讲解，然后手打打一遍代码，最后看些博客对每个模块细致理解。（推荐视频和博客将放到文末参考文献部分）
接下来会简短介绍一下流程图中的各个部分，之后在下一节详细说明并实现代码。
1. Embedding 此处的Embedding包含上图中的Input Embedding和Output Embedding，实际上这部分代码的实现是一样的。按Input Embedding为例，其分为Word Embedding和Positional Embedding（即图中Positional Encoding）。Word Embedding作用是将一个个单词转化为向量，即词嵌入，位置编码Positional Encoding这是给单词句子对应词向量添加位置信息。
论文对位置编码这块并没有解释说明，就直接通过sin和cos对词向量的每个位置元素进行操作，偶数位（2i）使用sin，奇数位（2i&#43;1）使用cos，然后将位置编码直接加（&#43;）到词向量上。
2. Multi-Head Attention 多头注意力机制，也可称为Self-attention自注意力机制，叫法不一。仔细看可以看到图中在Embedding后引申出4路，有3路进入Multi-Head Attention模块，另1路在之后和多头注意力模块的结果Norm后汇合（其实就是相加）。Multi-Head Attention是Transformer的核心，而这3路信息则是Multi-Head Attention的核心。3路分别为Query，Key和Value。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/1caf0ef7bbccae24379b6bc3b3f98d78/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-02T15:30:55+08:00" />
<meta property="article:modified_time" content="2023-04-02T15:30:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch从零开始实现Transformer (from scratch)</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/30/e4/w99wHyZM_o.jpg" alt="在这里插入图片描述"></p> 
<p></p> 
<div class="toc"> 
 <h4>Pytorch从零开始实现Transformer</h4> 
 <ul><li><a href="#_5" rel="nofollow">前言</a></li><li><a href="#Transformer_11" rel="nofollow">一、Transformer架构介绍</a></li><li><ul><li><a href="#1_Embedding_17" rel="nofollow">1. Embedding</a></li><li><a href="#2_MultiHead_Attention_21" rel="nofollow">2. Multi-Head Attention</a></li><li><ul><li><a href="#QueryKeyValue_23" rel="nofollow">Query，Key，Value</a></li></ul> 
   </li><li><a href="#3_Transformer_Block_29" rel="nofollow">3. Transformer Block</a></li><li><ul><li><a href="#LayerNorm_32" rel="nofollow">LayerNorm</a></li><li><a href="#Feed_Forward_39" rel="nofollow">Feed Forward</a></li></ul> 
   </li><li><a href="#4_Decoder_Block_41" rel="nofollow">4. Decoder Block</a></li></ul> 
  </li><li><a href="#Transformer_49" rel="nofollow">二、Transformer代码实现</a></li><li><ul><li><a href="#0__51" rel="nofollow">0. 导入库</a></li><li><a href="#1_Word_Embedding_62" rel="nofollow">1. Word Embedding</a></li><li><a href="#2_Positional_Encoding_88" rel="nofollow">2. Positional Encoding</a></li><li><a href="#3_MultiHead_Attention_135" rel="nofollow">3. Multi-Head Attention</a></li><li><a href="#4_Transformer_Encoder_246" rel="nofollow">4. Transformer Encoder</a></li><li><a href="#5_Transformer_Decoder_321" rel="nofollow">5. Transformer Decoder</a></li><li><ul><li><a href="#Decoder_attention_323" rel="nofollow">Decoder attention</a></li><li><a href="#Encoder_decoder_attention_329" rel="nofollow">Encoder decoder attention</a></li></ul> 
   </li><li><a href="#6_Overrall_Transformer_421" rel="nofollow">6. Overrall Transformer</a></li><li><a href="#7_Test_Code_503" rel="nofollow">7. Test Code</a></li></ul> 
  </li><li><a href="#_537" rel="nofollow">总结</a></li><li><a href="#_539" rel="nofollow">日志</a></li><li><a href="#_544" rel="nofollow">参考文献</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_5"></a>前言</h2> 
<p>2023年可以说是AI备受关注的一年，ChatGPT（GPT3.5）的问世引起了世界的轰动，近期GPT4的出现更是颠覆了各界对深度学习的认知，AI不止停留在特定任务的学习上，诸如此类的大模型已经初步符合人们最开始对AI的幻想——通用模型，通用模型可以处理多种场景下的难题。总所周知这些大模型背后的基础就是Transformer，于2017年提出后就席卷了整个学术界，各领域学者纷纷投入到Transformer的研究中，比如用Transformer取代ResNet作为Backbone取得更好的下游任务，或是单纯就Transformer的网络架构进行改进。“树要长多高，根就要扎多深”，在Transformer构建的金碧辉煌的大厦逐步建起时，不能忘记其最原初的根本。<br> 正值论文写作稍微告一段落，有时间深入理解Transformer工作原理，复现Transformer和Transformer相关问题在工作面试中也是比较常见的，对于深度学习初学者，从零开始实现Transformer即Code the Transformer from scratch可能会是一个很艰巨的任务，甚至望而却步，不可否认在写这篇博客前我也是这样。Transformer的衍生比如BERT、Vision Transformer这种比较大的模型，下意识就会被这个“大”所吓到，觉得凭自己贫瘠的代码功底应该是难以实现。可实际做起来并没有想象那么难，“Just do it！”<br> 希望每位读者看完这篇文章后能手撕Transformer或者对其有更深入的理解。</p> 
<hr> 
<h2><a id="Transformer_11"></a>一、Transformer架构介绍</h2> 
<img src="https://images2.imgbox.com/09/41/QxA901WF_o.png" width="50%"> 
<p>Transformer的模型流程图如上，取自原论文 <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">Attention is all you need</a>，目前引用量是69364。论文于2017年发表，由Google Brain和Google Research团队研发。据我所知，Transformer最初就是设计出来解决NLP领域问题的，比如机器翻译。<br> <strong>Tip：对于学习的建议是首先观看视频讲解，然后手打打一遍代码，最后看些博客对每个模块细致理解。（推荐视频和博客将放到文末参考文献部分）</strong><br> 接下来会简短介绍一下流程图中的各个部分，之后在下一节详细说明并实现代码。</p> 
<h3><a id="1_Embedding_17"></a>1. Embedding</h3> 
<p>此处的Embedding包含上图中的Input Embedding和Output Embedding，实际上这部分代码的实现是一样的。按Input Embedding为例，其分为Word Embedding和Positional Embedding（即图中Positional Encoding）。Word Embedding作用是将一个个单词转化为向量，即词嵌入，位置编码Positional Encoding这是给单词句子对应词向量添加位置信息。<br> <img src="https://images2.imgbox.com/27/24/cbIhwLwt_o.png" alt="在这里插入图片描述"><br> 论文对位置编码这块并没有解释说明，就直接通过sin和cos对词向量的每个位置元素进行操作，<strong>偶数位（2i）使用sin，奇数位（2i+1）使用cos</strong>，然后将位置编码直接加（+）到词向量上。</p> 
<h3><a id="2_MultiHead_Attention_21"></a>2. Multi-Head Attention</h3> 
<p>多头注意力机制，也可称为Self-attention自注意力机制，叫法不一。仔细看可以看到图中在Embedding后引申出<strong>4路</strong>，有<strong>3路</strong>进入<strong>Multi-Head Attention</strong>模块，另<strong>1路</strong>在之后和多头注意力模块的结果Norm后汇合（其实就是相加）。Multi-Head Attention是Transformer的核心，而这3路信息则是Multi-Head Attention的核心。3路分别为Query，Key和Value。</p> 
<h4><a id="QueryKeyValue_23"></a>Query，Key，Value</h4> 
<p>举个例子，图源自<a href="http://jalammar.github.io/illustrated-transformer/" rel="nofollow">The Illustrated Transformer</a>，<br> <img src="https://images2.imgbox.com/b0/64/pOztoGK4_o.png" width="90%"><br> 假如“Thinking”和“Machines”这两个单词经过Word Embedding和Positional Embedding后为x1和x2，<br> <strong>Tip：其实这里也体现了“Multi”，多头注意力模块把一整个Embedding（比如一个句子的词向量）切分为若干份（比如Heads为8就均分为8段）作为输入，旨在学习不同维度的特征（并没有证明，合情推理）。</strong><br> 然后将x1、x2分别和预先定义好的 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          Q 
         
        
       
      
        W^Q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span></span></span></span></span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          K 
         
        
       
      
        W^K 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span></span></span></span></span></span></span></span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          V 
         
        
       
      
        W^V 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em;">V</span></span></span></span></span></span></span></span></span></span></span></span>矩阵进行矩阵乘（或者说线性组合），即可得到q1，k1，v1（同理，q2，k2，v2）。从变量命名不难看出这是基于一套字典检索匹配的策略，给一个Query，然后找到对应的Key，而这个Value呢其实就是所要保存的值。Query是网络当前待查询的值，Key记录着上一次迭代网络的值，Value则负责保存这些值，可能听上去很抽象，类比RNN循环神经网络，Q是现在，K是过去，V用于记录和更新。而注意力所体现的地方就在于矩阵的乘法，类似余弦相似度，Q和K的转置相乘即可得到相似性，相似性越大则Q和K越匹配。如有不懂，可以观看第二节代码帮助理解。</p> 
<h3><a id="3_Transformer_Block_29"></a>3. Transformer Block</h3> 
<p>Transformer Block也指Encoder Block，在经过Multi-Head Attention和Add&amp;Norm后，此处必要一提，算是一个细节，Multi-Head Attention后的结果和之前未曾细说的那<strong>1路</strong>即<strong>Value</strong>进行相加（Add），这里<strong>足以体现Value的记录和存储作用，可见Value在模型中重要性</strong>。<br> 这种让模型某一路直接跳到某个模块后相加的操作（<strong>ShortCut操作</strong>）灵感来源于何恺明大神的ResNet，这是毋庸置疑的。残差的操作避免了梯度弥散的问题，同时也起到特征融合的作用，保留原本的特征。</p> 
<h4><a id="LayerNorm_32"></a>LayerNorm</h4> 
<p>必要一提，不同于ResNet这些卷积神经网络使用BatchNorm，Transformer使用的是LayerNorm。<br> LayerNorm论文发表于2016年，见<a href="https://arxiv.org/abs/1607.06450" rel="nofollow">Layer Normalization</a>，可以直接用于RNN神经网络的训练，因为RNN特质对整个Batch的归一化并不现实，这也决定LayerNorm对。<br> BatchNorm和LayerNorm的比较见下图，图源于<a href="https://paperswithcode.com/method/layer-normalization" rel="nofollow">paperswithcode.com/method/layer-normalization</a><br> <img src="https://images2.imgbox.com/ad/59/JxQKeghd_o.png" width="50%"><br> N为batch size，C为channel（通道数），H,W代表每个特征向量的高和宽。<br> 简单的理解就是BatchNorm是对一个batch size样本内的每个特征做归一化，LayerNorm是对每个样本的所有特征做归一化。（后续会补充细讲）</p> 
<h4><a id="Feed_Forward_39"></a>Feed Forward</h4> 
<p>使用两层全连接层（Linear)和ReLU激活函数。</p> 
<h3><a id="4_Decoder_Block_41"></a>4. Decoder Block</h3> 
<p>Decoder Block的架构大体上和Encoder Block是一致的，唯独不同的是Decoder比Encoder多了Masked Multi-Head Attention。<br> 那么<font color="red"><strong>为什么要在Decode阶段对特征（或者说tokens）做mask呢？</strong></font><br> Mask的样式如下，白格为0，蓝格为1, 1表示保留的部分，0表示mask的部分：<br> <img src="https://images2.imgbox.com/34/f3/slmyKR1r_o.png" alt="在这里插入图片描述"><br> Transformer针对序列化特征，比如文本数据，这些特征是有前后关系的，为了不让Decoder通过未来的单词（或者说特征）来猜测出当前的特征，将后面的token进行mask，这样模型只能通过过去的词或者说特征元素来预测当前特征元素。</p> 
<hr> 
<h2><a id="Transformer_49"></a>二、Transformer代码实现</h2> 
<p>本节的代码取自<a href="https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch/notebook" rel="nofollow">https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch/notebook</a>，但代码会有些许不同但保证是正确的（因为原来代码一些地方有瑕疵），本人做的工作更多是翻译和整理网上对Transformer的讲解。</p> 
<h3><a id="0__51"></a>0. 导入库</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> math
<span class="token keyword">import</span> warnings
warnings<span class="token punctuation">.</span>simplefilter<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="1_Word_Embedding_62"></a>1. Word Embedding</h3> 
<p>首先，我们需要将输入序列中的每个单词转换为embedding向量。<br> 假设每个embedding维数（embed_dim）是512维，vocabulary词汇表大小（vocab_size）是100，那么embedding词嵌入矩阵的大小将是100x512（行x列）。这些矩阵将在训练中学习，在inference即推理过程中，每个单词将被映射到相应的512维向量。如果batch size为32，序列长度（seq_len）为10(10个单词)。输出将是<strong>32x10x512</strong>。<br> <strong>Tip</strong>：inference推理过程在深度学习中指把从训练中学习到的能⼒应⽤到⼯作中去，也就是在训练完模型后将模型对实际的输入（可以是训练集里的数据也可以是其他数据）进行在线推演输出结果的过程。<br> 对于词嵌入矩阵的构造有专门的nn.Embedding，pytorch官方文档将其称为Sparse Layer（稀疏层），是一个保存了固定字典和大小的简单查找表。这个模块常用来保存词嵌入和用下标检索它们。模块的输入是一个下标的列表，输出是对应的词嵌入。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">WordEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            vocab_size: size of vocabulary
            embed_dim: dimension of embeddings
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>WordEmbedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            x: input vector
        Returns:
            out: embedding vector
        """</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out
</code></pre> 
<h3><a id="2_Positional_Encoding_88"></a>2. Positional Encoding</h3> 
<p>回顾到上一节位置编码的公式，公式细节解释：</p> 
<blockquote> 
 <p>pos --&gt;指的是当前词在句子中的顺序 [0,max_seq_len)<br> i --&gt;指沿embedding维度的位置，即 [0，512)</p> 
</blockquote> 
<p>这里注意pos范围不是[0,10)，10是输入序列的长度seq_len，因为输入的序列长度一般是小于等于max_seq_len的，也就是位置编码矩阵是固定大小，输入embedding序列是随机长度。<br> 前面我们已经得到32 x 10 x 512的embedding向量。类似地，我们将有尺寸为32 x 10 x 512的位置编码向量。然后我们把两者相加（实际上位置编码矩阵会更大）。</p> 
<p><img src="https://images2.imgbox.com/6c/ab/hLFD271r_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">PositionalEmbedding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> max_seq_len<span class="token punctuation">,</span> embed_model_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            seq_len: length of input sequence
            embed_model_dim: demension of embedding
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PositionalEmbedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_model_dim

        pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span>
        <span class="token keyword">for</span> pos <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                pe<span class="token punctuation">[</span>pos<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> math<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>pos <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">10000</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> i<span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                pe<span class="token punctuation">[</span>pos<span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> math<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>pos <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">10000</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        pe <span class="token operator">=</span> pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'pe'</span><span class="token punctuation">,</span> pe<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            x: input vector
        Returns:
            out : output
        """</span>

        <span class="token comment"># make embeddings relatively larger</span>
        x <span class="token operator">=</span> x <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span>
        <span class="token comment"># add constant to embedding</span>
        seq_len <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> x <span class="token operator">+</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> out 

</code></pre> 
<h3><a id="3_MultiHead_Attention_135"></a>3. Multi-Head Attention</h3> 
<p><strong>什么是self-attention自注意力机制？</strong><br> 对于某个句子比如“Dog is crossing the street because it saw the kitchen”，人类可以很容易理解这句话重点是在说狗，而对机器来说就不是那么容易的了。<br> 当模型处理每个单词时，self-attention可以查看输入序列中的其他位置以寻找线索。它将根据每个单词与其他单词的依赖关系创建一个向量。也就是所谓的“完形填空”。这里插播一些非计算机知识，算是一种联想来帮助读者理解。</p> 
<blockquote> 
 <p>格式塔系德文“Gestalt”的音译，主要指完形，即具有不同部分分离特性的有机整体。格式塔作为心理学术语，具有两种含义：一指事物的一般属性，即形式；一指事物的个别实体，即分离的整体，形式仅为其属性之一。也就是说，“假使有一种经验的现象，它的每一成分都牵连到其他成分；而且每一成分之所以有其特性，即因为它和其他部分具有关系，这种现象便称为格式塔。”总之，格式塔不是孤立不变的现象，而是指通体相关的完整的现象。完整的现象具有它本身完整的特性，它既不能割裂成简单的元素，同时它的特性又不包含于任何元素之内。——百度百科</p> 
</blockquote> 
<p><strong>下面进入到实例的讲解</strong>：<br> <strong>Step1</strong>：首先从Encoder的每个输入向量(在本例中，是每个单词的嵌入)中创建三个向量，分别是Query向量、Key向量和Value向量，每个向量的维数都是<strong>1x64</strong>。这里64是配合上文 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         512 
        
       
         / 
        
       
         8 
        
       
         = 
        
       
         64 
        
       
      
        512/8=64 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">512/8</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">64</span></span></span></span></span>，其中8为多头注意力的头的数量。我们将有一个Key矩阵、Query矩阵和一个Value矩阵来生成Key、Query和Value向量，这些矩阵都是在训练中可学习的。</p> 
<blockquote> 
 <p><strong>对于经过WordEmbedding和PositionEncoding后的输入张量32x10x512，将其调整为32x10x8x64</strong>。</p> 
</blockquote> 
<p><strong>Step2</strong>：对应Transformer原论文的Scaled Dot-Product Attention：<br> <img src="https://images2.imgbox.com/49/00/CBAPBwT2_o.png" width="30%"><br> <strong>Mask是optional（即可选项），Encoder时不需要Mask，Decoder需要Mask</strong>。<br> 根据公式：<br> <img src="https://images2.imgbox.com/9b/c9/uwAGxLDR_o.png" width="50%"><br> 将Query矩阵与Key矩阵相乘，[Q x <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          K 
         
        
          T 
         
        
       
      
        K^T 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span></span></span></span></span>]<br> 当前Key、Query和Key矩阵维度是<strong>32x10x8x64</strong>。QK相乘之前，我们将对它们进行转置（transpose）以方便相乘(32x8x10x64)。</p> 
<blockquote> 
 <p><strong>现在将Query矩阵与转置的Key矩阵相乘。即(32x8x10x64) x (32x8x64x10) -&gt; (32x8x10x10)。</strong></p> 
</blockquote> 
<p>然后将矩阵乘的结果除以Key维度，将32x8x10x10向量除以8，即除以64的平方根(Key矩阵的维数)</p> 
<p>接下来是softmax之后结果和Value矩阵(32x8x10x64)相乘：</p> 
<blockquote> 
 <p><strong>(32x8x10x10) x (32x8x10x64) -&gt; (32x8x10x64)</strong></p> 
</blockquote> 
<p><strong>Step3</strong>：通过Step2得到矩阵，将多头的结果进行合并（concat），然后让其通过线性层，这就形成了多头注意力的输出。图中h表示number of heads，即多头注意力的数量。<br> <img src="https://images2.imgbox.com/1c/30/vP00DwVW_o.png" width="30%"><br> <img src="https://images2.imgbox.com/74/90/n3cPDpMY_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p><strong>(32x8x10x64)张量被转置回为(32x10x8x64)，然后Concat为(32x10x512)。最后通过Linear线性层得到(32x10x512)的输出。</strong></p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            embed_dim: dimension of embeding vector output
            n_heads: number of self attention heads
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_dim  <span class="token comment"># 512 dim</span>
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> n_heads  <span class="token comment"># 8</span>
        self<span class="token punctuation">.</span>single_head_dim <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim <span class="token operator">/</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span>  <span class="token comment"># 512/8 = 64  . each key,query, value will be of 64d</span>

        <span class="token comment"># key,query and value matrixes    #64 x 64</span>
        self<span class="token punctuation">.</span>query_matrix <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">,</span>
                                      bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># single key matrix for all 8 keys #512x512</span>
        self<span class="token punctuation">.</span>key_matrix <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>value_matrix <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># batch_size x sequence_length x embedding_dim    # 32 x 10 x 512</span>

        <span class="token triple-quoted-string string">"""
        Args:
           key : key vector
           query : query vector
           value : value vector
           mask: mask for decoder

        Returns:
           output vector from multihead attention
        """</span>
        batch_size <span class="token operator">=</span> key<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        seq_length <span class="token operator">=</span> key<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># query dimension can change in decoder during inference.</span>
        <span class="token comment"># so we cant take general seq_length</span>
        seq_length_query <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># 32x10x512</span>
        key <span class="token operator">=</span> key<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">,</span>
                       self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">)</span>  <span class="token comment"># batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)</span>
        query <span class="token operator">=</span> query<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length_query<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">)</span>  <span class="token comment"># (32x10x8x64)</span>
        value <span class="token operator">=</span> value<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">)</span>  <span class="token comment"># (32x10x8x64)</span>

        k <span class="token operator">=</span> self<span class="token punctuation">.</span>key_matrix<span class="token punctuation">(</span>key<span class="token punctuation">)</span>  <span class="token comment"># (32x10x8x64)</span>
        q <span class="token operator">=</span> self<span class="token punctuation">.</span>query_matrix<span class="token punctuation">(</span>query<span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>value_matrix<span class="token punctuation">(</span>value<span class="token punctuation">)</span>

        q <span class="token operator">=</span> q<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)</span>
        k <span class="token operator">=</span> k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (batch_size, n_heads, seq_len, single_head_dim)</span>
        v <span class="token operator">=</span> v<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (batch_size, n_heads, seq_len, single_head_dim)</span>

        <span class="token comment"># computes attention</span>
        <span class="token comment"># adjust key for matrix multiplication</span>
        k_adjusted <span class="token operator">=</span> k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)</span>
        product <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k_adjusted<span class="token punctuation">)</span>  <span class="token comment"># (32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)</span>

        <span class="token comment"># fill those positions of product matrix as (-1e20) where mask positions are 0</span>
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            product <span class="token operator">=</span> product<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-1e20"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># divising by square root of key dimension</span>
        product <span class="token operator">=</span> product <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>single_head_dim<span class="token punctuation">)</span>  <span class="token comment"># / sqrt(64)</span>
        <span class="token comment"># applying softmax</span>
        scores <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>product<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># mutiply with value matrix</span>
        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> v<span class="token punctuation">)</span>  <span class="token comment">##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64)</span>
        <span class="token comment"># concatenated output</span>
        concat <span class="token operator">=</span> scores<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length_query<span class="token punctuation">,</span>
                                                          self<span class="token punctuation">.</span>single_head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span>  <span class="token comment"># (32x8x10x64) -&gt; (32x10x8x64)  -&gt; (32,10,512)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>concat<span class="token punctuation">)</span>  <span class="token comment"># (32,10,512) -&gt; (32,10,512)</span>

        <span class="token keyword">return</span> output

</code></pre> 
<p><strong>Warning</strong>: 在学习Vision Transformer代码时，回过头来发现以上的MultiHeadAttention这部分代码是有些许问题，因为这部分代码是借用Kaggle上的，所以写作当时没有意识到。不过也谈不上错误，只能说那么设计有些不恰当，不知道读者能否找出问题所在。答案将在<a href="https://blog.csdn.net/weixin_43594279/article/details/129802040">Pytorch动手实现Transformer机器翻译</a>这篇博客中给出，历时请对比代码。</p> 
<h3><a id="4_Transformer_Encoder_246"></a>4. Transformer Encoder</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> expansion_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TransformerBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">"""
        Args:
           embed_dim: dimension of the embedding
           expansion_factor: fator ehich determines output dimension of linear layer
           n_heads: number of attention heads
        """</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> n_heads<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> expansion_factor <span class="token operator">*</span> embed_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>expansion_factor <span class="token operator">*</span> embed_dim<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dropout1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
           key: key vector
           query: query vector
           value: value vector
           norm2_out: output of transformer block
        """</span>
        attention_out <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> value<span class="token punctuation">)</span>  <span class="token comment"># 32x10x512</span>
        attention_residual_out <span class="token operator">=</span> attention_out <span class="token operator">+</span> value  <span class="token comment"># 32x10x512</span>
        norm1_out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>attention_residual_out<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 32x10x512</span>

        feed_fwd_out <span class="token operator">=</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>norm1_out<span class="token punctuation">)</span>  <span class="token comment"># 32x10x512 -&gt; #32x10x2048 -&gt; 32x10x512</span>
        feed_fwd_residual_out <span class="token operator">=</span> feed_fwd_out <span class="token operator">+</span> norm1_out  <span class="token comment"># 32x10x512</span>
        norm2_out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>feed_fwd_residual_out<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 32x10x512</span>

        <span class="token keyword">return</span> norm2_out

<span class="token keyword">class</span> <span class="token class-name">TransformerEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Args:
        seq_len : length of input sequence
        embed_dim: dimension of embedding
        num_layers: number of encoder layers
        expansion_factor: factor which determines number of linear layers in feed forward layer
        n_heads: number of heads in multihead attention

    Returns:
        out: output of the encoder
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> expansion_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TransformerEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>embedding_layer <span class="token operator">=</span> WordEmbedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>positional_encoder <span class="token operator">=</span> PositionalEmbedding<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>TransformerBlock<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> expansion_factor<span class="token punctuation">,</span> n_heads<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        embed_out <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>positional_encoder<span class="token punctuation">(</span>embed_out<span class="token punctuation">)</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            out <span class="token operator">=</span> layer<span class="token punctuation">(</span>out<span class="token punctuation">,</span> out<span class="token punctuation">,</span> out<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out  <span class="token comment"># 32x10x512</span>

</code></pre> 
<h3><a id="5_Transformer_Decoder_321"></a>5. Transformer Decoder</h3> 
<p><strong>在Decoder中有两种multi-head attention，一种叫decoder attention，另一种叫encoder decoder attention。</strong></p> 
<h4><a id="Decoder_attention_323"></a>Decoder attention</h4> 
<p>首先是使用带<strong>mask</strong>的multi-head attention即decoder attention。<br> <font color="red"><strong>为什么要使用Mask呢？</strong></font><br> 使用<strong>mask掩码</strong>是因为在创建目标词的注意力时，我们不需要一个词来查看将来的词来检查依赖关系。其实也是一个历史问题，在NLP发展史中对文本序列token的处理类似GPT2的Auto-regressive（自回归）的Mask，就只对先前的tokens进行注意力学习，而不让模型通过未知的后续token来猜测当前token，否则模型会直接copy下一个单词的token。后来的BERT用的就是双向的策略，而不只是看过去的token而是从过去到未来再从未来到过去两个方向进行预测。<br> 例如:在单词“I am a student”中，我们不需要单词“a”来查找单词“student”，没有mask的话当模型看到“a”就会直接给出"student"的预测。<br> 对于Mask这一三角矩阵，如第一节中Decoder Block的图所示，对于所有零位置，用负无穷大填充，而在代码中，用一个非常小的数字填充它以避免除法错误(比如 -1e20)。</p> 
<h4><a id="Encoder_decoder_attention_329"></a>Encoder decoder attention</h4> 
<p>对于这个多头注意力，从Encoder的输出中创建Key和Value向量。Query是从上一个Decoder层的输出创建得到的。流程就如下图所示：<br> <img src="https://images2.imgbox.com/b8/5b/tmyOk7ck_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> expansion_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DecoderBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""
        Args:
           embed_dim: dimension of the embedding
           expansion_factor: fator ehich determines output dimension of linear layer
           n_heads: number of attention heads

        """</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>transformer_block <span class="token operator">=</span> TransformerBlock<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> expansion_factor<span class="token punctuation">,</span> n_heads<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
           key: key vector
           query: query vector
           x: value vector
           mask: mask to be given for multi head attention
        Returns:
           out: output of transformer block

        """</span>

        <span class="token comment"># we need to pass mask mask only to fst attention</span>
        attention <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>  <span class="token comment"># 32x10x512</span>
        value <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>attention <span class="token operator">+</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer_block<span class="token punctuation">(</span>key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> value<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out


<span class="token keyword">class</span> <span class="token class-name">TransformerDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> target_vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> expansion_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TransformerDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">"""  
        Args:
           target_vocab_size: vocabulary size of taget
           embed_dim: dimension of embedding
           seq_len : length of input sequence
           num_layers: number of encoder layers
           expansion_factor: factor which determines number of linear layers in feed forward layer
           n_heads: number of heads in multihead attention

        """</span>
        self<span class="token punctuation">.</span>word_embedding <span class="token operator">=</span> WordEmbedding<span class="token punctuation">(</span>target_vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>position_embedding <span class="token operator">=</span> PositionalEmbedding<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>
                DecoderBlock<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> expansion_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
                <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span>
            <span class="token punctuation">]</span>

        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> target_vocab_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> enc_out<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            x: input vector from target
            enc_out : output from encoder layer
            trg_mask: mask for decoder self attention
        Returns:
            out: output vector
        """</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>word_embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 32x10x512</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>position_embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 32x10x512</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>enc_out<span class="token punctuation">,</span> x<span class="token punctuation">,</span> enc_out<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>

        out <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc_out<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> out

</code></pre> 
<h3><a id="6_Overrall_Transformer_421"></a>6. Overrall Transformer</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> src_vocab_size<span class="token punctuation">,</span> target_vocab_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> expansion_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
                 n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">"""  
        Args:
           embed_dim:  dimension of embedding 
           src_vocab_size: vocabulary size of source
           target_vocab_size: vocabulary size of target
           seq_length : length of input sequence
           num_layers: number of encoder layers
           expansion_factor: factor which determines number of linear layers in feed forward layer
           n_heads: number of heads in multihead attention

        """</span>

        self<span class="token punctuation">.</span>target_vocab_size <span class="token operator">=</span> target_vocab_size

        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> TransformerEncoder<span class="token punctuation">(</span>seq_length<span class="token punctuation">,</span> src_vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> num_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span>
                                          expansion_factor<span class="token operator">=</span>expansion_factor<span class="token punctuation">,</span> n_heads<span class="token operator">=</span>n_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> TransformerDecoder<span class="token punctuation">(</span>target_vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> num_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span>
                                          expansion_factor<span class="token operator">=</span>expansion_factor<span class="token punctuation">,</span> n_heads<span class="token operator">=</span>n_heads<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">make_trg_mask</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> trg<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            trg: target sequence
        Returns:
            trg_mask: target mask
        """</span>
        batch_size<span class="token punctuation">,</span> trg_len <span class="token operator">=</span> trg<span class="token punctuation">.</span>shape
        <span class="token comment"># returns the lower triangular part of matrix filled with ones</span>
        trg_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>tril<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>trg_len<span class="token punctuation">,</span> trg_len<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>
            batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> trg_len<span class="token punctuation">,</span> trg_len
        <span class="token punctuation">)</span>
        <span class="token keyword">return</span> trg_mask

    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> trg<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        for inference
        Args:
            src: input to encoder
            trg: input to decoder
        out:
            out_labels : returns final prediction of sequence
        """</span>
        trg_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>make_trg_mask<span class="token punctuation">(</span>trg<span class="token punctuation">)</span>
        enc_out <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>src<span class="token punctuation">)</span>
        out_labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        batch_size<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> src<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> src<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)</span>
        out <span class="token operator">=</span> trg
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 10</span>
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>out<span class="token punctuation">,</span> enc_out<span class="token punctuation">,</span> trg_mask<span class="token punctuation">)</span>  <span class="token comment"># bs x seq_len x vocab_dim</span>
            <span class="token comment"># taking the last token</span>
            out <span class="token operator">=</span> out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

            out <span class="token operator">=</span> out<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            out_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>out<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            out <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>out<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> out_labels

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> trg<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            src: input to encoder
            trg: input to decoder
        out:
            out: final vector which returns probabilities of each target word
        """</span>
        trg_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>make_trg_mask<span class="token punctuation">(</span>trg<span class="token punctuation">)</span>
        enc_out <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>src<span class="token punctuation">)</span>

        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>trg<span class="token punctuation">,</span> enc_out<span class="token punctuation">,</span> trg_mask<span class="token punctuation">)</span>
        <span class="token keyword">return</span> outputs

</code></pre> 
<h3><a id="7_Test_Code_503"></a>7. Test Code</h3> 
<p>验证模型是否可正常运行，可以通过如下代码进行测试。如果没有报错，能输出结果那就没有问题。</p> 
<pre><code class="prism language-python"><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    src_vocab_size <span class="token operator">=</span> <span class="token number">11</span>
    target_vocab_size <span class="token operator">=</span> <span class="token number">11</span>
    num_layers <span class="token operator">=</span> <span class="token number">6</span>
    seq_length <span class="token operator">=</span> <span class="token number">12</span>

    <span class="token comment"># let 0 be sos(start operation signal) token and 1 be eos(end operation signal) token</span>
    src <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    target <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                           <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>src<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> target<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    model <span class="token operator">=</span> Transformer<span class="token punctuation">(</span>embed_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> src_vocab_size<span class="token operator">=</span>src_vocab_size<span class="token punctuation">,</span>
                        target_vocab_size<span class="token operator">=</span>target_vocab_size<span class="token punctuation">,</span> seq_length<span class="token operator">=</span>seq_length<span class="token punctuation">,</span>
                        num_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span> expansion_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> n_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
    <span class="token comment"># print(model)</span>
    out <span class="token operator">=</span> model<span class="token punctuation">(</span>src<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span> <span class="token operator">*</span> <span class="token number">50</span><span class="token punctuation">)</span>
    <span class="token comment"># inference</span>
    src <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    trg <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>src<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> trg<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    out <span class="token operator">=</span> model<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>src<span class="token punctuation">,</span> trg<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span>
</code></pre> 
<hr> 
<h2><a id="_537"></a>总结</h2> 
<p>即使现在ChatGPT等大模型的问世引起了社会广泛关注，但仍理性看待Transformer及其应用范围，在工业界仍旧是以ResNet为首的卷积神经网络占大头。而Transformer在学术界的地位是无法取代的，因此了解最基础的Transformer（Naive Transformer）也是很有必要性的。后续可以关注一下本博客衍生出的Vision Transformer的介绍和实现（挖个坑）……</p> 
<h2><a id="_539"></a>日志</h2> 
<p>2023.3.26.18:43 由于时间关系，先发布出来，后面会继续更进补充每个模块的要点和细节讲解。<br> 2023.3.27.13:22 补充Word Embedding和Position Encoding的讲解。<br> 2023.3.27.19:23 补充Multi-Head Attention讲解<br> 2023.3.28.15:27 补充Transformer Decoder讲解</p> 
<h2><a id="_544"></a>参考文献</h2> 
<p>B站视频链接：<a href="https://www.bilibili.com/video/BV1Di4y1c7Zm?p=1&amp;vd_source=141e1c55a2c44c3fa67bbdbec1600dcf" rel="nofollow">Transformer从零详细解读(可能是你见过最通俗易懂的讲解)</a><br> YouTube视频链接：<a href="https://www.youtube.com/watch?v=U0s0f995w14" rel="nofollow">Pytorch Transformers from Scratch (Attention is all you need)</a> 【友情提示：此视频的代码有略微错误】</p> 
<p>Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.<br> Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.</p> 
<p>https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch/notebook<br> http://jalammar.github.io/illustrated-transformer/<br> https://theaisummer.com/transformer/<br> https://towardsdatascience.com/7-things-you-didnt-know-about-the-transformer-a70d93ced6b2<br> http://nlp.seas.harvard.edu/2018/04/03/attention.html<br> https://peterbloem.nl/blog/transformers<br> https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec<br> https://github.com/hyunwoongko/transformer</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/91b5984469d16b160e277c740756346f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">selenium窗口切换和关闭指定窗口</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d6375d4c68945edcf62fe9ec15fd62d1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">小短腿的第一篇博客</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>