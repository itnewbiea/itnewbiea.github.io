<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>onnx网络中指定层的结果/中间层结果获取 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="onnx网络中指定层的结果/中间层结果获取" />
<meta property="og:description" content="背景 有时候我们需要对模型推理的中间层结果进行对比分析，这时候如何方便快捷地将指定中间层的结果保存下来就很重要了。具体方法如下： 核心：在模型推理之前将指定的中间层/所有的层放到输出节点中 代码 from onnx import load_model, save_model import onnx import onnxruntime as rt import torch import numpy as np from collections import OrderedDict # 该设置可以将每层tensor完整地输出，而非输出部分（即省略中间，只显示收尾） # tips：终端中通常不能全部显示大尺寸的tensor，可以重定向到文本中~ np.set_printoptions(threshold=np.inf) def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() def read_file(N, C, H, W, bin_path): input_size = N * C * H * W file = open(bin_path, &#34;rb&#34;) data = np.fromfile(file, dtype=np.uint8) data = data.astype(np.float32) / 255 data_tensor = torch.from_numpy(data[0:input_size]) return data_tensor." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/a642d4408c6e18ea167998e1a33ea41d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-28T00:09:08+08:00" />
<meta property="article:modified_time" content="2023-02-28T00:09:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">onnx网络中指定层的结果/中间层结果获取</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>背景</h2> 
<ul><li>有时候我们需要对模型推理的中间层结果进行对比分析，这时候如何方便快捷地将指定中间层的结果保存下来就很重要了。</li><li>具体方法如下： 
  <ul><li>核心：在模型推理之前将指定的中间层/所有的层放到输出节点中</li></ul> </li></ul> 
<h2><a id="_4"></a>代码</h2> 
<pre><code class="prism language-python"><span class="token keyword">from</span> onnx <span class="token keyword">import</span> load_model<span class="token punctuation">,</span> save_model
<span class="token keyword">import</span> onnx
<span class="token keyword">import</span> onnxruntime <span class="token keyword">as</span> rt
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict


<span class="token comment"># 该设置可以将每层tensor完整地输出，而非输出部分（即省略中间，只显示收尾）</span>
<span class="token comment"># tips：终端中通常不能全部显示大尺寸的tensor，可以重定向到文本中~</span>
np<span class="token punctuation">.</span>set_printoptions<span class="token punctuation">(</span>threshold<span class="token operator">=</span>np<span class="token punctuation">.</span>inf<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">to_numpy</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> tensor<span class="token punctuation">.</span>requires_grad <span class="token keyword">else</span> tensor<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">read_file</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> bin_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_size <span class="token operator">=</span> N <span class="token operator">*</span> C <span class="token operator">*</span> H <span class="token operator">*</span> W
    <span class="token builtin">file</span> <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>bin_path<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span>
    data <span class="token operator">=</span> np<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>
    data <span class="token operator">=</span> data<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255</span>
    data_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>input_size<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> data_tensor<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>

<span class="token comment"># 加载模型</span>
model <span class="token operator">=</span> onnx<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'./model.onnx'</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> node <span class="token keyword">in</span> model<span class="token punctuation">.</span>graph<span class="token punctuation">.</span>node<span class="token punctuation">:</span>
    <span class="token keyword">for</span> output <span class="token keyword">in</span> node<span class="token punctuation">.</span>output<span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>graph<span class="token punctuation">.</span>output<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>onnx<span class="token punctuation">.</span>ValueInfoProto<span class="token punctuation">(</span>name<span class="token operator">=</span>output<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'node is: '</span><span class="token punctuation">,</span> onnx<span class="token punctuation">.</span>ValueInfoProto<span class="token punctuation">(</span>name<span class="token operator">=</span>output<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 将指定结点打印出来，检查是否是自己想要的</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>graph<span class="token punctuation">.</span>output<span class="token punctuation">)</span>

<span class="token comment"># 将模型进行序列化为二进制格式</span>
session <span class="token operator">=</span> rt<span class="token punctuation">.</span>InferenceSession<span class="token punctuation">(</span>model<span class="token punctuation">.</span>SerializeToString<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 读取模型的输入</span>
x1 <span class="token operator">=</span> read_file<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token string">"./input1.bin"</span><span class="token punctuation">)</span>
<span class="token comment"># x2 = read_file(1, 1, 224, 224, "./input2.bin")</span>

<span class="token comment"># single input</span>
ort_inputs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>session<span class="token punctuation">.</span>get_inputs<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>name<span class="token punctuation">:</span> to_numpy<span class="token punctuation">(</span>x1<span class="token punctuation">)</span><span class="token punctuation">}</span>
<span class="token comment"># multiple inputs</span>
<span class="token comment"># ort_inputs = {session.get_inputs()[0].name: to_numpy(x1), session.get_inputs()[1].name: to_numpy(x2)}</span>
<span class="token comment"># 执行前向推理，保存结果</span>
ort_out <span class="token operator">=</span> session<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> ort_inputs<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>ort_out<span class="token punctuation">)</span>

<span class="token comment"># 获取前面循环中指定的所有节点输出</span>
outputs <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>name <span class="token keyword">for</span> x <span class="token keyword">in</span> session<span class="token punctuation">.</span>get_outputs<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token comment"># 将输出压缩在字典中，这样便于通过中间层名字作为key，获取具体的输出tensor值</span>
ort_out <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> ort_out<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># print(len(ort_out))</span>
<span class="token comment"># print(ort_out)</span>
<span class="token comment"># 通过中中间层名称访问其输出tensor</span>
<span class="token comment"># print(ort_out["layer_conv3"])</span>

</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/218593c664e574e1c9a9de84c013687a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java中的强制线程(插队)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/541c6470c303809768855b562b53bcba/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">一文读懂数据中台架构体系（收藏）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>