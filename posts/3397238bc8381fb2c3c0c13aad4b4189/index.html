<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pyspark_自定义udf_解析json列【附代码】 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pyspark_自定义udf_解析json列【附代码】" />
<meta property="og:description" content="pyspark_自定义udf_解析json列【附代码】 一、背景：二、调研方案：三、利用Pyspark &#43; udf自定义函数实现大数据并行计算整体流程案例代码运行结果：案例代码：代码地址：代码 一、背景： 车联网数据有很多车的时序数据，现有一套云端算法需要对每一辆车历史数据进行计算得到结果，每日将全部车算一遍存到hive数仓中
二、调研方案： 1、python脚本运行，利用pyhive拉取数据到pandas进行处理，将结果to_parquet后用hdfs_client存到数仓中
问题：数据量上亿，对内存要求极大，无法直接拉取到python脚本所在的服务器内存中运算
2、将算法内容改写成SQL或者SPARKSQL，每日调度
问题：代码改写SQL要重新梳理代码逻辑，且很多函数SQL实现复杂，有些函数不支持
三、利用Pyspark &#43; udf自定义函数实现大数据并行计算 整体流程 1、pyspark-spark sql拉取数据到spark df
2、spark df 按 车辆唯一标识分组，执行udf自定义函数（算法），每一个分组的返回值是String类型的json字符串，执行完成后返回的是result_df， spark_df【索引（车辆唯一标识）、数据（String类型的json字符串）】
3、解析json并拼接成spark_df
4、spark_df生成临时表，将临时表数据写入hive数仓
案例代码运行结果： 案例代码： 代码地址： https://github.com/SeafyLiang/Python_study/blob/master/pyspark_demo/pyspark_udf_json.py
代码 from pyspark.sql import SparkSession # SparkConf、SparkContext 和 SQLContext 都已经被封装在 SparkSession from pyspark.sql import functions as F import pandas as pd from pyspark.sql import types as T # spark df的数据类型 from pyspark.sql.functions import array, from_json, col, explode import sys def get_auc(id, date, vol): temp_df = pd." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/3397238bc8381fb2c3c0c13aad4b4189/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-02T13:51:16+08:00" />
<meta property="article:modified_time" content="2023-08-02T13:51:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pyspark_自定义udf_解析json列【附代码】</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>pyspark_自定义udf_解析json列【附代码】</h4> 
 <ul><li><ul><li><a href="#_1" rel="nofollow">一、背景：</a></li><li><a href="#_4" rel="nofollow">二、调研方案：</a></li><li><a href="#Pyspark__udf_10" rel="nofollow">三、利用Pyspark + udf自定义函数实现大数据并行计算</a></li><li><ul><li><a href="#_11" rel="nofollow">整体流程</a></li><li><a href="#_17" rel="nofollow">案例代码运行结果：</a></li><li><a href="#_20" rel="nofollow">案例代码：</a></li><li><a href="#_21" rel="nofollow">代码地址：</a></li><li><a href="#_24" rel="nofollow">代码</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_1"></a>一、背景：</h3> 
<p>车联网数据有很多车的时序数据，现有一套云端算法需要对每一辆车历史数据进行计算得到结果，每日将全部车算一遍存到hive数仓中</p> 
<h3><a id="_4"></a>二、调研方案：</h3> 
<p>1、python脚本运行，利用pyhive拉取数据到pandas进行处理，将结果to_parquet后用hdfs_client存到数仓中<br> 问题：数据量上亿，对内存要求极大，无法直接拉取到python脚本所在的服务器内存中运算<br> 2、将算法内容改写成SQL或者SPARKSQL，每日调度<br> 问题：代码改写SQL要重新梳理代码逻辑，且很多函数SQL实现复杂，有些函数不支持</p> 
<h3><a id="Pyspark__udf_10"></a>三、利用Pyspark + udf自定义函数实现大数据并行计算</h3> 
<h4><a id="_11"></a>整体流程</h4> 
<p>1、pyspark-spark sql拉取数据到spark df<br> 2、spark df 按 车辆唯一标识分组，执行udf自定义函数（算法），每一个分组的返回值是String类型的json字符串，执行完成后返回的是result_df， spark_df【索引（车辆唯一标识）、数据（String类型的json字符串）】<br> 3、解析json并拼接成spark_df<br> 4、spark_df生成临时表，将临时表数据写入hive数仓</p> 
<h4><a id="_17"></a>案例代码运行结果：</h4> 
<p><img src="https://images2.imgbox.com/41/89/2pQz5QRI_o.png" alt="案例代码运行结果"></p> 
<h4><a id="_20"></a>案例代码：</h4> 
<h4><a id="_21"></a>代码地址：</h4> 
<p>https://github.com/SeafyLiang/Python_study/blob/master/pyspark_demo/pyspark_udf_json.py</p> 
<h4><a id="_24"></a>代码</h4> 
<pre><code class="prism language-python"><span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession  <span class="token comment"># SparkConf、SparkContext 和 SQLContext 都已经被封装在 SparkSession</span>
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> functions <span class="token keyword">as</span> F
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> types <span class="token keyword">as</span> T  <span class="token comment"># spark df的数据类型</span>
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>functions <span class="token keyword">import</span> array<span class="token punctuation">,</span> from_json<span class="token punctuation">,</span> col<span class="token punctuation">,</span> explode
<span class="token keyword">import</span> sys


<span class="token keyword">def</span> <span class="token function">get_auc</span><span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">,</span> date<span class="token punctuation">,</span> vol<span class="token punctuation">)</span><span class="token punctuation">:</span>
    temp_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
        <span class="token string">'id'</span><span class="token punctuation">:</span> <span class="token builtin">id</span><span class="token punctuation">,</span>
        <span class="token string">'date'</span><span class="token punctuation">:</span> date<span class="token punctuation">,</span>
        <span class="token string">'vol'</span><span class="token punctuation">:</span> vol
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
    temp_df<span class="token punctuation">[</span><span class="token string">'date'</span><span class="token punctuation">]</span> <span class="token operator">=</span> temp_df<span class="token punctuation">[</span><span class="token string">'date'</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">+</span> <span class="token string">'aaa'</span><span class="token punctuation">)</span>
    temp_df_json <span class="token operator">=</span> temp_df<span class="token punctuation">.</span>to_json<span class="token punctuation">(</span>orient<span class="token operator">=</span><span class="token string">'records'</span><span class="token punctuation">)</span>  <span class="token comment"># orient='records'是关键，可以把json转成array&lt;json&gt;</span>
    <span class="token keyword">return</span> temp_df_json


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'test_sklearn_pyspark'</span><span class="token punctuation">)</span> \
        <span class="token punctuation">.</span>config<span class="token punctuation">(</span><span class="token string">"spark.sql.warehouse.dir"</span><span class="token punctuation">,</span> <span class="token string">"hdfs://nameservice1/user/hive/warehouse"</span><span class="token punctuation">)</span> \
        <span class="token punctuation">.</span>config<span class="token punctuation">(</span><span class="token string">"hive.exec.dynamici.partition"</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span> \
        <span class="token punctuation">.</span>config<span class="token punctuation">(</span><span class="token string">"hive.exec.dynamic.partition.mode"</span><span class="token punctuation">,</span> <span class="token string">"nonstrict"</span><span class="token punctuation">)</span> \
        <span class="token punctuation">.</span>config<span class="token punctuation">(</span><span class="token string">"spark.sql.crossJoin.enabled"</span><span class="token punctuation">,</span> <span class="token string">"true"</span><span class="token punctuation">)</span><span class="token punctuation">.</span> \
        config<span class="token punctuation">(</span><span class="token string">"spark.serializer"</span><span class="token punctuation">,</span> <span class="token string">"org.apache.spark.serializer.KryoSerializer"</span><span class="token punctuation">)</span> \
        <span class="token punctuation">.</span>enableHiveSupport<span class="token punctuation">(</span><span class="token punctuation">)</span> \
        <span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>spark<span class="token punctuation">)</span>

    temp_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">'id'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">'date'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'2022-05-01'</span><span class="token punctuation">,</span> <span class="token string">'2022-05-02'</span><span class="token punctuation">,</span> <span class="token string">'2022-05-03'</span><span class="token punctuation">,</span> <span class="token string">'2022-05-04'</span><span class="token punctuation">,</span> <span class="token string">'2022-05-05'</span><span class="token punctuation">,</span> <span class="token string">'2022-05-05'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">'vol'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">68.22</span><span class="token punctuation">,</span> <span class="token number">45.10</span><span class="token punctuation">,</span> <span class="token number">899.33</span><span class="token punctuation">,</span> <span class="token number">45.11</span><span class="token punctuation">,</span> <span class="token number">32.22</span><span class="token punctuation">,</span> <span class="token number">99.33</span><span class="token punctuation">]</span>
    <span class="token punctuation">}</span>
    tempdf <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>temp_dict<span class="token punctuation">)</span>
    df <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>tempdf<span class="token punctuation">)</span>

    <span class="token comment"># 自定义函数（计算AUC），并且变成UDF</span>
    <span class="token triple-quoted-string string">"""注意：自定义函数的重点在于定义返回值的数据类型，这个返回值的数据类型必须与该函数return值的数据类型一致，否则会报错。
    该例子中，该函数return的值auc，是string类型，在将该函数定义成udf的时候，指定的返回值类型，也必须是string！！"""</span>

    get_auc_udfs <span class="token operator">=</span> F<span class="token punctuation">.</span>udf<span class="token punctuation">(</span>get_auc<span class="token punctuation">,</span> returnType<span class="token operator">=</span>T<span class="token punctuation">.</span>StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 定义成udf,并且此udf的返回值类型为string</span>

    <span class="token comment"># 分组聚合操作：分别计算每月样本量、逾期率、AUC</span>
    <span class="token triple-quoted-string string">"""使用上面定义的UDF，结合F.collect_list(col)来实现UDAF的功能。
    F.collect_lits(col)的作用是将列col的值变成一个list返回."""</span>

    df_result <span class="token operator">=</span> df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">'id'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>agg<span class="token punctuation">(</span>get_auc_udfs<span class="token punctuation">(</span>
        F<span class="token punctuation">.</span>collect_list<span class="token punctuation">(</span>F<span class="token punctuation">.</span>col<span class="token punctuation">(</span><span class="token string">'id'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cast<span class="token punctuation">(</span><span class="token string">'int'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        F<span class="token punctuation">.</span>collect_list<span class="token punctuation">(</span>F<span class="token punctuation">.</span>col<span class="token punctuation">(</span><span class="token string">'date'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cast<span class="token punctuation">(</span><span class="token string">'string'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        F<span class="token punctuation">.</span>collect_list<span class="token punctuation">(</span>F<span class="token punctuation">.</span>col<span class="token punctuation">(</span><span class="token string">'vol'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cast<span class="token punctuation">(</span><span class="token string">'double'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'json_str'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 利用自定的UDF，实现指定聚合计算</span>

    df_result<span class="token punctuation">.</span>show<span class="token punctuation">(</span>truncate<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    opn <span class="token operator">=</span> <span class="token number">2</span>
    <span class="token keyword">if</span> opn <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token comment"># 【不推荐】方式一：spark_df转成pandas_df，拼接json成pandas_all_df后再转成spark_df写入</span>
        <span class="token comment"># 数据量大时会把大量数据拉到driver本地，导致内存溢出</span>
        all_result_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">)</span>
        df_result_pandas <span class="token operator">=</span> df_result<span class="token punctuation">.</span>toPandas<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> row <span class="token keyword">in</span> df_result_pandas<span class="token punctuation">.</span>itertuples<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>row<span class="token punctuation">.</span>json_str<span class="token punctuation">)</span>
            temp_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_json<span class="token punctuation">(</span>row<span class="token punctuation">.</span>json_str<span class="token punctuation">)</span>
            all_result_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>all_result_df<span class="token punctuation">,</span> temp_df<span class="token punctuation">]</span><span class="token punctuation">,</span> ignore_index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>all_result_df<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> opn <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
        <span class="token comment"># 【推荐】方式二：解析json成新的spark_df</span>
        json_schema <span class="token operator">=</span> T<span class="token punctuation">.</span>ArrayType<span class="token punctuation">(</span>
            T<span class="token punctuation">.</span>StructType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token string">"id"</span><span class="token punctuation">,</span> T<span class="token punctuation">.</span>IntegerType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token string">"date"</span><span class="token punctuation">,</span> T<span class="token punctuation">.</span>StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token string">"vol"</span><span class="token punctuation">,</span> T<span class="token punctuation">.</span>DoubleType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        df_result <span class="token operator">=</span> df_result<span class="token punctuation">.</span>withColumn<span class="token punctuation">(</span><span class="token string">'parsed_json'</span><span class="token punctuation">,</span> from_json<span class="token punctuation">(</span>col<span class="token punctuation">(</span><span class="token string">'json_str'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> json_schema<span class="token punctuation">)</span><span class="token punctuation">)</span>
        df_result<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
        df_result<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">'parsed_json'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> truncate<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        df_result <span class="token operator">=</span> df_result<span class="token punctuation">.</span>select<span class="token punctuation">(</span>explode<span class="token punctuation">(</span>col<span class="token punctuation">(</span><span class="token string">'parsed_json'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'parsed_json_explode'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        df_result<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
        df_result <span class="token operator">=</span> df_result<span class="token punctuation">.</span>select<span class="token punctuation">(</span>col<span class="token punctuation">(</span><span class="token string">'parsed_json_explode.id'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'id'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                     col<span class="token punctuation">(</span><span class="token string">'parsed_json_explode.date'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'date'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                     col<span class="token punctuation">(</span><span class="token string">'parsed_json_explode.vol'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'vol'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        df_result<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'df_result:'</span><span class="token punctuation">,</span> df_result<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 写入hive表</span>
        <span class="token comment"># dt_before1day = sys.argv[1]</span>
        <span class="token comment"># print('dt_before1day:', dt_before1day)</span>
        <span class="token comment"># # df 转为临时表/临时视图</span>
        <span class="token comment"># df_result.createOrReplaceTempView("df_tmp_view")</span>
        <span class="token comment"># # spark.sql 插入hive</span>
        <span class="token comment"># spark.sql("""</span>
        <span class="token comment">#         insert overwrite table table_name partition(dt='{DT}')</span>
        <span class="token comment">#         select</span>
        <span class="token comment">#         *</span>
        <span class="token comment">#         from df_tmp_view</span>
        <span class="token comment">#         """.format(DT=dt_before1day))</span>
        <span class="token comment"># print('spark write end!')</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'end'</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7adab7198fca55c032a9fa043ddbcde0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Hive的Sort By与Distribute By</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/611e599805ebd4df1fc2274790400575/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">List集合删除指定元素-四种方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>