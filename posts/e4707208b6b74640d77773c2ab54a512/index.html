<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【2023最新】Scrapy框架教程一-Scrapy的创建与启动及Scrapy基础命令 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【2023最新】Scrapy框架教程一-Scrapy的创建与启动及Scrapy基础命令" />
<meta property="og:description" content="文章目录 Scrapy框架Scrapy五大组件Scrapy五大文件 Scrapy创建项目Scrapy启动项目启动项目第一种方法启动项目第一种方法 Scrapy总结基础命令 Scrapy框架 Scrapy 是一个快速的高级网络爬虫和网络抓取框架，用于 抓取网站并从其页面中提取结构化数据。它可以被使用 用途广泛，从数据挖掘到监控和自动化 测试。
scrapy官网学习网址：https://docs.scrapy.org/en/latest/
Scrapy五大组件 &#39;&#39;&#39; Scrapy是一个Python编写的开源网络爬虫框架，它的五大核心组件包括： 1.引擎（Engine）：是Scrapy的核心，负责控制整个爬虫流程的运行，包括调度器（Scheduler）、下载器（Downloader）和管道（Pipeline）等组件的协调工作。 2.调度器（Scheduler）：负责接受引擎发送过来的请求，并将其加入到队列中，队列会自动去重，等待下载器进行下载。同时，调度器还会根据一定的策略，从队列中选择一定数量的请求发送给下载器进行处理。 3.下载器（Downloader）：负责将调度器发送过来的请求进行处理，并将响应数据返回给引擎。下载器还可以处理一些请求的中间件，如代理、用户代理等。 4.爬虫（Spider）：负责定义爬取网站的规则，包括起始URL、如何跟踪链接、如何解析页面等。爬虫会将爬取到的数据交给管道进行处理。 5.管道（Pipeline）：负责处理爬虫爬取到的数据，包括清洗、去重、存储等。管道可以定义多个，用于对不同类型的数据进行处理 工作流程： 引擎先获取爬虫文件的起始url连接，传递给调度器，由调度器进行安排顺序（同时去除重复连接），之后便让引擎给下载器，引擎给下载器的需要过下载中间件的手，该做代理做代理，该换请求头换请求头，下载器拿到包装好的需要请求的东西，向互联网进行请求，获取下载来的数据（response）给下载中间件，下载中间件过一手让引擎给爬虫文件，爬虫文件开始解析，如果还需要请求的，再重复上面的步骤，需要存储的，通过item给管道，管道开始存储，保存为Mysql,Mongodb,csv等 &#39;&#39;&#39; Scrapy五大文件 items.py ： 管道文件，用来传输文件到pipelines.py进行保存 middlewares.py ： 中间件，可以用来设置ua，ip代理，selenium等等等，可为 Scrapy 添加其他功能。我们可以将很多开源中间件附加到 Scrapy 以获得额外的功能。 pipelines.py：保存文件的地方，可以把管道文件传输过来的数据进行保存，保存为Mysql，mongodb,csv等等等 settings.py：设置爬虫的地方，像管道保存是否开启，日志输出等等等 spiders下的爬虫文件：是我们核心的文件，用来爬取数据，分析数据的文件 Scrapy创建项目 首先安装scrapy
# 使用清华源安装scrapy pip install -i https://pypi.tuna.tsinghua.edu.cn/simple Scrapy 前提:路径切换 cd 需要cd到你准备放scrapy的文件下 1. 创建scrapy项目 scrapy startproject 项目名 例： scrapy startproject douban 创建后显示这个样就成功了
他的意思是让我们创建爬虫文件，因为爬虫文件需要我们输入爬虫名字和域名
2.1 先cd到路径 cd douban 2.2 创建爬虫程序 scrapy genspider example example.com scrapy genspider：固定的 example：爬虫程序的名字(不固定的) 不能和项目名一样 example." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/e4707208b6b74640d77773c2ab54a512/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-19T19:50:47+08:00" />
<meta property="article:modified_time" content="2023-11-19T19:50:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【2023最新】Scrapy框架教程一-Scrapy的创建与启动及Scrapy基础命令</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Scrapy_1" rel="nofollow">Scrapy框架</a></li><li><ul><li><a href="#Scrapy_7" rel="nofollow">Scrapy五大组件</a></li><li><a href="#Scrapy_30" rel="nofollow">Scrapy五大文件</a></li></ul> 
  </li><li><a href="#Scrapy_46" rel="nofollow">Scrapy创建项目</a></li><li><a href="#Scrapy_118" rel="nofollow">Scrapy启动项目</a></li><li><ul><li><a href="#_165" rel="nofollow">启动项目第一种方法</a></li><li><a href="#_186" rel="nofollow">启动项目第一种方法</a></li></ul> 
  </li><li><a href="#Scrapy_231" rel="nofollow">Scrapy总结基础命令</a></li></ul> 
</div> 
<p></p> 
<h2><a id="Scrapy_1"></a>Scrapy框架</h2> 
<p>Scrapy 是一个快速的高级网络<a href="https://en.wikipedia.org/wiki/Web_crawler" rel="nofollow">爬虫</a>和<a href="https://en.wikipedia.org/wiki/Web_scraping" rel="nofollow">网络抓取</a>框架，用于 抓取网站并从其页面中提取结构化数据。它可以被使用 用途广泛，从数据挖掘到监控和自动化 测试。</p> 
<p>scrapy官网学习网址：https://docs.scrapy.org/en/latest/</p> 
<h3><a id="Scrapy_7"></a>Scrapy五大组件</h3> 
<p><img src="https://images2.imgbox.com/73/ba/P9nnDuvs_o.png" alt="image-20231112215719179"></p> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">'''
Scrapy是一个Python编写的开源网络爬虫框架，它的五大核心组件包括：

1.引擎（Engine）：是Scrapy的核心，负责控制整个爬虫流程的运行，包括调度器（Scheduler）、下载器（Downloader）和管道（Pipeline）等组件的协调工作。

2.调度器（Scheduler）：负责接受引擎发送过来的请求，并将其加入到队列中，队列会自动去重，等待下载器进行下载。同时，调度器还会根据一定的策略，从队列中选择一定数量的请求发送给下载器进行处理。

3.下载器（Downloader）：负责将调度器发送过来的请求进行处理，并将响应数据返回给引擎。下载器还可以处理一些请求的中间件，如代理、用户代理等。

4.爬虫（Spider）：负责定义爬取网站的规则，包括起始URL、如何跟踪链接、如何解析页面等。爬虫会将爬取到的数据交给管道进行处理。

5.管道（Pipeline）：负责处理爬虫爬取到的数据，包括清洗、去重、存储等。管道可以定义多个，用于对不同类型的数据进行处理

工作流程：
引擎先获取爬虫文件的起始url连接，传递给调度器，由调度器进行安排顺序（同时去除重复连接），之后便让引擎给下载器，引擎给下载器的需要过下载中间件的手，该做代理做代理，该换请求头换请求头，下载器拿到包装好的需要请求的东西，向互联网进行请求，获取下载来的数据（response）给下载中间件，下载中间件过一手让引擎给爬虫文件，爬虫文件开始解析，如果还需要请求的，再重复上面的步骤，需要存储的，通过item给管道，管道开始存储，保存为Mysql,Mongodb,csv等
'''</span>
</code></pre> 
<h3><a id="Scrapy_30"></a>Scrapy五大文件</h3> 
<pre><code class="prism language-python">items<span class="token punctuation">.</span>py ： 管道文件，用来传输文件到pipelines<span class="token punctuation">.</span>py进行保存

middlewares<span class="token punctuation">.</span>py ： 中间件，可以用来设置ua，ip代理，selenium等等等，可为 Scrapy 添加其他功能。我们可以将很多开源中间件附加到 Scrapy 以获得额外的功能。

pipelines<span class="token punctuation">.</span>py：保存文件的地方，可以把管道文件传输过来的数据进行保存，保存为Mysql，mongodb<span class="token punctuation">,</span>csv等等等

settings<span class="token punctuation">.</span>py：设置爬虫的地方，像管道保存是否开启，日志输出等等等

spiders下的爬虫文件：是我们核心的文件，用来爬取数据，分析数据的文件
</code></pre> 
<h2><a id="Scrapy_46"></a>Scrapy创建项目</h2> 
<p>首先安装scrapy</p> 
<pre><code class="prism language-python"><span class="token comment"># 使用清华源安装scrapy</span>
pip install <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>pypi<span class="token punctuation">.</span>tuna<span class="token punctuation">.</span>tsinghua<span class="token punctuation">.</span>edu<span class="token punctuation">.</span>cn<span class="token operator">/</span>simple Scrapy
</code></pre> 
<pre><code class="prism language-python">前提<span class="token punctuation">:</span>路径切换 cd  
需要cd到你准备放scrapy的文件下

<span class="token number">1.</span> 创建scrapy项目
scrapy startproject 项目名

例：
scrapy startproject douban
</code></pre> 
<p><img src="https://images2.imgbox.com/16/c5/dqKqDkt5_o.png" alt="image-20231112194901199"></p> 
<p>创建后显示这个样就成功了</p> 
<p>他的意思是让我们创建爬虫文件，因为爬虫文件需要我们输入爬虫名字和域名</p> 
<pre><code class="prism language-python"><span class="token number">2.1</span> 先cd到路径
cd douban

<span class="token number">2.2</span> 创建爬虫程序
scrapy genspider example example<span class="token punctuation">.</span>com

scrapy genspider：固定的
example：爬虫程序的名字<span class="token punctuation">(</span>不固定的<span class="token punctuation">)</span> 不能和项目名一样
example<span class="token punctuation">.</span>com：可以允许爬取的范围<span class="token punctuation">(</span>不固定的<span class="token punctuation">)</span> 是根据你的目标url来指定的 其实很重要 后面是可以修改的

例：
目标url：https<span class="token punctuation">:</span><span class="token operator">//</span>www<span class="token punctuation">.</span>douban<span class="token punctuation">.</span>com<span class="token operator">/</span>doulist<span class="token operator">/</span><span class="token number">124084417</span><span class="token operator">/</span>?start<span class="token operator">=</span><span class="token number">0</span><span class="token operator">&amp;</span>sort<span class="token operator">=</span>seq<span class="token operator">&amp;</span>playable<span class="token operator">=</span><span class="token number">0</span><span class="token operator">&amp;</span>sub_type<span class="token operator">=</span>

scrapy genspider douban_data www<span class="token punctuation">.</span>douban<span class="token punctuation">.</span>com
</code></pre> 
<p><img src="https://images2.imgbox.com/56/74/yqdAxTKE_o.png" alt="image-20231112200144684"></p> 
<p>douban_data.py 解读</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> scrapy  <span class="token comment"># 导入模块</span>

<span class="token comment"># 爬虫类，项目启动后会自动运行这个类</span>
<span class="token keyword">class</span> <span class="token class-name">DoubanDataSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 爬虫程序的名字</span>
    name <span class="token operator">=</span> <span class="token string">'douban_data'</span>
    <span class="token comment"># 可以爬取的范围</span>
    <span class="token comment"># 有可能我们在实际进行爬取的时候  第一页可能是xxx.com 第三页可能就变成了xxx.cn </span>
    <span class="token comment"># 或者xxx.yy 那么可能就会爬取不到数据</span>
    <span class="token comment"># 所以我们需要对allowed_domains进行一个列表的添加</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"www.douban.com"</span><span class="token punctuation">]</span>
    <span class="token comment"># 起始url地址  会根据我们的allowed_domains对网页前缀进行一定的补全 </span>
    <span class="token comment"># 但有时候补全的url不对 所以我们也要去对他进行修改</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"https://www.douban.com"</span><span class="token punctuation">]</span>

    <span class="token comment"># 专门用于解析数据的</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>

</code></pre> 
<h2><a id="Scrapy_118"></a>Scrapy启动项目</h2> 
<p>scrapy启动之前需要先设置</p> 
<p>关于settings文件点击查看 <a href="https://blog.csdn.net/m0_73689941/article/details/134365715">Scrapy_settings配置文件设置</a></p> 
<p>settings.py 设置</p> 
<pre><code class="prism language-python"><span class="token comment"># 设置UA，但不常用，一般都是在MiddleWare中添加</span>
<span class="token keyword">from</span> fake_useragent <span class="token keyword">import</span> UserAgent
USER_AGENT <span class="token operator">=</span> UserAgent<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>random

<span class="token comment"># 遵循robots.txt中的爬虫规则</span>
ROBOTSTXT_OBEY <span class="token operator">=</span> <span class="token boolean">False</span><span class="token comment"># 需要改成False,要不然很多东西爬不了</span>
</code></pre> 
<p>这里先随便设置一个UA,后面会详细讲解，目前在不保存文件的情况下，只需要更改这两个设置就可以了</p> 
<p>在设置好之后，我们在简单写一下爬虫文件</p> 
<p>douban_data.py</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">DoubanDataSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"douban_data"</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"www.douban.com"</span><span class="token punctuation">]</span>
    <span class="token comment"># 把url换成我们需要爬取的</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"https://www.douban.com/doulist/124084417/?start=0&amp;sort=seq&amp;playable=0&amp;sub_type="</span><span class="token punctuation">]</span>
    
    <span class="token comment"># 爬虫函数 添加**kwargs就不会报黄</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
</code></pre> 
<p>爬虫文件我们只修改了要爬取的url</p> 
<p>打印响应的数据</p> 
<p>接下来就可以启动项目了</p> 
<p>启动项目有三种方法</p> 
<h3><a id="_165"></a>启动项目第一种方法</h3> 
<p>在项目的终端输入启动命令</p> 
<pre><code class="prism language-python"><span class="token number">4.</span> 执行爬虫程序
scrapy crawl example

scrapy crawl：固定的
example：执行的爬虫程序的名字，就是创建爬虫程序scrapy genspider example example<span class="token punctuation">.</span>com 

例：
scrapy crawl douban_data
</code></pre> 
<p><img src="https://images2.imgbox.com/cb/8a/xrjNBvtF_o.png" alt="image-20231112212427113"></p> 
<p><img src="https://images2.imgbox.com/c5/ef/gyQMgrxB_o.png" alt="image-20231112212551353"></p> 
<p>可以看到启动成功，但是他在终端不能选择数据啥的，及其的不方便</p> 
<h3><a id="_186"></a>启动项目第一种方法</h3> 
<p><img src="https://images2.imgbox.com/53/82/xpIFjIk9_o.png" alt="image-20231112212728508"></p> 
<p>在settings同级文件下创建一个启动项目的文件</p> 
<p>start.py文件</p> 
<pre><code class="prism language-python"><span class="token comment"># 第一种</span>
<span class="token keyword">from</span> scrapy <span class="token keyword">import</span> cmdline
cmdline<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">"scrapy crawl douban_data"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># # 启动不带日志第一种方法</span>
<span class="token comment"># --nolog 关闭日志文件输出</span>
<span class="token comment"># cmdline.execute('scrapy crawl douban_data --nolog'.split())  # 启动爬虫命令</span>

<span class="token comment"># 启动不带日志第二种方法</span>
<span class="token comment"># -s LOG_ENABLED=False 关闭日志文件输出</span>
<span class="token comment"># cmdline.execute('scrapy crawl douban_data -s LOG_ENABLED=False'.split())  # 启动爬虫命令</span>


<span class="token comment"># 第二种</span>
<span class="token comment"># 使用系统模块的system函数运行命令</span>
<span class="token keyword">import</span> os
<span class="token comment"># os.system('scrapy crawl douban_data')  # 启动爬虫命令</span>


<span class="token comment"># 启动不带日志第一种方法</span>
<span class="token comment"># --nolog 关闭日志文件输出</span>
<span class="token comment"># os.system('scrapy crawl douban_data --nolog')  # 启动爬虫命令</span>

<span class="token comment"># 启动不带日志第二种方法</span>
<span class="token comment"># -s LOG_ENABLED=False 关闭日志文件输出</span>
<span class="token comment"># os.system('scrapy crawl douban_data -s LOG_ENABLED=False')  # 启动爬虫命令</span>
</code></pre> 
<p>这里给大家列举好了，随便选一种取消注释运行即可</p> 
<p>关于日志输出，可以在启动时临时设置</p> 
<p>也可以在settings文件永久设置</p> 
<p><img src="https://images2.imgbox.com/24/12/zmfGlT8a_o.png" alt="image-20231112214127641"></p> 
<h2><a id="Scrapy_231"></a>Scrapy总结基础命令</h2> 
<pre><code class="prism language-python"><span class="token number">1.</span> 创建scrapy项目
scrapy startproject 项目名

例：scrapy startproject douban


<span class="token number">2.1</span> 先cd到路径
cd douban

<span class="token number">2.2</span> 创建爬虫程序
scrapy genspider example example<span class="token punctuation">.</span>com

scrapy genspider：固定的
example：爬虫程序的名字<span class="token punctuation">(</span>不固定的<span class="token punctuation">)</span> 不能和项目名一样
example<span class="token punctuation">.</span>com：可以允许爬取的范围<span class="token punctuation">(</span>不固定的<span class="token punctuation">)</span> 是根据你的目标url来指定的 其实很重要 后面是可以修改的

例：目标url：https<span class="token punctuation">:</span><span class="token operator">//</span>www<span class="token punctuation">.</span>douban<span class="token punctuation">.</span>com<span class="token operator">/</span>doulist<span class="token operator">/</span><span class="token number">124084417</span><span class="token operator">/</span>?start<span class="token operator">=</span><span class="token number">0</span><span class="token operator">&amp;</span>sort<span class="token operator">=</span>seq<span class="token operator">&amp;</span>playable<span class="token operator">=</span><span class="token number">0</span><span class="token operator">&amp;</span>sub_type<span class="token operator">=</span>

scrapy genspider douban_data www<span class="token punctuation">.</span>douban<span class="token punctuation">.</span>com


<span class="token number">4.</span> 执行爬虫程序
scrapy crawl example

scrapy crawl：固定的
example：执行的爬虫程序的名字，就是创建爬虫程序scrapy genspider example example<span class="token punctuation">.</span>com 

例：scrapy crawl douban_data

可以通过start<span class="token punctuation">.</span>py文件执行爬虫项目：
<span class="token keyword">from</span> scrapy <span class="token keyword">import</span> cmdline
cmdline<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">"scrapy crawl bd"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>scrapy其实就是把我们平时写的爬虫进行了四分五裂式的改造. 对每个功能进行了单独的封装, 并且, 各个模块之间互相的不做依赖. 一切都由引擎进行调配. 这种思想希望你能知道. 让模块与模块之间的关联性更加的松散. 这样我们如果希望替换某一模块的时候会非常的容易. 对其他模块也不会产生任何的影响。</p> 
<p>下一篇文章讲Scrapy的管道文件，以及Scrapy翻页处理和scrapy请求二级网址</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c09c671a9e1b6529e67c7a245a017acd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C&#43;&#43;多线程编程（2）：四种线程管理方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ac89bb5e26924949a204df81e4f72fb6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">APP测试常见功能测试点汇总</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>