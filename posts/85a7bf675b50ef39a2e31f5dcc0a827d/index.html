<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch的两个函数 .detach() .detach_() 的作用和区别 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch的两个函数 .detach() .detach_() 的作用和区别" />
<meta property="og:description" content="前言：当我们再训练网络的时候可能希望保持一部分的网络参数不变，只对其中一部分的参数进行调整；或者值训练部分分支网络，并不让其梯度对主网络的梯度造成影响，这时候我们就需要使用detach()函数来切断一些分支的反向传播
一、tensor.detach() 返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。
即使之后重新将它的requires_grad置为true,它也不会具有梯度grad
这样我们就会继续使用这个新的tensor进行计算，后面当我们进行反向传播时，到该调用detach()的tensor就会停止，不能再继续向前进行传播
注意：
使用detach返回的tensor和原始的tensor共同一个内存，即一个修改另一个也会跟着改变。
比如正常的例子是：
import torch a = torch.tensor([ 1, 2, 3.], requires_grad= True) print(a.grad) out = a.sigmoid() out.sum().backward() print(a.grad) &#39;&#39;&#39;返回： None tensor([0.1966, 0.1050, 0.0452]) &#39;&#39;&#39; 1.1 当使用detach()分离tensor但是没有更改这个tensor时，并不会影响backward():
import torch a = torch.tensor([ 1, 2, 3.], requires_grad= True) print(a.grad) out = a.sigmoid() print(out) #添加detach(),c的requires_grad为False c = out.detach() print(c) #这时候没有对c进行更改，所以并不会影响backward() out.sum().backward() print(a.grad) &#39;&#39;&#39;返回： None tensor([0.7311, 0.8808, 0.9526], grad_fn=&lt;SigmoidBackward&gt;) tensor([0.7311, 0.8808, 0.9526]) tensor([0.1966, 0.1050, 0.0452]) &#39;&#39;&#39; 从上可见tensor c是由out分离得到的，但是我也没有去改变这个c，这个时候依然对原来的out求导是不会有错误的，即" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/85a7bf675b50ef39a2e31f5dcc0a827d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-21T21:11:06+08:00" />
<meta property="article:modified_time" content="2021-02-21T21:11:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch的两个函数 .detach() .detach_() 的作用和区别</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>前言：</strong>当我们再训练网络的时候可能希望保持一部分的网络参数不变，只对其中一部分的参数进行调整；或者值训练部分分支网络，并不让其梯度对主网络的梯度造成影响，这时候我们就需要使用detach()函数来切断一些分支的反向传播</p> 
<h3>一、tensor.detach()</h3> 
<p>返回一个新的<code>tensor</code>，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个<code>tensor</code>永远不需要计算其梯度，不具有grad。</p> 
<p><strong>即使之后重新将它的requires_grad置为true,它也不会具有梯度grad</strong></p> 
<p>这样我们就会继续使用这个新的<code>tensor进行计算，后面当我们进行</code>反向传播时，到该调用detach()的<code>tensor</code>就会停止，不能再继续向前进行传播</p> 
<p><strong><code>注意：</code></strong></p> 
<p>使用detach返回的<code>tensor</code>和原始的<code>tensor</code>共同一个<code>内存，即一个修改另一个也会跟着改变</code>。</p> 
<p>比如正常的例子是：</p> 
<pre class="has"><code class="language-python hljs"></code>
 
 <ol class="hljs-ln"><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-keyword">import</span> torch
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     a = torch.tensor([
     
     <span class="hljs-number">1</span>, 
     
     <span class="hljs-number">2</span>, 
     
     <span class="hljs-number">3.</span>], requires_grad=
     
     <span class="hljs-literal">True</span>)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(a.grad)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     out = a.sigmoid()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     out.sum().backward()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(a.grad)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string"><span class="hljs-string">'''返回：</span></span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">None</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0.1966, 0.1050, 0.0452])</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">'''</span>
    
    </div>
   
   </div></li></ol>
 
 <div class="hljs-button {2}"></div></pre> 
<p><strong>1.1 当使用detach()分离tensor但是没有更改这个tensor时，并不会影响backward():</strong></p> 
<pre class="has"><code class="language-python hljs"></code>
 
 <ol class="hljs-ln"><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-keyword">import</span> torch
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     a = torch.tensor([
     
     <span class="hljs-number">1</span>, 
     
     <span class="hljs-number">2</span>, 
     
     <span class="hljs-number">3.</span>], requires_grad=
     
     <span class="hljs-literal">True</span>)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(a.grad)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     out = a.sigmoid()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(out)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-comment">#添加detach(),c的requires_grad为False</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     c = out.detach()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(c)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-comment">#这时候没有对c进行更改，所以并不会影响backward()</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     out.sum().backward()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(a.grad)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string"><span class="hljs-string">'''返回：</span></span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">None</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0.7311, 0.8808, 0.9526], grad_fn=&lt;SigmoidBackward&gt;)</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0.7311, 0.8808, 0.9526])</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0.1966, 0.1050, 0.0452])</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">'''</span>
    
    </div>
   
   </div></li></ol>
 
 <div class="hljs-button {2}"></div></pre> 
<p>从上可见tensor  c是由out分离得到的，但是我也没有去改变这个c，这个时候依然对原来的out求导是不会有错误的，即</p> 
<p>c,out之间的区别是c是没有梯度的，out是有梯度的,但是需要注意的是下面两种情况是汇报错的，</p> 
<p><strong>1.2 当使用detach()分离tensor，然后用这个分离出来的tensor去求导数，会影响backward()，会出现错误</strong></p> 
<pre class="has"><code class="language-python hljs"></code>
 
 <ol class="hljs-ln"><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-keyword">import</span> torch
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     a = torch.tensor([
     
     <span class="hljs-number">1</span>, 
     
     <span class="hljs-number">2</span>, 
     
     <span class="hljs-number">3.</span>], requires_grad=
     
     <span class="hljs-literal">True</span>)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(a.grad)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     out = a.sigmoid()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(out)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-comment">#添加detach(),c的requires_grad为False</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     c = out.detach()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(c)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-comment">#使用新生成的Variable进行反向传播</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     c.sum().backward()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(a.grad)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string"><span class="hljs-string">'''返回：</span></span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">None</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0.7311, 0.8808, 0.9526], grad_fn=&lt;SigmoidBackward&gt;)</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0.7311, 0.8808, 0.9526])</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">Traceback (most recent call last):</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">  File "test.py", line 13, in &lt;module&gt;</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">    c.sum().backward()</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">  File "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">    torch.autograd.backward(self, gradient, retain_graph, create_graph)</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">  File "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">    allow_unreachable=True)  # allow_unreachable flag</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">'''</span>
    
    </div>
   
   </div></li></ol>
 
 <div class="hljs-button {2}"></div></pre> 
<p><strong>1.3 当使用detach()分离tensor并且更改这个tensor时，即使再对原来的out求导数，会影响backward()，会出现错误</strong></p> 
<p>如果此时对c进行了更改，这个更改会被autograd追踪，在对out.sum()进行backward()时也会报错，因为此时的值进行backward()得到的梯度是错误的：</p> 
<pre class="has"><code class="language-python hljs"></code>
 
 <ol class="hljs-ln"><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-keyword">import</span> torch
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     a = torch.tensor([
     
     <span class="hljs-number">1</span>, 
     
     <span class="hljs-number">2</span>, 
     
     <span class="hljs-number">3.</span>], requires_grad=
     
     <span class="hljs-literal">True</span>)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(a.grad)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     out = a.sigmoid()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(out)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-comment">#添加detach(),c的requires_grad为False</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     c = out.detach()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(c)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     c.zero_() 
     
     <span class="hljs-comment">#使用in place函数对其进行修改</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-comment">#会发现c的修改同时会影响out的值</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(c)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(out)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-comment">#这时候对c进行更改，所以会影响backward()，这时候就不能进行backward()，会报错</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     out.sum().backward()
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     print(a.grad)
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line"> 
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string"><span class="hljs-string">'''返回：</span></span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">None</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0.7311, 0.8808, 0.9526], grad_fn=&lt;SigmoidBackward&gt;)</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0.7311, 0.8808, 0.9526])</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0., 0., 0.])</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">tensor([0., 0., 0.], grad_fn=&lt;SigmoidBackward&gt;)</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">Traceback (most recent call last):</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">  File "test.py", line 16, in &lt;module&gt;</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">    out.sum().backward()</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">  File "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">    torch.autograd.backward(self, gradient, retain_graph, create_graph)</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">  File "/anaconda3/envs/deeplearning/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">    allow_unreachable=True)  # allow_unreachable flag</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">RuntimeError: one of the variables needed for gradient computation has been modified </span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">by an inplace operation</span>
    
    </div>
   
   </div></li><li>
   
   <div class="hljs-ln-numbers">
    
    <div class="hljs-ln-line hljs-ln-n"></div>
   
   </div>
   
   <div class="hljs-ln-code">
    
    <div class="hljs-ln-line">
     
     <span class="hljs-string">'''</span>
    
    </div>
   
   </div></li></ol>
 
 <div class="hljs-button {2}"></div></pre> 
<h3>二、tensor.detach_()</h3> 
<p>将一个<code>tensor</code>从创建它的图中分离，并把它设置成叶子<code>tensor</code></p> 
<p>其实就相当于变量之间的关系本来是<strong>x -&gt; m -&gt; y</strong>,这里的叶子tensor是x，但是这个时候对m进行了m.detach_()操作,其实就是进行了两个操作：</p> 
<ul><li>将m的grad_fn的值设置为None,这样m就不会再与前一个节点x关联，这里的关系就会变成<strong>x, m -&gt; y,</strong>此时的m就变成了叶子结点</li><li>然后会将m的requires_grad设置为False，这样对y进行backward()时就不会求m的梯度</li></ul> 
<p> </p> 
<p><strong>总结：</strong>其实detach()和detach_()很像，两个的区别就是detach_()是对本身的更改，detach()则是生成了一个新的tensor</p> 
<p>比如x -&gt; m -&gt; y中如果对m进行detach()，后面如果反悔想还是对原来的计算图进行操作还是可以的</p> 
<p>但是如果是进行了detach_()，那么原来的计算图也发生了变化，就不能反悔了</p> 
<div> 
 <div></div> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b935650c5189b3ae5281daa907a998b5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">边缘测量</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ba4f91305a38ca891a17791e4ebf7158/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">strassen矩阵乘法 java_分治法-Strassen矩阵乘法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>