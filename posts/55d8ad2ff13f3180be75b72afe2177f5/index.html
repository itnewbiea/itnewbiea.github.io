<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark编程实验三：Spark SQL编程 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Spark编程实验三：Spark SQL编程" />
<meta property="og:description" content="目录
一、目的与要求
二、实验内容
三、实验步骤
1、Spark SQL基本操作
2、编程实现将RDD转换为DataFrame
3、编程实现利用DataFrame读写MySQL的数据
四、结果分析与实验体会
一、目的与要求 1、通过实验掌握Spark SQL的基本编程方法；
2、熟悉RDD到DataFrame的转化方法；
3、熟悉利用Spark SQL管理来自不同数据源的数据。
二、实验内容 1、Spark SQL基本操作
将下列JSON格式数据复制到Linux系统中，并保存命名为employee.json。
{ &#34;id&#34;:1 , &#34;name&#34;:&#34;Ella&#34; , &#34;age&#34;:36 }
{ &#34;id&#34;:2, &#34;name&#34;:&#34;Bob&#34;,&#34;age&#34;:29 }
{ &#34;id&#34;:3 , &#34;name&#34;:&#34;Jack&#34;,&#34;age&#34;:29 }
{ &#34;id&#34;:4 , &#34;name&#34;:&#34;Jim&#34;,&#34;age&#34;:28 }
{ &#34;id&#34;:5 , &#34;name&#34;:&#34;Damon&#34; }
{ &#34;id&#34;:5 , &#34;name&#34;:&#34;Damon&#34; }
为employee.json创建DataFrame，并写出Python语句完成下列操作：
（1）查询所有数据；
（2）查询所有数据，并去除重复的数据；
（3）查询所有数据，打印时去除id字段；
（4）筛选出age&gt;30的记录；
（5）将数据按age分组；
（6）将数据按name升序排列；
（7）取出前3行数据；
（8）查询所有记录的name列，并为其取别名为username；
（9）查询年龄age的平均值；
（10）查询年龄age的最小值。
2、编程实现将RDD转换为DataFrame
源文件内容如下（包含id,name,age）：
1,Ella,36
2,Bob,29
3,Jack,29
请将数据复制保存到Linux系统中，命名为employee.txt，实现从RDD转换得到DataFrame，并按“id:1,name:Ella,age:36”的格式打印出DataFrame的所有数据。请写出程序代码。
3、编程实现利用DataFrame读写MySQL的数据
（1）在MySQL数据库中新建数据库sparktest，再创建表employee，包含如表所示的两行数据。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/55d8ad2ff13f3180be75b72afe2177f5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-24T15:50:48+08:00" />
<meta property="article:modified_time" content="2023-12-24T15:50:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark编程实验三：Spark SQL编程</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E7%9B%AE%E7%9A%84%E4%B8%8E%E8%A6%81%E6%B1%82-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81%E7%9B%AE%E7%9A%84%E4%B8%8E%E8%A6%81%E6%B1%82" rel="nofollow">一、目的与要求</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9" rel="nofollow">二、实验内容</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4" rel="nofollow">三、实验步骤</a></p> 
<p id="1%E3%80%81Spark%20SQL%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-toc" style="margin-left:80px;"><a href="#1%E3%80%81Spark%20SQL%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C" rel="nofollow">1、Spark SQL基本操作</a></p> 
<p id="2%E3%80%81%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B0%86RDD%E8%BD%AC%E6%8D%A2%E4%B8%BADataFrame-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B0%86RDD%E8%BD%AC%E6%8D%A2%E4%B8%BADataFrame" rel="nofollow">2、编程实现将RDD转换为DataFrame</a></p> 
<p id="3%E3%80%81%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%88%A9%E7%94%A8DataFrame%E8%AF%BB%E5%86%99MySQL%E7%9A%84%E6%95%B0%E6%8D%AE-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%88%A9%E7%94%A8DataFrame%E8%AF%BB%E5%86%99MySQL%E7%9A%84%E6%95%B0%E6%8D%AE" rel="nofollow">3、编程实现利用DataFrame读写MySQL的数据</a></p> 
<p id="%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A-toc" style="margin-left:40px;"><a href="#%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A" rel="nofollow">四、结果分析与实验体会</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3 style="margin-left:.0001pt;text-align:justify;">一、目的与要求</h3> 
<p style="margin-left:.0001pt;text-align:justify;">1、通过实验掌握Spark SQL的基本编程方法；<br> 2、熟悉RDD到DataFrame的转化方法；<br> 3、熟悉利用Spark SQL管理来自不同数据源的数据。</p> 
<h3 id="%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9" style="margin-left:.0001pt;text-align:justify;"><strong>二、实验内容</strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>1、Spark SQL基本操作</strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">        将下列JSON格式数据复制到Linux系统中，并保存命名为employee.json。</p> 
<blockquote> 
 <p>{ "id":1 , "name":"Ella" , "age":36 }</p> 
 <p>{ "id":2, "name":"Bob","age":29 }</p> 
 <p>{ "id":3 , "name":"Jack","age":29 }</p> 
 <p>{ "id":4 , "name":"Jim","age":28 }</p> 
 <p>{ "id":5 , "name":"Damon" }</p> 
 <p style="margin-left:.0001pt;text-align:justify;">{ "id":5 , "name":"Damon" }</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">为employee.json创建DataFrame，并写出Python语句完成下列操作：<br> （1）查询所有数据；<br> （2）查询所有数据，并去除重复的数据；<br> （3）查询所有数据，打印时去除id字段；<br> （4）筛选出age&gt;30的记录；<br> （5）将数据按age分组；<br> （6）将数据按name升序排列；<br> （7）取出前3行数据；<br> （8）查询所有记录的name列，并为其取别名为username；<br> （9）查询年龄age的平均值；<br> （10）查询年龄age的最小值。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>2、编程实现将RDD转换为DataFrame</strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">        源文件内容如下（包含id,name,age）：</span></span></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">1,Ella,36</p> 
 <p>2,Bob,29</p> 
 <p style="margin-left:.0001pt;text-align:justify;">3,Jack,29</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">请<span style="background-color:#fefefe;"><span style="color:#333333;">将数据复制保存到Linux系统中，命名为</span></span>employee.txt<span style="background-color:#fefefe;"><span style="color:#333333;">，实现从RDD转换得到DataFrame，并按“id:1,name:</span></span>Ella<span style="background-color:#fefefe;"><span style="color:#333333;">,age:36”的格式打印出DataFrame的所有数据。请写出程序代码。</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>3、编程实现利用DataFrame读写MySQL的数据</strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">（1）在MySQL数据库中新建数据库sparktest，再创建表employee，包含如表所示的两行数据。</p> 
<p class="img-center"><img alt="" height="136" src="https://images2.imgbox.com/99/ee/oecBUr0e_o.png" width="800"></p> 
<p>（2）配置Spark通过JDBC连接数据库MySQL，编程实现利用DataFrame插入如表所示的三行数据到MySQL中，最后打印出age的最大值和age的总和。</p> 
<p class="img-center"><img alt="" height="175" src="https://images2.imgbox.com/c4/a5/WwEsey6K_o.png" width="800"></p> 
<h3 id="%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4">三、实验步骤</h3> 
<h4 id="1%E3%80%81Spark%20SQL%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C" style="margin-left:.0001pt;text-align:justify;">1、Spark SQL基本操作</h4> 
<p style="margin-left:.0001pt;text-align:justify;">        将下列JSON格式数据复制到Linux系统中，并保存命名为employee.json。</p> 
<blockquote> 
 <p>{ "id":1 , "name":"Ella" , "age":36 }</p> 
 <p>{ "id":2, "name":"Bob","age":29 }</p> 
 <p>{ "id":3 , "name":"Jack","age":29 }</p> 
 <p>{ "id":4 , "name":"Jim","age":28 }</p> 
 <p>{ "id":5 , "name":"Damon" }</p> 
 <p style="margin-left:.0001pt;text-align:justify;">{ "id":5 , "name":"Damon" }</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">为employee.json创建DataFrame，并写出Python语句完成下列操作：</p> 
<pre><code class="language-bash">&gt;&gt;&gt; spark=SparkSession.builder.getOrCreate()
&gt;&gt;&gt; df = spark.read.json("file:///home/zhc/mycode/sparksql/employee.json")</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">（1）查询所有数据；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.show()</code></pre> 
<p><img alt="" height="148" src="https://images2.imgbox.com/9d/74/5THCPeP0_o.png" width="300"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（2）查询所有数据，并去除重复的数据；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.distinct().show()</code></pre> 
<p><img alt="" height="148" src="https://images2.imgbox.com/4e/46/zicbuNCL_o.png" width="300"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（3）查询所有数据，打印时去除id字段；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.drop("id").show()</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="160" src="https://images2.imgbox.com/dc/21/mQ9cvzBz_o.png" width="300"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（4）筛选出age&gt;30的记录；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.filter(df.age &gt; 30).show()
</code></pre> 
<p><img alt="" height="82" src="https://images2.imgbox.com/b6/63/HpFFX0CK_o.png" width="300"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（5）将数据按age分组；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.groupBy("age").count().show()</code></pre> 
<p><img alt="" height="129" src="https://images2.imgbox.com/af/36/P3ZeG9iq_o.png" width="300"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（6）将数据按name升序排列；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.sort(df.name.asc()).show()</code></pre> 
<p><img alt="" height="168" src="https://images2.imgbox.com/f8/dd/NruU7g9q_o.png" width="300"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（7）取出前3行数据；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.take(3)</code></pre> 
<p><img alt="" height="41" src="https://images2.imgbox.com/aa/8c/6s7HzNeX_o.png" width="882"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（8）查询所有记录的name列，并为其取别名为username；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.select(df.name.alias("username")).show()</code></pre> 
<p><img alt="" height="152" src="https://images2.imgbox.com/7b/04/ghiRpSYd_o.png" width="350"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（9）查询年龄age的平均值；</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.agg({"age": "mean"}).show()</code></pre> 
<p><img alt="" height="87" src="https://images2.imgbox.com/d0/52/cSHrYaJu_o.png" width="300"></p> 
<p style="margin-left:.0001pt;text-align:justify;">（10）查询年龄age的最小值。</p> 
<pre><code class="language-bash">&gt;&gt;&gt; df.agg({"age": "min"}).show()</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="99" src="https://images2.imgbox.com/b6/8f/RzvOGeZZ_o.png" width="300"></p> 
<h4 id="2%E3%80%81%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B0%86RDD%E8%BD%AC%E6%8D%A2%E4%B8%BADataFrame" style="margin-left:.0001pt;text-align:justify;">2、编程实现将RDD转换为DataFrame</h4> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">        源文件内容如下（包含id,name,age）：</span></span></p> 
<blockquote> 
 <p>1,Ella,36</p> 
 <p>2,Bob,29</p> 
 <p style="margin-left:.0001pt;text-align:justify;">3,Jack,29</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">请<span style="background-color:#fefefe;"><span style="color:#333333;">将数据复制保存到Linux系统中，命名为</span></span>employee.txt<span style="background-color:#fefefe;"><span style="color:#333333;">，实现从RDD转换得到DataFrame，并按“id:1,name:</span></span>Ella<span style="background-color:#fefefe;"><span style="color:#333333;">,age:36”的格式打印出DataFrame的所有数据。请写出程序代码。</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">首先，在“/home/zhc/mycode/sparksql”目录下创建文件employee.txt</span></span></p> 
<pre><code class="language-css">​​​​​​​[root@bigdata sparksql]# vi employee.txt</code></pre> 
<p>然后，在该目录下新建一个py文件命名为rddtodf.py，然后写入如下py程序：</p> 
<pre><code class="language-css">[root@bigdata sparksql]# vi rddtodf.py</code></pre> 
<pre><code class="language-python">#/home/zhc/mycode/sparksql/rddtodf.py
from pyspark.conf import SparkConf
from pyspark.sql.session import SparkSession
from pyspark import SparkContext
from pyspark.sql.types import Row
from pyspark.sql import SQLContext
if __name__ == "__main__":
        sc = SparkContext("local","Simple App")
        spark=SparkSession(sc)
        peopleRDD = spark.sparkContext.textFile("file:home/zhc/mycode/sparksql/employee.txt")
        rowRDD = peopleRDD.map(lambda line : line.split(",")).map(lambda attributes : Row(int(attributes[0]),attributes[1],int(attributes[2]))).toDF()
        rowRDD.createOrReplaceTempView("employee")
        personsDF = spark.sql("select * from employee")
        personsDF.rdd.map(lambda t : "id:"+str(t[0])+","+"Name:"+t[1]+","+"age:"+str(t[2])).foreach(print)
</code></pre> 
<p>最后，运行该程序：</p> 
<pre><code class="language-css">[root@bigdata sparksql]# python3 rddtodf.py</code></pre> 
<p><img alt="" height="147" src="https://images2.imgbox.com/8d/eb/rbgUQ7WZ_o.png" width="1200"></p> 
<h4 id="3%E3%80%81%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%88%A9%E7%94%A8DataFrame%E8%AF%BB%E5%86%99MySQL%E7%9A%84%E6%95%B0%E6%8D%AE" style="margin-left:.0001pt;text-align:justify;">3、编程实现利用DataFrame读写MySQL的数据</h4> 
<p style="margin-left:.0001pt;text-align:justify;">（1）在MySQL数据库中新建数据库sparktest，再创建表employee，包含如表所示的两行数据。</p> 
<p class="img-center"><img alt="" height="136" src="https://images2.imgbox.com/a1/1f/ylsdz7vF_o.png" width="800"></p> 
<p>（2）配置Spark通过JDBC连接数据库MySQL，编程实现利用DataFrame插入如表所示的三行数据到MySQL中，最后打印出age的最大值和age的总和。</p> 
<p class="img-center"><img alt="" height="175" src="https://images2.imgbox.com/3f/dc/Z1rrRJhi_o.png" width="800"></p> 
<p>首先，启动mysql服务并进入到mysql数据库中：</p> 
<pre><code class="language-css">[root@bigdata sparksql]# systemctl start mysqld.service
[root@bigdata sparksql]# mysql -u root -p
</code></pre> 
<p>然后开始接下来的操作。</p> 
<p>（1）在MySQL数据库中新建数据库sparktest，再创建表employee，包含如表所示的两行数据。</p> 
<pre><code class="language-sql">mysql&gt; create database sparktest;
mysql&gt; use sparktest;
mysql&gt; create table employee (id int(4), name char(20), gender char(4), age int(4));
mysql&gt; insert into employee values(1,'Alice','F',22);
mysql&gt; insert into employee values(2,'John','M',25);</code></pre> 
<p><img alt="" height="270" src="https://images2.imgbox.com/9a/05/iuW73VHC_o.png" width="800"></p> 
<p>（2）配置Spark通过JDBC连接数据库MySQL，编程实现利用DataFrame插入如表所示的三行数据到MySQL中，最后打印出age的最大值和age的总和。</p> 
<p>首先，在<span style="background-color:#fefefe;"><span style="color:#333333;">“/home/zhc/mycode/sparksql”</span></span>目录下面新建一个py程序并命名为mysqltest.py。</p> 
<pre><code class="language-css">[root@bigdata sparksql]# vi mysqltest.py</code></pre> 
<p>接着，写入如下py程序： </p> 
<pre><code class="language-python">#/home/zhc/mycode/sparksql/mysqltest.py
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark import SparkContext,SparkConf
from pyspark.sql import SparkSession
spark = SparkSession.builder.config(conf = SparkConf()).getOrCreate()
#下面设置模式信息
schema = StructType([StructField("id",IntegerType(),True),StructField("name", StringType(), True),StructField("gender", StringType(), True),StructField("age",IntegerType(), True)])
employeeRDD = spark.sparkContext.parallelize(["3 Mary F 26","4 Tom M 23","5 zhanghc M 21"]).map(lambda x:x.split(" "))
#下面创建Row对象，每个Row对象都是rowRDD中的一行
rowRDD = employeeRDD.map(lambda p:Row(int(p[0].strip()), p[1].strip(), p[2].strip(), int(p[3].strip())))
#建立起Row对象和模式之间的对应关系，也就是把数据和模式对应起来
employeeDF = spark.createDataFrame(rowRDD, schema)
#写入数据库
prop = {}
prop['user'] = 'root'
prop['password'] = 'MYsql123!'
prop['driver'] = "com.mysql.jdbc.Driver"
employeeDF.write.jdbc("jdbc:mysql://localhost:3306/sparktest?useSSL=false",'employee','append', prop)
employeeDF.collect()
employeeDF.agg({"age": "max"}).show()
employeeDF.agg({"age": "sum"}).show()
</code></pre> 
<p>然后，直接运行该py程序即可得到结果：</p> 
<pre><code class="language-css">[root@bigdata sparksql]# python3 mysqltest.py</code></pre> 
<p><img alt="" height="294" src="https://images2.imgbox.com/5e/cf/NNYjCXSC_o.png" width="1200"></p> 
<p>最后，到MySQL Shell中，即可查看employee表中的所有信息。</p> 
<pre><code class="language-bash">mysql&gt; select * from employee;</code></pre> 
<p><img alt="" height="172" src="https://images2.imgbox.com/ea/bc/rIQFWwv5_o.png" width="400"></p> 
<h3 id="%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A" style="margin-left:.0001pt;text-align:justify;">四、结果分析与实验体会</h3> 
<p style="margin-left:0px;text-align:justify;">        Spark SQL是Apache Spark中用于处理结构化数据的模块。它提供了一种类似于SQL的编程接口，可以用于查询和分析数据。通过实验掌握了Spark SQL的基本编程方法，SparkSession支持从不同的数据源加载数据，并把数据转换成DataFrame，并且支持把DataFrame转换成SQLContext自身中的表，然后使用SQL语句来操作数据。<br>         在使用Spark SQL之前，需要创建一个SparkSession对象。可以使用SparkSession的read方法加载数据。可以使用DataFrame的createOrReplaceTempView方法将DataFrame注册为一个临时视图。可以使用SparkSession的sql方法执行SQL查询。除了使用SQL查询外，还可以使用DataFrame的API进行数据操作和转换。可以使用DataFrame的write方法将数据写入外部存储。在使用完SparkSession后，应该调用其close方法来关闭SparkSession。<br>         最后，还掌握了RDD到DataFrame的转化方法，并可以利用Spark SQL管理来自不同数据源的数据。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c8919f36a363dee5ff03571e7f3dc6f9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">gradle--groovy-dsl和kotlin-dsl对比</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3bc3e72708638338ff148a77af956587/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">记录一次ocr识别</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>