<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch之warm-up预热学习策略 - IT学习者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch之warm-up预热学习策略" />
<meta property="og:description" content="文章目录 一、warm-up1、什么是Warmup2、为什么使用Warmup3、Warmup的实现方法3.1 constant warmup3.1 gradual warmup3.3 WarmupMultiStepLR3.3.1 build_lr_scheduler3.3.2 WarmupMultiSetpLR3.3.3 仿真 4、总结 二、PyTorch学习之六个学习率调整策略1、等间隔调整学习率 StepLR1.1 参数1.2 示例· 2、按需调整学习率 MultiStepLR2.1 参数2.2 示例 3、指数衰减调整学习率 ExponentialLR3.1 参数3.2 示例 4、余弦退火调整学习率 CosineAnnealingLR4.1 参数4.2 示例 5、自适应调整学习率 ReduceLROnPlateau5.1 参数 6、自定义调整学习率 LambdaLR6.1 参数： 一、warm-up 学习率是神经网络训练中最重要的超参数之一,针对学习率的优化方式很多,Warmup是其中的一种
1、什么是Warmup Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。
2、为什么使用Warmup 由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。
Example：
3、Warmup的实现方法 3.1 constant warmup Resnet论文中使用一个110层的ResNet在cifar10上训练时，先用0.01的学习率训练直到训练误差低于80%(大概训练了400个steps)，然后使用0.1的学习率进行训练。
3.1 gradual warmup constant warmup的不足之处在于从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。于是18年Facebook提出了gradual warmup来解决这个问题，即从最初的小学习率开始，每个step增大一点点，直到达到最初设置的比较大的学习率时，采用最初设置的学习率进行训练。gradual warmup的实现模拟代码如下:
&#34;&#34;&#34; Implements gradual warmup, if train_steps &lt; warmup_steps, the learning rate will be `train_steps/warmup_steps * init_lr`. Args: warmup_steps:warmup步长阈值,即train_steps&lt;warmup_steps,使用预热学习率,否则使用预设值学习率 train_steps:训练了的步长数 init_lr:预设置学习率 &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://itnewbiea.github.io/posts/f2fce04d72a6048fd9f497b1a43e4bc9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-27T10:19:04+08:00" />
<meta property="article:modified_time" content="2021-05-27T10:19:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="IT学习者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">IT学习者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch之warm-up预热学习策略</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#warmup_1" rel="nofollow">一、warm-up</a></li><li><ul><li><a href="#1Warmup_3" rel="nofollow">1、什么是Warmup</a></li><li><a href="#2Warmup_6" rel="nofollow">2、为什么使用Warmup</a></li><li><a href="#3Warmup_9" rel="nofollow">3、Warmup的实现方法</a></li><li><ul><li><a href="#31_constant_warmup_10" rel="nofollow">3.1 constant warmup</a></li><li><a href="#31_gradual_warmup_15" rel="nofollow">3.1 gradual warmup</a></li><li><a href="#33_WarmupMultiStepLR_46" rel="nofollow">3.3 WarmupMultiStepLR</a></li><li><ul><li><a href="#331_build_lr_scheduler_47" rel="nofollow">3.3.1 build_lr_scheduler</a></li><li><a href="#332_WarmupMultiSetpLR_76" rel="nofollow">3.3.2 WarmupMultiSetpLR</a></li><li><a href="#333__145" rel="nofollow">3.3.3 仿真</a></li></ul> 
   </li></ul> 
   </li><li><a href="#4_170" rel="nofollow">4、总结</a></li></ul> 
  </li><li><a href="#PyTorch_178" rel="nofollow">二、PyTorch学习之六个学习率调整策略</a></li><li><ul><li><a href="#1_StepLR_184" rel="nofollow">1、等间隔调整学习率 StepLR</a></li><li><ul><li><a href="#11__191" rel="nofollow">1.1 参数</a></li><li><a href="#12__197" rel="nofollow">1.2 示例·</a></li></ul> 
   </li><li><a href="#2_MultiStepLR_240" rel="nofollow">2、按需调整学习率 MultiStepLR</a></li><li><ul><li><a href="#21__247" rel="nofollow">2.1 参数</a></li><li><a href="#22__252" rel="nofollow">2.2 示例</a></li></ul> 
   </li><li><a href="#3_ExponentialLR_288" rel="nofollow">3、指数衰减调整学习率 ExponentialLR</a></li><li><ul><li><a href="#31__293" rel="nofollow">3.1 参数</a></li><li><a href="#32__296" rel="nofollow">3.2 示例</a></li></ul> 
   </li><li><a href="#4_CosineAnnealingLR_300" rel="nofollow">4、余弦退火调整学习率 CosineAnnealingLR</a></li><li><ul><li><a href="#41__308" rel="nofollow">4.1 参数</a></li><li><a href="#42__312" rel="nofollow">4.2 示例</a></li></ul> 
   </li><li><a href="#5_ReduceLROnPlateau_315" rel="nofollow">5、自适应调整学习率 ReduceLROnPlateau</a></li><li><ul><li><a href="#51__322" rel="nofollow">5.1 参数</a></li></ul> 
   </li><li><a href="#6_LambdaLR_336" rel="nofollow">6、自定义调整学习率 LambdaLR</a></li><li><ul><li><a href="#61__342" rel="nofollow">6.1 参数：</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="warmup_1"></a>一、warm-up</h2> 
<p>学习率是神经网络训练中最重要的超参数之一,针对学习率的优化方式很多,Warmup是其中的一种</p> 
<h3><a id="1Warmup_3"></a>1、什么是Warmup</h3> 
<p>Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。</p> 
<h3><a id="2Warmup_6"></a>2、为什么使用Warmup</h3> 
<p>由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p> 
<blockquote> 
 <p>Example：</p> 
</blockquote> 
<h3><a id="3Warmup_9"></a>3、Warmup的实现方法</h3> 
<h4><a id="31_constant_warmup_10"></a>3.1 constant warmup</h4> 
<p>Resnet论文中使用一个110层的ResNet在cifar10上训练时，先用0.01的学习率训练直到训练误差低于80%(大概训练了400个steps)，然后使用0.1的学习率进行训练。</p> 
<h4><a id="31_gradual_warmup_15"></a>3.1 gradual warmup</h4> 
<p>constant warmup的不足之处在于从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。于是18年Facebook提出了gradual warmup来解决这个问题，即从最初的小学习率开始，每个step增大一点点，直到达到最初设置的比较大的学习率时，采用最初设置的学习率进行训练。<strong>gradual warmup的实现模拟代码如下:</strong></p> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">"""
Implements gradual warmup, if train_steps &lt; warmup_steps, the
learning rate will be `train_steps/warmup_steps * init_lr`.
Args:
    warmup_steps:warmup步长阈值,即train_steps&lt;warmup_steps,使用预热学习率,否则使用预设值学习率
    train_steps:训练了的步长数
    init_lr:预设置学习率
"""</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
warmup_steps <span class="token operator">=</span> <span class="token number">2500</span>
init_lr <span class="token operator">=</span> <span class="token number">0.1</span>  
<span class="token comment"># 模拟训练15000步</span>
max_steps <span class="token operator">=</span> <span class="token number">15000</span>
<span class="token keyword">for</span> train_steps <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> warmup_steps <span class="token keyword">and</span> train_steps <span class="token operator">&lt;</span> warmup_steps<span class="token punctuation">:</span>
        warmup_percent_done <span class="token operator">=</span> train_steps <span class="token operator">/</span> warmup_steps
        warmup_learning_rate <span class="token operator">=</span> init_lr <span class="token operator">*</span> warmup_percent_done  <span class="token comment">#gradual warmup_lr</span>
        learning_rate <span class="token operator">=</span> warmup_learning_rate
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment">#learning_rate = np.sin(learning_rate)  #预热学习率结束后,学习率呈sin衰减</span>
        learning_rate <span class="token operator">=</span> learning_rate<span class="token operator">**</span><span class="token number">1.0001</span> <span class="token comment">#预热学习率结束后,学习率呈指数衰减(近似模拟指数衰减)</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>train_steps<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
             <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"train_steps:%.3f--warmup_steps:%.3f--learning_rate:%.3f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>
                 train_steps<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>warmup_steps<span class="token punctuation">,</span>learning_rate<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>上述代码实现的Warmup预热学习率以及学习率预热完成后衰减(sin or exp decay)的曲线图如下:<br> <img src="https://images2.imgbox.com/a1/84/KtCMaLkC_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="33_WarmupMultiStepLR_46"></a>3.3 WarmupMultiStepLR</h4> 
<h5><a id="331_build_lr_scheduler_47"></a>3.3.1 build_lr_scheduler</h5> 
<pre><code class="prism language-python"><span class="token comment"># -&gt;箭头表示函数输出返回值类型</span>
<span class="token comment"># LR_SCHEDULER_NAME有两种WarmupMultiSetpLR, WarmupCosineLR</span>
<span class="token keyword">def</span> <span class="token function">build_lr_scheduler</span><span class="token punctuation">(</span>cfg<span class="token punctuation">:</span> CfgNode<span class="token punctuation">,</span> optimizer<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>_LRScheduler<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Build a LR scheduler from config.
    """</span>
    name <span class="token operator">=</span> cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>LR_SCHEDULER_NAME
    <span class="token keyword">if</span> name <span class="token operator">==</span> <span class="token string">"WarmupMultiStepLR"</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> WarmupMultiStepLR<span class="token punctuation">(</span>
            optimizer<span class="token punctuation">,</span>
            cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>STEPS<span class="token punctuation">,</span>                         <span class="token comment"># tuple (300, 400)</span>
            cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>GAMMA<span class="token punctuation">,</span>                         <span class="token comment">#[0.1]</span>
            warmup_factor<span class="token operator">=</span>cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>WARMUP_FACTOR<span class="token punctuation">,</span>   <span class="token comment"># 0.001</span>
            warmup_iters<span class="token operator">=</span>cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>WARMUP_ITERS<span class="token punctuation">,</span>     <span class="token comment"># 1000</span>
            warmup_method<span class="token operator">=</span>cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>WARMUP_METHOD<span class="token punctuation">,</span>   <span class="token comment"># 'linear'</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">elif</span> name <span class="token operator">==</span> <span class="token string">"WarmupCosineLR"</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> WarmupCosineLR<span class="token punctuation">(</span>
            optimizer<span class="token punctuation">,</span>
            cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>MAX_ITER<span class="token punctuation">,</span>
            warmup_factor<span class="token operator">=</span>cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>WARMUP_FACTOR<span class="token punctuation">,</span>
            warmup_iters<span class="token operator">=</span>cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>WARMUP_ITERS<span class="token punctuation">,</span>
            warmup_method<span class="token operator">=</span>cfg<span class="token punctuation">.</span>SOLVER<span class="token punctuation">.</span>WARMUP_METHOD<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Unknown LR scheduler: {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">)</span>        
</code></pre> 
<h5><a id="332_WarmupMultiSetpLR_76"></a>3.3.2 WarmupMultiSetpLR</h5> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">WarmupMultiStepLR</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>_LRScheduler<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        optimizer<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">,</span>
        milestones<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        gamma<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
        warmup_factor<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0.001</span><span class="token punctuation">,</span>
        warmup_iters<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">,</span>
        warmup_method<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"linear"</span><span class="token punctuation">,</span>
        last_epoch<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">list</span><span class="token punctuation">(</span>milestones<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>milestones<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string">"Milestones should be a list of"</span> <span class="token string">" increasing integers. Got {}"</span><span class="token punctuation">,</span> milestones
            <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>milestones <span class="token operator">=</span> milestones
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>warmup_factor <span class="token operator">=</span> warmup_factor
        self<span class="token punctuation">.</span>warmup_iters <span class="token operator">=</span> warmup_iters
        self<span class="token punctuation">.</span>warmup_method <span class="token operator">=</span> warmup_method
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> last_epoch<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">get_lr</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        warmup_factor <span class="token operator">=</span> _get_warmup_factor_at_iter<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>warmup_method<span class="token punctuation">,</span> self<span class="token punctuation">.</span>last_epoch<span class="token punctuation">,</span> self<span class="token punctuation">.</span>warmup_iters<span class="token punctuation">,</span> self<span class="token punctuation">.</span>warmup_factor
        <span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>
            base_lr <span class="token operator">*</span> warmup_factor <span class="token operator">*</span> self<span class="token punctuation">.</span>gamma <span class="token operator">**</span> bisect_right<span class="token punctuation">(</span>self<span class="token punctuation">.</span>milestones<span class="token punctuation">,</span> self<span class="token punctuation">.</span>last_epoch<span class="token punctuation">)</span>
            <span class="token keyword">for</span> base_lr <span class="token keyword">in</span> self<span class="token punctuation">.</span>base_lrs 
<span class="token comment">##################################################</span>
<span class="token comment">## self.base_lrs 【0.001，.... 0.001】 len = 84</span>
<span class="token comment">#################################################</span>
        <span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">_compute_values</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token comment"># The new interface</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>get_lr<span class="token punctuation">(</span><span class="token punctuation">)</span>



<span class="token keyword">def</span> <span class="token function">_get_warmup_factor_at_iter</span><span class="token punctuation">(</span>method<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">iter</span><span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> warmup_iters<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> warmup_factor<span class="token punctuation">:</span> <span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">float</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Return the learning rate warmup factor at a specific iteration.
    See https://arxiv.org/abs/1706.02677 for more details.

    Args:
        method (str): warmup method; either "constant" or "linear".
        iter (int): iteration at which to calculate the warmup factor.
        warmup_iters (int): the number of warmup iterations.
        warmup_factor (float): the base warmup factor (the meaning changes according
            to the method used).

    Returns:
        float: the effective warmup factor at the given iteration.
    """</span>
    <span class="token keyword">if</span> <span class="token builtin">iter</span> <span class="token operator">&gt;=</span> warmup_iters<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">1.0</span>

    <span class="token keyword">if</span> method <span class="token operator">==</span> <span class="token string">"constant"</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> warmup_factor
    <span class="token keyword">elif</span> method <span class="token operator">==</span> <span class="token string">"linear"</span><span class="token punctuation">:</span>
        alpha <span class="token operator">=</span> <span class="token builtin">iter</span> <span class="token operator">/</span> warmup_iters
        <span class="token keyword">return</span> warmup_factor <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> alpha<span class="token punctuation">)</span> <span class="token operator">+</span> alpha
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Unknown warmup method: {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>method<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="333__145"></a>3.3.3 仿真</h5> 
<p><img src="https://images2.imgbox.com/83/ec/ObHmtJ2T_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> bisect
<span class="token keyword">from</span> bisect <span class="token keyword">import</span> bisect_right
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
warmup_factor <span class="token operator">=</span> <span class="token number">0.001</span>
Steps <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">,</span><span class="token number">400</span><span class="token punctuation">)</span>
gamma <span class="token operator">=</span> <span class="token number">0.1</span>
warmup_iters <span class="token operator">=</span> <span class="token number">1000</span>
base_lr <span class="token operator">=</span> <span class="token number">0.001</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
lr <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
iters<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> <span class="token builtin">iter</span> <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    alpha <span class="token operator">=</span> <span class="token builtin">iter</span><span class="token operator">/</span>warmup_iters
    warmup_factor <span class="token operator">=</span> warmup_factor<span class="token operator">*</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>alpha<span class="token punctuation">)</span><span class="token operator">+</span>alpha
    lr<span class="token punctuation">.</span>append<span class="token punctuation">(</span> base_lr <span class="token operator">*</span> warmup_factor <span class="token operator">*</span> gamma <span class="token operator">**</span> bisect_right<span class="token punctuation">(</span>Steps<span class="token punctuation">,</span> <span class="token builtin">iter</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    iters<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>iters<span class="token punctuation">,</span>lr<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/8c/53/449bckQD_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4_170"></a>4、总结</h3> 
<p>使用Warmup预热学习率的方式,即先用最初的小学习率训练，然后每个step增大一点点，直到达到最初设置的比较大的学习率时（注：此时预热学习率完成），之后采用最初设置的学习率进行训练（注：预热学习率完成后的训练过程，学习率是衰减的），有助于使模型收敛速度变快，效果更佳。</p> 
<p>参考：https://blog.csdn.net/sinat_36618660/article/details/99650804<br> 参考：https://zhuanlan.zhihu.com/p/99568607</p> 
<h2><a id="PyTorch_178"></a>二、PyTorch学习之六个学习率调整策略</h2> 
<p>PyTorch学习率调整策略通过<code>torch.optim.lr_scheduler</code>接口实现。PyTorch提供的学习率调整策略分为三大类，分别是</p> 
<ul><li>有序调整：等间隔调整(Step)，按需调整学习率(MultiStep)，指数衰减调整(Exponential)和余弦退火CosineAnnealing。</li><li>自适应调整：自适应调整学习率 ReduceLROnPlateau。</li><li>自定义调整：自定义调整学习率 LambdaLR。</li></ul> 
<h3><a id="1_StepLR_184"></a>1、等间隔调整学习率 StepLR</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> step_size<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> last_epoch<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p>等间隔调整学习率，调整倍数为 gamma 倍，调整间隔为 step_size。间隔单位是step。<code>需要注意的是， step 通常是指 epoch，不要弄成 iteration 了</code></p> 
<h4><a id="11__191"></a>1.1 参数</h4> 
<ul><li>optimizer：优化器</li><li>step_size(int)： 学习率下降间隔数，若为 30，则会在 30、 60、 90…个 step 时，将学习率调整为 lr*gamma。</li><li>gamma(float)：学习率调整倍数，默认为 0.1 倍，即下降 10 倍。</li><li>last_epoch(int)：上一个 epoch 数，这个变量用来指示学习率是否需要调整。当last_epoch 符合设定的间隔时，就会对学习率进行调整。当为-1 时，学习率设置为初始值。</li></ul> 
<h4><a id="12__197"></a>1.2 示例·</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> lr_scheduler
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">import</span> AlexNet
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt


model <span class="token operator">=</span> AlexNet<span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token operator">=</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token comment"># ----------------------------------------------------------------------------------------------------------------------------------------</span>
<span class="token comment"># 官方用法</span>
<span class="token comment"># lr_scheduler.StepLR()</span>
<span class="token comment"># Assuming optimizer uses lr = 0.05 for all groups</span>
<span class="token comment"># lr = 0.05     if epoch &lt; 30</span>
<span class="token comment"># lr = 0.005    if 30 &lt;= epoch &lt; 60</span>
<span class="token comment"># lr = 0.0005   if 60 &lt;= epoch &lt; 90</span>
<span class="token comment">#</span>
<span class="token comment"># scheduler = StepLR(optimizer, step_size=30, gamma=0.1)</span>
<span class="token comment"># for epoch in range(100):</span>
<span class="token comment">#     train(...)</span>
<span class="token comment">#     validate(...)</span>
<span class="token comment">#     scheduler.step()</span>
<span class="token comment"># ----------------------------------------------------------------------------------------------------------------------------------------</span>

scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    lr <span class="token operator">=</span> scheduler<span class="token punctuation">.</span>get_lr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> scheduler<span class="token punctuation">.</span>get_lr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>scheduler<span class="token punctuation">.</span>get_lr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"learning rate"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ec/d8/7u38OYjJ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2_MultiStepLR_240"></a>2、按需调整学习率 MultiStepLR</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> last_epoch<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>按设定的间隔调整学习率。这个方法适合后期调试使用，观察 loss 曲线，为每个实验定制学习率调整时机。</li><li><code>与StepLR的区别是，调节的epoch是自己定义，无须一定是【30， 60， 90】 这种等差数列；请注意，这种衰减是由外部的设置来更改的</code></li></ul> 
<h4><a id="21__247"></a>2.1 参数</h4> 
<ul><li>milestones(list)：一个 list，每一个元素代表何时调整学习率， list 元素必须是递增的。如milestones=[30,80,120]</li><li>gamma(float)- 学习率调整倍数，默认为 0.1 倍，即下降 10 倍</li><li>last_epoch(int)：上一个epoch数，这个变量用来指示学习率是否需要调整。当last_epoch符合设定的间隔时，就会对学习率进行调整；当为-1时，学习率设置为初始值。</li></ul> 
<h4><a id="22__252"></a>2.2 示例</h4> 
<pre><code class="prism language-python">model <span class="token operator">=</span> AlexNet<span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params <span class="token operator">=</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token comment"># ----------------------------------------------------------------------------------------------------------------------------------------</span>
<span class="token comment"># Assuming optimizer uses lr = 0.05 for all groups</span>
<span class="token comment"># lr = 0.05     if epoch &lt; 30</span>
<span class="token comment"># lr = 0.005    if 30 &lt;= epoch &lt; 80</span>
<span class="token comment"># lr = 0.0005   if epoch &gt;= 80</span>
<span class="token comment"># scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)</span>
<span class="token comment"># for epoch in range(100):</span>
<span class="token comment">#     train(...)</span>
<span class="token comment">#     validate(...)</span>
<span class="token comment">#     scheduler.step()</span>
<span class="token comment"># ----------------------------------------------------------------------------------------------------------------------------------------</span>


<span class="token comment">#在指定的epoch值，如[5,20,25,80]处对学习率进行衰减，lr = lr * gamma</span>
scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">25</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    lr <span class="token operator">=</span> scheduler<span class="token punctuation">.</span>get_lr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> scheduler<span class="token punctuation">.</span>get_lr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>scheduler<span class="token punctuation">.</span>get_lr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"learning rate"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="3_ExponentialLR_288"></a>3、指数衰减调整学习率 ExponentialLR</h3> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> last_epoch<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>指数衰减调整学习率的调整公式：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          l 
         
        
          r 
         
        
          = 
         
        
          l 
         
        
          r 
         
        
          ∗ 
         
        
          g 
         
        
          a 
         
        
          m 
         
        
          m 
         
         
         
           a 
          
          
          
            e 
           
          
            p 
           
          
            o 
           
          
            c 
           
          
            h 
           
          
         
        
       
         lr=lr∗gamma^{epoch} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.04355em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mord">∗</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">m</span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span></li></ul> 
<h4><a id="31__293"></a>3.1 参数</h4> 
<ul><li>gamma：学习率调衰减的底数，选择不同的gamma值可以获得幅度不同的衰减曲线，指数为 epoch，即 gamma**epoch(或<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          g 
         
        
          a 
         
        
          m 
         
        
          m 
         
         
         
           a 
          
          
          
            e 
           
          
            p 
           
          
            o 
           
          
            c 
           
          
            h 
           
          
         
        
       
         gamma^{epoch} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04355em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">m</span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span>)</li></ul> 
<h4><a id="32__296"></a>3.2 示例</h4> 
<p><img src="https://images2.imgbox.com/81/3b/B2KliF2T_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4_CosineAnnealingLR_300"></a>4、余弦退火调整学习率 CosineAnnealingLR</h3> 
<ul><li>以初始学习率为最大学习率，以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          2 
         
        
          ∗ 
         
        
          T 
         
        
          _ 
         
        
          m 
         
        
          a 
         
        
          x 
         
        
       
         2 ∗ T\_max 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.99333em; vertical-align: -0.31em;"></span><span class="mord">2</span><span class="mord">∗</span><span class="mord mathdefault" style="margin-right: 0.13889em;">T</span><span class="mord" style="margin-right: 0.02778em;">_</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span></span></span></span></span> 为周期，在一个周期内先下降，后上升。</li></ul> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>CosineAnnealingLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> T_max<span class="token punctuation">,</span> eta_min<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> last_epoch<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="41__308"></a>4.1 参数</h4> 
<ul><li>T_max(int)：学习率下降到最小值时的epoch数，即当epoch=T_max时，学习率下降到余弦函数最小值，当epoch&gt;T_max时，学习率将增大</li><li>eta_min(float)：学习率的最小值，即在一个周期中，学习率最小会下降到 eta_min，默认值为 0</li><li>上一个epoch数，这个变量用来指示学习率是否需要调整。当last_epoch符合设定的间隔时，就会对学习率进行调整；当为-1时，学习率设置为初始值。</li></ul> 
<h4><a id="42__312"></a>4.2 示例</h4> 
<p><img src="https://images2.imgbox.com/93/51/paddbtB1_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="5_ReduceLROnPlateau_315"></a>5、自适应调整学习率 ReduceLROnPlateau</h3> 
<p>当某指标不再变化（下降或升高），调整学习率，这是非常实用的学习率调整策略。例如，当验证集的 loss 不再下降时，进行学习率调整；或者监测验证集的 accuracy，当accuracy 不再上升时，则调整学习率。</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ReduceLROnPlateau<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'min'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> threshold_mode<span class="token operator">=</span><span class="token string">'rel'</span><span class="token punctuation">,</span> cooldown<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="51__322"></a>5.1 参数</h4> 
<ul><li>mode(str)- 模式选择，有 min 和 max 两种模式， min 表示当指标不再降低(如监测loss)， max 表示当指标不再升高(如监测 accuracy)。</li><li>factor(float)- 学习率调整倍数(等同于其它方法的 gamma)，即学习率更新为 lr = lr * factor</li><li>patience(int)- 忍受该指标多少个 step 不变化，当忍无可忍时，调整学习率。</li><li>verbose(bool)- 是否打印学习率信息， print(‘Epoch {:5d}: reducing learning rate of group {} to {:.4e}.’.format(epoch, i, new_lr))</li><li>threshold_mode(str)- 选择判断指标是否达最优的模式，有两种模式， rel 和 abs。</li></ul> 
<blockquote> 
 <ul><li>当 threshold_mode == rel，并且 mode == max 时， dynamic_threshold = best * (1 +threshold )；</li><li>当 threshold_mode == rel，并且 mode == min 时，dynamic_threshold = best * ( 1 -threshold )；</li><li>当 threshold_mode <mark>abs，并且 mode</mark> max 时， dynamic_threshold = best + threshold ； &gt; - 当threshold_mode == rel，并且 mode == max 时， dynamic_threshold = best - threshold； threshold(float)- 配合 threshold_mode 使用。</li></ul> 
</blockquote> 
<ul><li>cooldown(int)- “冷却时间“，当调整学习率之后，让学习率调整策略冷静一下，让模型再训练一段时间，再重启监测模式。</li><li>min_lr(float or list)- 学习率下限，可为 float，或者 list，当有多个参数组时，可用 list 进行设置。</li><li>eps(float)- 学习率衰减的最小值，当学习率变化小于 eps 时，则不调整学习率。</li></ul> 
<h3><a id="6_LambdaLR_336"></a>6、自定义调整学习率 LambdaLR</h3> 
<p>为不同参数组设定不同学习率调整策略。调整规则为，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         r 
        
       
         = 
        
       
         b 
        
       
         a 
        
       
         s 
        
        
        
          e 
         
        
          l 
         
        
       
         r 
        
       
         ∗ 
        
       
         l 
        
       
         m 
        
       
         b 
        
       
         d 
        
       
         a 
        
       
         ( 
        
       
         s 
        
       
         e 
        
       
         l 
        
       
         f 
        
       
         . 
        
       
         l 
        
       
         a 
        
       
         s 
        
        
        
          t 
         
        
          e 
         
        
       
         p 
        
       
         o 
        
       
         c 
        
       
         h 
        
       
         ) 
        
       
      
        lr=base_lr∗lmbda(self.last_epoch) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">b</span><span class="mord mathdefault">a</span><span class="mord mathdefault">s</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mord">∗</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">m</span><span class="mord mathdefault">b</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mopen">(</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mord">.</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">a</span><span class="mord mathdefault">s</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault">p</span><span class="mord mathdefault">o</span><span class="mord mathdefault">c</span><span class="mord mathdefault">h</span><span class="mclose">)</span></span></span></span></span>，这在fine-tune 中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略。</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> lr_lambda<span class="token punctuation">,</span> last_epoch<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="61__342"></a>6.1 参数：</h4> 
<ul><li>lr_lambda(function or list)- 一个计算学习率调整倍数的函数，输入通常为 step，当有多个参数组时，设为 list。</li></ul> 
<p>参考链接：https://blog.csdn.net/shanglianlm/article/details/85143614<br> 参考：https://www.jianshu.com/p/26a7dbc15246</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/66e08a3889a4cca2302d5577c557191d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【es】将 elasticsearch 写入速度优化到极限</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1ae060672cb05921e5890f73cb16e50b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">android 标题被顶出去,解决安卓虚拟键盘把标题栏顶出屏幕外</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 IT学习者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://itnewbiea.github.io/js/foot.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>